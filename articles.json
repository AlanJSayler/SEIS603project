[{"abstract": "We present a robust method to correct for motion and deformations for in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI requires robust alignment across time in the presence of substantial and unpredictable motion. We make a Markov assumption on the nature of deformations to take advantage of the temporal structure in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We demonstrate the utility of the temporal model by showing that its use improves the accuracy of the segmentation propagation through temporal registration. Our results suggest that the proposed model captures accurately the temporal dynamics of deformations in in-utero MRI time series.\n        \u25b3 Less", "author": "Elfar Adalsteinsson"}, {"abstract": "A linear inverse problem is proposed that requires the determination of multiple unknown signal vectors. Each unknown vector passes through a different system matrix and the results are added to yield a single observation vector. Given the matrices and lone observation, the objective is to find a simultaneously sparse set of unknown vectors that solves the system. We will refer to this as the multiple-system single-output (MSSO) simultaneous sparsity problem. This manuscript contrasts the MSSO problem with other simultaneous sparsity problems and conducts a thorough initial exploration of algorithms with which to solve it. Seven algorithms are formulated that approximately solve this NP-Hard problem. Three greedy techniques are developed (matching pursuit, orthogonal matching pursuit, and least squares matching pursuit) along with four methods based on a convex relaxation (iteratively reweighted least squares, two forms of iterative shrinkage, and formulation as a second-order cone program). The algorithms are evaluated across three experiments: the first and second involve sparsity profile recovery in noiseless and noisy scenarios, respectively, while the third deals with magnetic resonance imaging radio-frequency excitation pulse design.\n        \u25b3 Less", "author": "Elfar Adalsteinsson"}, {"abstract": "Theorems from Part 1 of this paper are generalized to \u03c8-mixing sources in this paper. Application to Markoff chains and order m Markoff chains is presented. The main result is the generalization of Theorem 1 in Part 1.\n        \u25b3 Less", "author": "Anant Agarwal"}, {"abstract": "In this paper, the problem of communication over an essentially unknown channel, which is known to be able to communicate a source to a destination to within a certain distortion level, is considered from a behavioral, interconnection view-point. Rates of reliable communication are derived and source-channel separation for communication with fidelity criteria is proved. The results are then generalized to the multi-user setting under certain assumptions. Other applications of this problem problem which follow from this perspective are discussed.\n        \u25b3 Less", "author": "Anant Agarwal"}, {"abstract": "This is a three part paper.\n  Optimality of source-channel separation for communication with a fidelity criterion when the channel is compound as defined by Csiszar and Korner in their book and general as defined by Verdu and Han, is proved in Part I. It is assumed that random codes are permitted. The word \"universal\" in the title of this paper refers to the fact that the channel model is compound. The proof uses a layered black-box or a layered input-output view-point. In particular, only the end-to-end description of the channel as being capable of communicating a source to within a certain distortion level is used when proving separation. This implies that the channel model does not play any role for separation to hold as long as there is a source model. Further implications of the layered black-box view-point are discussed.\n  Optimality of source-medium separation for multi-user communication with fidelity criteria over a general, compound medium in the unicast setting is proved in Part II, thus generalizing Part I to the unicast, multi-user setting.\n  Part III gets to an understanding of the question, \"Why is a channel which is capable of communicating a source to within a certain distortion level, also capable of communicating bits at any rate less than the infimum of the rates needed to code the source to within the distortion level\": this lies at the heart of why optimality of separation for communication with a fidelity criterion holds. The perspective taken to get to this understanding is a randomized covering-packing perspective, and the proof is operational.\n        \u25b3 Less", "author": "Anant Agarwal"}, {"abstract": "An operational perspective is used to understand the relationship between source and channel coding. This is based on a direct reduction of one problem to another that uses random coding (and hence common randomness) but unlike all prior work, does not involve any functional computations, in particular, no mutual-information computations. This result is then used to prove a universal source-channel separation theorem in the rate-distortion context where universality is in the sense of a compound ``general channel.''\n        \u25b3 Less", "author": "Anant Agarwal"}, {"abstract": "Shannon proved that if we can transmit bits reliably at rates larger than the rate distortion function $R(D)$, then we can transmit this source to within a distortion $D$. We answer the converse question ``If we can transmit a source to within a distortion $D$, can we transmit bits reliably at rates less than the rate distortion function?'' in the affirmative. This can be viewed as a direct converse of the rate distortion theorem.\n        \u25b3 Less", "author": "Anant Agarwal"}, {"abstract": "Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems, however, use simple generalized heuristics and ignore workload structure, since developing and tuning a bespoke heuristic for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically.\n  Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond specifying a high-level objective such as minimizing average job completion time. Off-the-shelf RL techniques, however, cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent new RL training methods for continuous job arrivals.\n  Our prototype integration with Spark on a 25-node cluster shows that Decima outperforms several heuristics, including hand-tuned ones, by at least 21%. Further experiments with an industrial production workload trace demonstrate that Decima delivers up to a 17% reduction in average job completion time and scales to large clusters.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "With the growing usage of Bitcoin and other cryptocurrencies, many scalability challenges have emerged. A promising scaling solution, exemplified by the Lightning Network, uses a network of bidirectional payment channels that allows fast transactions between two parties. However, routing payments on these networks efficiently is non-trivial, since payments require finding paths with sufficient funds, and channels can become unidirectional over time blocking further transactions through them. Today's payment channel networks exacerbate these problems by attempting to deliver all payments atomically. In this paper, we present the Spider network, a new packet-switched architecture for payment channel networks. Spider splits payments into transaction units and transmits them over time across different paths. Spider uses congestion control, payment scheduling, and imbalance-aware routing to optimize delivery of payments. Our results show that Spider improves the volume and number of successful payments on the network by 10-45% and 5-40% respectively compared to state-of-the-art approaches.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Prior research has proposed technical solutions to use peer-to-peer (P2P) content delivery to serve Internet video, showing that it can reduce costs to content providers. Yet, such methods have not become widespread except for a few niche instances. An important challenge is incentivization: what tangible benefits does P2P content delivery offer users who bring resources to the table? In this paper, we ask whether monetary incentives can help attract peers in P2P content delivery systems. We commissioned a professional survey of people around theUnited States to answer several relevant questions. We found that 51% of the 876 respondents--substantially larger than our expectations--answered \"yes\" to whether they would participate for suitable financial incentives. Encouraged by the results of the survey, we propose Gringotts, a system to structure incentives and securely incorporate P2P delivery into content delivery systems. Gringotts provides a novel Proof of Delivery mechanism that allows content providers to verify correct delivery of their files, and shows how to use cryptocurrency to pay peers while guarding against liars and Sybil attacks.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Homa is a new transport protocol for datacenter networks. It provides exceptionally low latency, especially for workloads with a high volume of very short messages, and it also supports large messages and high network utilization. Homa uses in-network priority queues to ensure low latency for short messages; priority allocation is managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Homa also uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high load. Our implementation of Homa delivers 99th percentile round-trip times less than 15\u03bcs for short messages on a 10 Gbps network running at 80% load. These latencies are almost 100x lower than the best published measurements of an implementation. In simulations, Homa's latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "This paper develops a technique to detect whether the cross traffic competing with a flow is elastic or not, and shows how to use the elasticity detector to improve congestion control. If the cross traffic is elastic, i.e., made up of flows like Cubic or NewReno that increase their rate when they perceive available bandwidth, then one should use a scheme that competes well with such traffic. Such a scheme will not be able to control delays because the cross traffic will not cooperate to maintain low delays. If, however, cross traffic is inelastic, then one can use a suitable delay-controlled algorithm. Our elasticity detector uses an asymmetric sinusoidal pulse pattern and estimates elasticity by computing the frequency response (FFT) of the cross traffic estimate; we have measured its accuracy to be over 90%. We present the design and evaluation of Nimbus, a congestion control protocol that uses the elasticity detector to switch between delay-control and TCP-competitive modes. Our results on emulated and real-world paths show that Nimbus achieves throughput comparable to or better than Cubic always, but with delays that are much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to Cubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which also switches between a delay-controlling and a TCP-competitive mode, Nimbus is more robust at correctly detecting the nature of cross traffic, and unlike Copa, it is usable by a variety of delay-based and TCP-competitive methods.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Neural networks have been shown to be an effective tool for learning algorithms over graph-structured data. However, graph representation techniques---that convert graphs to real-valued vectors for use with neural networks---are still in their infancy. Recent works have proposed several approaches (e.g., graph convolutional networks), but these methods have difficulty scaling and generalizing to graphs with different sizes and shapes. We present Graph2Seq, a new technique that represents vertices of graphs as infinite time-series. By not limiting the representation to a fixed dimension, Graph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq is also reversible, allowing full recovery of the graph structure from the sequences. By analyzing a formal computational model for graph representation, we show that an unbounded sequence is necessary for scalability. Our experimental results with Graph2Seq show strong generalization and new state-of-the-art performance on a variety of graph combinatorial optimization problems.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Mapping road networks is currently both expensive and labor-intensive. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct. We propose RoadTracer, a new method to automatically construct accurate road network maps from aerial images. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We compare our approach with a segmentation method on fifteen cities, and find that at a 5% error rate, RoadTracer correctly captures 45% more junctions across these cities.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "We define the Fitzpatrick function of a $\u03c3$-monotone operator in a way similar to the original definition given by Fitzpatrick. We show that some well-known properties of Fitzpatrick function remain valid for the larger class of premonotone operators. Also, we find some conditions under which the Fitzpatrick function of a $\u03c3$-monotone operator is proper, and give some results in Hilbert spaces.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "As its price per bit drops, SSD is increasingly becoming the default storage medium for cloud application databases. However, it has not become the preferred storage medium for key-value caches, even though SSD offers more than 10x lower price per bit and sufficient performance compared to DRAM. This is because key-value caches need to frequently insert, update and evict small objects. This causes excessive writes and erasures on flash storage, since flash only supports writes and erasures of large chunks of data. These excessive writes and erasures significantly shorten the lifetime of flash, rendering it impractical to use for key-value caches. We present Flashield, a hybrid key-value cache that uses DRAM as a \"filter\" to minimize writes to SSD. Flashield performs light-weight machine learning profiling to predict which objects are likely to be read frequently before getting updated; these objects, which are prime candidates to be stored on SSD, are written to SSD in large chunks sequentially. In order to efficiently utilize the cache's available memory, we design a novel in-memory index for the variable-sized objects stored on flash that requires only 4 bytes per object in DRAM. We describe Flashield's design and implementation and, we evaluate it on a real-world cache trace. Compared to state-of-the-art systems that suffer a write amplification of 2.5x or more, Flashield maintains a median write amplification of 0.5x without any loss of hit rate or throughput.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Switches today provide a small set of scheduling algorithms. While we can tweak scheduling parameters, we cannot modify algorithmic logic, or add a completely new algorithm, after the switch has been designed. This paper presents a design for a programmable packet scheduler, which allows scheduling algorithms---potentially algorithms that are unknown today---to be programmed into a switch without requiring hardware redesign.\n  Our design builds on the observation that scheduling algorithms make two decisions: in what order to schedule packets and when to schedule them. Further, in many scheduling algorithms these decisions can be made when packets are enqueued. We leverage this observation to build a programmable scheduler using a single abstraction: the push-in first-out queue (PIFO), a priority queue that maintains the scheduling order and time for such algorithms.\n  We show that a programmable scheduler using PIFOs lets us program a wide variety of scheduling algorithms. We present a detailed hardware design for this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area overhead on a 16-nm standard-cell library. Our design lets us program many sophisticated algorithms, such as a 5-level hierarchical scheduler with programmable scheduling algorithms at each level.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Many algorithms for congestion control, scheduling, network measurement, active queue management, security, and load balancing require custom processing of packets as they traverse the data plane of a network switch. To run at line rate, these data-plane algorithms must be in hardware. With today's switch hardware, algorithms cannot be changed, nor new algorithms installed, after a switch has been built.\n  This paper shows how to program data-plane algorithms in a high-level language and compile those programs into low-level microcode that can run on emerging programmable line-rate switching chipsets. The key challenge is that these algorithms create and modify algorithmic state. The key idea to achieve line-rate programmability for stateful algorithms is the notion of a packet transaction : a sequential code block that is atomic and isolated from other such code blocks. We have developed this idea in Domino, a C-like imperative language to express data-plane algorithms. We show with many examples that Domino provides a convenient and natural way to express sophisticated data-plane algorithms, and show that these algorithms can be run at line rate with modest estimated die-area overhead.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Hybrid switching - in which a high bandwidth circuit switch (optical or wireless) is used in conjunction with a low bandwidth packet switch - is a promising alternative to interconnect servers in today's large scale data-centers. Circuit switches offer a very high link rate, but incur a non-trivial reconfiguration delay which makes their scheduling challenging. In this paper, we demonstrate a lightweight, simple and nearly-optimal scheduling algorithm that trades-off configuration costs with the benefits of reconfiguration that match the traffic demands. The algorithm has strong connections to submodular optimization, has performance at least half that of the optimal schedule and strictly outperforms state of the art in a variety of traffic demand settings. These ideas naturally generalize: we see that indirect routing leads to exponential connectivity; this is another phenomenon of the power of multi hop routing, distinct from the well-known load balancing effects.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Let $\u03c3(A)$, $\u03c1(A)$ and $r(A)$ denote the spectrum, spectral radius and numerical radius of a bounded linear operator $A$ on a Hilbert space $H$, respectively. We show that a linear operator $A$ satisfying $$\u03c1(AB)\\le r(A)r(B) \\quad\\text{ for all bounded linear operators } B$$ if and only if there is a unique $\u03bc\\in \u03c3(A)$ satisfying $|\u03bc| = \u03c1(A)$ and $A = \\frac{\u03bc(I + L)}{2}$ for a contraction $L$ with $1\\in\u03c3(L)$. One can get the same conclusion on $A$ if $\u03c1(AB) \\le r(A)r(B)$ for all rank one operators $B$. If $H$ is of finite dimension, we can further decompose $L$ as a direct sum of $C \\oplus 0$ under a suitable choice of orthonormal basis so that $Re(C^{-1}x,x) \\ge 1$ for all unit vector $x$.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "This paper presents a practical approach to rapidly introduce new dataplane functionality into networks: End-hosts embed tiny programs into packets to actively query and manipulate a network's internal state. We show how this \"tiny packet program\" (TPP) interface gives end-hosts unprecedented visibility into network behavior, enabling them to work with the network to achieve a common goal. Our design leverages what each component does best: (a) switches forward and execute tiny packet programs (at most 5 instructions) at line rate, and (b) end-hosts perform arbitrary computation on network state, which are easy to evolve. Using a hardware prototype on a NetFPGA, we show our design is feasible, at a reasonable cost. By implementing three different research proposals, we show that TPPs are also useful. And finally, we present an architecture in which they can be made secure.\n        \u25b3 Less", "author": "Mohammad Alizadeh"}, {"abstract": "Statically estimating the number of processor clock cycles it takes to execute a basic block of assembly instructions in steady state (throughput) is important for compiler backend optimizations such as register allocation, instruction selection and instruction scheduling. This is complicated specially in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures. Traditionally, compiler writers invest time experimenting and referring to processor manuals to analytically model modern processors with incomplete specifications. This is tedious, error prone and should be done for each processor generation. We present Ithemal, the first automatically learnt estimator to statically predict throughput of a set of basic block instructions using machine learning. Ithemal uses a novel Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven approach for throughput estimation. We show that Ithemal is accurate than state-of-the-art hand written tools used in compiler backends and static machine code analyzers. In particular, our model has a worst case average error of 10.53% on actual throughput values when compared to best case average errors of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's machine code analyzer when compared on three different microarchitectures, while predicting throughput values at a faster rate than aforementioned tools. We also show that Ithemal is portable, learning throughput estimation for Intel Nehalem, Haswell and Skylake microarchitectures without requiring changes to its structure.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "Modern out-of-order processors have increased capacity to exploit instruction level parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide superscalar pipelines and vector execution units, as well as deep buffers for in-flight memory requests. These resources, however, often exhibit poor utilization rates on workloads with large working sets, e.g., in-memory databases, key-value stores, and graph analytics, as compilers and hardware struggle to expose ILP and MLP from the instruction stream automatically.\n  In this paper, we introduce the IMLP (Instruction and Memory Level Parallelism) task programming model. IMLP tasks execute as coroutines that yield execution at annotated long-latency operations, e.g., memory accesses, divisions, or unpredictable branches. IMLP tasks are interleaved on a single thread, and integrate well with thread parallelism and vectorization. Our DSL embedded in C++, Cimple, allows exploration of task scheduling and transformations, such as buffering, vectorization, pipelining, and prefetching.\n  We demonstrate state-of-the-art performance on core algorithms used in in-memory databases that operate on arrays, hash tables, trees, and skip lists. Cimple applications reach 2.5x throughput gains over hardware multithreading on a multi-core, and 6.4x single thread speedup.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. Programmers must try different combinations of a large set of techniques to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks lack flexibility, supporting only a limited set of optimizations.\n  This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a scheduling language. The algorithm language simplifies expressing the algorithms. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together optimizations. We also built an autotuner to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8$\\times$, and is never more than 43% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The extensions include explicit communication, synchronization, and mapping buffers to different memory hierarchies. Tiramisu relies on a flexible representation based on the polyhedral model and explicitly uses a well-defined four-level IR that allows full separation between the algorithms, loop transformations, data-layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing and stencil benchmarks and compare it with state-of-the-art compilers. We show that Tiramisu matches or outperforms existing compilers on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "This paper shows how to build a sparse tensor algebra compiler that is agnostic to tensor formats (data layouts). We develop an interface that describes formats in terms of their capabilities and properties, and show how to build a modular code generator where new formats can be added as plugins. We then describe six implementations of the interface that compose to form the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants thereof. With these implementations at hand, our code generator can generate code to compute any tensor algebra expression on any combination of the aforementioned formats.\n  To demonstrate our technique, we have implemented it in the taco tensor algebra compiler. Our modular code generator design makes it simple to add support for new tensor formats, and the performance of the generated code is competitive with hand-optimized implementations. Furthermore, by extending taco to support a wider range of formats specialized for different application and data characteristics, we can improve end-user application performance. For example, if input data is provided in the COO format, our technique allows computing a single matrix-vector multiplication directly with the data in COO, which is up to 3.6$\\times$ faster than by first converting the data to CSR.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit superword level parallelism (SLP), a type of fine-grained parallelism. Current SLP auto-vectorization techniques use heuristics to discover vectorization opportunities in high-level language code. These heuristics are fragile, local and typically only present one vectorization strategy that is either accepted or rejected by a cost model. We present goSLP, a novel SLP auto-vectorization framework which solves the statement packing problem in a pairwise optimal manner. Using an integer linear programming (ILP) solver, goSLP searches the entire space of statement packing opportunities for a whole function at a time, while limiting total compilation time to a few minutes. Furthermore, goSLP optimally solves the vector permutation selection problem using dynamic programming. We implemented goSLP in the LLVM compiler infrastructure, achieving a geometric mean speedup of 7.58% on SPEC2017fp, 2.42% on SPEC2006fp and 4.07% on NAS benchmarks compared to LLVM's existing SLP auto-vectorizer.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "High-performance DSL developers work hard to take advantage of modern hardware. The DSL compilers have to build their own complex middle-ends before they can target a common back-end such as LLVM, which only handles single instruction streams with SIMD instructions. We introduce Tiramisu, a common middle-end that can generate efficient code for modern processors and accelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu introduces a novel three-level IR that separates the algorithm, how that algorithm is executed, and where intermediate data are stored. This separation simplifies optimization and makes targeting multiple hardware architectures from the same algorithm easier. As a result, DSL compilers can be made considerably less complex with no loss of performance while immediately targeting multiple hardware or hardware combinations such as distributed nodes with both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for the Halide and Julia compilers. We show that Tiramisu extends Halide and Julia with many new capabilities including the ability to: express new algorithms (such as recurrent filters and non-rectangular iteration spaces), perform new complex loop nest transformations (such as wavefront parallelization, loop shifting and loop fusion) and generate efficient code for more architectures (such as combinations of distributed clusters, multicores, GPUs and FPGAs). Finally, we demonstrate that Tiramisu can generate very efficient code that matches the highly optimized Intel MKL gemm (generalized matrix multiplication) implementation, we also show speedups reaching 4X in Halide and 16X in Julia due to optimizations enabled by Tiramisu.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "This paper shows how to optimize sparse tensor algebraic expressions by introducing temporary tensors, called workspaces, into the resulting loop nests. We develop a new intermediate language for tensor operations called concrete index notation that extends tensor index notation. Concrete index notation expresses when and where sub-computations occur and what tensor they are stored into. We then describe the workspace optimization in this language, and how to compile it to sparse code by building on prior work in the literature.\n  We demonstrate the importance of the optimization on several important sparse tensor kernels, including sparse matrix-matrix multiplication (SpMM), sparse tensor addition (SpAdd), and the matricized tensor times Khatri-Rao product (MTTKRP) used to factorize tensors. Our results show improvements over prior work on tensor algebra compilation and brings the performance of these kernels on par with state-of-the-art hand-optimized implementations. For example, SpMM was not supported by prior tensor algebra compilers, the performance of MTTKRP on the nell-2 data set improves by 35%, and MTTKRP can for the first time have sparse results.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "Data analytics applications combine multiple functions from different libraries and frameworks. Even when each function is optimized in isolation, the performance of the combined application can be an order of magnitude below hardware limits due to extensive data movement across these functions. To address this problem, we propose Weld, a new interface between data-intensive libraries that can optimize across disjoint libraries and functions. Weld exposes a lazily-evaluated API where diverse functions can submit their computations in a simple but general intermediate representation that captures their data-parallel structure. It then optimizes data movement across these functions and emits efficient code for diverse hardware. Weld can be integrated into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without changing their user-facing APIs. We demonstrate that Weld can speed up applications using these frameworks by up to 29x.\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "Modern hardware systems are heavily underutilized when running large-scale graph applications. While many in-memory graph frameworks have made substantial progress in optimizing these applications, we show that it is still possible to achieve up to 4 $\\times$ speedups over the fastest frameworks by greatly improving cache utilization. Previous systems have applied out-of-core processing techniques from the memory/disk boundary to the cache/DRAM boundary. However, we find that blindly applying such techniques is ineffective because of the much smaller performance gap between DRAM and cache. We present two techniques that take advantage of the cache with minimal or no instruction overhead. The first, frequency based clustering, groups together frequently accessed vertices to improve the utilization of each cache line with no runtime overhead. The second, CSR segmenting, partitions the graph to restrict all random accesses to the cache, makes all DRAM access sequential, and merges partition results using a very low overhead cache-aware merge. Both techniques can be easily implemented on top of optimized graph frameworks. Our techniques combined give speedups of up to 4 $\\times$ for PageRank, Label Propagation and Collaborative Filtering, and 2 $\\times$ for Betweenness Centrality over the best published results\n        \u25b3 Less", "author": "Saman Amarasinghe"}, {"abstract": "We present a compact model for Tunnel Field Effect Transistors (TFET), that captures sev- eral non-idealities such as the Trap Assisted Tunneling (TAT) originating from interface traps (Dit), along with Verilog-A implementation. We show that the TAT, together with band edge non-abruptness known as the Urbach tail, sets the lower limit of the sub-threshold swing and the minimum achievable current at a given temperature. Presence of charged trap states also contributes to reduced gate efficiency. We show that we can decouple the contribution of each of these processes and extract the intrinsic sub-threshold swing from a given experimental data. We derive closed form expressions of channel potential, electric field and effective tunnel energy window to accurately capture the essential device physics of TFETs. We test the model against recently published exper- imental data, and simulate simple TFET circuits using the Verilog-A model. The compact model provides a framework for TFET technology projections with improved device metrics such as better electrostatic design, reduced TAT, material with better transport properties etc.\n        \u25b3 Less", "author": "Dimitri Antoniadis"}, {"abstract": "We provide a detailed study of the interface Trap Assisted Tunneling (TAT) mechanism in tunnel field effect transistors to show how it contributes a major leakage current path before the Band To Band Tunneling (BTBT) is initiated. With a modified Shockley-Read-Hall formalism, we show that at room temperature, the phonon assisted TAT current always dominates and obscures the steep turn ON of the BTBT current for common densities of traps. Our results are applicable to top gate, double gate and gate all around structures where the traps are positioned between the source-channel tunneling region. Since the TAT has strong dependence on electric field, any effort to increase the BTBT current by enhancing local electric field also increases the leakage current. Unless the BTBT current can be increased separately, calculations show that the trap density Dit has to be decreased by 40-100 times compared with the state of the art in order for the steep turn ON (for III-V materials) to be clearly observable at room temperature. We find that the combination of the intrinsic sharpness of the band edges (Urbach tail) and the surface trap density determines the subthreshold swing.\n        \u25b3 Less", "author": "Dimitri Antoniadis"}, {"abstract": "We examine the phenomenology of the majoron portal: a simplified model of fermionic dark matter coupled to a light scalar mediator carrying lepton number 2. We find that the mediator can be very light and still consistent with laboratory and cosmological bounds. This model satisfies the thermal relic condition for natural values of dimensionless coupling constants and admits a mediator in the $10 - 100 ~\\text{MeV}$ mass range favored by small scale structure observations. As such, this model provides an excellent candidate for self-interacting dark matter.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "The magnetic moments of baryon decuplet are studied in vacuum as well as in the symmetric nuclear matter at finite temperature using a chiral SU(3) quark mean field model approach. The contributions coming from the valence quarks, quark sea and the orbital angular momentum of the quark sea have been considered to calculate magnetic moment of decuplet baryons. The decuplet baryon masses decrease, whereas, the magnetic moments increase significantly with the rise of baryonic density of the symmetric nuclear medium. This is because of the reason that constituent quark magnetic moment and the quark spin polarizations show considerable variation in the nuclear medium especially in the low temperature and baryonic density regime.\n  The increase is however quantitatively less as compared to the case of octet baryon members.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We investigate graph properties and parameters that are invariant under Weisfeiler-Leman equivalence, focussing especially on 1-dimensional Weisfeiler-Leman equivalence and on subgraph counts that are preserved. Our main results are summarized below:\n  -- For 1-dimensional Weisfeiler-Leman equivalence, we completely characterize graphs whose: (a) counts are determined by 1-dimensional Weisfeiler-Leman equivalence, and (b) whose presence is determined by 1-dimensional Weisfeiler-Leman equivalence.\n  -- Extending an old result due to Beezer and Farrell for distance-regular graphs, we show that 2-dimensional Weisfeiler-Leman equivalence preserves the first five coefficients of the matching polynomial (and no more). We determine exactly which paths and cycles have their counts determined by 2-dimensional WL-equivalence.\n  -- We also study a notion of \"approximate invariance\" of graph parameters under Weisfeiler-Leman equivalence. We show that the fractional matching number and fractional domination number are preserved by 1-dimensional Weisfeiler-Leman equivalence.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Small system collectivity observed at the LHC energies along with enhancement of strangeness makes high-multiplicity proton+proton collisions very interesting in order to look for QGP-like features, usually found in heavy-ion collisions. It may be interesting to perform a double differential study of different observables in pp collisions in terms of charged particle multiplicity and event shape in order to understand the new dimensions in high-multiplicity pp physics. We study the correlation between the number of multi-partonic interactions (nMPI), event shape (transverse spherocity) and charged particle multiplicity classes. We report the simulation results on the spherocity and multiplicity dependent study of identified particle production in pp collisions at $\\sqrt{s}$= 13 TeV using PYTHIA 8. We explore the event shape dependence of the transverse momentum ($p_{\\rm{T}}$) spectra, integrated yield, mean transverse momentum ($\\langle p_{\\rm{T}} \\rangle$) and particle ratios of the identified particles. A clear spherocity dependence of $p_{\\rm{T}}$-spectra is observed for all the particles. The $p_{\\rm T}$-crossing point of the ratios of jetty and isotropic events to the spherocity-integrated ones, depend on the multiplicity classes. The p/$\u03c6$ ratio at low-$p_{\\rm T}$ shows a weak $p_{\\rm T}$ dependence for high-multiplicity isotropic events, which is a hydrodynamic-like behavior. Larger dependence of integrated yield on spherocity is observed for high multiplicity compared to the low multiplicity pp collisions. However, the $p_{\\rm{T}}$-integrated particle ratio shows less dependence on spherocity which suggests that, the relative increase in integrated yield for different particles as a function of spherocity are similar.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Hierarchical models in Bayesian inverse problems are characterized by an assumed prior probability distribution for the unknown state and measurement error precision, and hyper-priors for the prior parameters. Combining these probability models using Bayes' law often yields a posterior distribution that cannot be sampled from directly, even for a linear model with Gaussian measurement error and Gaussian prior. Gibbs sampling can be used to sample from the posterior, but problems arise when the dimension of the state is large. This is because the Gaussian sample required for each iteration can be prohibitively expensive to compute, and because the statistical efficiency of the Markov chain degrades as the dimension of the state increases. The latter problem can be mitigated using marginalization-based techniques, but these can be computationally prohibitive as well. In this paper, we combine the low-rank techniques of Brown, Saibaba, and Vallelian (2018) with the marginalization approach of Rue and Held (2005). We consider two variants of this approach: delayed acceptance and pseudo-marginalization. We provide a detailed analysis of the acceptance rates and computational costs associated with our proposed algorithms, and compare their performances on two numerical test cases---image deblurring and inverse heat equation.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Context. Being the most numerous and oldest stars in the galaxy, M dwarfs are objects of great interest for exoplanet searches. The presence of molecules in their atmosphere complicates our understanding of their atmospheric properties. But great advances have recently been made in the modeling of M dwarfs due to the revision of solar abundances. Aims. We aim to determine stellar parameters of M dwarfs using high resolution spectra (R = 90 000) simultaneously in the visible and the near-infrared. The high resolution spectra and broad wavelength coverage provide an unique opportunity to understand the onset of dust and cloud formation at cool temperatures. Furthermore, this study will help in understanding the physical processes which occur in a cool atmospheres, particularly, the redistribution of energy from the optical to the near-infrared. Methods. The stellar parameters of M dwarfs in our sample have been determined by comparing the high resolution spectra both in the optical and in the near-infrared simultaneously observed by CARMENES with the synthetic spectra obtained from the BT-Settl model atmosphere. The detailed spectral synthesis of these observed spectra both in the optical and in the near-infrared helps to understand the missing continuum opacity. Results. For the first time, we derive fundamental stellar parameters of M dwarfs using the high resolution optical and near-infrared spectra simultaneously. We determine Teff , log g and [M/H] for 292 M dwarfs of spectral type M0 to M9, where the formation of dust and clouds are important. The derived Teff for the sample ranges from 2300 to 4000 K, values of log g ranges from 4.5 geq log g leq 5.5 and the resulting metallicity ranges from -0.5 geq [M/H] leq +0.5. We have also explored the possible differences in Teff , log g and [M/H] by comparing them with other studies of the same sample of M dwarfs.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "The viscosity of dark matter in cosmological models may cause an accelerated expansion and when this effect is sufficiently large, it can explain the dark energy. In this work, attributing the origin of viscosity to self-interaction of dark matter, we study the viscous cosmology at small redshift $(0\\leq z\\leq2.5)$. Assuming the cluster scale to be virialized and by modeling a power law behavior of velocity gradients, we calculate the Hubble expansion rate, $H(z)$ and the deceleration parameter, $q(z)$. We then perform a $\u03c7^{2}$ analysis to estimate the best fit model parameters. By using the best fit values, we explain the cosmic chronometer and type Ia supernova data. We conclude that if the dissipative effects become prominent only at the late time of cosmic evolution and are smaller at higher redshift, we can explain the observational data without requiring any dark energy component. Our analysis is independent of any specific model of self interacting dark matter.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "The magnetic moment of the $\u03c4$ lepton is an interesting quantity that is potentially sensitive to physics beyond the Standard Model. Electroweak gauge invariance implies that a heavy new physics contribution to it takes the form of an operator which involves the Higgs boson, implying that rare Higgs decays are able to probe the same physics as $a_\u03c4$. We examine the prospects for rare Higgs decays at future high energy lepton (electron or muon) colliders, and find that such a project collecting a few ab$^{-1}$ would be able to advance our understanding of this physics by roughly a factor of 10 compared to the expected reach of the high luminosity LHC.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We investigate the mass-shift of vector channel $J/\u03c8$ and pseudoscalar channel $\u03b7_c$ in strongly magnetized asymmetric nuclear medium at finite temperature using the conjunction of chiral $SU(3)$ model with the QCD sum rules. The magnetic field dependence of scalar gluon condensate $\\left\\langle \\frac{\u03b1_{s}}\u03c0 G^a_{\u03bc\u03bd} {G^a}^{\u03bc\u03bd} \\right\\rangle$ as well as the twist-2 gluon condensate $\\left\\langle \\frac{\u03b1_{s}}\u03c0 G^a_{\u03bc\u03c3} {{G^a}_\u03bd}^\u03c3 \\right\\rangle $ calculated from chiral SU($3$) model, are implemented in QCD sum rules to calculate the magnetic field dependence of $J/\u03c8$ and $\u03b7_c$ meson masses. The effects of constant external magnetic field at finite density and temperature of the medium are found to be appreciable in symmetric and asymmetric nuclear matter. The results of the present investigation may be helpful to understand the experimental observables arising from the Compressed Baryonic Matter (CBM) produced in asymmetric non-central heavy ion collision experiments.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We consider the flow network model to solve the multiprocessor real-time task scheduling problems. Using the flow network model or its generic form, linear programming (LP) formulation, for the problems is not new. However, the previous works have limitations, for example, that they are classified as offline scheduling techniques since they establish a flow network model or an LP problem considering a very long time interval. In this study, we propose how to construct the flow network model for online scheduling periodic real-time tasks on multiprocessors. Our key idea is to construct the flow network only for the active instances of tasks at the current scheduling time, while guaranteeing the existence of an optimal schedule for the future instances of the tasks. The optimal scheduling is here defined to ensure that all real-time tasks meet their deadlines when the total utilization demand of the given tasks does not exceed the total processing capacity. We then propose the flow network model-based polynomial-time scheduling algorithms. Advantageously, the flow network model allows the task workload to be collected unfairly within a certain time interval without losing the optimality. It thus leads us to designing three unfair-but-optimal scheduling algorithms on both continuous and discrete-time models. Especially, our unfair-but-optimal scheduling algorithm on a discrete-time model is, to the best of our knowledge, the first in the problem domain. We experimentally demonstrate that it significantly alleviates the scheduling overheads, i.e., the reduced number of preemptions with the comparable number of task migrations across processors.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We describe physical tests validating progress made toward acceleration and automation of hydrodynamic codes in the regime of developed turbulence by two {\\bf Deep Learning} (DL) Neural Network (NN) schemes trained on {\\bf Direct Numerical Simulations} of turbulence. Even the bare DL solutions, which do not take into account any physics of turbulence explicitly, are impressively good overall when it comes to qualitative description of important features of turbulence. However, the early tests have also uncovered some caveats of the DL approaches. We observe that the static DL scheme, implementing Convolutional GAN and trained on spatial snapshots of turbulence, fails to reproduce intermittency of turbulent fluctuations at small scales and details of the turbulence geometry at large scales. We show that the dynamic NN scheme, LAT-NET, trained on a temporal sequence of turbulence snapshots is capable to correct for the small-scale caveat of the static NN. We suggest a path forward towards improving reproducibility of the large-scale geometry of turbulence with NN.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Radiomics involves the study of tumor images to identify quantitative markers explaining cancer heterogeneity. The predominant approach is to extract hundreds to thousands of image features, including histogram features comprised of summaries of the marginal distribution of pixel intensities, which leads to multiple testing problems and can miss out on insights not contained in the selected features. In this paper, we present methods to model the entire marginal distribution of pixel intensities via the quantile function as functional data, regressed on a set of demographic, clinical, and genetic predictors. We call this approach quantile functional regression, regressing subject-specific marginal distributions across repeated measurements on a set of covariates, allowing us to assess which covariates are associated with the distribution in a global sense, as well as to identify distributional features characterizing these differences, including mean, variance, skewness, and various upper and lower quantiles. To account for smoothness in the quantile functions, we introduce custom basis functions we call quantlets that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given data set. We fit this model using a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and provides fully Bayesian inference after fitting a Markov chain Monte Carlo. We demonstrate the benefit of the basis space modeling through simulation studies, and apply the method to Magnetic resonance imaging (MRI) based radiomic dataset from Glioblastoma Multiforme to relate imaging-based quantile functions to demographic, clinical, and genetic predictors, finding specific differences in tumor pixel intensity distribution between males and females and between tumors with and without DDIT3 mutations.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Living cells communicate information about physiological conditions by producing signaling molecules in a specific timed manner. Different conditions can result in the same total amount of a signaling molecule, differing only in the pattern of the molecular concentration over time. Such temporally coded information can be completely invisible to even state-of-the-art molecular sensors with high chemical specificity that respond only to the total amount of the signaling molecule. Here, we demonstrate design principles for circuits with temporal specificity, that is, molecular circuits that respond to specific temporal patterns in a molecular concentration. We consider pulsatile patterns in a molecular concentration characterized by three fundamental temporal features - time period, duty fraction and number of pulses. We develop circuits that respond to each one of these features while being insensitive to the others. We demonstrate our design principles using abstract Chemical Reaction Networks and with explicit simulations of DNA strand displacement reactions. In this way, our work develops building blocks for temporal pattern recognition through molecular computation.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Continuous attractors have been used to understand recent neuroscience experiments where persistent activity patterns encode internal representations of external attributes like head direction or spatial location. However, the conditions under which the emergent bump of neural activity in such networks can be manipulated by space and time-dependent external sensory or motor signals are not understood. Here, we find fundamental limits on how rapidly internal representations encoded along continuous attractors can be updated by an external signal. We apply these results to place cell networks to derive a velocity-dependent non-equilibrium memory capacity.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "In recent times, Resistive RAMs (ReRAMs) have gained significant prominence due to their unique feature of supporting both non-volatile storage and logic capabilities. ReRAM is also reported to provide extremely low power consumption compared to the standard CMOS storage devices. As a result, researchers have explored the mapping and design of diverse applications, ranging from arithmetic to neuromorphic computing structures to ReRAM-based platforms. ReVAMP, a general-purpose ReRAM computing platform, has been proposed recently to leverage the parallelism exhibited in a crossbar structure. However, the technology mapping on ReVAMP remains an open challenge. Though the technology mapping with device/area-constraints have been proposed, crossbar constraints are not considered so far. In this work, we address this problem. Two technology mapping flows are proposed, considering different runtime-efficiency trade-offs. Both the mapping flows take crossbar constraints into account and generate feasible mapping for a variety of crossbar dimensions. Our proposed algorithms are highly scalable and reveal important design hints for ReRAM-based implementations.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We have applied the non-extensive statistical mechanics to free electrons in several metals to calculate the electronic specific heat at low temperature. In this case, the Fermi-Dirac (FD) function is modified from its Boltzmann-Gibbs (BG) form, with the exponential part going to a $q$-exponential, in its non-extensive form. In most cases the non-extensive parameter, $q$, is found to be slightly greater than 1 to produce the correct thermal effective mass, $m^*$, of electrons. The ratio $m^{*}/m$ is found to show a parabolic dependence on $q$. These results are tempting to conclude that electrons in free-electron metals are slightly away from thermal equilibrium with local (spatial) temperature fluctuations.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "The security of most existing cryptocurrencies is based on a concept called Proof-of-Work, in which users must solve a computationally hard cryptopuzzle to authorize transactions (`one unit of computation, one vote'). This leads to enormous expenditure on hardware and electricity in order to collect the rewards associated with transaction authorization. Proof-of-Stake is an alternative concept that instead selects users to authorize transactions proportional to their wealth (`one coin, one vote'). Some aspects of the two paradigms are the same. For instance, obtaining voting power in Proof-of-Stake has a monetary cost just as in Proof-of-Work: a coin cannot be freely duplicated any more easily than a unit of computation. However some aspects are fundamentally different. In particular, exactly because Proof-of-Stake is wasteless, there is no inherent resource cost to deviating (commonly referred to as the `Nothing-at-Stake' problem).\n  In contrast to prior work, we focus on incentive-driven deviations (any participant will deviate if doing so yields higher revenue) instead of adversarial corruption (an adversary may take over a significant fraction of the network, but the remaining players follow the protocol). The main results of this paper are several formal barriers to designing incentive-compatible proof-of-stake cryptocurrencies (that don't apply to proof-of-work).\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "In this work, we have studied the isothermal compressibility ($\u03ba_T$) as a function of temperature, baryon chemical potential and centre-of-mass energy ($\\sqrt{s_{NN}}$) using hadron resonance gas (HRG) and excluded-volume hadron resonance gas (EV-HRG) models. A mass cut-off dependence of isothermal compressibility has been studied for a physical resonance gas. Further, we study the effect of heavier resonances ($>$ 2 GeV) on the isothermal compressibility by considering the Hagedorn mass spectrum, $\u03c1(m)\\sim{\\exp(bm)}/{(m^2+m_0^2)^{5/4}}$. Here, the parameters, $b$ and $m_0$ are extracted after comparing the results of recent lattice QCD simulations at finite baryonic chemical potential. We find a significant difference between the results obtained in EV-HRG and HRG models at a higher temperatures and higher baryochemical potentials. The inclusion of the Hagedorn mass spectrum in the partition function for hadron gas has a large effect at a higher temperature. A higher mass cut-off in the Hagedorn mass spectrum takes the isothermal compressibility to a minimum value, which occurs near the Hagedorn temperature ($T_H$). We show explicitly that at the future low energy accelerator facilities like FAIR (CBM), Darmstadt and NICA, Dubna the created matter would be incompressible compared to the high energy facilities like RHIC and LHC.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Recent studies exploited external periodic synchronous signals to synchronize a pair of network nodes to address a threat of delaying the communications between the nodes. However, the sensing-based synchronization may yield faults due to nonmalicious signal and sensor noises. This paper considers a system of N nodes that will fuse their peer-to-peer synchronization results to correct the faults. Our analysis gives the lower bound of the number of faults that the system can tolerate when N is up to 12. If the number of faults is no greater than the lower bound, the faults can be identified and corrected. We also prove that the system cannot tolerate more than N-2 faults. Our results can guide the design of resilient sensing-based clock synchronization systems.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Encrypted databases have been studied for more than 10 years and are quickly emerging as a critical technology for the cloud. The current state of the art is to use property-preserving encrypting techniques (e.g., deterministic encryption) to protect the confidentiality of the data and support query processing at the same time. Unfortunately, these techniques have many limitations. Recently, trusted computing platforms (e.g., Intel SGX) have emerged as an alternative to implement encrypted databases. This paper demonstrates some vulnerabilities and the limitations of this technology, but it also shows how to make best use of it in order to improve on confidentiality, functionality, and performance.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Online advertisements that masquerade as non-advertising content pose numerous risks to users. Such hidden advertisements appear on social media platforms when content creators or \"influencers\" endorse products and brands in their content. While the Federal Trade Commission (FTC) requires content creators to disclose their endorsements in order to prevent deception and harm to users, we do not know whether and how content creators comply with the FTC's guidelines. In this paper, we studied disclosures within affiliate marketing, an endorsement-based advertising strategy used by social media content creators. We examined whether content creators follow the FTC's disclosure guidelines, how they word the disclosures, and whether these disclosures help users identify affiliate marketing content as advertisements. To do so, we first measured the prevalence of and identified the types of disclosures in over 500,000 YouTube videos and 2.1 million Pinterest pins. We then conducted a user study with 1,791 participants to test the efficacy of these disclosures. Our findings reveal that only about 10% of affiliate marketing content on both platforms contains any disclosures at all. Further, users fail to understand shorter, non-explanatory disclosures. Based on our findings, we make various design and policy suggestions to help improve advertising disclosure practices on social media platforms.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Stewards of social science data face a fundamental tension. On one hand, they want to make their data accessible to as many researchers as possible to facilitate new discoveries. At the same time, they want to restrict access to their data as much as possible in order to protect the people represented in the data. In this paper, we provide a case study addressing this common tension in an uncommon setting: the Fragile Families Challenge, a scientific mass collaboration designed to yield insights that could improve the lives of disadvantaged children in the United States. We describe our process of threat modeling, threat mitigation, and third-party guidance. We also describe the ethical principles that formed the basis of our process. We are open about our process and the trade-offs that we made in the hopes that others can improve on what we have done.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Let $\\mathbb{F}[X]$ be the polynomial ring over the variables $X=\\{x_1,x_2, \\ldots, x_n\\}$. An ideal $I=\\langle p_1(x_1), \\ldots, p_n(x_n)\\rangle$ generated by univariate polynomials $\\{p_i(x_i)\\}_{i=1}^n$ is a \\emph{univariate ideal}. We study the ideal membership problem for the univariate ideals and show the following results.\n  \\item Let $f(X)\\in\\mathbb{F}[\\ell_1, \\ldots, \\ell_r]$ be a (low rank) polynomial given by an arithmetic circuit where $\\ell_i : 1\\leq i\\leq r$ are linear forms, and $I=\\langle p_1(x_1), \\ldots, p_n(x_n)\\rangle$ be a univariate ideal. Given $\\vec\u03b1\\in {\\mathbb{F}}^n$, the (unique) remainder $f(X) \\pmod I$ can be evaluated at $\\vec\u03b1$ in deterministic time $d^{O(r)}\\cdot poly(n)$, where $d=\\max\\{\u00b0(f),\u00b0(p_1)\\ldots,\u00b0(p_n)\\}$. This yields an $n^{O(r)}$ algorithm for minimum vertex cover in graphs with rank-$r$ adjacency matrices. It also yields an $n^{O(r)}$ algorithm for evaluating the permanent of a $n\\times n$ matrix of rank $r$, over any field $\\mathbb{F}$. Over $\\mathbb{Q}$, an algorithm of similar run time for low rank permanent is due to Barvinok[Bar96] via a different technique.\n  \\item Let $f(X)\\in\\mathbb{F}[X]$ be given by an arithmetic circuit of degree $k$ ($k$ treated as fixed parameter) and $I=\\langle p_1(x_1), \\ldots, p_n(x_n)\\rangle$. We show in the special case when $I=\\langle x_1^{e_1}, \\ldots, x_n^{e_n}\\rangle$, we obtain a randomized $O^*(4.08^k)$ algorithm that uses $poly(n,k)$ space.\n  \\item Given $f(X)\\in\\mathbb{F}[X]$ by an arithmetic circuit and $I=\\langle p_1(x_1), \\ldots, p_k(x_k) \\rangle$, membership testing is $W[1]$-hard, parameterized by $k$. The problem is $MINI[1]$-hard in the special case when $I=\\langle x_1^{e_1}, \\ldots, x_k^{e_k}\\rangle$.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "For linear inverse problems with a large number of unknown parameters, uncertainty quantification remains a challenging task. In this work, we use Krylov subspace methods to approximate the posterior covariance matrix and describe efficient methods for exploring the posterior distribution. Assuming that Krylov methods (e.g., based on the generalized Golub-Kahan bidiagonalization) have been used to compute an estimate of the solution, we get an approximation of the posterior covariance matrix for `free.' We provide theoretical results that quantify the accuracy of the approximation and of the resulting posterior distribution. Then, we describe efficient methods that use the approximation to compute measures of uncertainty, including the Kullback-Liebler divergence. We present two methods that use preconditioned Lanczos methods to efficiently generate samples from the posterior distribution. Numerical examples from tomography demonstrate the effectiveness of the described approaches.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Voltage current characteristics of a PN-junction diode are intrinsically nonlinear in nature. It is shown in this paper that a mathematical form of nonlinearity of a PN-junction diode resembles the nonlinear response of electric polarization of a dielectric medium to the electric field. Nonlinearity of a PN-junction can be expressed in a series of successively increasing orders of the nonlinearity. For a PN-junction diode, higher order nonlinear terms become significant as a voltage across the diode is increased. In this paper, a gradual emergence of a nonlinear regime with the amplitude of a sinusoidal voltage is presented. Higher order harmonics are produced by utilizing the nonlinearity of a single PN-junction diode. An experimental realization of a frequency comb with the highest frequency up to the twentieth harmonics is also presented. In addition, in the same circuit by making the nonlinearity significant up to the second order, an experiment on generation of the sum and difference of frequencies is realized.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We present a quantum key distribution (QKD) protocol based on long lived coherent states prepared on superconducting rings with a mesoscopic Josephson junction (dc-SQUIDs). This enables storage of the prepared states for long durations before actually performing the key distribution. Our on-demand QKD protocol is closely related to the coherent state based continuous variable quantum key distribution protocol. A detailed analysis of preparation, evolution and different measurement schemes that are required to be implemented on dc-SQUIDs to carry out the QKD is provided. We present two variants of the protocol, one requiring time stamping of states and offering a higher key rate and the other without time stamping and a lower key rate. This is a step towards having non-photon based QKD protocols which will be eventually desirable as photon states cannot be stored for long and therefore the key distribution has to be implemented immediately after photon exchange has occurred. Our protocol offers an innovative scheme to perform QKD and can be realized using current experimental techniques.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We study a multispecies generalization of a left-permeable asymmetric exclusion process (LPASEP) in one dimension with open boundaries. We determine all phases in the phase diagram using an exact projection to the LPASEP solved by us in a previous work. In most phases, we observe the phenomenon of dynamical expulsion of one or more species. We explain the density profiles in each phase using interacting shocks. This explanation is corroborated by simulations.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We consider circuit complexity in certain interacting scalar quantum field theories, mainly focusing on the $\u03c6^4$ theory. We work out the circuit complexity for evolving from a nearly Gaussian unentangled reference state to the entangled ground state of the theory. Our approach uses Nielsen's geometric method, which translates into working out the geodesic equation arising from a certain cost functional. We present a general method, making use of integral transforms, to do the required lattice sums analytically and give explicit expressions for the $d=2,3$ cases. Our method enables a study of circuit complexity in the epsilon expansion for the Wilson-Fisher fixed point. We find that with increasing dimensionality the circuit depth increases in the presence of the $\u03c6^4$ interaction eventually causing the perturbative calculation to breakdown. We discuss how circuit complexity relates with the renormalization group.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We use concurrence as an entanglement measure and experimentally demonstrate the entanglement classification of arbitrary three-qubit pure states on a nuclear magnetic resonance (NMR) quantum information processor. Computing the concurrence experimentally under three different bipartitions, for an arbitrary three-qubit pure state, reveals the entanglement class of the state. The experiment involves measuring the expectation values of Pauli operators. This was achieved by mapping the desired expectation values onto the local $z$ magnetization of a single qubit. We tested the entanglement classification protocol on twenty seven different generic states and successfully detected their entanglement class. Full quantum state tomography was performed to construct experimental tomographs of each state and negativity was calculated from them, to validate the experimental results.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We analyse the transverse momentum ($p_{T}$)-spectra as a function of charged-particle multiplicity at midrapidity ($|y| < 0.5$) for various identified particles such as $\u03c0^{\\pm}$, $K^{\\pm}$, $K_S^0$, $p+\\overline{p}$, $\u03c6$, $K^{*0} + \\overline {K^{*0}}$, and $\u039b$ + $\\bar\u039b$ in proton-proton collisions at $\\sqrt{s}$ = 7 TeV using Boltzmann-Gibbs Blast Wave (BGBW) model and thermodynamically consistent Tsallis distribution function. We obtain the multiplicity dependent kinetic freeze-out temperature ($T_{kin}$) and radial flow ($\u03b2$) of various particles after fitting the $p_T$-distribution with BGBW model. Here, $T_{kin}$ exhibits mild dependence on multiplicity class while $\u03b2$ shows almost independent behaviour. The information regarding Tsallis temperature and the non-extensivity parameter ($q$) are drawn by fitting the $p_{T}$-spectra with Tsallis distribution function. The extracted parameters of these particles are studied as a function of charged particle multiplicity density ($dN_{ch}/d\u03b7$). In addition to this, we also study these parameters as a function of particle mass to observe any possible mass ordering. All the identified hadrons show a mass ordering in temperature, non-extensive parameter and also a strong dependence on multiplicity classes, except the lighter particles. It is observed that as the particle multiplicity increases, the $q$-parameter approaches to Boltzmann-Gibbs value, hence a conclusion can be drawn that system tends to thermal equilibrium. The observations are consistent with a differential freeze-out scenario of the produced particles.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Ride-hailing services have expanded the role of shared mobility in passenger transportation systems, creating new markets and creative planning solutions for major urban centers. In this paper, we consider their use for last-mile passenger transportation in coordination with a mass transit service to provide a seamless multimodal transportation experience for the user. A system that provides passengers with predictable information on travel and waiting times in their commutes is immensely valuable. We envision that the passengers will inform the system in advance of their desired travel and arrival windows so that the system can jointly optimize the schedules of passengers. The problem we study balances minimizing travel time and the number of trips taken by the last-mile vehicles, so that long-term planning, maintenance, and environmental impact considerations can be taken into account. We focus our attention on the problem where the last-mile service aggregates passengers by destination. We show that this problem is NP-hard, and propose a decision diagram-based branch-and-price decomposition model that can solve instances of real-world size (10,000 passengers, 50 last-mile destinations, 600 last-mile vehicles) in time (~1 minute) that is orders-of-magnitude faster than other methods appearing in the literature. Our experiments also indicate that single-destination last-mile service provides high-quality solutions to more general settings.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "The propagation of a cylindrical shock wave in a self-gravitating, rotating axisymmetric dusty gas under the action of monochromatic radiation with a constant intensity per unit area, which has variable azimuthal and axial components of fluid velocity, is investigated. The gas is assumed to be grey and opaque, and the shock is assumed to be transparent. The dusty gas is considered as a mixture of non-ideal gas and small solid particles, in which solid particles are continuously distributed. To obtain some essential features of shock propagation, small solid particles are considered as a pseudo-fluid, and it is assumed that the equilibrium flow condition is maintained in the entire flow-field. Similarity solutions are obtained as well as the effects of the variation of the radiation parameter, the gravitation parameter, the non-idealness parameter of the gas, the mass concentration of solid particles in the mixture, the ratio of the density of solid particles to the initial density of the gas are worked out in detail. The similarity solution exists under the constant initial angular velocity, and the shock strength is independent from the radiation parameter and the gravitation parameter. It is found that radiation parameter dominates the effect of dusty gas parameters on the variation of radiation heat flux. The total energy of the flow-field behind the shock front is not constant but varies as fourth power of the shock radius.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Let $G$ be a finite simple graph on $n$ vertices and $J_G$ denote the corresponding binomial edge ideal in the polynomial ring $S = K[x_1, \\ldots, x_n, y_1, \\ldots, y_n].$ In this article, we compute the Hilbert series of binomial edge ideal of decomposable graphs in terms of Hilbert series of its indecomposable subgraphs. Also, we compute the Hilbert series of binomial edge ideal of join of two graphs and as a consequence we obtain the Hilbert series of complete $k$-partite graph, fan graph, multi-fan graph and wheel graph.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We revisit an unpublished paper of Vervoort (2002) on the once reinforced random walk, and prove that this process is recurrent on any graph of the form $\\mathbb{Z}\\times \u0393$, with $\u0393$ a finite graph, for sufficiently large reinforcement parameter. We also obtain a shape theorem for the set of visited sites, and show that the fluctuations around this shape are of polynomial order. The proof involves sharp general estimates on the time spent on subgraphs of the ambiant graph which might be of independent interest.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Weak values inferred from weak measurements have been proposed as a tool to investigate trajectories of pre- and post-selected quantum systems. Are the inferences drawn from the weak values about the past of a quantum particle fully true? Can the two-state vector formalism predict everything that the standard formalism of quantum mechanics can? To investigate these questions we present a \"which-path\" gedanken experiment in which the information revealed by a pre- and post-selected quantum system is surprisingly different from what one would expect from the weak values computed using the two-state vector formalism. In our gedanken experiment, a particle reveals its presence in locations where the weak value of the projection operator onto those locations was vanishingly small. Therefore our predictions turn out to be in contradistinction to those made based on the nonvanishing weak values as the presence indicators of the quantum particle. We propose a six port photon-based interferometer setup as a possible physical realization of our gedanken experiment.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "In this paper we develop an efficient procedure for computing a (scaled) Hadamard product for \\emph{commutative} polynomials. This serves as a tool to obtain faster algorithms for several problems. Our main algorithmic results include the following: 1) Given an arithmetic circuit $C$ of $\\text{poly}(n)$ size computing a polynomial $f\\in\\mathbb{F}[X]$ and a parameter $k$, we give a deterministic algorithm of run time $O^*(n^{k/2+c\\log k})$ for some constant $c$ to compute the sum of the coefficients of multilinear monomials of degree $k$ in $f$, that answers an open question mentioned by Koutis and Williams in~\\cite{KW16}. 2) Given an arithmetic circuit $C$ of size $s$ computing a polynomial $f\\in\\mathbb{F}[X]$ (where $\\mathbb{F}$ could be any field where the field arithmetic is efficient), and a parameter $k$, we give a randomized algorithm of run time $4.32^k\\cdot\\text{poly}(n,s)$ to check if $f$ contains a multilinear monomial of degree $k$ or not. Our algorithm uses $\\text{poly}(n,k,s)$ space. The recent algorithm of Brand et al. \\cite{BDH18} solves this problem over fields of characteristic zero using exterior algebra. 3) If the given circuit $C$ is a depth-three homogeneous circuit computing $f \\in \\mathbb{Q}[X]$ of degree $k$, we give a \\emph{deterministic} parameterized algorithm of run time $4^k \\cdot \\text{poly}(n,s)$ to detect degree $k$ multilinear terms, and an algorithm of run time $2^k \\cdot \\text{poly}(n,s)$ to compute the sum of their coefficients in $f$. For finite fields also we can detect degree $k$ multilinear terms in $f$ in deterministic $e^k k^{O(\\log k)}(2^{ck} + 2^k)\\cdot\\text{poly}(n,s)$ time for $c\\leq 5$.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Hardware acceleration is an enabler for ubiquitous and efficient deep learning. With hardware accelerators being introduced in datacenter and edge devices, it is time to acknowledge that hardware specialization is central to the deep learning system stack.\n  This technical report presents the Versatile Tensor Accelerator (VTA), an open, generic, and customizable deep learning accelerator design. VTA is a programmable accelerator that exposes a RISC-like programming abstraction to describe operations at the tensor level. We designed VTA to expose the most salient and common characteristics of mainstream deep learning accelerators, such as tensor operations, DMA load/stores, and explicit compute/memory arbitration.\n  VTA is more than a standalone accelerator design: it's an end-to-end solution that includes drivers, a JIT runtime, and an optimizing compiler stack based on TVM. The current release of VTA includes a behavioral hardware simulator, as well as the infrastructure to deploy VTA on low-cost FPGA development boards for fast prototyping.\n  By extending the TVM stack with a customizable, and open source deep learning hardware accelerator design, we are exposing a transparent end-to-end deep learning stack from the high-level deep learning framework, down to the actual hardware design and implementation. This forms a truly end-to-end, from software-to-hardware open source stack for deep learning systems.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We extend our previous study of Markov chains on finite commutative rings to arbitrary finite rings with identity. At each step, we either add or multiply by a randomly chosen element of the ring, where the addition (resp. multiplication) distribution is uniform (resp. conjugacy invariant). We present explicit formulas both for the eigenvalues of the transition matrix as well as the stationary distribution. For the special case of the rings $M_2(\\mathbb F_q),$ we prove that the mixing time is bounded by an absolute constant.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "This paper proposed a new probability distribution named as inverse xgamma distribution (IXGD). Different mathematical and statistical properties,viz., reliability characteristics, moments, inverse moments, stochastic ordering and order statistics of the proposed distribution have been derived and discussed. The estimation of the parameter of IXGD has been approached by different methods of estimation, namely, maximum likelihood method of estimation (MLE), Least square method of estimation (LSE), Weighted least square method of estimation (WLSE), Cram'er-von-Mises method of estimation (CME) and maximum product spacing method of estimation (MPSE). Asymptotic confidence interval (ACI) of the parameter is also obtained. A simulation study has been carried out to compare the performance of the obtained estimators and corresponding ACI in terms of average widths and corresponding coverage probabilities. Finally, two real data sets have been used to demonstrate the applicability of IXGD in real life situations.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "The advent of RoCE (RDMA over Converged Ethernet) has led to a significant increase in the use of RDMA in datacenter networks. To achieve good performance, RoCE requires a lossless network which is in turn achieved by enabling Priority Flow Control (PFC) within the network. However, PFC brings with it a host of problems such as head-of-the-line blocking, congestion spreading, and occasional deadlocks. Rather than seek to fix these issues, we instead ask: is PFC fundamentally required to support RDMA over Ethernet?\n  We show that the need for PFC is an artifact of current RoCE NIC designs rather than a fundamental requirement. We propose an improved RoCE NIC (IRN) design that makes a few simple changes to the RoCE NIC for better handling of packet losses. We show that IRN (without PFC) outperforms RoCE (with PFC) by 6-83% for typical network scenarios. Thus not only does IRN eliminate the need for PFC, it improves performance in the process! We further show that the changes that IRN introduces can be implemented with modest overheads of about 3-10% to NIC resources. Based on our results, we argue that research and industry should rethink the current trajectory of network support for RDMA.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We present an experimental setup to demonstrate normal modes and symmetry breaking in a two-dimensional pendulum. In our experiment we have used two modes of a single oscillator to demonstrate normal modes, as opposed to two single oscillators used in standard setups of two-dimensional pendulums. Breaking of the cylindrical symmetry of the pendulum is achieved by attaching a spring in the suspension. This leads to interesting visual patterns in the motion, wherein the plane of the oscillator shifts with time, the motion then becomes elliptical, shifts back again to planar, before finally returning to planar motion in the original plane. The symmetry breaking leads to non-degenerate normal modes of oscillation, whose interplay gives rise to the observed motion patterns. This also explains why for a real pendulum, the plane of motion always shifts, unlike the ideal two-dimensional pendulum where the plane of oscillation is supposed to remain fixed. This curious fact also contributes to the difficulties involved in building a Foucault's pendulum, where the plane of rotation due to Coriolis force needs to be accurately measured. The strength of the symmetry breaking in our system can be quantified by a parameter the \"return time\", which is defined as the time over which the pendulum returns to its original motion pattern. We propose this setup as a pedagogical tool to introduce the concepts of normal modes and symmetry breaking in a physics laboratory. The motion patterns that emerge have a high visual impact and we have also described in detail the quantitative observations can be made with this setup.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Invariant tensors play an important role in gauge theories, for example, in dualities of N=1 gauge theories. However, for theories with fields in representations larger than the fundamental, the full set of invariant tensors is often difficult to construct. We present a new approach to the construction of these tensors, and use it to find the complete set of invariant tensors of a theory of SO(3) with fields in the symmetric tensor representation.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Let $G$ be a finite simple graph on $n$ vertices and $J_G$ denote the corresponding binomial edge ideal in $S = K[x_1, \\ldots, x_n, y_1, \\ldots, y_n].$ In this article, we prove that if $G$ is a fan graph of a complete graph, then $reg(S/J_G) \\leq c(G)$, where $c(G)$ denote the number of maximal cliques in $G$. Further, we show that if $G$ is a $k$-pure fan graph, then $reg(S/J_G) = k+1$. We then compute a precise expression for the regularity of Cohen-Macaulay bipartite graphs.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Flagella and cilia are examples of actively oscillating, whiplike biological filaments that are crucial to processes as diverse as locomotion, mucus clearance, embryogenesis and cell motility. Elastic driven rod-like filaments subjected to compressive follower forces provide a way to mimic oscillatory beating in synthetic settings. In the continuum limit, this spatiotemporal response is an emergent phenomenon resulting from the interplay between the structural elastic instability of the slender rods subjected to the non-conservative follower forces, geometric constraints that control the onset of this instability, and viscous dissipation due to fluid drag by ambient media. In this paper, we use an elastic rod model to characterize beating frequencies, the critical follower forces and the non-linear rod shapes, for prestressed, clamped rods subject to two types of fluid drag forces, namely, linear Stokes drag and non-linear Morrison drag. We find that the critical follower force depends strongly on the initial slack and weakly on the nature of the drag force. The emergent frequencies however, depend strongly on both the extent of pre-stress as well as the nature of the fluid drag.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware-specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Distributed deep neural network (DDNN) training constitutes an increasingly important workload that frequently runs in the cloud. Larger DNN models and faster compute engines are shifting DDNN training bottlenecks from computation to communication. This paper characterizes DDNN training to precisely pinpoint these bottlenecks. We found that timely training requires high performance parameter servers (PSs) with optimized network stacks and gradient processing pipelines, as well as server and network hardware with balanced computation and communication resources. We therefore propose PHub, a high performance multi-tenant, rack-scale PS design. PHub co-designs the PS software and hardware to accelerate rack-level and hierarchical cross-rack parameter exchange, with an API compatible with many DDNN training frameworks. PHub provides a performance improvement of up to 2.7x compared to state-of-the-art distributed training techniques for cloud-based ImageNet workloads, with 25% better throughput per dollar.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Weak memory models are a consequence of the desire on part of architects to preserve all the uniprocessor optimizations while building a shared memory multiprocessor. The efforts to formalize weak memory models of ARM and POWER over the last decades are mostly empirical -- they try to capture empirically observed behaviors -- and end up providing no insight into the inherent nature of weak memory models. This paper takes a constructive approach to find a common base for weak memory models: we explore what a weak memory would look like if we constructed it with the explicit goal of preserving all the uniprocessor optimizations. We will disallow some optimizations which break a programmer's intuition in highly unexpected ways. The constructed model, which we call General Atomic Memory Model (GAM), allows all four load/store reorderings. We give the construction procedure of GAM, and provide insights which are used to define its operational and axiomatic semantics. Though no attempt is made to match GAM to any existing weak memory model, we show by simulation that GAM has comparable performance with other models. No deep knowledge of memory models is needed to read this paper.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Let $C$ be a depth-3 arithmetic circuit of size at most $s$, computing a polynomial $ f \\in \\mathbb{F}[x_1,\\ldots, x_n] $ (where $\\mathbb{F}$ = $\\mathbb{Q}$ or $\\mathbb{C}$) and the fan-in of the product gates of $C$ is bounded by $d$. We give a deterministic polynomial identity testing algorithm to check whether $f\\equiv 0$ or not in time $ 2^d \\text{ poly}(n,s) $.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Propagating interfaces are ubiquitous in nature, underlying instabilities and pattern formation in biology and material science. Physical principles governing interface growth are well understood in passive settings; however, our understanding of interfaces in active systems is still in its infancy. Here, we study the evolution of an active-passive interface using a model active matter system, bacterial swarms. We use ultra-violet light exposure to create compact domains of passive bacteria within Serratia marcescens swarms, thereby creating interfaces separating motile and immotile cells. Post-exposure, the boundary re-shapes and erodes due to self-emergent collective flows. We demonstrate that the active-passive boundary acts as a diffuse interface with mechanical properties set by the flow. Intriguingly, interfacial velocity couples to local swarm speed and interface curvature, suggesting that an active analogue to classic Gibbs-Thomson-Stefan conditions controls boundary propagation. Our results generalize interface theories to mixing and segregation in active systems with collective flows.\n        \u25b3 Less", "author": "Arvind"}, {"abstract": "Prior research has proposed technical solutions to use peer-to-peer (P2P) content delivery to serve Internet video, showing that it can reduce costs to content providers. Yet, such methods have not become widespread except for a few niche instances. An important challenge is incentivization: what tangible benefits does P2P content delivery offer users who bring resources to the table? In this paper, we ask whether monetary incentives can help attract peers in P2P content delivery systems. We commissioned a professional survey of people around theUnited States to answer several relevant questions. We found that 51% of the 876 respondents--substantially larger than our expectations--answered \"yes\" to whether they would participate for suitable financial incentives. Encouraged by the results of the survey, we propose Gringotts, a system to structure incentives and securely incorporate P2P delivery into content delivery systems. Gringotts provides a novel Proof of Delivery mechanism that allows content providers to verify correct delivery of their files, and shows how to use cryptocurrency to pay peers while guarding against liars and Sybil attacks.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "This paper develops a technique to detect whether the cross traffic competing with a flow is elastic or not, and shows how to use the elasticity detector to improve congestion control. If the cross traffic is elastic, i.e., made up of flows like Cubic or NewReno that increase their rate when they perceive available bandwidth, then one should use a scheme that competes well with such traffic. Such a scheme will not be able to control delays because the cross traffic will not cooperate to maintain low delays. If, however, cross traffic is inelastic, then one can use a suitable delay-controlled algorithm. Our elasticity detector uses an asymmetric sinusoidal pulse pattern and estimates elasticity by computing the frequency response (FFT) of the cross traffic estimate; we have measured its accuracy to be over 90%. We present the design and evaluation of Nimbus, a congestion control protocol that uses the elasticity detector to switch between delay-control and TCP-competitive modes. Our results on emulated and real-world paths show that Nimbus achieves throughput comparable to or better than Cubic always, but with delays that are much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to Cubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which also switches between a delay-controlling and a TCP-competitive mode, Nimbus is more robust at correctly detecting the nature of cross traffic, and unlike Copa, it is usable by a variety of delay-based and TCP-competitive methods.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "Mapping road networks is currently both expensive and labor-intensive. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct. We propose RoadTracer, a new method to automatically construct accurate road network maps from aerial images. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We compare our approach with a segmentation method on fifteen cities, and find that at a 5% error rate, RoadTracer correctly captures 45% more junctions across these cities.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "Switches today provide a small set of scheduling algorithms. While we can tweak scheduling parameters, we cannot modify algorithmic logic, or add a completely new algorithm, after the switch has been designed. This paper presents a design for a programmable packet scheduler, which allows scheduling algorithms---potentially algorithms that are unknown today---to be programmed into a switch without requiring hardware redesign.\n  Our design builds on the observation that scheduling algorithms make two decisions: in what order to schedule packets and when to schedule them. Further, in many scheduling algorithms these decisions can be made when packets are enqueued. We leverage this observation to build a programmable scheduler using a single abstraction: the push-in first-out queue (PIFO), a priority queue that maintains the scheduling order and time for such algorithms.\n  We show that a programmable scheduler using PIFOs lets us program a wide variety of scheduling algorithms. We present a detailed hardware design for this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area overhead on a 16-nm standard-cell library. Our design lets us program many sophisticated algorithms, such as a 5-level hierarchical scheduler with programmable scheduling algorithms at each level.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "Many algorithms for congestion control, scheduling, network measurement, active queue management, security, and load balancing require custom processing of packets as they traverse the data plane of a network switch. To run at line rate, these data-plane algorithms must be in hardware. With today's switch hardware, algorithms cannot be changed, nor new algorithms installed, after a switch has been built.\n  This paper shows how to program data-plane algorithms in a high-level language and compile those programs into low-level microcode that can run on emerging programmable line-rate switching chipsets. The key challenge is that these algorithms create and modify algorithmic state. The key idea to achieve line-rate programmability for stateful algorithms is the notion of a packet transaction : a sequential code block that is atomic and isolated from other such code blocks. We have developed this idea in Domino, a C-like imperative language to express data-plane algorithms. We show with many examples that Domino provides a convenient and natural way to express sophisticated data-plane algorithms, and show that these algorithms can be run at line rate with modest estimated die-area overhead.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "Fractional Interference Alignment (FIA) is a transmission scheme which achieves any value between [0,1] for the Symbols transmitted per Antenna per Channel use (SpAC). FIA was designed in [1] specifically for Finite Alphabet (FA) signals, under the constraint that the Minimum Distance (MD) detector is used at all the receivers. Similar to classical interference alignment, the FIA precoder also needs perfect channel state information at all the transmitters (CSIT). In this work, a novel Blind Fractional Interference Alignment (B-FIA) scheme is introduced, where the basic assumption is that CSIT is not available. We consider two popular channel models, namely: Broadcast channel, and Interference channel. For these two channel models, the maximum achievable value of SpAC satisfying the constraints of the MD detector is obtained, but with no CSIT, and also a precoder design is provided to obtain any value of SpAC in the achievable range.\n  Further, the precoder structure provided has one distinct advantage: interference channel state information at the receiver (I-CSIR) is not needed, when all the transmitters and receivers are equipped with one antenna each. When two or more antennas are used at both ends, I-CSIR must be available to obtain the maximum achievable value of SpAC. The receiver designs for both the Minimum Distance and the Maximum Likelihood (ML) decoders are discussed, where the interference statistics is estimated from the received signal samples. Simulation results of the B-FIA show that the ML decoder with estimated statistics achieves a significantly better error rate performance when compared to the MD decoder with known statistics, since the MD decoder assumes the interference plus noise term as colored Gaussian noise.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "Interference Alignment (IA) is a transmission scheme which achieves 1/2 Degrees-of-Freedom (DoF) per transmit-antenna per user. The constraints imposed on the scheme are based on the linear receiver since conventional IA assumes Gaussian signaling. However, when the transmitters employ Finite Alphabet (FA) signaling, neither the conventional IA precoders nor the linear receiver are optimal structures. Therefore, a novel Fractional Interference Alignment (FIA) scheme is introduced when FA signals are used, where the alignment constraints are now based on the non-linear, minimum distance (MD) detector. Since DoF is defined only as signal-to-noise ratio tends to infinity, we introduce a new metric called SpAC (number of Symbols transmitted-per-transmit Antenna-per-Channel use) for analyzing the FIA scheme. The maximum SpAC is one, and the FIA achieves any value of SpAC in the range [0,1]. The key motivation for this work is that numerical simulations with FA signals and MD detector for fixed SpAC (=1/2, as in IA) over a set of optimization problems, like minimizing bit error rate or maximizing the mutual information, achieves a significantly better error rate performance when compared to the existing algorithms that minimize mean square error or maximize signal-to-interference plus noise ratio.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "This paper presents an analysis of spinal codes, a class of rateless codes proposed recently. We prove that spinal codes achieve Shannon capacity for the binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN) channel with an efficient polynomial-time encoder and decoder. They are the first rateless codes with proofs of these properties for BSC and AWGN. The key idea in the spinal code is the sequential application of a hash function over the message bits. The sequential structure of the code turns out to be crucial for efficient decoding. Moreover, counter to the wisdom of having an expander structure in good codes, we show that the spinal code, despite its sequential structure, achieves capacity. The pseudo-randomness provided by a hash function suffices for this purpose. Our proof introduces a variant of Gallager's result characterizing the error exponent of random codes for any memoryless channel. We present a novel application of these error-exponent results within the framework of an efficient sequential code. The application of a hash function over the message bits provides a methodical and effective way to de-randomize Shannon's random codebook construction.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "This paper describes the implementation and evaluation of an operating system module, the Congestion Manager (CM), which provides integrated network flow management and exports a convenient programming interface that allows applications to be notified of, and adapt to, changing network conditions. We describe the API by which applications interface with the CM, and the architectural considerations that factored into the design. To evaluate the architecture and API, we describe our implementations of TCP; a streaming layered audio/video application; and an interactive audio application using the CM, and show that they achieve adaptive behavior without incurring much end-system overhead. All flows including TCP benefit from the sharing of congestion information, and applications are able to incorporate new functionality such as congestion control and adaptive behavior.\n        \u25b3 Less", "author": "Hari Balakrishnan"}, {"abstract": "High-quality micro-lasers are key ingredients in non-linear optics, communication, sensing and low-threshold solar-pumped lasers. However, such micro-lasers exhibit negligible absorption of free-space broadband pump light. Recently, this limitation was lifted by cascade energy transfer, in which the absorption and quality factor are modulated with wavelength, enabling non-resonant pumping of high-quality micro-lasers and solar-pumped laser to operate at record low solar concentration. Here, we present a generic theoretical framework for modeling the absorption, emission and energy transfer of incoherent radiation between cascade sensitizer and laser gain media. Our model is based on linear equations of the modified net radiation method and is therefore robust, fast converging and has low complexity. We apply this formalism to compute the optimal parameters of low-threshold solar-pumped lasers. It is revealed that the interplay between the absorption and self-absorption of such lasers defines the optimal pump absorption below the maximal value, which is in contrast to conventional lasers for which full pump absorption is desired. Numerical results are compared to experimental data on a sensitized Nd:YAG cavity, and quantitative agreement with theoretical models is found. Our work modularizes the gain and sensitizing components and paves the way for the optimal design of broadband-pumped high-quality micro-lasers and efficient solar-pumped lasers.\n        \u25b3 Less", "author": "Marc A. Baldo"}, {"abstract": "Plexcitons are polaritonic modes that result from the strong coupling between excitons and plasmons. We consider plexcitons emerging from the interaction of excitons in an organic molecular layer with surface plasmons in a metallic film. We predict the emergence of Dirac cones in the two-dimensional bandstructure of plexcitons due to the inherent alignment of the excitonic transitions in the organic layer. These Dirac cones may open up in energy by simultaneously interfacing the metal with a magneto-optical layer and subjecting the whole system to a perpendicular magnetic field. The resulting energy gap becomes populated with topologically protected one-way modes which travel at the interface of this plexcitonic system. Our theoretical proposal suggests that plexcitons are a convenient and simple platform for the exploration of exotic phases of matter as well as of novel ways to direct energy flow at the nanoscale.\n        \u25b3 Less", "author": "Marc A. Baldo"}, {"abstract": "The formation of 360\u00b0 magnetic domain walls (360DWs) in Co and Ni80Fe20 thin film wires was demonstrated experimentally for different wire widths, by successively injecting two 180\u00b0 domain walls (180DWs) into the wire. For narrow wires (less than 50 nm wide for Co), edge roughness prevented the combination of the 180DWs into a 360DW, and for wide wires (200 nm for Co) the 360DW collapsed, but over an intermediate range of wire widths, reproducible 360DW formation occurred. The annihilation and dissociation of 360DWs was demonstrated by applying a magnetic field parallel to the wire, showing that annihilation fields were several times higher than dissociation fields in agreement with micromagnetic modeling. The annihilation of a 360DW by current pulsing was demonstrated.\n        \u25b3 Less", "author": "Marc A. Baldo"}, {"abstract": "Organic light emitting devices and solar cells are machines that create, manipulate and destroy excited states in organic semiconductors. It is crucial to characterize these excited states, or excitons, to optimize device performance in applications like displays and solar energy harvesting. This is complicated if the excited state is a triplet because the electronic transition is dark with a vanishing oscillator strength. As a consequence, triplet state spectroscopy must usually be performed at cryogenic temperatures to reduce competition from non-radiative rates. Here, we control non-radiative rates by engineering a solid-state host matrix containing the target molecule, allowing the observation of phosphorescence at room temperature and alleviating constraints of cryogenic experiments. We test these techniques on a wide range of materials with functionalities spanning multi-exciton generation (singlet exciton fission), organic light emitting device host materials, and thermally activated delayed fluorescence type emitters. Control of non-radiative modes in the matrix surrounding a target molecule may also have broader applications in light emitting and photovoltaic devices.\n        \u25b3 Less", "author": "Marc A. Baldo"}, {"abstract": "We report highly efficient, simultaneous fluorescence and phosphorescence (74% yield) at room temperature from a single molecule ensemble of (BzP)PB dispersed into a polymer host. The slow phosphorescence (208 ms lifetime) is very efficient (50%) at room temperature and only possible because the non-radiative rate for the triplet state is extremely low. The ability of an organic molecule to function as an efficient dual state emitter at room temperature is unusual and opens new fields of applications including the use as broadband down-conversion emitters, optical sensors and attenuators, exciton probes, and spin-independent intermediates for F\u00f6rster resonant energy transfer.\n        \u25b3 Less", "author": "Marc A. Baldo"}, {"abstract": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and focus on modelling local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing both local and non-local dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions and exploits the richer representation to improve word level predictions. The framework is evaluated on three different tasks, namely social media, textual and visual information extraction. Results show that GraphIE outperforms a competitive baseline (BiLSTM+CRF) in all tasks by a significant margin.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a point-to-set metric, determines how to combine predictors trained on various domains. The metric is learned in an unsupervised fashion using meta-training. Experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15% average error reduction on benchmark datasets.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized representation to effectively utilize entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment. We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our model with reinforcement learning using a variant of generalized value iteration. The model outperforms state-of-the-art approaches on several metrics, yielding a 45% reduction in goal localization error.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We present a novel technique for automatic program correction in MOOCs, capable of fixing both syntactic and semantic errors without manual, problem specific correction strategies. Given an incorrect student program, it generates candidate programs from a distribution of likely corrections, and checks each candidate for correctness against a test suite.\n  The key observation is that in MOOCs many programs share similar code fragments, and the seq2seq neural network model, used in the natural-language processing task of machine translation, can be modified and trained to recover these fragments.\n  Experiment shows our scheme can correct 29% of all incorrect submissions and out-performs state of the art approach which requires manual, problem specific correction strategies.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis.  Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect.  This approach directly enables discovery of highly-rated or inconsistent aspects of a product.  Our generative model admits an efficient variational mean-field inference algorithm.  It is also easily extensible, and we describe several modifications and their effects on model structure and inference.  We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries.  We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy.  We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "Domain knowledge is crucial for effective performance in autonomous control systems.  Typically, human effort is required to encode this knowledge into a control algorithm.  In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance.  Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application.  To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure.  This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space.  We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer.  Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games.  We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide.  Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations.  Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as a real bargain or good value. These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing.  To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases.  The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews.  Our approach is implemented as a hierarchical Bayesian model with joint inference.  We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties.  Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We consider the problem of modeling the content structure of texts within a specific domain, in terms of the topics the texts address and the order in which these topics appear. We first present an effective knowledge-lean method for learning content models from un-annotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models. We then apply our method to two complementary tasks: information ordering and extractive summarization. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "We address the text-to-text generation problem of sentence-level paraphrasing -- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "An important component of any generation system is the mapping dictionary, a lexicon of elementary semantic expressions and corresponding natural language realizations. Typically, labor-intensive knowledge-based methods are used to construct the dictionary. We instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment, a technique commonly used in bioinformatics. Crucially, our method leverages latent information contained in multi-parallel corpora -- datasets that supply several verbalizations of the corresponding semantics rather than just one.\n  We used our techniques to generate natural language versions of computer-generated mathematical proofs, with good results on both a per-component and overall-output basis. For example, in evaluations involving a dozen human judges, our system produced output whose readability and faithfulness to the semantic input rivaled that of a traditional generation system.\n        \u25b3 Less", "author": "Regina Barzilay"}, {"abstract": "Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA greater than 1 Kbp with high sequence identity that are copied to other regions of the genome. SDs are among the most important sources of evolution, a common cause of genomic structural variation, and several are associated with diseases of genomic origin. Despite their functional importance, SDs present one of the major hurdles for de novo genome assembly due to the ambiguity they cause in building and traversing both state-of-the-art overlap-layout-consensus and de Bruijn graphs. This causes SD regions to be misassembled, collapsed into a unique representation, or completely missing from assembled reference genomes for various organisms. In turn, this missing or incorrect information limits our ability to fully understand the evolution and the architecture of the genomes. Despite the essential need to accurately characterize SDs in assemblies, there is only one tool that has been developed for this purpose, called Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several steps that employ different tools and custom scripts, which makes it difficult and time consuming to use. Thus there is still a need for algorithms to characterize within-assembly SDs quickly, accurately, and in a user friendly manner.\n  Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to rapidly detect SDs through sophisticated filtering strategies based on Jaccard similarity and local chaining. We show that SEDEF accurately detects SDs while maintaining substantial speed up over WGAC that translates into practical run times of minutes instead of weeks. Notably, our algorithm captures up to 25% pairwise error between segments, where previous studies focused on only 10%, allowing us to more deeply track the evolutionary history of the genome.\n  SEDEF is available at https://github.com/vpc-ccg/sedef\n        \u25b3 Less", "author": "Bonnie A. Berger"}, {"abstract": "Cryo-electron microscopy (cryoEM) is an increasingly popular method for protein structure determination. However, identifying a sufficient number of particles for analysis (often >100,000) can take months of manual effort. Current computational approaches are limited by high false positive rates and require significant ad-hoc post-processing, especially for unusually shaped particles. To address this shortcoming, we develop Topaz, an efficient and accurate particle picking pipeline using neural networks trained with few labeled particles by newly leveraging the remaining unlabeled particles through the framework of positive-unlabeled (PU) learning. Remarkably, despite using minimal labeled particles, Topaz allows us to improve reconstruction resolution by up to 0.15 \u00c5 over published particles on three public cryoEM datasets without any post-processing. Furthermore, we show that our novel generalized-expectation criteria approach to PU learning outperforms existing general PU learning approaches when applied to particle detection, especially for challenging datasets of non-globular proteins. We expect Topaz to be an essential component of cryoEM analysis.\n        \u25b3 Less", "author": "Bonnie A. Berger"}, {"abstract": "We sequenced genomes from a $\\sim$7,000 year old early farmer from Stuttgart in Germany, an $\\sim$8,000 year old hunter-gatherer from Luxembourg, and seven $\\sim$8,000 year old hunter-gatherers from southern Sweden. We analyzed these data together with other ancient genomes and 2,345 contemporary humans to show that the great majority of present-day Europeans derive from at least three highly differentiated populations: West European Hunter-Gatherers (WHG), who contributed ancestry to all Europeans but not to Near Easterners; Ancient North Eurasians (ANE), who were most closely related to Upper Paleolithic Siberians and contributed to both Europeans and Near Easterners; and Early European Farmers (EEF), who were mainly of Near Eastern origin but also harbored WHG-related ancestry. We model these populations' deep relationships and show that EEF had $\\sim$44% ancestry from a \"Basal Eurasian\" lineage that split prior to the diversification of all other non-African lineages.\n        \u25b3 Less", "author": "Bonnie A. Berger"}, {"abstract": "The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess.\n        \u25b3 Less", "author": "Bonnie A. Berger"}, {"abstract": "The recent explosion in available genetic data has led to significant advances in understanding the demographic histories of and relationships among human populations. It is still a challenge, however, to infer reliable parameter values for complicated models involving many populations. Here we present MixMapper, an efficient, interactive method for constructing phylogenetic trees including admixture events using single nucleotide polymorphism (SNP) genotype data. MixMapper implements a novel two-phase approach to admixture inference using moment statistics, first building an unadmixed scaffold tree and then adding admixed populations by solving systems of equations that express allele frequency divergences in terms of mixture parameters. Importantly, all features of the model, including topology, sources of gene flow, branch lengths, and mixture proportions, are optimized automatically from the data and include estimates of statistical uncertainty. MixMapper also uses a new method to express branch lengths in easily interpretable drift units. We apply MixMapper to recently published data for HGDP individuals genotyped on a SNP array designed especially for use in population genetics studies, obtaining confident results for 30 populations, 20 of them admixed. Notably, we confirm a signal of ancient admixture in European populations---including previously undetected admixture in Sardinians and Basques---involving a proportion of 20--40% ancient northern Eurasian ancestry.\n        \u25b3 Less", "author": "Bonnie A. Berger"}, {"abstract": "The method of negative-tone-PMMA electron-beam lithography is investigated to improve the performance of nanowire-based superconducting detectors. Using this approach, the superconducting nanowire single-photon detectors (SNSPDs) have been fabricated from thick 5-nm NbN film sputtered at the room temperature. To investigate the impact of this process, SNSPDs were prepared by positive-tone and negative-tone-PMMA lithography, and their electrical and photodetection characteristics at 4.2 K were compared. The SNSPDs made by negative-tone-PMMA lithography show higher critical-current density and higher photon count rate at various wavelengths. Our results suggest a higher negative-tone-PMMA technology may be preferable to the standard positive-tone-PMMA lithography for this application.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Conventional readout of a superconducting nanowire single-photon detector (SNSPD) sets an upper bound on the output voltage to be the product of the bias current and the load impedance, $I_\\mathrm{B}\\times Z_\\mathrm{load}$, where $Z_\\mathrm{load}$ is limited to 50 $\u03a9$ in standard r.f. electronics. Here, we break this limit by interfacing the 50 $\u03a9$ load and the SNSPD using an integrated superconducting transmission line taper. The taper is a transformer that effectively loads the SNSPD with high impedance without latching. It increases the amplitude of the detector output while preserving the fast rising edge. Using a taper with a starting width of 500 nm, we experimentally observed a 3.6$\\times$ higher pulse amplitude, 3.7$\\times$ faster slew rate, and 25.1 ps smaller timing jitter. The results match our numerical simulation, which incorporates both the hotspot dynamics in the SNSPD and the distributed nature in the transmission line taper. The taper studied here may become a useful tool to interface high-impedance superconducting nanowire devices to conventional low-impedance circuits.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "The basis for superconducting electronics can broadly be divided between two technologies: the Josephson junction and the superconducting nanowire. While the Josephson junction (JJ) remains the dominant technology due to its high speed and low power dissipation, recently proposed nanowire devices offer improvements such as gain, high fanout, and compatibility with CMOS circuits. Despite these benefits, nanowire-based electronics have largely been limited to binary operations, with devices switching between the superconducting state and a high-impedance resistive state dominated by uncontrolled hotspot dynamics. Unlike the JJ, they cannot increment an output through successive switching, and their operation speeds are limited by their slow thermal reset times. Thus, there is a need for an intermediate device with the interfacing capabilities of a nanowire but a faster, moderated response allowing for modulation of the output. Here, we present a nanowire device based on controlled fluxon transport. We show that the device is capable of responding proportionally to the strength of its input, unlike other nanowire technologies. The device can be operated to produce a multilevel output with distinguishable states, which can be tuned by circuit parameters. Agreement between experimental results and electrothermal circuit simulations demonstrates that the device is classical and may be readily engineered for applications including use as a multilevel memory.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Semi-transparent mirrors are standard elements in light optics for splitting light beams or creating two versions of the same image. Such mirrors do not exist in electron optics, although they could be beneficial in existing techniques such as electron interferometry and holography and enable novel electron imaging and spectroscopy techniques. We propose a design for an electron beam splitter using the concept of quantum interaction-free measurement (IFM). The design combines an electron resonator with a weak phase grating. Fast switching gates allow electrons to enter and exit the resonator. While in the resonator, the phase grating transfers intensity from the direct beam into one of the weakly diffracted beams at each pass. To make the beam splitter an efficient two-port splitter, the intensity in all other diffracted beams is blocked by an aperture. The IFM principle minimizes the loss of total intensity by this aperture. We use a scattering matrix method to analyze the performance of the beam splitter, including the effects of inelastic scattering in the phase grating. This design can be generalized to beam splitters for not only electrons, but also photons, neutrons, atoms, and other quantum mechanical systems.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "To analyze the switching dynamics and output performance of a superconducting nanowire single photon detector (SNSPD), the nanowire is usually modelled as an inductor in series with a time-varying resistor induced by absorption of a photon. Our recent experimental results show that, due to the effect of kinetic inductance, for a SNSPD made of a nanowire of sufficient length, its geometry length can be comparable to or even longer than the effective wavelength of frequencies contained in the output pulse. In other words, a superconducting nanowire can behave as a distributed transmission line so that the readout pulse depends on the photon detection location and the transmission line properties of the nanowire. Here, we develop a distributed model for a superconducting nanowire and apply it to simulate the output performance of a long nanowire designed into a coplanar waveguide. We compare this coplanar waveguide geometry to a conventional meander nanowire geometry. The simulation results agree well with our experimental observations. With this distributed model, we discussed the importance of microwave design of a nanowire and how impedance matching can affect the output pulse shape. We also discuss how the distributed model affects the growth and decay of the photon-triggered resistive hotspot.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We analyze the origin of the intrinsic timing jitter in superconducting nanowire single photon detectors (SNSPDs) in terms of fluctuations in the latency of the detector response, which is determined by the microscopic physics of the photon detection process. We demonstrate that fluctuations in the physical parameters which determine the latency give rise to the intrinsic timing jitter. We develop a general description of latency by introducing the explicit time dependence of the internal detection efficiency. By considering the dynamic Fano fluctuations together with static spatial inhomogeneities, we study the details of the connection between latency and timing jitter. We develop both a simple phenomenological model and a more general microscopic model of detector latency and timing jitter based on the solution of the generalized time-dependent Ginzburg-Landau equations for the 1D hotbelt geometry. While the analytical model is sufficient for qualitative interpretation of recent data, the general approach establishes the framework for a quantitative analysis of detector latency and the fundamental limits of intrinsic timing jitter. These theoretical advances can be used to interpret the results of recent experiments measuring the dependence of detection latency and timing jitter on photon energy to the few-picosecond level.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Report of the first workshop to identify approaches and techniques in the domain of quantum sensing that can be utilized by future High Energy Physics applications to further the scientific goals of High Energy Physics.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "The International Linear Collider is now proposed with a staged machine design, with the first stage at $\\sqrt{s}=$~250 GeV and an integrated luminosity goal of 2~ab$^{-1}$. One of the questions for the machine design is the importance of positron polarization. In this report, we review the impact of positron polarization on the physics goals of the $250$ GeV stage of the ILC and demonstrate that positron polarization has distinct advantages.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Coincidence detection of single photons is crucial in numerous quantum technologies and usually requires multiple time-resolved single-photon detectors. However, the electronic readout becomes a major challenge when the measurement basis scales to large numbers of spatial modes. Here, we address this problem by introducing a two-terminal coincidence detector that enables scalable readout of an array of detector segments based on superconducting nanowire microstrip transmission line. Exploiting timing logic, we demonstrate a 16-element detector that resolves all 136 possible single-photon and two-photon coincidence events. We further explore the pulse shapes of the detector output and resolve up to four-photon coincidence events in a 4-element device, giving the detector photon-number-resolving capability. This new detector architecture and operating scheme will be particularly useful for multi-photon coincidence detection in large-scale photonic integrated circuits.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "A superconducting loop stores persistent current without any ohmic loss, making it an ideal platform for energy efficient memories. Conventional superconducting memories use an architecture based on Josephson junctions (JJs) and have demonstrated access times less than 10 ps and power dissipation as low as $10^{-19}$ J. However, their scalability has been slow to develop due to the challenges in reducing the dimensions of JJs and minimizing the area of the superconducting loops. In addition to the memory itself, complex readout circuits require additional JJs and inductors for coupling signals, increasing the overall area. Here, we have demonstrated a superconducting memory based solely on lithographic nanowires. The small dimensions of the nanowire ensure that the device can be fabricated in a dense area in multiple layers, while the high kinetic inductance makes the loop essentially independent of geometric inductance, allowing it to be scaled down without sacrificing performance. The memory is operated by a group of nanowire cryotrons patterned alongside the storage loop, enabling us to reduce the entire memory cell to 3 \u03bcm $\\times $ 7 \u03bcm in our proof-of-concept device. In this work we present the operation principles of a superconducting nanowire memory (nMem) and characterize its bit error rate, speed, and power dissipation.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We present the performance of a superconducting nanowire that can be operated in two detection modes: i) as a kinetic inductance detector (KID) or ii) as a single-photon detector (SPD). Two superconducting nanowires developed for use as single-photon detectors (SNSPDs) are embedded as the inductive (L) component in resonant inductor/capacitor (LC) circuits coupled to a microwave transmission line. The capacitors are low loss commercial chip capacitors and limit the internal quality factor of the resonators to approximately $Q_i = 170$. The resonator quality factor, $Q_r \\simeq 23$, is dominated by the coupling to the feedline and limits the detection bandwidth to on the order of 1MHz. When operated in KID mode, the detectors are AC biased with tones at their resonant frequencies of 45.85 and 91.81MHz. In the low-bias, standard KID mode, a single photon produces a hot spot that does not turn an entire section of the line normal but only increases the kinetic inductance. In the high-bias, critical KID mode, a photon event turns a section of the line normal and the resonance is destroyed until the normal region is dissipated. When operated as an SPD in Geiger mode, the resonators are DC biased through cryogenic bias tees and each photon produces a sharp voltage step followed by a ringdown signal at the resonant frequency of the detector which is converted to a standard pulse with an envelop detector. We show that AC biasing in the critical KID mode is inferior to the sensitivity achieved in DC-biased SPD mode due to the small fraction of time spent near the critical current with an AC bias.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "The International Linear Collider is now proposed with a staged machine design, with the first stage at 250 GeV with a luminosity goal of 2 ab-1. In this paper, we review the physics expectations for this machine. These include precision measurements of Higgs boson couplings, searches for exotic Higgs decays, other searches for particles that decay with zero or small visible energy, and measurements of e+e- annihilation to W+W- and 2-fermion states with improved sensitivity. A summary table gives projections for the achievable levels of precision based on the latest full simulation studies.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Recent advances in the fabrication of nanostructures and nanoscale features in metasurfaces offer a new prospect for generating visible, light emission from low energy electrons. In this paper, we present the experimental observation of visible light emission from low-energy free electrons interacting with nanoscale periodic surfaces through the Smith-Purcell (SP) effect. SP radiation is emitted when electrons pass in close proximity over a periodic structure, inducing collective charge motion or dipole excitations near the surface, thereby giving rise to electromagnetic radiation. We demonstrate a controlled emission of SP light from nanoscale gold gratings with periodicity as small as 50 nm, enabling the observation of visible SP radiation by low energy electrons (1.5 to 6 keV), an order of magnitude lower than previously reported. We study the emission wavelength and intensity dependence on the grating pitch and electron energy, showing agreement between experiment and theory. Further reduction of structure periodicity should enable the production of SP-based devices that operate with even slower electrons that allow an even smaller footprint and facilitate the investigation of quantum effects for light generation in nanoscale devices. A tunable light source integrated in an electron microscope would enable the development of novel electron-optical correlated spectroscopic techniques, with additional applications ranging from biological imaging to solid-state lighting.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Many superconducting technologies such as rapid single flux quantum computing (RSFQ) and superconducting quantum interference devices (SQUIDs) rely on the modulation of nonlinear dynamics in Josephson junctions for functionality. More recently, however, superconducting devices have been developed based on the switching and thermal heating of nanowires for use in fields such as single photon detection and digital logic. In this paper, we use resistive shunting to control the nonlinear heating of a superconducting nanowire and compare the resulting dynamics to those observed in Josephson junctions. We show that interaction of the hotspot growth with the external shunt produces high frequency relaxation oscillations with similar behavior as observed in Josephson junctions due to their rapid time constants and ability to be modulated by a weak periodic signal. In particular, we use a microwave drive to pull and mix the oscillation frequency, resulting in phase locked features that resemble the AC Josephson effect. New nanowire devices based on these conclusions have promising applications in fields such as parametric amplification and frequency multiplexing.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "The lack of energy dissipation and abrupt electrical phase transition of superconductors favorite them for nanoscale technologies, including radiation detectors, and quantum technologies. Moreover, understanding the nanoscale behavior of superconductivity is significant for revealing the onset of collective-electron behavior in nature. Nevertheless, the limited number of accessible superconductors restricts availability of the superconducting properties, encumbering the realization of their potential. Superconducting nanowire single photon detectors (SNSPDs) sense single-IR photons faster and more efficient with respect to competing technologies. However, these advantageous properties are material-dependent causing an undesirable speed-efficiency payoff. Usually, SNSPDs based on granular materials are faster, while those based on amorphous materials are more efficient. Here we optimized ultrathin films of granular NbN on SiO2 and of amorphous W5Si3. We showed that hybrid superconducting nanowire single photon detectors (SNSPDs) made of 2-nm-thick W5Si3 films over 2-nm-thick NbN films exhibit advantageous coexistence of timing (< 5-ns reset time and 52-ps timing jitter) and efficiency (> 96% quantum efficiency) performance. We propose that the governing mechanism of this hybridization is the presence of a dual superconducting behavior: native superconductivity of each of the films and superconductivity that is induced from the neighboring film via the proximity effect. In addition to improvement in SNSPDs performance, our results suggest that such hybridization can expand the range of available superconducting properties, impacting nano-superconducting technologies. Lastly, this hybridization may be used to tune the amorphous character of superconducting films and to illuminate the elusive onset of collective-electron behavior near the superconducting-to-insulating transition.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Integration with conventional electronics offers a straightforward and economical approach to upgrading existing superconducting technologies, such as scaling up superconducting detectors into large arrays and combining single flux quantum (SFQ) digital circuits with semiconductor logic and memories. However, direct output signals from superconducting devices (e.g., Josephson junctions) are usually not compatible with the input requirements of conventional devices (e.g., transistors). Here, we demonstrate the use of a single three-terminal superconducting-nanowire device, called the nanocryotron (nTron), as a digital comparator to combine SFQ circuits with mature semiconductor circuits such as complementary metal oxide semiconductor (CMOS) circuits. Since SFQ circuits can digitize output signals from general superconducting devices and CMOS circuits can interface existing CMOS-compatible electronics, our results demonstrate the feasibility of a general architecture that uses an nTron as an interface to realize a super-hybrid system consisting of superconducting detectors, superconducting quantum electronics, CMOS logic and memories, and other conventional electronics.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We report a self-aligned, monolithic electron interferometer, consisting of two 45 nm thick silicon layers separated by 20 $\u03bc$m. This interferometer was fabricated from a single crystal silicon cantilever on a transmission electron microscope grid by gallium focused ion-beam milling. Using this interferometer, we demonstrate beam path-separation, and obtain interference fringes in a Mach-Zehnder geometry, in an unmodified 200 kV transmission electron microscope. The fringes have a period of 0.32 nm, which corresponds to the $\\left[\\bar{1}\\bar{1}1\\right]$ lattice planes of silicon, and a maximum contrast of 15 %. This design can potentially be scaled to millimeter-scale, and used in electron holography. It can also be applied to perform fundamental physics experiments, such as interaction-free measurement with electrons.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Detection jitter quantifies variance introduced by the detector in the determination of photon arrival time. It is a crucial performance parameter for systems using superconducting nanowire single photon detectors (SNSPDs). In this work, we have demonstrated that the detection timing jitter is limited in part by the spatial variation of photon detection events along the length of the wire. This distribution causes the generated electrical pulses to arrive at the readout at varied times. We define this jitter source as geometric jitter since it is related to the length and area of the SNSPD. To characterize the geometric jitter, we have constructed a novel differential cryogenic readout with less than 7 ps of electronic jitter that can amplify the pulses generated from the two ends of an SNSPD. By differencing the measured arrival times of the two electrical pulses, we were able to partially cancel out the difference of the propagation times and thus reduce the uncertainty of the photon arrival time. Our experimental data indicates that the variation of the differential propagation time was a few ps for a 3 \u03bcm x 3 \u03bcm device while it increased up to 50 ps for a 20 \u03bcm x 20 \u03bcm device. In a 20 \u03bcm x 20 \u03bcm large SNSPD, we achieved a 20% reduction in the overall detection timing jitter for detecting telecom-wavelength photons by using the differential cryogenic readout. The geometric jitter hypothesis was further confirmed by studying jitter in devices that consisted of long wires with 1-\u03bcm-long narrowed regions used for sensing photons.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We describe a superconducting three-terminal device that uses a simple geometric effect known as current crowding to sense the flow of current and actuate a readout signal. The device consists of a \"Y\"-shaped current combiner, with two currents (sense and bias) entering through the top arms of the \"Y\", intersecting, and then exiting through the bottom leg of the \"Y\"'. This geometry--mixing two inputs at a sharp intersection point--takes its inspiration from Y-shaped combiners in fluid flow systems, where variations in the input pressures can produce at turbulence and mixing at the intersection. When current is added to or removed from one of the arms (the sense arm), the superconducting critical current in the other arm (the bias arm) is modulated. The current in the sense arm can thus be determined by measuring the critical current of the bias arm. The dependence of the bias critical current on the sense current is possible because current crowding causes the sense current to interact locally with the bias arm. Measurement of the critical current in the bias arm does not break the superconducting state of the sense arm or of the bottom leg, and thus the signal to be sensed is fully restored after the measurement process. This device thus has potential for broad applicability across superconducting technologies and materials.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Detecting spatial and temporal information of individual photons by using single-photon-detector (SPD) arrays is critical to applications in spectroscopy, communication, biological imaging, astronomical observation, and quantum-information processing. Among the current SPDs1,detectors based on superconducting nanowires have outstanding performance2, but are limited in their ability to be integrated into large scale arrays due to the engineering difficulty of high-bandwidth cryogenic electronic readout3-8. Here, we address this problem by demonstrating a scalable single-photon imager using a single continuous photon-sensitive superconducting nanowire microwave-plasmon transmission line. By appropriately designing the nanowire's local electromagnetic environment so that the nanowire guides microwave plasmons, the propagating voltages signals generated by a photon-detection event were slowed down to ~ 2% of the speed of light. As a result, the time difference between arrivals of the signals at the two ends of the nanowire naturally encoded the position and time of absorption of the photon. Thus, with only two readout lines, we demonstrated that a 19.7-mm-long nanowire meandered across an area of 286 \u03bcm * 193 \u03bcm was capable of resolving ~590 effective pixels while simultaneously recording the arrival times of photons with a temporal resolution of 50 ps. The nanowire imager presents a scalable approach to realizing high-resolution photon imaging in time and space.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We study the microwave impedance of extremely high aspect ratio (length/width ~ 5,000) superconducting niobium nitride nanowires. The nanowires are fabricated in a compact meander geometry that is in series with the center conductor of a 50 ohm coplanar waveguide transmission line. The transmission coefficient of the sample is measured up to 20 GHz. At high frequency, a peak in the transmission coefficient is seen. Numerical simulations show that this is a half-wave resonance along the length of the nanowire, where the nanowire acts as a high impedance, slow wave transmission line. This resonance sets the upper frequency limit for these nanowires as inductive elements. Fitting simulations to the measured resonance enables a precise determination of the nanowire's complex sheet impedance at the resonance frequency. The real part is a measure of dissipation, while the imaginary part is dominated by kinetic inductance. We characterize the dependence of the sheet resistance and sheet inductance on both temperature and current and compare the results to recent theoretical predictions for disordered superconductors. These results can aid in the understanding of high frequency devices based on superconducting nanowires. They may also lead to the development of novel superconducting devices such as ultra-compact resonators and slow-wave structures.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "This paper describes the construction of a cryostat and an optical system with a free-space coupling efficiency of 56.5% +/- 3.4% to a superconducting nanowire single-photon detector (SNSPD) for infrared quantum communication and spectrum analysis. A 1K pot decreases the base temperature to T = 1.7 K from the 2.9 K reached by the cold head cooled by a pulse-tube cryocooler. The minimum spot size coupled to the detector chip was 6.6 +/- 0.11 \u03bcm starting from a fiber source at wavelength, \u03bb = 1.55 \u03bcm. We demonstrated efficient photon counting on a detector with an 8 x 7.3 \u03bcm^2 area. We measured a dark count rate of 95 +/- 3.35 kcps and a system detection efficiency of 1.64% +/- 0.13%. We explain the key steps that are required to further improve the coupling efficiency.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "One of the astounding consequences of quantum mechanics is that it allows the detection of a target using an incident probe, with only a low probability of interaction of the probe and the target. This 'quantum weirdness' could be applied in the field of electron microscopy to generate images of beam-sensitive specimens with substantially reduced damage to the specimen. A reduction of beam-induced damage to specimens is especially of great importance if it can enable imaging of biological specimens with atomic resolution. Following a recent suggestion that interaction-free measurements are possible with electrons, we now analyze the difficulties of actually building an atomic resolution interaction-free electron microscope, or \"quantum electron microscope\". A quantum electron microscope would require a number of unique components not found in conventional transmission electron microscopes. These components include a coherent electron-beam splitter or two-state-coupler, and a resonator structure to allow each electron to interrogate the specimen multiple times, thus supporting high success probabilities for interaction-free detection of the specimen. Different system designs are presented here, which are based on four different choices of two-state-couplers: a thin crystal, a grating mirror, a standing light wave and an electro-dynamical pseudopotential. Challenges for the detailed electron optical design are identified as future directions for development. While it is concluded that it should be possible to build an atomic resolution quantum electron microscope, we have also identified a number of hurdles to the development of such a microscope and further theoretical investigations that will be required to enable a complete interpretation of the images produced by such a microscope.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Methods for patterning biomolecules on a substrate at the single molecule level have been studied as a route to sensors with single-molecular sensitivity or as a way to probe biological phenomena at the single-molecule level. However, the arrangement and orientation of single biomolecules on substrates has been less investigated. Here, we examined the arrangement and orientation of two rod-like coiled-coil proteins, cortexillin and tropomyosin, around patterned gold nanostructures. The high aspect ratio of the coiled coils made it possible to study their orientations and to pursue a strategy of protein orientation via two-point attachment. The proteins were anchored to the surfaces using thiol groups, and the number of cysteine residues in tropomyosin was varied to test how this variation affected the structure and arrangement of the surface-attached proteins. Molecular dynamics studies were used to interpret the observed positional distributions. Based on initial studies of protein attachment to gold post structures, two 31-nm-long tropomyosin molecules were aligned between the two sidewalls of a trench with a width of 68 nm. Because the approach presented in this study uses one of twenty natural amino acids, this method provides a convenient way to pattern biomolecules on substrates using standard chemistry.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We present an optical setup that can be used to characterize the thicknesses of thin NbN films to screen samples for fabrication and to better model the performance of the resulting superconducting nanowire single photon detectors. The infrared transmissometer reported here is easy to use, gives results within minutes and is non-destructive. Thus, the thickness measurement can be easily integrated into the workflow of deposition and characterization. Comparison to a similar visible-wavelength transmissometer is provided.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Superconducting nanowire avalanche single-photon detectors (SNAPs) with n parallel nanowires are advantageous over single-nanowire detectors because their output signal amplitude scales linearly with n. However, the SNAP architecture has not been viably demonstrated for n > 4. To increase n for larger signal amplification, we designed a multi-stage, successive-avalanche architecture which used nanowires, connected via choke inductors in a binary-tree layout. We demonstrated an avalanche detector with n = 8 parallel nanowires and achieved eight-fold signal amplification, with a timing jitter of 54 ps.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Thin superconducting films form a unique platform for geometrically-confined, strongly-interacting electrons. They allow an inherent competition between disorder and superconductivity, which in turn enables the intriguing superconducting-to-insulator transition and believed to facilitate the comprehension of high-Tc superconductivity. Furthermore, understanding thin film superconductivity is technologically essential e.g. for photo-detectors, and quantum-computers. Consequently, the absence of an established universal relationships between critical temperature ($T_c$), film thickness ($d$) and sheet resistance ($R_s$) hinders both our understanding of the onset of the superconductivity and the development of miniaturised superconducting devices. We report that in thin films, superconductivity scales as $d^.$$T_c(R_s)$. We demonstrated this scaling by analysing the data published over the past 46 years for different materials (and facilitated this database for further analysis). Moreover, we experimentally confirmed the discovered scaling for NbN films, quantified it with a power law, explored its possible origin and demonstrated its usefulness for superconducting film-based devices.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Photonic integrated circuits (PICs) have emerged as a scalable platform for complex quantum technologies using photonic and atomic systems. A central goal has been to integrate photon-resolving detectors to reduce optical losses, latency, and wiring complexity associated with off-chip detectors. Superconducting nanowire single-photon detectors (SNSPDs) are particularly attractive because of high detection efficiency, sub-50-ps timing jitter, nanosecond-scale reset time, and sensitivity from the visible to the mid-infrared spectrum. However, while single SNSPDs have been incorporated into individual waveguides, the system efficiency of multiple SNSPDs in one photonic circuit has been limited below 0.2% due to low device yield. Here we introduce a micrometer-scale flip-chip process that enables scalable integration of SNSPDs on a range of PICs. Ten low-jitter detectors were integrated on one PIC with 100% device yield. With an average system efficiency beyond 10% for multiple SNSPDs on one PIC, we demonstrate high-fidelity on-chip photon correlation measurements of non-classical light.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "In existing superconducting electronic systems, Josephson junctions play a central role in processing and transmitting small-amplitude electrical signals. However, Josephson-junction-based devices have a number of limitations including: (1) sensitivity to magnetic fields, (2) limited gain, (3) inability to drive large impedances, and (4) difficulty in controlling the junction critical current (which depends sensitively on sub-Angstrom-scale thickness variation of the tunneling barrier). Here we present a nanowire-based superconducting electronic device, which we call the nanocryotron (nTron), that does not rely on Josephson junctions and can be patterned from a single thin film of superconducting material with conventional electron-beam lithography. The nTron is a 3-terminal, T-shaped planar device with a gain of ~20 that is capable of driving impedances of more than 100 k\u03a9, and operates in typical ambient magnetic fields at temperatures of 4.2K. The device uses a localized, Joule-heated hotspot formed in the gate to modulate current flow in a perpendicular superconducting channel. We have characterized the nTron, matched it to a theoretical framework, and applied it both as a digital logic element in a half-adder circuit, and as a digital amplifier for superconducting nanowire single-photon detectors pulses. The nTron has immediate applications in classical and quantum communications, photon sensing and astronomy, and its performance characteristics make it compatible with existing superconducting technologies. Furthermore, because the hotspot effect occurs in all known superconductors, we expect the design to be extensible to other materials, providing a path to digital logic, switching, and amplification in high-temperature superconductors.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "The optimal orientations are determined for polarized substrate side illumination of three superconducting nanowire single-photon detector (SNSPD) designs: (1) periodic niobium-nitride (NbN) stripes standing in air with dimensions according to conventional SNSPDs, (2) same NbN patterns below ~quarter-wavelength hydrogensilsesquioxane-filled nano-cavity, (3) analogous NbN patterns in HSQ nano-cavity closed by a thin gold reflector. Numerical computation results have shown that the optical response and near-field distribution vary significantly with polar-angle, fi, and these variations are analogous across all azimuthal-angles, gamma, but are fundamentally different in various device designs. Larger absorptance is available due to p-polarized illumination of NbN patterns in P-structure configuration, while s-polarized illumination results in higher absorptance in S-structure arrangement. As a result of p-polarized illumination a global maximum appears on absorptance of bare NbN pattern at polar angle corresponding to NbN-related ATIR; integration with HSQ nano-cavity results in a global absorptance maximum at polar angle corresponding to TIR at sapphire-air interface; while the highest absorptance is observable at perpendicular incidence on P-structures aligned below gold reflector covered HSQ nano-cavity. S-polarized light illumination results in a global absorptance maximum at TIR on bare NbN patterns; the highest absorptance is available below HSQ nano-cavity at polar angle corresponding to ATIR phenomenon; while the benefit of gold reflector is large and polar angle independent absorptance.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "In this paper we calculate the critical currents in thin superconducting strips with sharp right-angle turns, 180-degree turnarounds, and more complicated geometries, where all the line widths are much smaller than the Pearl length $\u039b= 2 \u03bb^2/d$. We define the critical current as the current that reduces the Gibbs free-energy barrier to zero. We show that current crowding, which occurs whenever the current rounds a sharp turn, tends to reduce the critical current, but we also show that when the radius of curvature is less than the coherence length this effect is partially compensated by a radius-of-curvature effect. We propose several patterns with rounded corners to avoid critical-current reduction due to current crowding. These results are relevant to superconducting nanowire single-photon detectors, where they suggest a means of improving the bias conditions and reducing dark counts. These results also have relevance to normal-metal nanocircuits, as these patterns can reduce the electrical resistance, electromigration, and hot spots caused by nonuniform heating.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "A novel finite-element method for calculating the illumination-dependence of absorption in three-dimensional nanostructures is presented based on the RF module of the COMSOL software package. This method is capable of numerically determining the optical response and near-field distribution of sub-wavelength periodic structures as a function of illumination orientations specified by polar angle, fi, and azimuthal angle, gamma. The method was applied to determine the illumination-angle-dependent absorptance in cavity-based superconducting-nanowire single-photon detector (SNSPD) designs. Niobium-nitride stripes based on dimensions of conventional SNSPDs and integrated with ~ quarter-wavelength hydrogensilsesquioxane-filled nano-optical cavities and covered by a thin gold film acting as a reflector were illuminated from below by p-polarized light in this study. The numerical results were compared to results from complementary transfer-matrix-method calculations on composite layers made of analogous film-stacks. This comparison helped to uncover the optical phenomena contributing to the appearance of extrema in the optical response. This paper presents an approach to optimizing the absorptance of different sensing and detecting devices via simultaneous numerical optimization of the polar and azimuthal illumination angles.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We developed an electro thermal model of NbN superconducting nanowire avalanche photodetectors (SNAPs) on sapphire substrates. SNAPs are single photon detectors consisting of the parallel connection of N superconducting nanowires. We extrapolated the physical constants of the model from experimental data and we simulated the time evolution of the device resistance, temperature and current by solving two coupled electrical and thermal differential equations describing the nanowires. The predictions of the model were in good quantitative agreement with the experimental results.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We fabricate superconducting ion traps with niobium and niobium nitride and trap single 88Sr ions at cryogenic temperatures. The superconducting transition is verified and characterized by measuring the resistance and critical current using a 4-wire measurement on the trap structure, and observing change in the rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz at 6 K and shows no significant change across the superconducting transition, suggesting that anomalous heating is primarily caused by noise sources on the surface. This demonstration of superconducting ion traps opens up possibilities for integrating trapped ions and molecular ions with superconducting devices.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Transitions in an artificial atom, driven non-adiabatically through an energy-level avoided crossing, can be controlled by carefully engineering the driving protocol. We have driven a superconducting persistent-current qubit with a large-amplitude, radio-frequency field. By applying a bi-harmonic waveform generated by a digital source, we demonstrate a mapping between the amplitude and phase of the harmonics produced at the source and those received by the device. This allows us to image the actual waveform at the device. This information is used to engineer a desired time dependence, as confirmed by detailed comparison with simulation.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We investigate the role of electrothermal feedback in the operation of superconducting nanowire single-photon detectors (SNSPDs). It is found that the desired mode of operation for SNSPDs is only achieved if this feedback is unstable, which happens naturally through the slow electrical response associated with their relatively large kinetic inductance. If this response is sped up in an effort to increase the device count rate, the electrothermal feedback becomes stable and results in an effect known as latching, where the device is locked in a resistive state and can no longer detect photons. We present a set of experiments which elucidate this effect, and a simple model which quantitatively explains the results.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We measured the optical absorptance of superconducting nanowire single photon detectors. We found that 200-nm-pitch, 50%-fill-factor devices had an average absorptance of 21% for normally-incident front-illumination of 1.55-um-wavelength light polarized parallel to the nanowires, and only 10% for perpendicularly-polarized light. We also measured devices with lower fill-factors and narrower wires that were five times more sensitive to parallel-polarized photons than perpendicular-polarized photons. We developed a numerical model that predicts the absorptance of our structures. We also used our measurements, coupled with measurements of device detection efficiencies, to determine the probability of photon detection after an absorption event. We found that, remarkably, absorbed parallel-polarized photons were more likely to result in detection events than perpendicular-polarized photons, and we present a hypothesis that qualitatively explains this result. Finally, we also determined the enhancement of device detection efficiency and absorptance due to the inclusion of an integrated optical cavity over a range of wavelengths (700-1700 nm) on a number of devices, and found good agreement with our numerical model.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "A photon-number-resolving detector based on a four-element superconducting nanowire single photon detector is demonstrated to have sub-30-ps resolution in measuring the arrival time of individual photons. This detector can be used to characterize the photon statistics of non-pulsed light sources and to mitigate dead-time effects in high-speed photon counting applications. Furthermore, a 25% system detection efficiency at 1550 nm was demonstrated, making the detector useful for both low-flux source characterization and high-speed photon-counting and quantum communication applications. The design, fabrication and testing of this detector are described, and a comparison between the measured and theoretical performance is presented.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "The energy-level structure of a quantum system plays a fundamental role in determining its behavior and manifests itself in a discrete absorption and emission spectrum. Conventionally, spectra are probed via frequency spectroscopy whereby the frequency \u03bdof a harmonic driving field is varied to fulfill the conditions \u0394E = h \u03bd, where the driving field is resonant with the level separation \u0394E (h is Planck's constant). Although this technique has been successfully employed in a variety of physical systems, including natural and artificial atoms and molecules, its application is not universally straightforward, and becomes extremely challenging for frequencies in the range of 10's and 100's of gigahertz. Here we demonstrate an alternative approach, whereby a harmonic driving field sweeps the atom through its energy-level avoided crossings at a fixed frequency, surmounting many of the limitations of the conventional approach. Spectroscopic information is obtained from the amplitude dependence of the system response. The resulting ``spectroscopy diamonds'' contain interference patterns and population inversion that serve as a fingerprint of the atom's spectrum. By analyzing these features, we determine the energy spectrum of a manifold of states with energies from 0.01 to 120 GHz \\times h in a superconducting artificial atom, using a driving frequency near 0.1 GHz. This approach provides a means to manipulate and characterize systems over a broad bandwidth, using only a single driving frequency that may be orders of magnitude smaller than the energy scales being probed.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Novel optical phenomena, including electromagnetically induced transparency, slow light, superluminal light propagation, have recently been demonstrated in diverse physical implementations. These phenomena are challenging to realize in practical systems because they require quantum coherence as well as careful preparation and control of prescribed quantum states. Here we present a unified approach to engineering optical materials that exhibit these phenomena by using mixtures of active and passive optical materials at frequencies near their resonances. Our approach does not depend on quantum coherence and can realize large and small (much less than 1) indices of refraction and negative permittivity ($\u03b5<0$), normal and anomalous dispersion, all while maintaining transparency.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We investigate the source of large variations in the observed detection effiiencies of superconducting nanowire single-photon detectors between many nominally identical devices. Through both electrical and optical measurements, we infer that these variations arise from \"constrictions:\" highly localized regions of the nanowires where the effective cross-sectional area for superconducting current is reduced. These constrictions limit the DC bias current density to well below its critical value over the remainder of the wire, and thus prevent the detection efficiency from reaching the high values that occur in these devices only when they are biased near the critical current density.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "A nonlinear resonant circuit comprising a SQUID magnetometer and a parallel capacitor is studied as a readout scheme for a persistent-current (PC) qubit. The flux state of the qubit is detected as a change in the Josephson inductance of the SQUID magnetometer, which in turn mediates a shift in the resonance frequency of the readout circuit. The nonlinearity and resulting hysteresis in the resonant behavior are characterized as a function of the power of both the input drive and the associated resonance peak response. Numerical simulations based on a phenomenological circuit model are presented which display the features of the observed nonlinearity.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We demonstrate Mach-Zehnder-type interferometry in a superconducting flux qubit. The qubit is a tunable artificial atom, whose ground and excited states exhibit an avoided crossing. Strongly driving the qubit with harmonic excitation sweeps it through the avoided crossing two times per period. As the induced Landau-Zener transitions act as coherent beamsplitters, the accumulated phase between transitions, which varies with microwave amplitude, results in quantum interference fringes for n=1...20 photon transitions. The generalization of optical Mach-Zehnder interferometry, performed in qubit phase space, provides an alternative means to manipulate and characterize the qubit in the strongly-driven regime.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We investigate the recovery of superconducting NbN-nanowire photon counters after detection of an optical pulse at a wavelength of 1550 nm, and present a model that quantitatively accounts for our observations. The reset time is found to be limited by the large kinetic inductance of these nanowires, which forces a tradeoff between counting rate and either detection efficiency or active area. Devices of usable size and high detection efficiency are found to have reset times orders of magnitude longer than their intrinsic photoresponse time.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "Quantum optical techniques may yield immersion fluids with high indices of refraction without absorption. We describe one such technique in which a probe field experiences a large index of refraction with amplification rather than absorption, and examine its practicality for an immersion lithography application. Enhanced index can be observed in a three-level system with a tunable, near-resonant, coherent probe and incoherent pump field that inverts population of the probe transition. This observation contradicts the common belief that large indices of refraction are impossible without absorption, however it is well in accord with existing electromagnetic theory and practice. Calculations show that a refractive index >> 2 is possible with practical experimental parameters. A scheme with an incoherent mixture of pumped and unpumped atoms is also examined, and is seen to have a lower refractive index (~2) accompanied by neither gain nor loss.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We have implemented a resonant circuit that uses a SQUID as a flux-sensitive Josephson inductor for qubit readout. In contrast to the conventional switching current measurement that generates undesired quasi-particles when the SQUID switches to the voltage state, our approach keeps the readout SQUID biased along the supercurrent branch during the measurement. By incorporating the SQUID inductor in a high-Q resonant circuit, we can distinguish the two flux states of a niobium persistent-current (PC) qubit by observing a shift in the resonant frequency of both the magnitude and the phase spectra. The readout circuit was also characterized in the nonlinear regime to investigate its potential use as a nonlinear amplifier.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "We measured the intrawell energy relaxation time \u03c4_{d} between macroscopic quantum levels in the double well potential of a Nb persistent-current qubit. Interwell population transitions were generated by irradiating the qubit with microwaves. Zero population in the initial well was then observed due to a multi-level decay process in which the initial population relaxed to the lower energy levels during transitions. The qubit's decoherence time, determined from \u03c4_{d}, is longer than 20 microseconds, holding the promise of building a quantum computer with Nb-based superconducting qubits.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "A numerical method for solving Schrodinger's equation based upon a Baker-Campbell-Hausdorff (BCH) expansion of the time evolution operator is presented herein. The technique manifestly preserves wavefunction norm, and it can be applied to problems in any number of spatial dimensions. We also identify a particular dimensionless ratio of potential to kinetic energies as a key coupling constant. This coupling establishes characteristic length and time scales for a large class of low energy quantum states, and it guides the choice of step sizes in numerical work. Using the BCH method in conjunction with an imaginary time rotation, we compute low energy eigenstates for several quantum systems coupled to non-trivial background potentials. The approach is subsequently applied to the study of 1D propagating wave packets and 2D bound state time development. Failures of classical expectations uncovered by simulations of these simple systems help develop quantum intuition.\n  Finally, we investigate the response of a Superconducting Quantum Interference Device (SQUID) to a time dependent potential. We discuss how to engineer the potential's energy and time scales so that the SQUID acts as a quantum NOT gate. The notional simulation we present for this gate provides useful insight into the design of one candidate building block for a quantum computer.\n        \u25b3 Less", "author": "Karl Berggren"}, {"abstract": "The Semantic Web drives towards the use of the Web for interacting with logically interconnected data. Through knowledge models such as Resource Description Framework (RDF), the Semantic Web provides a unifying representation of richly structured data. Adding logic to the Web implies the use of rules to make inferences, choose courses of action, and answer questions. This logic must be powerful enough to describe complex properties of objects but not so powerful that agents can be tricked by being asked to consider a paradox. The Web has several characteristics that can lead to problems when existing logics are used, in particular, the inconsistencies that inevitably arise due to the openness of the Web, where anyone can assert anything. N3Logic is a logic that allows rules to be expressed in a Web environment. It extends RDF with syntax for nested graphs and quantified variables and with predicates for implication and accessing resources on the Web, and functions including cryptographic, string, math. The main goal of N3Logic is to be a minimal extension to the RDF data model such that the same language can be used for logic and data. In this paper, we describe N3Logic and illustrate through examples why it is an appropriate logic for the Web.\n        \u25b3 Less", "author": "Tim Berners-Lee"}, {"abstract": "In this paper we discuss policy iteration methods for approximate solution of a finite-state discounted Markov decision problem, with a focus on feature-based aggregation methods and their connection with deep reinforcement learning schemes. We introduce features of the states of the original problem, and we formulate a smaller \"aggregate\" Markov decision problem, whose states relate to the features. We discuss properties and possible implementations of this type of aggregation, including a new approach to approximate policy iteration. In this approach the policy improvement operation combines feature-based aggregation with feature construction using deep neural networks or other calculations. We argue that the cost function of a policy may be approximated much more accurately by the nonlinear function of the features provided by aggregation, than by the linear function of the features provided by neural network-based reinforcement learning, thereby potentially leading to more effective policy improvement.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We consider discrete-time infinite horizon deterministic optimal control problems with nonnegative cost per stage, and a destination that is cost-free and absorbing. The classical linear-quadratic regulator problem is a special case. Our assumptions are very general, and allow the possibility that the optimal policy may not be stabilizing the system, e.g., may not reach the destination either asymptotically or in a finite number of steps. We introduce a new unifying notion of stable feedback policy, based on perturbation of the cost per stage, which in addition to implying convergence of the generated states to the destination, quantifies the speed of convergence. We consider the properties of two distinct cost functions: $\\jstar$, the overall optimal, and $\\hat J$, the restricted optimal over just the stable policies. Different classes of stable policies (with different speeds of convergence) may yield different values of $\\hat J$. We show that for any class of stable policies, $\\hat J$ is a solution of Bellman's equation, and we characterize the smallest and the largest solutions: they are $\\jstar$, and $J^+$, the restricted optimal cost function over the class of (finitely) terminating policies. We also characterize the regions of convergence of various modified versions of value and policy iteration algorithms, as substitutes for the standard algorithms, which may not work in general.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We consider stochastic shortest path problems with infinite state and control spaces, a nonnegative cost per stage, and a termination state. We extend the notion of a proper policy, a policy that terminates within a finite expected number of steps, from the context of finite state space to the context of infinite state space. We consider the optimal cost function $\\jstar$, and the optimal cost function $\\hat J$ over just the proper policies. We show that $\\jstar$ and $\\hat J$ are the smallest and largest solutions of Bellman's equation, respectively, within a suitable class of Lyapounov-like functions. If the cost per stage is bounded, these functions are those that are bounded over the effective domain of $\\hat J$. The standard value iteration algorithm may be attracted to either $\\jstar$ or $\\hat J$, depending on the initial condition. In the favorable case where $\\jstar=\\hat J$, strong analytical and algorithmic results are obtained.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We consider large linear and nonlinear fixed point problems, and solution with proximal algorithms. We show that there is a close connection between two seemingly different types of methods from distinct fields: 1) Proximal iterations for linear systems of equations, which are prominent in numerical analysis and convex optimization, and 2) Temporal difference (TD) type methods, such as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in simulation-based approximate dynamic programming/reinforcement learning (DP/RL), and its recent prominent successes in large-scale game contexts, among others.\n  One benefit of this connection is a new and simple way to accelerate the standard proximal algorithm by extrapolation towards the TD iteration, which generically has a faster convergence rate. Another benefit is the potential integration into the proximal algorithmic context of several new ideas that have emerged in the DP/RL context. We discuss some of the possibilities, and in particular, algorithms that project each proximal iterate onto the subspace spanned by a small number of basis functions, using low-dimensional calculations and simulation. A third benefit is that insights and analysis from proximal algorithms can be brought to bear on the enhancement of TD methods.\n  The linear fixed point methodology can be extended to nonlinear fixed point problems involving a contraction, thus providing guaranteed and potentially substantial acceleration of the proximal and forward backward splitting algorithms at no extra cost. Moreover, the connection of proximal and TD methods can be extended to nonlinear (nondifferentiable) fixed point problems through new proximal-like algorithms that involve successive linearization, similar to policy iteration in DP.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We consider challenging dynamic programming models where the associated Bellman equation, and the value and policy iteration algorithms commonly exhibit complex and even pathological behavior. Our analysis is based on the new notion of regular policies. These are policies that are well-behaved with respect to value and policy iteration, and are patterned after proper policies, which are central in the theory of stochastic shortest path problems. We show that the optimal cost function over regular policies may have favorable value and policy iteration properties, which the optimal cost function over all policies need not have. We accordingly develop a unifying methodology to address long standing analytical and algorithmic issues in broad classes of undiscounted models, including stochastic and minimax shortest path problems, as well as positive cost, negative cost, risk-sensitive, and multiplicative cost problems.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "In this paper we consider shortest path problems in a directed graph where the transitions between nodes are subject to uncertainty. We use a minimax formulation, where the objective is to guarantee that a special destination state is reached with a minimum cost path under the worst possible instance of the uncertainty. Problems of this type arise, among others, in planning and pursuit-evasion contexts, and in model predictive control. Our analysis makes use of the recently developed theory of abstract semicontractive dynamic programming models. We investigate questions of existence and uniqueness of solution of the optimality equation, existence of optimal paths, and the validity of various algorithms patterned after the classical methods of value and policy iteration, as well as a Dijkstra-like algorithm for problems with nonnegative arc lengths.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "In this paper we consider a broad class of infinite horizon discrete-time optimal control models that involve a nonnegative cost function and an affine mapping in their dynamic programming equation. They include as special cases classical models such as stochastic undiscounted nonnegative cost problems, stochastic multiplicative cost problems, and risk-sensitive problems with exponential cost. We focus on the case where the state space is finite and the control space has some compactness properties. We assume that the affine mapping has a semicontractive character, whereby for some policies it is a contraction, while for others it is not. In one line of analysis, we impose assumptions that guarantee that the latter policies cannot be optimal. Under these assumptions, we prove strong results that resemble those for discounted Markovian decision problems, such as the uniqueness of solution of Bellman's equation, and the validity of forms of value and policy iteration. In the absence of these assumptions, the results are weaker and unusual in character: the optimal cost function need not be a solution of Bellman's equation, and an optimal policy may not be found by value or policy iteration. Instead the optimal cost function over just the contractive policies solves Bellman's equation, and can be computed by a variety of algorithms.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We consider minimization of the sum of a large number of convex functions, and we propose an incremental aggregated version of the proximal algorithm, which bears similarity to the incremental aggregated gradient and subgradient methods that have received a lot of recent attention. Under cost function differentiability and strong convexity assumptions, we show linear convergence for a sufficiently small constant stepsize. This result also applies to distributed asynchronous variants of the method, involving bounded interprocessor communication delays.\n  We then consider dual versions of incremental proximal algorithms, which are incremental augmented Lagrangian methods for separable equality-constrained optimization problems. Contrary to the standard augmented Lagrangian method, these methods admit decomposition in the minimization of the augmented Lagrangian, and update the multipliers far more frequently. Our incremental aggregated augmented Lagrangian methods bear similarity to several known decomposition algorithms, including the alternating direction method of multipliers (ADMM) and more recent variations. We compare these methods in terms of their properties, and highlight their potential advantages and limitations.\n  We also address the solution of separable inequality-constrained optimization problems through the use of nonquadratic augmented Lagrangiias such as the exponential, and we dually consider a corresponding incremental aggregated version of the proximal algorithm that uses nonquadratic regularization, such as an entropy function. We finally propose a closely related linearly convergent method for minimization of large differentiable sums subject to an orthant constraint, which may be viewed as an incremental aggregated version of the mirror descent method.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We survey incremental methods for minimizing a sum $\\sum_{i=1}^mf_i(x)$ consisting of a large number of convex component functions $f_i$. Our methods consist of iterations applied to single components, and have proved very effective in practice. We introduce a unified algorithmic framework for a variety of such methods, some involving gradient and subgradient iterations, which are known, and some involving combinations of subgradient and proximal methods, which are new and offer greater flexibility in exploiting the special structure of $f_i$. We provide an analysis of the convergence and rate of convergence properties of these methods, including the advantages offered by randomization in the selection of components. We also survey applications in inference/machine learning, signal processing, and large-scale and distributed optimization.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "In this paper we discuss $\u0142$-policy iteration, a method for exact and approximate dynamic programming. It is intermediate between the classical value iteration (VI) and policy iteration (PI) methods, and it is closely related to optimistic (also known as modified) PI, whereby each policy evaluation is done approximately, using a finite number of VI. We review the theory of the method and associated questions of bias and exploration arising in simulation-based cost function approximation. We then discuss various implementations, which offer advantages over well-established PI methods that use LSPE($\u0142$), LSTD($\u0142$), or TD($\u0142$) for policy evaluation with cost function approximation. One of these implementations is based on a new simulation scheme, called geometric sampling, which uses multiple short trajectories rather than a single infinitely long trajectory.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "In this paper, we consider discrete-time infinite horizon problems of optimal control to a terminal set of states. These are the problems that are often taken as the starting point for adaptive dynamic programming. Under very general assumptions, we establish the uniqueness of solution of Bellman's equation, and we provide convergence results for value and policy iteration.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We consider Newton methods for common types of single commodity and multi-commodity network flow problems. Despite the potentially very large dimension of the problem, they can be implemented using the conjugate gradient method and low-dimensional network operations, as shown nearly thirty years ago. We revisit these methods, compare them to more recent proposals, and describe how they can be implemented in a distributed computing system. We also discuss generalizations, including the treatment of arc gains, linear side constraints, and related special structures.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "We consider stochastic control models with Borel spaces and universally measurable policies. For such models the standard policy iteration is known to have difficult measurability issues and cannot be carried out in general. We present a mixed value and policy iteration method that circumvents this difficulty. The method allows the use of stationary policies in computing the optimal cost function, in a manner that resembles policy iteration. It can also be used to address similar difficulties of policy iteration in the context of upper and lower semicontinuous models. We analyze the convergence of the method in infinite horizon total cost problems, for the discounted case where the one-stage costs are bounded, and for the undiscounted case where the one-stage costs are nonpositive or nonnegative.\n  For undiscounted total cost problems with nonnegative one-stage costs, we also give a new convergence theorem for value iteration, which shows that value iteration converges whenever it is initialized with a function that is above the optimal cost function and yet bounded by a multiple of the optimal cost function. This condition resembles Whittle's bridging condition and is partly motivated by it. The theorem is also partly motivated by a result of Maitra and Sudderth, which showed that value iteration, when initialized with the constant function zero, could require a transfinite number of iterations to converge. We use the new convergence theorem for value iteration to establish the convergence of our mixed value and policy iteration method for the nonnegative cost case.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "In this paper, we propose a new lower approximation scheme for POMDP with discounted and average cost criterion. The approximating functions are determined by their values at a finite number of belief points, and can be computed efficiently using value iteration algorithms for finite-state MDP. While for discounted problems several lower approximation schemes have been proposed earlier, ours seems the first of its kind for average cost problems. We focus primarily on the average cost case, and we show that the corresponding approximation can be computed efficiently using multi-chain algorithms for finite-state MDP. We give a preliminary analysis showing that regardless of the existence of the optimal average cost J in the POMDP, the approximation obtained is a lower bound of the liminf optimal average cost function, and can also be used to calculate an upper bound on the limsup optimal average cost function, as well as bounds on the cost of executing the stationary policy associated with the approximation. Weshow the convergence of the cost approximation, when the optimal average cost is constant and the optimal differential cost is continuous.\n        \u25b3 Less", "author": "Dimitri Bertsekas"}, {"abstract": "While long short-term memory (LSTM) neural net architectures are designed to capture sequence information, human language is generally composed of hierarchical structures. This raises the question as to whether LSTMs can learn hierarchical structures. We explore this question with a well-formed bracket prediction task using two types of brackets modeled by an LSTM. Demonstrating that such a system is learnable by an LSTM is the first step in demonstrating that the entire class of CFLs is also learnable. We observe that the model requires exponential memory in terms of the number of characters and embedded depth, where a sub-linear memory should suffice. Still, the model does more than memorize the training input. It learns how to distinguish between relevant and irrelevant information. On the other hand, we also observe that the model does not generalize well. We conclude that LSTMs do not learn the relevant underlying context-free rules, suggesting the good overall performance is attained rather by an efficient way of evaluating nuisance variables. LSTMs are a way to quickly reach good results for many natural language tasks, but to understand and generate natural language one has to investigate other concepts that can make more direct use of natural language's structural nature.\n        \u25b3 Less", "author": "Robert Berwick"}, {"abstract": "We consider two different data sets of syntactic parameters and we discuss how to detect relations between parameters through a heat kernel method developed by Belkin-Niyogi, which produces low dimensional representations of the data, based on Laplace eigenfunctions, that preserve neighborhood information. We analyze the different connectivity and clustering structures that arise in the two datasets, and the regions of maximal variance in the two-parameter space of the Belkin-Niyogi construction, which identify preferable choices of independent variables. We compute clustering coefficients and their variance.\n        \u25b3 Less", "author": "Robert Berwick"}, {"abstract": "Using Phylogenetic Algebraic Geometry, we analyze computationally the phylogenetic tree of subfamilies of the Indo-European language family, using data of syntactic structures. The two main sources of syntactic data are the SSWL database and Longobardi's recent data of syntactic parameters. We compute phylogenetic invariants and likelihood functions for two sets of Germanic languages, a set of Romance languages, a set of Slavic languages and a set of early Indo-European languages, and we compare the results with what is known through historical linguistics.\n        \u25b3 Less", "author": "Robert Berwick"}, {"abstract": "In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the basis of Zipf rank frequency data that noncoding DNA sequence regions are more like natural languages than coding regions. We argue on the contrary that an empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to natural languages. Although DNA is a presumably an ``organized system of signs'' in Mandelbrot's (1961) sense, an observation of statistical features of the sort presented in the Mantegna et al. paper does not shed light on the similarity between DNA's ``grammar'' and natural language grammars, just as the observation of exact Zipf-like behavior cannot distinguish between the underlying processes of tossing an $M$ sided die or a finite-state branching process.\n        \u25b3 Less", "author": "Robert Berwick"}, {"abstract": "Modellers of large scale genome rearrangement events, in which segments of DNA are inverted, moved, swapped, or even inserted or deleted, have found a natural syntax in the language of permutations. Despite this, there has been a wide range of modelling choices, assumptions and interpretations that make navigating the literature a significant challenge. Indeed, even authors of papers that use permutations to model genome rearrangement can struggle to interpret each others' work, because of subtle differences in basic assumptions that are often deeply ingrained (and consequently sometimes not even mentioned). In this paper, we describe the different ways in which permutations have been used to model genomes and genome rearrangement events, presenting some features and limitations of each approach, and show how the various models are related. This paper will help researchers navigate the landscape of genome rearrangement models, and make it easier for authors to present clear and consistent models.\n        \u25b3 Less", "author": "Sangeeta Bhatia"}, {"abstract": "Establishing a distance between genomes is a significant problem in computational genomics, because its solution can be used to establish evolutionary relationships including phylogeny.\n  The \"double cut and join\" (DCJ) model of chromosomal rearrangement proposed by Yancopoulos et al. has received attention as it can model inversions, translocations, fusion and fission on a multichromosomal genome that may contain both linear and circular chromosomes. In this paper, we realize the DCJ operator as a group action on the space of multichromosomal genomes. We study this group action, deriving some properties of the group and finding group-theoretic analogues for the key results in the DCJ theory.\n        \u25b3 Less", "author": "Sangeeta Bhatia"}, {"abstract": "Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer CAV17]. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or delivering low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms Fast-Lin and Fast-Lip that are able to certify non-trivial lower bounds of minimum distortions, by bounding the ReLU units with appropriate linear functions Fast-Lin, or by bounding the local Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods deliver bounds close to (the gap is 2-3X) exact minimum distortion found by Reluplex in small MNIST networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core.\n  In addition, we show that, in fact, there is no polynomial time algorithm that can approximately find the minimum $\\ell_1$ adversarial distortion of a ReLU network with a $0.99\\ln n$ approximation ratio unless $\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network.\n        \u25b3 Less", "author": "Duane Boning"}, {"abstract": "In this paper, we propose a novel method to estimate and characterize spatial variations on dies or wafers. This new technique exploits recent developments in matrix completion, enabling estimation of spatial variation across wafers or dies with a small number of randomly picked sampling points while still achieving fairly high accuracy. This new approach can be easily generalized, including for estimation of mixed spatial and structure or device type information.\n        \u25b3 Less", "author": "Duane Boning"}, {"abstract": "Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but an explicit connection between the two had not been made. We show how to efficiently transform a black-box solver for SLR into an algorithm for SPCA: assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the SPCA algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomialtime algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. We provide experimental results on simulated data comparing our proposed framework to other algorithms for SPCA.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "The prototypical high-dimensional statistics problem entails finding a structured signal in noise. Many of these problems exhibit an intriguing phenomenon: the amount of data needed by all known computationally efficient algorithms far exceeds what is needed for inefficient algorithms that search over all possible structures. A line of work initiated by Berthet and Rigollet in 2013 has aimed to explain these statistical-computational gaps by reducing from conjecturally hard average-case problems in computer science. However, the delicate nature of average-case reductions has limited the applicability of this approach. In this work we introduce several new techniques to give a web of average-case reductions showing strong computational lower bounds based on the planted clique conjecture using natural problems as intermediates. These include tight lower bounds for Planted Independent Set, Planted Dense Subgraph, Sparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block Model and a biased variant of Sparse PCA. We also give algorithms matching our lower bounds and identify the information-theoretic limits of the models we consider.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Graphical models are a rich language for describing high-dimensional distributions in terms of their dependence structure. While there are algorithms with provable guarantees for learning undirected graphical models in a variety of settings, there has been much less progress in the important scenario when there are latent variables. Here we study Restricted Boltzmann Machines (or RBMs), which are a popular model with wide-ranging applications in dimensionality reduction, collaborative filtering, topic modeling, feature extraction and deep learning.\n  The main message of our paper is a strong dichotomy in the feasibility of learning RBMs, depending on the nature of the interactions between variables: ferromagnetic models can be learned efficiently, while general models cannot. In particular, we give a simple greedy algorithm based on influence maximization to learn ferromagnetic RBMs with bounded degree. In fact, we learn a description of the distribution on the observed variables as a Markov Random Field. Our analysis is based on tools from mathematical physics that were developed to show the concavity of magnetization. Our algorithm extends straighforwardly to general ferromagnetic Ising models with latent variables.\n  Conversely, we show that even for a contant number of latent variables with constant degree, without ferromagneticity the problem is as hard as sparse parity with noise. This hardness result is based on a sharp and surprising characterization of the representational power of bounded degree RBMs: the distribution on their observed variables can simulate any bounded order MRF. This result is of independent interest since RBMs are the building blocks of deep belief networks.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Most information systems store data by modifying the local state of matter, in the hope that atomic (or sub-atomic) local interactions would stabilize the state for a sufficiently long time, thereby allowing later recovery. In this work we initiate the study of information retention in locally-interacting systems. The evolution in time of the interacting particles is modeled via the stochastic Ising model (SIM). The initial spin configuration $X_0$ serves as the user-controlled input. The output configuration $X_t$ is produced by running $t$ steps of the Glauber chain. Our main goal is to evaluate the information capacity $I_n(t)\\triangleq\\max_{p_{X_0}}I(X_0;X_t)$ when the time $t$ scales with the size of the system $n$. For the zero-temperature SIM on the two-dimensional $\\sqrt{n}\\times\\sqrt{n}$ grid and free boundary conditions, it is easy to show that $I_n(t) = \u0398(n)$ for $t=O(n)$. In addition, we show that on the order of $\\sqrt{n}$ bits can be stored for infinite time in striped configurations. The $\\sqrt{n}$ achievability is optimal when $t\\to\\infty$ and $n$ is fixed.\n  One of the main results of this work is an achievability scheme that stores more than $\\sqrt{n}$ bits (in orders of magnitude) for superlinear (in $n$) times. The analysis of the scheme decomposes the system into $\u03a9(\\sqrt{n})$ independent Z-channels whose crossover probability is found via the (recently rigorously established) Lifshitz law of phase boundary movement. We also provide results for the positive but small temperature regime. We show that an initial configuration drawn according to the Gibbs measure cannot retain more than a single bit for $t\\geq e^{cn^{\\frac{1}{4}+\u03b5}}$. On the other hand, when scaling time with $\u03b2$, the stripe-based coding scheme (that stores for infinite time at zero temperature) is shown to retain its bits for time that is exponential in $\u03b2$.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We study the problem of testing, using only a single sample, between mean field distributions (like Curie-Weiss, Erd\u0151s-R\u00e9nyi) and structured Gibbs distributions (like Ising model on sparse graphs and Exponential Random Graphs). Our goal is to test without knowing the parameter values of the underlying models: only the \\emph{structure} of dependencies is known. We develop a new approach that applies to both the Ising and Exponential Random Graph settings based on a general and natural statistical test. The test can distinguish the hypotheses with high probability above a certain threshold in the (inverse) temperature parameter, and is optimal in that below the threshold no test can distinguish the hypotheses.\n  The thresholds do not correspond to the presence of long-range order in the models. By aggregating information at a global scale, our test works even at very high temperatures.\n  The proofs are based on distributional approximation and sharp concentration of quadratic forms, when restricted to Hamming spheres. The restriction to Hamming spheres is necessary, since otherwise any scalar statistic is useless without explicit knowledge of the temperature parameter. At the same time, this restriction radically changes the behavior of the functions under consideration, resulting in a much smaller variance than in the independent setting; this makes it hard to directly apply standard methods (i.e., Stein's method) for concentration of weakly dependent variables. Instead, we carry out an additional tensorization argument using a Markov chain that respects the symmetry of the Hamming sphere.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We develop a new technique, based on Stein's method, for comparing two stationary distributions of irreducible Markov Chains whose update rules are `close enough'. We apply this technique to compare Ising models on $d$-regular expander graphs to the Curie-Weiss model (complete graph) in terms of pairwise correlations and more generally $k$th order moments. Concretely, we show that $d$-regular Ramanujan graphs approximate the $k$th order moments of the Curie-Weiss model to within average error $k/\\sqrt{d}$ (averaged over the size $k$ subsets). The result applies even in the low-temperature regime; we also derive some simpler approximation results for functionals of Ising models that hold only at high enough temperatures.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We consider an online model for recommendation systems, with each user being recommended an item at each time-step and providing 'like' or 'dislike' feedback. A latent variable model specifies the user preferences: both users and items are clustered into types. All users of a given type have identical preferences for the items, and similarly, items of a given type are either all liked or all disliked by a given user. The model captures structure in both the item and user spaces, and in this paper, we assume that the type preference matrix is randomly generated. We describe two algorithms inspired by user-user and item-item collaborative filtering (CF), modified to explicitly make exploratory recommendations, and prove performance guarantees in terms of their expected regret. For two regimes of model parameters, with structure only in item space or only in user space, we prove information-theoretic lower bounds on regret that match our upper bounds up to logarithmic factors. Our analysis elucidates system operating regimes in which existing CF algorithms are nearly optimal.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We study the problem of learning a tree Ising model from samples such that subsequent predictions made using the model are accurate. The prediction task considered in this paper is that of predicting the values of a subset of variables given values of some other subset of variables. Virtually all previous work on graphical model learning has focused on recovering the true underlying graph. We define a distance (\"small set TV\" or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\\mathcal{S}$ of a given size, of the total variation between the marginals of $P$ and $Q$ on $\\mathcal{S}$; this distance captures the accuracy of the prediction task of interest. We derive non-asymptotic bounds on the number of samples needed to get a distribution (from the same class) with small ssTV relative to the one generating the samples. One of the main messages of this paper is that far fewer samples are needed than for recovering the underlying tree, which means that accurate predictions are possible using the wrong tree.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "There is much empirical evidence that item-item collaborative filtering works well in practice. Motivated to understand this, we provide a framework to design and analyze various recommendation algorithms. The setup amounts to online binary matrix completion, where at each time a random user requests a recommendation and the algorithm chooses an entry to reveal in the user's row. The goal is to minimize regret, or equivalently to maximize the number of +1 entries revealed at any time. We analyze an item-item collaborative filtering algorithm that can achieve fundamentally better performance compared to user-user collaborative filtering. The algorithm achieves good \"cold-start\" performance (appropriately defined) by quickly making good recommendations to new users about whom there is little information.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. We first observe that the notoriously difficult problem of learning parities with noise can be captured as a special case of learning graphical models. This leads to an unconditional computational lower bound of $\u03a9(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of so-called statistical algorithms recently introduced by Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime required to exhaustively search over neighborhoods cannot be significantly improved without restricting the class of models.\n  Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari (2009) showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time $O(p^2)$. We provide an algorithm whose performance interpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the repulsion.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the \"online\" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We consider the problem of reconstructing the graph underlying an Ising model from i.i.d. samples. Over the last fifteen years this problem has been of significant interest in the statistics, machine learning, and statistical physics communities, and much of the effort has been directed towards finding algorithms with low computational cost for various restricted classes of models. Nevertheless, for learning Ising models on general graphs with $p$ nodes of degree at most $d$, it is not known whether or not it is possible to improve upon the $p^{d}$ computation needed to exhaustively search over all possible neighborhoods for each node.\n  In this paper we show that a simple greedy procedure allows to learn the structure of an Ising model on an arbitrary bounded-degree graph in time on the order of $p^2$. We make no assumptions on the parameters except what is necessary for identifiability of the model, and in particular the results hold at low-temperatures as well as for highly non-uniform models. The proof rests on a new structural property of Ising models: we show that for any node there exists at least one neighbor with which it has a high mutual information. This structural property may be of independent interest.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "In this paper we consider the problem of learning undirected graphical models from data generated according to the Glauber dynamics. The Glauber dynamics is a Markov chain that sequentially updates individual nodes (variables) in a graphical model and it is frequently used to sample from the stationary distribution (to which it converges given sufficient time). Additionally, the Glauber dynamics is a natural dynamical model in a variety of settings. This work deviates from the standard formulation of graphical model learning in the literature, where one assumes access to i.i.d. samples from the distribution.\n  Much of the research on graphical model learning has been directed towards finding algorithms with low computational cost. As the main result of this work, we establish that the problem of reconstructing binary pairwise graphical models is computationally tractable when we observe the Glauber dynamics. Specifically, we show that a binary pairwise graphical model on $p$ nodes with maximum degree $d$ can be learned in time $f(d)p^2\\log p$, for a function $f(d)$, using nearly the information-theoretic minimum number of samples.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan (2008)) but no proof was known.\n  Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We study vector space interference alignment for the MIMO interference channel with no time or frequency diversity, and no symbol extensions. We prove both necessary and sufficient conditions for alignment. In particular, we characterize the feasibility of alignment for the symmetric three-user channel where all users transmit along d dimensions, all transmitters have M antennas and all receivers have N antennas, as well as feasibility of alignment for the fully symmetric (M=N) channel with an arbitrary number of users.\n  An implication of our results is that the total degrees of freedom available in a K-user interference channel, using only spatial diversity from the multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees of freedom shown to be possible by Cadambe and Jafar with arbitrarily large time or frequency diversity.\n  Moving beyond the question of feasibility, we additionally discuss computation of the number of solutions using Schubert calculus in cases where there are a finite number of solutions.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "We present a framework for the design of optimal assembly algorithms for shotgun sequencing under the criterion of complete reconstruction. We derive a lower bound on the read length and the coverage depth required for reconstruction in terms of the repeat statistics of the genome. Building on earlier works, we design a de Brujin graph based assembly algorithm which can achieve very close to the lower bound for repeat statistics of a wide range of sequenced genomes, including the GAGE datasets. The results are based on a set of necessary and sufficient conditions on the DNA sequence and the reads for reconstruction. The conditions can be viewed as the shotgun sequencing analogue of Ukkonen-Pevzner's necessary and sufficient conditions for Sequencing by Hybridization.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "DNA sequencing is the basic workhorse of modern day biology and medicine. Shotgun sequencing is the dominant technique used: many randomly located short fragments called reads are extracted from the DNA sequence, and these reads are assembled to reconstruct the original sequence. A basic question is: given a sequencing technology and the statistics of the DNA sequence, what is the minimum number of reads required for reliable reconstruction? This number provides a fundamental limit to the performance of {\\em any} assembly algorithm. For a simple statistical model of the DNA sequence and the read process, we show that the answer admits a critical phenomena in the asymptotic limit of long DNA sequences: if the read length is below a threshold, reconstruction is impossible no matter how many reads are observed, and if the read length is above the threshold, having enough reads to cover the DNA sequence is sufficient to reconstruct. The threshold is computed in terms of the Renyi entropy rate of the DNA sequence. We also study the impact of noise in the read process on the performance.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "This paper studies vector space interference alignment for the three-user MIMO interference channel with no time or frequency diversity. The main result is a characterization of the feasibility of interference alignment in the symmetric case where all transmitters have M antennas and all receivers have N antennas. If N >= M and all users desire d transmit dimensions, then alignment is feasible if and only if (2r+1)d <= max(rN,(r+1)M) for all nonnegative integers r. The analogous result holds with M and N switched if M >= N.\n  It turns out that, just as for the 3-user parallel interference channel \\cite{BT09}, the length of alignment paths captures the essence of the problem. In fact, for each feasible value of M and N the maximum alignment path length dictates both the converse and achievability arguments.\n  One of the implications of our feasibility criterion is that simply counting equations and comparing to the number of variables does not predict feasibility. Instead, a more careful investigation of the geometry of the alignment problem is required. The necessary condition obtained by counting equations is implied by our new feasibility criterion.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Determining the feasibility conditions for vector space interference alignment in the K-user MIMO interference channel with constant channel coefficients has attracted much recent attention yet remains unsolved. The main result of this paper is restricted to the symmetric square case where all transmitters and receivers have N antennas, and each user desires d transmit dimensions. We prove that alignment is possible if and only if the number of antennas satisfies N>= d(K+1)/2. We also show a necessary condition for feasibility of alignment with arbitrary system parameters. An algebraic geometry approach is central to the results.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Exponential random graphs are used extensively in the sociology literature. This model seeks to incorporate in random graphs the notion of reciprocity, that is, the larger than expected number of triangles and other small subgraphs. Sampling from these distributions is crucial for parameter estimation hypothesis testing, and more generally for understanding basic features of the network model itself. In practice sampling is typically carried out using Markov chain Monte Carlo, in particular either the Glauber dynamics or the Metropolis-Hasting procedure.\n  In this paper we characterize the high and low temperature regimes of the exponential random graph model. We establish that in the high temperature regime the mixing time of the Glauber dynamics is $\u0398(n^2 \\log n)$, where $n$ is the number of vertices in the graph; in contrast, we show that in the low temperature regime the mixing is exponentially slow for any local Markov chain. Our results, moreover, give a rigorous basis for criticisms made of such models. In the high temperature regime, where sampling with MCMC is possible, we show that any finite collection of edges are asymptotically independent; thus, the model does not possess the desired reciprocity property, and is not appreciably different from the Erd\u0151s-R\u00e9nyi random graph.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Recently, Etkin, Tse, and Wang found the capacity region of the two-user Gaussian interference channel to within one bit/s/Hz. A natural goal is to apply this approach to the Gaussian interference channel with an arbitrary number of users. We make progress towards this goal by finding the capacity region of the many-to-one and one-to-many Gaussian interference channels to within a constant number of bits. The result makes use of a deterministic model to provide insight into the Gaussian channel. The deterministic model makes explicit the dimension of signal scale. A central theme emerges: the use of lattice codes for alignment of interfering signals on the signal scale.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "This paper explores the two-user Gaussian interference channel through the lens of a natural deterministic channel model. The main result is that the deterministic channel uniformly approximates the Gaussian channel, the capacity regions differing by a universal constant. The problem of finding the capacity of the Gaussian channel to within a constant error is therefore reduced to that of finding the capacity of the far simpler deterministic channel. Thus, the paper provides an alternative derivation of the recent constant gap capacity characterization of Etkin, Tse, and Wang. Additionally, the deterministic model gives significant insight towards the Gaussian channel.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Markov random fields are used to model high dimensional distributions in a number of applied areas. Much recent interest has been devoted to the reconstruction of the dependency structure from independent samples from the Markov random fields. We analyze a simple algorithm for reconstructing the underlying graph defining a Markov random field on $n$ nodes and maximum degree $d$ given observations. We show that under mild non-degeneracy conditions it reconstructs the generating graph with high probability using $\u0398(d \u03b5^{-2}\u03b4^{-4} \\log n)$ samples where $\u03b5,\u03b4$ depend on the local interactions. For most local interaction $\\eps,\u03b4$ are of order $\\exp(-O(d))$.\n Our results are optimal as a function of $n$ up to a multiplicative constant depending on $d$ and the strength of the local interactions.  Our results seem to be the first results for general models that guarantee that {\\em the} generating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2} \u03b5^{-2}\u03b4^{-4}  \\log n)$ running time bound. In cases where the measure on the graph has correlation decay, the running time is $O(n^2 \\log n)$ for all fixed $d$. We also discuss the effect of observing noisy samples and show that as long as the noise level is low, our algorithm is effective. On the other hand, we construct an example where large noise implies non-identifiability even for generic noise and interactions. Finally, we briefly show that in some simple cases, models with hidden nodes can also be recovered.\n        \u25b3 Less", "author": "Guy Bresler"}, {"abstract": "Until recently, transcriptomics was limited to bulk RNA sequencing, obscuring the underlying expression patterns of individual cells in favor of a global average. Thanks to technological advances, we can now profile gene expression across thousands or millions of individual cells in parallel. This new type of data has led to the intriguing discovery that individual cell profiles can reflect the imprint of time or dynamic processes. However, synthesizing this information to reconstruct dynamic biological phenomena from data that are noisy, heterogenous, and sparse---and from processes that may unfold asynchronously---poses a complex computational and statistical challenge. Here, we develop a full generative model for probabilistically reconstructing trees of cellular differentiation from single-cell RNA-seq data. Specifically, we extend the framework of the classical Dirichlet diffusion tree to simultaneously infer branch topology and latent cell states along continuous trajectories over the full tree. In tandem, we construct a novel Markov chain Monte Carlo sampler that interleaves Metropolis-Hastings and message passing to leverage model structure for efficient inference. Finally, we demonstrate that these techniques can recover latent trajectories from simulated single-cell transcriptomes. While this work is motivated by cellular differentiation, we derive a tractable model that provides flexible densities for any data (coupled with an appropriate noise model) that arise from continuous evolution along a latent nonparametric tree.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "A central question in many probabilistic clustering problems is how many distinct clusters are present in a particular dataset. A Bayesian nonparametric (BNP) model addresses this question by placing a generative process on cluster assignment. However, like all Bayesian approaches, BNP requires the specification of a prior. In practice, it is important to quantitatively establish that the prior is not too informative, particularly when the particular form of the prior is chosen for mathematical convenience rather than because of a considered subjective belief.\n  We derive local sensitivity measures for a truncated variational Bayes (VB) approximation and approximate nonlinear dependence of a VB optimum on prior parameters using a local Taylor series approximation. Using a stick-breaking representation of a Dirichlet process, we consider perturbations both to the scalar concentration parameter and to the functional form of the stick- breaking distribution.\n  Unlike previous work on local Bayesian sensitivity for BNP, we pay special attention to the ability of our sensitivity measures to extrapolate to different priors, rather than treating the sensitivity as a measure of robustness per se. Extrapolation motivates the use of multiplicative perturbations to the functional form of the prior for VB. Additionally, we linearly approximate only the computationally intensive part of inference -- the optimization of the global parameters -- and retain the nonlinearity of easily computed quantities as functions of the global parameters.\n  We apply our methods to estimate sensitivity of the expected number of distinct clusters present in the Iris dataset to the BNP prior specification. We evaluate the accuracy of our approximations by comparing to the much more expensive process of re-fitting the model.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Kernel methods offer the flexibility to learn complex relationships in modern, large data sets while enjoying strong theoretical guarantees on quality. Unfortunately, these methods typically require cubic running time in the data set size, a prohibitive cost in the large-data setting. Random feature maps (RFMs) and the Nystrom method both consider low-rank approximations to the kernel matrix as a potential solution. But, in order to achieve desirable theoretical guarantees, the former may require a prohibitively large number of features J+, and the latter may be prohibitively expensive for high-dimensional problems. We propose to combine the simplicity and generality of RFMs with a data-dependent feature selection scheme to achieve desirable theoretical approximation properties of Nystrom with just O(log J+) features. Our key insight is to begin with a large set of random features, then reduce them to a small number of weighted features in a data-dependent, computationally efficient way, while preserving the statistical guarantees of using the original large set of features. We demonstrate the efficacy of our method with theory and experiments--including on a data set with over 50 million observations. In particular, we show that our method achieves small kernel matrix approximation error and better test set accuracy with provably fewer random features than state- of-the-art methods.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Bayesian inference typically requires the computation of an approximation to the posterior distribution. An important requirement for an approximate Bayesian inference algorithm is to output high-accuracy posterior mean and uncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain Monte Carlo, remain the gold standard for approximate Bayesian inference because they have a robust finite-sample theory and reliable convergence diagnostics. However, alternative methods, which are more scalable or apply to problems where Markov Chain Monte Carlo cannot be used, lack the same finite-data approximation theory and tools for evaluating their accuracy. In this work, we develop a flexible new approach to bounding the error of mean and uncertainty estimates of scalable inference algorithms. Our strategy is to control the estimation errors in terms of Wasserstein distance, then bound the Wasserstein distance via a generalized notion of Fisher distance. Unlike computing the Wasserstein distance, which requires access to the normalized posterior distribution, the Fisher distance is tractable to compute because it requires access only to the gradient of the log posterior density. We demonstrate the usefulness of our Fisher distance approach by deriving bounds on the Wasserstein error of the Laplace approximation and Hilbert coresets. We anticipate that our approach will be applicable to many other approximate inference methods such as the integrated Laplace approximation, variational inference, and approximate Bayesian computation\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Gaussian processes (GPs) offer a flexible class of priors for nonparametric Bayesian regression, but popular GP posterior inference methods are typically prohibitively slow or lack desirable finite-data guarantees on quality. We develop an approach to scalable approximate GP regression with finite-data guarantees on the accuracy of pointwise posterior mean and variance estimates. Our main contribution is a novel objective for approximate inference in the nonparametric setting: the preconditioned Fisher (pF) divergence. We show that unlike the Kullback--Leibler divergence (used in variational inference), the pF divergence bounds the 2-Wasserstein distance, which in turn provides tight bounds the pointwise difference of the mean and variance functions. We demonstrate that, for sparse GP likelihood approximations, we can minimize the pF divergence efficiently. Our experiments show that optimizing the pF divergence has the same computational requirements as variational sparse GPs while providing comparable empirical performance--in addition to our novel finite-data quality guarantees.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "The error or variability of machine learning algorithms is often assessed by repeatedly re-fitting a model with different weighted versions of the observed data. The ubiquitous tools of cross-validation (CV) and the bootstrap are examples of this technique. These methods are powerful in large part due to their model agnosticism but can be slow to run on modern, large data sets due to the need to repeatedly re-fit the model. In this work, we use a linear approximation to the dependence of the fitting procedure on the weights, producing results that can be faster than repeated re-fitting by orders of magnitude. This linear approximation is sometimes known as the \"infinitesimal jackknife\" in the statistics literature, where it is mostly used to as a theoretical tool to prove asymptotic results. We provide explicit finite-sample error bounds for the infinitesimal jackknife in terms of a small number of simple, verifiable assumptions. Our results apply whether the weights and data are stochastic, deterministic, or even adversarially chosen, and so can be used as a tool for proving the accuracy of the infinitesimal jackknife on a wide variety of problems. As a corollary, we state mild regularity conditions under which our approximation consistently estimates true leave-k-out cross-validation for any fixed k. These theoretical results, together with modern automatic differentiation software, support the application of the infinitesimal jackknife to a wide variety of practical problems in machine learning, providing a \"Swiss Army infinitesimal jackknife.\" We demonstrate the accuracy of our methods on a range of simulated and real datasets.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationships. However, traditional methods often fail in modern applications, which exhibit a larger number of observed variables than data points. The resulting uncertainty about the underlying network as well as the desire to incorporate prior information recommend a Bayesian approach to learning the BN, but the highly combinatorial structure of BNs poses a striking challenge for inference. The current state-of-the-art methods such as order MCMC are faster than previous methods but prevent the use of many natural structural priors and still have running time exponential in the maximum indegree of the true directed acyclic graph (DAG) of the BN. We here propose an alternative posterior approximation based on the observation that, if we incorporate empirical conditional independence tests, we can focus on a high-probability DAG associated with each order of the vertices. We show that our method allows the desired flexibility in prior specification, removes timing dependence on the maximum indegree and yields provably good posterior approximations; in addition, we show that it achieves superior accuracy, scalability, and sampler mixing on several datasets.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Clustering procedures typically estimate which data points are clustered together, a quantity of primary importance in many analyses. Often used as a preliminary step for dimensionality reduction or to facilitate interpretation, finding robust and stable clusters is often crucial for appropriate for downstream analysis. In the present work, we consider Bayesian nonparametric (BNP) models, a particularly popular set of Bayesian models for clustering due to their flexibility. Because of its complexity, the Bayesian posterior often cannot be computed exactly, and approximations must be employed. Mean-field variational Bayes forms a posterior approximation by solving an optimization problem and is widely used due to its speed. An exact BNP posterior might vary dramatically when presented with different data. As such, stability and robustness of the clustering should be assessed.\n  A popular mean to assess stability is to apply the bootstrap by resampling the data, and rerun the clustering for each simulated data set. The time cost is thus often very expensive, especially for the sort of exploratory analysis where clustering is typically used. We propose to use a fast and automatic approximation to the full bootstrap called the \"linear bootstrap\", which can be seen by local data perturbation. In this work, we demonstrate how to apply this idea to a data analysis pipeline, consisting of an MFVB approximation to a BNP clustering posterior of time course gene expression data. We show that using auto-differentiation tools, the necessary calculations can be done automatically, and that the linear bootstrap is a fast but approximate alternative to the bootstrap.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Generalized linear models (GLMs) -- such as logistic regression, Poisson regression, and robust regression -- provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of speed and multiple measures of accuracy -- including on an advertising data set with 40 million data points and 20,000 covariates.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior inference technique that is increasingly popular due to its fast runtimes on large-scale datasets. However, even when MFVB provides accurate posterior means for certain parameters, it often mis-estimates variances and covariances. Furthermore, prior robustness measures have remained undeveloped for MFVB. By deriving a simple formula for the effect of infinitesimal model perturbations on MFVB posterior means, we provide both improved covariance estimates and local robustness measures for MFVB, thus greatly expanding the practical usefulness of MFVB posterior approximations. The estimates for MFVB posterior covariances rely on a result from the classical Bayesian robustness literature relating derivatives of posterior expectations to posterior covariances and include the Laplace approximation as a special case. Our key condition is that the MFVB approximation provides good estimates of a select subset of posterior means---an assumption that has been shown to hold in many practical settings. In our experiments, we demonstrate that our methods are simple, general, and fast, providing accurate posterior uncertainty estimates and robustness measures with runtimes that can be an order of magnitude faster than MCMC.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse. We present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity. To do so, we outline a general framework for graph generative models; by contrast to the pioneering work of Caron and Fox (2015), models within our framework are stationary across steps of the graph sequence. In particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior, since this choice is made by the modeler and is often somewhat subjective. A different, equally subjectively plausible choice of prior may result in a substantially different posterior, and so different conclusions drawn from the data. Were this to be the case, our conclusions would not be robust to the choice of prior. To determine whether our model is robust, we must quantify how sensitive our posterior is to perturbations of our prior. Despite the importance of the problem and a considerable body of literature, generic, easy-to-use methods to quantify Bayesian robustness are still lacking.\n  Abstract In this paper, we demonstrate that powerful measures of robustness can be easily calculated from Variational Bayes (VB) approximate posteriors. We begin with local robustness, which measures the effect of infinitesimal changes to the prior on a posterior mean of interest. In particular, we show that the influence function of Gustafson (2012) has a simple, easy-to-calculate closed form expression for VB approximations. We then demonstrate how local robustness measures can be inadequate for non-local prior changes, such as replacing one prior entirely with another. We propose a simple approximate non-local robustness measure and demonstrate its effectiveness on a simulated data set.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Variational inference (VI) provides fast approximations of a Bayesian posterior in part because it formulates posterior approximation as an optimization problem: to find the closest distribution to the exact posterior over some family of distributions. For practical reasons, the family of distributions in VI is usually constrained so that it does not include the exact posterior, even as a limit point. Thus, no matter how long VI is run, the resulting approximation will not approach the exact posterior. We propose to instead consider a more flexible approximating family consisting of all possible finite mixtures of a parametric base distribution (e.g., Gaussian). For efficient inference, we borrow ideas from gradient boosting to develop an algorithm we call boosting variational inference (BVI). BVI iteratively improves the current approximation by mixing it with a new component from the base distribution family and thereby yields progressively more accurate posterior approximations as more computing time is spent. Unlike a number of common VI variants including mean-field VI, BVI is able to capture multimodality, general posterior covariance, and nonstandard posterior shapes.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Trait allocations are a class of combinatorial structures in which data may belong to multiple groups and may have different levels of belonging in each group. Often the data are also exchangeable, i.e., their joint distribution is invariant to reordering. In clustering---a special case of trait allocation---exchangeability implies the existence of both a de Finetti representation and an exchangeable partition probability function (EPPF), distributional representations useful for computational and theoretical purposes. In this work, we develop the analogous de Finetti representation and exchangeable trait probability function (ETPF) for trait allocations, along with a characterization of all trait allocations with an ETPF. Unlike previous feature allocation characterizations, our proofs fully capture single-occurrence \"dust\" groups. We further introduce a novel constrained version of the ETPF that we use to establish an intuitive connection between the probability functions for clustering, feature allocations, and trait allocations. As an application of our general theory, we characterize the distribution of all edge-exchangeable graphs, a class of recently-developed models that captures realistic sparse graph sequences.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Bayesian hierarchical models are increasing popular in economics. When using hierarchical models, it is useful not only to calculate posterior expectations, but also to measure the robustness of these expectations to reasonable alternative prior choices. We use variational Bayes and linear response methods to provide fast, accurate posterior means and robustness measures with an application to measuring the effectiveness of microcredit in the developing world.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Network data appear in a number of applications, such as online social networks and biological networks, and there is growing interest in both developing models for networks as well as studying the properties of such data. Since individual network datasets continue to grow in size, it is necessary to develop models that accurately represent the real-life scaling properties of networks. One behavior of interest is having a power law in the degree distribution. However, other types of power laws that have been observed empirically and considered for applications such as clustering and feature allocation models have not been studied as frequently in models for graph data. In this paper, we enumerate desirable asymptotic behavior that may be of interest for modeling graph data, including sparsity and several types of power laws. We outline a general framework for graph generative models using completely random measures; by contrast to the pioneering work of Caron and Fox (2015), we consider instantiating more of the existing atoms of the random measure as the dataset size increases rather than adding new atoms to the measure. We see that these two models can be complementary; they respectively yield interpretations as (1) time passing among existing members of a network and (2) new individuals joining a network. We detail a particular instance of this framework and show simulated results that suggest this model exhibits some desirable asymptotic power-law behavior.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "A known failing of many popular random graph models is that the Aldous-Hoover Theorem guarantees these graphs are dense with probability one; that is, the number of edges grows quadratically with the number of nodes. This behavior is considered unrealistic in observed graphs. We define a notion of edge exchangeability for random graphs in contrast to the established notion of infinite exchangeability for random graphs --- which has traditionally relied on exchangeability of nodes (rather than edges) in a graph. We show that, unlike node exchangeability, edge exchangeability encompasses models that are known to provide a projective sequence of random graphs that circumvent the Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the number of edges with the number of nodes. We show how edge-exchangeability of graphs relates naturally to existing notions of exchangeability from clustering (a.k.a. partitions) and other familiar combinatorial structures.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Completely random measures (CRMs) and their normalizations are a rich source of Bayesian nonparametric priors. Examples include the beta, gamma, and Dirichlet processes. In this paper we detail two major classes of sequential CRM representations---series representations and superposition representations---within which we organize both novel and existing sequential representations that can be used for simulation and posterior inference. These two classes and their constituent representations subsume existing ones that have previously been developed in an ad hoc manner for specific processes. Since a complete infinite-dimensional CRM cannot be used explicitly for computation, sequential representations are often truncated for tractability. We provide truncation error analyses for each type of sequential representation, as well as their normalized versions, thereby generalizing and improving upon existing truncation error bounds in the literature. We analyze the computational complexity of the sequential representations, which in conjunction with our error bounds allows us to directly compare representations and discuss their relative efficiency. We include numerous applications of our theoretical results to commonly-used (normalized) CRMs, demonstrating that our results enable a straightforward representation and analysis of CRMs that has not previously been available in a Bayesian nonparametric context.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "In Bayesian analysis, the posterior follows from the data and a choice of a prior and a likelihood. One hopes that the posterior is robust to reasonable variation in the choice of prior and likelihood, since this choice is made by the modeler and is necessarily somewhat subjective. Despite the fundamental importance of the problem and a considerable body of literature, the tools of robust Bayes are not commonly used in practice. This is in large part due to the difficulty of calculating robustness measures from MCMC draws. Although methods for computing robustness measures from MCMC draws exist, they lack generality and often require additional coding or computation.\n  In contrast to MCMC, variational Bayes (VB) techniques are readily amenable to robustness analysis. The derivative of a posterior expectation with respect to a prior or data perturbation is a measure of local robustness to the prior or likelihood. Because VB casts posterior inference as an optimization problem, its methodology is built on the ability to calculate derivatives of posterior quantities with respect to model parameters, even in very complex models. In the present work, we develop local prior robustness measures for mean-field variational Bayes(MFVB), a VB technique which imposes a particular factorization assumption on the variational posterior approximation. We start by outlining existing local prior measures of robustness. Next, we use these results to derive closed-form measures of the sensitivity of mean-field variational posterior approximation to prior specification. We demonstrate our method on a meta-analysis of randomized controlled interventions in access to microcredit in developing countries.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "This article is a translation of Bruno de Finetti's paper \"Funzione Caratteristica di un fenomeno aleatorio\" which appeared in Atti del Congresso Internazionale dei Matematici, Bologna 3-10 Settembre 1928, Tomo VI, pp. 179-190, originally published by Nicola Zanichelli Editore S.p.A. The translation was made as close as possible to the original in form and style, except for apparent mistakes found in the original document, which were corrected and are mentioned as footnotes. Most of these were resolved by comparing against a longer version of this work by de Finetti, published shortly after this one under the same titlea. The interested reader is highly encouraged to consult this other version for a more detailed treatment of the topics covered here. Footnotes regarding the translation are labeled with letters to distinguish them from de Finetti's original footnotes.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance.\n  We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables---both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family, LRVB has a simple, analytic form, even for non-conjugate models. Indeed, we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We develop a fast, general methodology for exponential families that augments MFVB to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables. MFVB for exponential families defines a fixed-point equation in the means of the approximating posterior, and our approach yields a covariance estimate by perturbing this fixed point. Inspired by linear response theory, we call our method linear response variational Bayes (LRVB). We also show how LRVB can be used to quickly calculate a measure of the influence of individual data points on parameter point estimates. We demonstrate the accuracy and scalability of our method by learning Gaussian mixture models for both simulated and real data.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Mean Field Variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is its (sometimes severe) underestimates of the uncertainty of model variables and lack of information about model variable covariance. We develop a fast, general methodology for exponential families that augments MFVB to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables. MFVB for exponential families defines a fixed-point equation in the means of the approximating posterior, and our approach yields a covariance estimate by perturbing this fixed point. Inspired by linear response theory, we call our method linear response variational Bayes (LRVB). We demonstrate the accuracy of our method on simulated data sets.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "We demonstrate how to calculate posteriors for general CRM-based priors and likelihoods for Bayesian nonparametric models. We further show how to represent Bayesian nonparametric priors as a sequence of finite draws using a size-biasing approach---and how to represent full Bayesian nonparametric models via finite marginals. Motivated by conjugate priors based on exponential family representations of likelihoods, we introduce a notion of exponential families for CRMs, which we call exponential CRMs. This construction allows us to specify automatic Bayesian nonparametric conjugate priors for exponential CRM likelihoods. We demonstrate that our exponential CRMs allow particularly straightforward recipes for size-biased and marginal representations of Bayesian nonparametric models. Along the way, we prove that the gamma process is a conjugate prior for the Poisson likelihood process and the beta prime process is a conjugate prior for a process we call the odds Bernoulli process. We deliver a size-biased representation of the gamma process and a marginal representation of the gamma process coupled with a Poisson likelihood process.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Bayesian entity resolution merges together multiple, noisy databases and returns the minimal collection of unique individuals represented, together with their true, latent record values. Bayesian methods allow flexible generative models that share power across databases as well as principled quantification of uncertainty for queries of the final, resolved database. However, existing Bayesian methods for entity resolution use Markov monte Carlo method (MCMC) approximations and are too slow to run on modern databases containing millions or billions of records. Instead, we propose applying variational approximations to allow scalable Bayesian inference in these models. We derive a coordinate-ascent approximation for mean-field variational Bayes, qualitatively compare our algorithm to existing methods, note unique challenges for inference that arise from the expected distribution of cluster sizes in entity resolution, and discuss directions for future work in this domain.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Research on distributed machine learning algorithms has focused primarily on one of two extremes - algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this \"optimistic concurrency control\" paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "We present SDA-Bayes, a framework for (S)treaming, (D)istributed, (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation batch primitive. We demonstrate the usefulness of our framework, with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet allocation model to two large-scale document collections. We demonstrate the advantages of our algorithm over stochastic variational inference (SVI) by comparing the two after a single pass through a known amount of data---a case where SVI may be applied---and in the streaming setting, where SVI does not apply.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "The problem of inferring a clustering of a data set has been the subject of much research in Bayesian analysis, and there currently exists a solid mathematical foundation for Bayesian approaches to clustering. In particular, the class of probability distributions over partitions of a data set has been characterized in a number of ways, including via exchangeable partition probability functions (EPPFs) and the Kingman paintbox. Here, we develop a generalization of the clustering problem, called feature allocation, where we allow each data point to belong to an arbitrary, non-negative integer number of groups, now called features or topics. We define and study an \"exchangeable feature probability function\" (EFPF)---analogous to the EPPF in the clustering setting---for certain types of feature models. Moreover, we introduce a \"feature paintbox\" characterization---analogous to the Kingman paintbox for clustering---of the class of exchangeable feature models. We provide a further characterization of the subclass of feature allocations that have EFPF representations.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "We develop algorithms for performing semiparametric regression analysis in real time, with data processed as it is collected and made immediately available via modern telecommunications technologies. Our definition of semiparametric regression is quite broad and includes, as special cases, generalized linear mixed models, generalized additive models, geostatistical models, wavelet nonparametric regression models and their various combinations. Fast updating of regression fits is achieved by couching semiparametric regression into a Bayesian hierarchical model or, equivalently, graphical model framework and employing online mean field variational ideas. An internet site attached to this article, realtime-semiparametric-regression.net, illustrates the methodology for continually arriving stock market, real estate and airline data. Flexible real-time analyses, based on increasingly ubiquitous streaming data sources stand to benefit.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "One of the focal points of the modern literature on Bayesian nonparametrics has been the problem of clustering, or partitioning, where each data point is modeled as being associated with one and only one of some collection of groups called clusters or partition blocks. Underlying these Bayesian nonparametric models are a set of interrelated stochastic processes, most notably the Dirichlet process and the Chinese restaurant process. In this paper we provide a formal development of an analogous problem, called feature modeling, for associating data points with arbitrary nonnegative integer numbers of groups, now called features or topics. We review the existing combinatorial stochastic process representations for the clustering problem and develop analogous representations for the feature modeling problem. These representations include the beta process and the Indian buffet process as well as new representations that provide insight into the connections between these processes. We thereby bring the same level of completeness to the treatment of Bayesian nonparametric feature modeling that has previously been achieved for Bayesian nonparametric clustering.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "We introduce a new graphical model for tracking radio-tagged animals and learning their movement patterns. The model provides a principled way to combine radio telemetry data with an arbitrary set of userdefined, spatial features. We describe an efficient stochastic gradient algorithm for fitting model parameters to data and demonstrate its effectiveness via asymptotic analysis and synthetic experiments. We also apply our model to real datasets, and show that it outperforms the most popular radio telemetry software package used in ecology. We conclude that integration of different data sources under a single statistical framework, coupled with appropriate parameter and state estimation procedures, produces both accurate location estimates and an interpretable statistical model of animal movement.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "As the number of observed Gamma-Ray Bursts (GRBs) continues to grow, follow-up resources need to be used more efficiently in order to maximize science output from limited telescope time. As such, it is becoming increasingly important to rapidly identify bursts of interest as soon as possible after the event, before the afterglows fade beyond detectability. Studying the most distant (highest redshift) events, for instance, remains a primary goal for many in the field. Here we present our Random forest Automated Triage Estimator for GRB redshifts (RATE GRB-z) for rapid identification of high-redshift candidates using early-time metrics from the three telescopes onboard Swift. While the basic RATE methodology is generalizable to a number of resource allocation problems, here we demonstrate its utility for telescope-constrained follow-up efforts with the primary goal to identify and study high-z GRBs. For each new GRB, RATE GRB-z provides a recommendation - based on the available telescope time - of whether the event warrants additional follow-up resources. We train RATE GRB-z using a set consisting of 135 Swift bursts with known redshifts, only 18 of which are z > 4. Cross-validated performance metrics on this training data suggest that ~56% of high-z bursts can be captured from following up the top 20% of the ranked candidates, and ~84% of high-z bursts are identified after following up the top ~40% of candidates. We further use the method to rank 200+ Swift bursts with unknown redshifts according to their likelihood of being high-z.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "We develop a Bayesian nonparametric approach to a general family of latent class problems in which individuals can belong simultaneously to multiple classes and where each class can be exhibited multiple times by an individual. We introduce a combinatorial stochastic process known as the negative binomial process (NBP) as an infinite-dimensional prior appropriate for such problems. We show that the NBP is conjugate to the beta process, and we characterize the posterior distribution under the beta-negative binomial process (BNBP) and hierarchical models based on the BNBP (the HBNBP). We study the asymptotic properties of the BNBP and develop a three-parameter extension of the BNBP that exhibits power-law behavior. We derive MCMC algorithms for posterior inference under the HBNBP, and we present experiments using these algorithms in the domains of image segmentation, object recognition, and document analysis.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "The beta-Bernoulli process provides a Bayesian nonparametric prior for models involving collections of binary-valued features. A draw from the beta process yields an infinite collection of probabilities in the unit interval, and a draw from the Bernoulli process turns these into binary-valued features. Recent work has provided stick-breaking representations for the beta process analogous to the well-known stick-breaking representation for the Dirichlet process. We derive one such stick-breaking representation directly from the characterization of the beta process as a completely random measure. This approach motivates a three-parameter generalization of the beta process, and we study the power laws that can be obtained from this generalized beta process. We present a posterior inference algorithm for the beta-Bernoulli process that exploits the stick-breaking representation, and we present experimental results for a discrete factor-analysis model.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Selection methods that require only a single-switch input, such as a button click or blink, are potentially useful for individuals with motor impairments, mobile technology users, and individuals wishing to transmit information securely. We present a single-switch selection method, \"Nomon,\" that is general and efficient. Existing single-switch selection methods require selectable options to be arranged in ways that limit potential applications. By contrast, traditional operating systems, web browsers, and free-form applications (such as drawing) place options at arbitrary points on the screen. Nomon, however, has the flexibility to select any point on a screen. Nomon adapts automatically to an individual's clicking ability; it allows a person who clicks precisely to make a selection quickly and allows a person who clicks imprecisely more time to make a selection without error. Nomon reaps gains in information rate by allowing the specification of beliefs (priors) about option selection probabilities and by avoiding tree-based selection schemes in favor of direct (posterior) inference. We have developed both a Nomon-based writing application and a drawing application. To evaluate Nomon's performance, we compared the writing application with a popular existing method for single-switch writing (row-column scanning). Novice users wrote 35% faster with the Nomon interface than with the scanning interface. An experienced user (author TB, with > 10 hours practice) wrote at speeds of 9.3 words per minute with Nomon, using 1.2 clicks per character and making no errors in the final text.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Recognizing the successes of treed Gaussian process (TGP) models as an interpretable and thrifty model for nonparametric regression, we seek to extend the model to classification. Both treed models and Gaussian processes (GPs) have, separately, enjoyed great success in application to classification problems. An example of the former is Bayesian CART. In the latter, real-valued GP output may be utilized for classification via latent variables, which provide classification rules by means of a softmax function. We formulate a Bayesian model averaging scheme to combine these two models and describe a Monte Carlo method for sampling from the full posterior distribution with joint proposals for the tree topology and the GP parameters corresponding to latent variables at the leaves. We concentrate on efficient sampling of the latent variables, which is important to obtain good mixing in the expanded parameter space. The tree structure is particularly helpful for this task and also for developing an efficient scheme for handling categorical predictors, which commonly arise in classification problems. Our proposed classification TGP (CTGP) methodology is illustrated on a collection of synthetic and real data sets. We assess performance relative to existing methods and thereby show how CTGP is highly flexible, offers tractable inference, produces rules that are easy to interpret, and performs well out of sample.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Recent work has shown that probabilistic models based on pairwise interactions-in the simplest case, the Ising model-provide surprisingly accurate descriptions of experiments on real biological networks ranging from neurons to genes. Finding these models requires us to solve an inverse problem: given experimentally measured expectation values, what are the parameters of the underlying Hamiltonian? This problem sits at the intersection of statistical physics and machine learning, and we suggest that more efficient solutions are possible by merging ideas from the two fields. We use a combination of recent coordinate descent algorithms with an adaptation of the histogram Monte Carlo method, and implement these techniques to take advantage of the sparseness found in data on real neurons. The resulting algorithm learns the parameters of an Ising model describing a network of forty neurons within a few minutes. This opens the possibility of analyzing much larger data sets now emerging, and thus testing hypotheses about the collective behaviors of these networks.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "We present the results of attempts to detect the ellipticity of dark matter halos using galaxy-galaxy weak lensing with SDSS data. We use 2,020,256 galaxies brighter than r=19 with photometric redshifts (divided into colour and luminosity subsamples) as lenses and 31,697,869 source galaxies. We search for and identify several signal contaminants, which if not removed lead to a spurious detection. These include systematic shear that leads to a slight spurious alignment of lens and source ellipticities, intrinsic alignments (due to contamination of the source sample by physically-associated lens source pairs), and anisotropic magnification bias. We develop methods that allow us to remove these contaminants to the signal. We split the analysis into blue (spiral) and red (elliptical) galaxies. Assuming Gaussian errors as in previous work and a power-law profile, we find f_h=e_h/e_g=0.1+/-0.06 for red galaxies and -0.8+/-0.4 for blue galaxies using 20-300 kpc/h, averaged over luminosity. Inclusion of the more realistic non-Gaussian error distributions and of the NFW density profile (which predicts much smaller ellipticity of the shear for scales above the scale radius) yields 0.60+/-0.38 for ellipticals and -1.4+1.7-2.0 for spirals. While there is no concrete detection of alignment in either case, there is a suggestion in the data of a positive alignment in the brightest lens sample of ellipticals. Our results appear to be mildly inconsistent with a previously reported detection by Hoekstra et al. (2004), but more data and further tests are needed to clarify whether the discrepancy is real or a consequence of differences in the lens galaxy samples used and analysis methods.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "We investigate the required redshift accuracy of type Ia supernova and cluster number-count surveys in order for the redshift uncertainties not to contribute appreciably to the dark energy parameter error budget. For the SNAP supernova experiment, we find that, without the assistance of ground-based measurements, individual supernova redshifts would need to be determined to about 0.002 or better, which is a challenging but feasible requirement for a low-resolution spectrograph. However, we find that accurate redshifts for z<0.1 supernovae, obtained with ground-based experiments, are sufficient to immunize the results against even relatively large redshift errors at high z. For the future cluster number-count surveys such as the South Pole Telescope, Planck or DUET, we find that the purely statistical error in photometric redshift is less important, and that the irreducible, systematic bias in redshift drives the requirements. The redshift bias will have to be kept below 0.001-0.005 per redshift bin (which is determined by the filter set), depending on the sky coverage and details of the definition of the minimal mass of the survey. Furthermore, we find that X-ray surveys have a more stringent required redshift accuracy than Sunyaev-Zeldovich (SZ) effect surveys since they use a shorter lever arm in redshift; conversely, SZ surveys benefit from their high redshift reach only so long as some redshift information is available for distant (z>1) clusters.\n        \u25b3 Less", "author": "Tamara Broderick"}, {"abstract": "Transneptunian objects (TNOs) are a source of invaluable information to access the history and evolution of the outer solar system. However, observing these faint objects is a difficult task. As a consequence, important properties such as size and albedo are known for only a small fraction of them. Now, with the results from deep sky surveys and the Gaia space mission, a new exciting era is within reach as accurate predictions of stellar occultations by numerous distant small solar system bodies become available. From them, diameters with kilometer accuracies can be determined. Albedos, in turn, can be obtained from diameters and absolute magnitudes. We use observations from the Dark Energy Survey (DES) from November 2012 until February 2016, amounting to 4292847 CCD frames. We searched them for all known small solar system bodies and recovered a total of 202 TNOs and Centaurs, 63 of which have been discovered by the DES collaboration until the date of this writing. Their positions were determined using the Gaia Data Release 2 as reference and their orbits were refined. Stellar occultations were then predicted using these refined orbits plus stellar positions from Gaia. These predictions are maintained, and updated, in a dedicated web service. The techniques developed here are also part of an ambitious preparation to use the data from the Large Synoptic Survey Telescope (LSST), that expects to obtain accurate positions and multifilter photometry for tens of thousands of TNOs.\n        \u25b3 Less", "author": "Rodney Brooks"}, {"abstract": "The measurement problem and three other vexing experiments in quantum physics are described. It is shown how Quantum Field Theory, as formulated by Julian Schwinger, provides simple solutions for all four experiments. It is also shown how this theory resolves many other problems of Quantum Mechanics and Relativity, including a new and simple derivation of E = mc2.\n        \u25b3 Less", "author": "Rodney Brooks"}, {"abstract": "Halide perovskites are promising semiconductors for inexpensive, high-performance optoelectronics. Despite a remarkable defect tolerance compared to conventional semiconductors, perovskite thin films still show substantial microscale heterogeneity in key properties such as luminescence efficiency and device performance. This behavior has been attributed to spatial fluctuations in the population of sub-bandgap electronic states that act as trap-mediated non-radiative recombination sites. However, the origin of the variations, trap states and extent of the defect tolerance remains a topic of debate, and a precise understanding is critical to the rational design of defect management strategies. By combining scanning X-ray diffraction beamlines at two different synchrotrons with high-resolution transmission electron microscopy, we reveal levels of heterogeneity on the ten-micrometer scale (super-grains) and even ten-nanometer scale (sub-grain domains). We find that local strain is associated with enhanced defect concentrations, and correlations between the local structure and time-resolved photoluminescence reveal that these strain-related defects are the cause of non-radiative recombination. We reveal a direct connection between defect concentrations and non-radiative losses, as well as complex heterogeneity across multiple length scales, shedding new light on the presence and influence of structural defects in halide perovskites.\n        \u25b3 Less", "author": "Vladimir Bulovic"}, {"abstract": "Unique optical properties of colloidal semiconductor quantum dots (QDs), arising from quantum mechanical confinement of charge within these structures, present a versatile testbed for the study of how high electric fields affect the electronic structure of nanostructured solids. Earlier studies of quasi-DC electric field modulation of QD properties have been limited by the electrostatic breakdown processes under the high externally applied electric fields, which have restricted the range of modulation of QD properties. In contrast, in the present work we drive CdSe:CdS core:shell QD films with high-field THz-frequency electromagnetic pulses whose duration is only a few picoseconds. Surprisingly, in response to the THz excitation we observe QD luminescence even in the absence of an external charge source. Our experiments show that QD luminescence is associated with a remarkably high and rapid modulation of the QD band-gap, which is changing by more than 0.5 eV (corresponding to 25% of the unperturbed bandgap energy) within the picosecond timeframe of THz field profile. We show that these colossal energy shifts can be consistently explained by the quantum confined Stark effect. Our work demonstrates a route to extreme modulation of material properties without configurational changes in material sets or geometries. Additionally, we expect that this platform can be adapted to a novel compact THz detection scheme where conversion of THz fields (with meV-scale photon energies) to the visible/near-IR band (with eV-scale photon energies) can be achieved at room temperature with high bandwidth and sensitivity.\n        \u25b3 Less", "author": "Vladimir Bulovic"}, {"abstract": "Plexcitons are polaritonic modes that result from the strong coupling between excitons and plasmons. We consider plexcitons emerging from the interaction of excitons in an organic molecular layer with surface plasmons in a metallic film. We predict the emergence of Dirac cones in the two-dimensional bandstructure of plexcitons due to the inherent alignment of the excitonic transitions in the organic layer. These Dirac cones may open up in energy by simultaneously interfacing the metal with a magneto-optical layer and subjecting the whole system to a perpendicular magnetic field. The resulting energy gap becomes populated with topologically protected one-way modes which travel at the interface of this plexcitonic system. Our theoretical proposal suggests that plexcitons are a convenient and simple platform for the exploration of exotic phases of matter as well as of novel ways to direct energy flow at the nanoscale.\n        \u25b3 Less", "author": "Vladimir Bulovic"}, {"abstract": "When a computational task tolerates a relaxation of its specification or when an algorithm tolerates the effects of noise in its execution, hardware, programming languages, and system software can trade deviations from correct behavior for lower resource usage. We present, for the first time, a synthesis of research results on computing systems that only make as many errors as their users can tolerate, from across the disciplines of computer aided design of circuits, digital system design, computer architecture, programming languages, operating systems, and information theory.\n  Rather than over-provisioning resources at each layer to avoid errors, it can be more efficient to exploit the masking of errors occurring at one layer which can prevent them from propagating to a higher layer. We survey tradeoffs for individual layers of computing systems from the circuit level to the operating system level and illustrate the potential benefits of end-to-end approaches using two illustrative examples. To tie together the survey, we present a consistent formalization of terminology, across the layers, which does not significantly deviate from the terminology traditionally used by research communities in their layer of focus.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "Statically estimating the number of processor clock cycles it takes to execute a basic block of assembly instructions in steady state (throughput) is important for compiler backend optimizations such as register allocation, instruction selection and instruction scheduling. This is complicated specially in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures. Traditionally, compiler writers invest time experimenting and referring to processor manuals to analytically model modern processors with incomplete specifications. This is tedious, error prone and should be done for each processor generation. We present Ithemal, the first automatically learnt estimator to statically predict throughput of a set of basic block instructions using machine learning. Ithemal uses a novel Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven approach for throughput estimation. We show that Ithemal is accurate than state-of-the-art hand written tools used in compiler backends and static machine code analyzers. In particular, our model has a worst case average error of 10.53% on actual throughput values when compared to best case average errors of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's machine code analyzer when compared on three different microarchitectures, while predicting throughput values at a faster rate than aforementioned tools. We also show that Ithemal is portable, learning throughput estimation for Intel Nehalem, Haswell and Skylake microarchitectures without requiring changes to its structure.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "Researchers have recently designed a number of application-specific fault tolerance mechanisms that enable applications to either be naturally resilient to errors or include additional detection and correction steps that can bring the overall execution of an application back into an envelope for which an acceptable execution is eventually guaranteed. A major challenge to building an application that leverages these mechanisms, however, is to verify that the implementation satisfies the basic invariants that these mechanisms require--given a model of how faults may manifest during the application's execution.\n  To this end we present Leto, an SMT based automatic verification system that enables developers to verify their applications with respect to a first-class execution model specification. Namely, Leto enables software and platform developers to programmatically specify the execution semantics of the underlying hardware system as well as verify assertions about the behavior of the application's resulting execution. In this paper, we present the Leto programming language and its corresponding verification system. We also demonstrate Leto on several applications that leverage application-specific fault tolerance mechanisms.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "Researchers have recently proposed several systems that ease the process of performing Bayesian probabilistic inference. These include systems for automatic inference algorithm synthesis as well as stronger abstractions for manual algorithm development. However, existing systems whose performance relies on the developer manually constructing a part of the inference algorithm have limited support for reasoning about the correctness of the resulting algorithm.\n  In this paper, we present Shuffle, a programming language for manually developing inference procedures that 1) enforces the basic rules of probability theory, 2) enforces the statistical dependencies of the algorithm's corresponding probabilistic model, and 3) generates an optimized implementation. We have used Shuffle to develop inference algorithms for several standard probabilistic models. Our results demonstrate that Shuffle enables a developer to deliver correct and performant implementations of these algorithms.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "Though many safety-critical software systems use floating point to represent real-world input and output, programmers usually have idealized versions in mind that compute with real numbers. Significant deviations from the ideal can cause errors and jeopardize safety. Some programming systems implement exact real arithmetic, which resolves this matter but complicates others, such as decision making. In these systems, it is impossible to compute (total and deterministic) discrete decisions based on connected spaces such as $\\mathbb{R}$. We present programming-language semantics based on constructive topology with variants allowing nondeterminism and/or partiality. Either nondeterminism or partiality suffices to allow computable decision making on connected spaces such as $\\mathbb{R}$. We then introduce pattern matching on spaces, a language construct for creating programs on spaces, generalizing pattern matching in functional programming, where patterns need not represent decidable predicates and also may overlap or be inexhaustive, giving rise to nondeterminism or partiality, respectively. Nondeterminism and/or partiality also yield formal logics for constructing approximate decision procedures. We implemented these constructs in the Marshall language for exact real arithmetic.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\n  We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis\": dense, randomly-initialized feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.\n  We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "We propose a novel approach to improving software security called Cryptographic Path Hardening, which is aimed at hiding security vulnerabilities in software from attackers through the use of provably secure and obfuscated cryptographic devices to harden paths in programs.\n  By \"harden\" we mean that certain error-checking if-conditionals in a given program P are replaced by equivalent\" we mean that adversaries cannot use semi-automatic program analysis techniques to reason about the hardened program paths and thus cannot discover as-yet-unknown errors along those paths, except perhaps through black-box dictionary attacks or random testing (which we can never prevent).\n  Other than these unpreventable attack methods, we can make program analysis aimed at error-finding \"provably hard\" for a resource-bounded attacker, in the same sense that cryptographic schemes are hard to break. Unlike security-through-obscurity, in Cryptographic Path Hardening we use provably-secure crypto devices to hide errors and our mathematical arguments of security are the same as the standard ones used in cryptography.\n  One application of Cryptographic Path Hardening is that software patches or filters often reveal enough information to an attacker that they can be used to construct error-revealing inputs to exploit an unpatched version of the program. By \"hardening\" the patch we make it difficult for the attacker to analyze the patched program to construct error-revealing inputs, and thus prevent him from potentially constructing exploits.\n        \u25b3 Less", "author": "Michael Carbin"}, {"abstract": "Shot noise is an important ingredient to any measurement or theoretical modeling of discrete tracers of the large scale structure. Recent work has shown that the shot noise in the halo power spectrum becomes increasingly sub-Poissonian at high mass. Interestingly, while the halo model predicts a shot noise power spectrum in qualitative agreement with the data, it leads to an unphysical white noise in the cross halo-matter and matter power spectrum. In this work, we show that absorbing all the halo model sources of shot noise into the halo fluctuation field leads to meaningful predictions for the shot noise contributions to halo clustering statistics and remove the unphysical white noise from the cross halo-matter statistics. Our prescription straightforwardly maps onto the general bias expansion, so that the renormalized shot noise terms can be expressed as combinations of the halo model shot noises. Furthermore, we demonstrate that non-Poissonian contributions are related to volume integrals over correlation functions and their response to long-wavelength density perturbations. This leads to a new class of consistency relations for discrete tracers, which appear to be satisfied by our reformulation of the halo model. We test our theoretical predictions against measurements of halo shot noise bispectra extracted from a large suite of numerical simulations. Our model reproduces qualitatively the observed sub-Poissonian noise, although it underestimates the magnitude of this effect.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "Regular medical records are useful for medical practitioners to analyze and monitor patient health status especially for those with chronic disease, but such records are usually incomplete due to unpunctuality and absence of patients. In order to resolve the missing data problem over time, tensor-based model is suggested for missing data imputation in recent papers because this approach makes use of low rank tensor assumption for highly correlated data. However, when the time intervals between records are long, the data correlation is not high along temporal direction and such assumption is not valid. To address this problem, we propose to decompose a matrix with missing data into its latent factors. Then, the locally linear constraint is imposed on these factors for matrix completion in this paper. By using a publicly available dataset and two medical datasets collected from hospital, experimental results show that the proposed algorithm achieves the best performance by comparing with the existing methods.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "The Galactic Center black hole Sagittarius A* (Sgr A*) is a prime observing target for the Event Horizon Telescope (EHT), which can resolve the 1.3 mm emission from this source on angular scales comparable to that of the general relativistic shadow. Previous EHT observations have used visibility amplitudes to infer the morphology of the millimeter-wavelength emission. Potentially much richer source information is contained in the phases. We report on 1.3 mm phase information on Sgr A* obtained with the EHT on a total of 13 observing nights over 4 years. Closure phases, the sum of visibility phases along a closed triangle of interferometer baselines, are used because they are robust against phase corruptions introduced by instrumentation and the rapidly variable atmosphere. The median closure phase on a triangle including telescopes in California, Hawaii, and Arizona is nonzero. This result conclusively demonstrates that the millimeter emission is asymmetric on scales of a few Schwarzschild radii and can be used to break 180-degree rotational ambiguities inherent from amplitude data alone. The stability of the sign of the closure phase over most observing nights indicates persistent asymmetry in the image of Sgr A* that is not obscured by refraction due to interstellar electrons along the line of sight.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "We study the halo-matter cross bispectrum in the presence of primordial non-Gaussianity of the local type. We restrict ourselves to the squeezed limit, for which the calculation are straightforward, and perform the measurements in the initial conditions of N-body simulations, to mitigate the contamination induced by nonlinear gravitational evolution. Interestingly, the halo-matter cross bispectrum is not trivial even in this simple limit as it is strongly sensitive to the scale-dependence of the quadratic and third-order halo bias. Therefore, it can be used to test biasing prescriptions. We consider three different prescription for halo clustering: excursion set peaks (ESP), local bias and a model in which the halo bias parameters are explicitly derived from a peak-background split. In all cases, the model parameters are fully constrained with statistics other than the cross bispectrum. We measure the cross bispectrum involving one halo fluctuation field and two mass overdensity fields for various halo masses and collapse redshifts. We find that the ESP is in reasonably good agreement with the numerical data, while the other alternatives we consider fail in various cases. This suggests that the scale-dependence of halo bias also is a crucial ingredient to the squeezed limit of the halo bispectrum.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "We study the clustering of voids using $N$-body simulations and simple theoretical models. The excursion-set formalism describes fairly well the abundance of voids identified with the watershed algorithm, although the void formation threshold required is quite different from the spherical collapse value. The void cross bias $b_{\\rm c} $ is measured and its large-scale value is found to be consistent with the peak background split results. A simple fitting formula for $b_{\\rm c} $ is found. We model the void auto-power spectrum taking into account the void biasing and exclusion effect. A good fit to the simulation data is obtained for voids with radii $\\gtrsim$ 30 Mpc/$h$, especially when the void biasing model is extended to 1-loop order. However, the best-fit bias parameters do not agree well with the peak-background split results. Being able to fit the void auto-power spectrum is particularly important not only because it is the direct observable in galaxy surveys, but also our method enables us to treat the bias parameters as nuisance parameters, which are sensitive to the techniques used to identify voids.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "We investigate nonlocal Lagrangian bias contributions involving gradients of the linear density field, for which we have predictions from the excursion set peak formalism. We begin by writing down a bias expansion which includes all the bias terms, including the nonlocal ones. Having checked that the model furnishes a reasonable fit to the halo mass function, we develop a 1-point cross-correlation technique to measure bias factors associated with 2-distributed quantities. We validate the method with numerical realizations of peaks of Gaussian random fields before we apply it to N-body simulations. We focus on the lowest (quadratic) order nonlocal contributions. We can reproduce our measurement of \u03c7_{10} if we allow for an offset between the Lagrangian halo center-of-mass and the peak position. The sign and magnitude of \u03c7_{10} is consistent with Lagrangian haloes sitting near linear density maxima. The resulting contribution to the halo bias can safely be ignored for M = 10^13 Msun/h, but could become relevant at larger halo masses. For the second nonlocal bias \u03c7_{01} however, we measure a much larger magnitude than predicted by our model. We speculate that some of this discrepancy might originate from nonlocal Lagrangian contributions induced by nonspherical collapse.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "Let $E \\subseteq R^n$ be a closed set of Hausdorff dimension $\u03b1$. For $m \\geq n$, let $\\{B_1,\\ldots,B_k\\}$ be $n \\times (m-n)$ matrices. We prove that if the system of matrices $B_j$ is non-degenerate in a suitable sense, $\u03b1$ is sufficiently close to $n$, and if $E$ supports a probability measure obeying appropriate dimensionality and Fourier decay conditions, then for a range of $m$ depending on $n$ and $k$, the set $E$ contains a translate of a non-trivial $k$-point configuration $\\{B_1y,\\ldots,B_ky\\}$. As a consequence, we are able to establish existence of certain geometric configurations in Salem sets (such as parallelograms in $ R^n$ and isosceles right triangles in $R^2$). This can be viewed as a multidimensional analogue of an earlier result of Laba and Pramanik on 3-term arithmetic progressions in subsets of $R$.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "We explore the scale dependence of halo bias using real space cross-correlation measurements in N-body simulations and in Pinocchio, an algorithm based on Lagrangian Perturbation Theory. Recent work has shown how to interpret such real space measurements in terms of k-dependent bias in Fourier space, and how to remove the k-dependence to reconstruct the k-independent peak-background split halo bias parameters. We compare our reconstruction of the linear bias, which requires no free parameters, with previous estimates from N-body simulations which were obtained directly in Fourier space at large scales, and find very good agreement. Our reconstruction of the quadratic bias is similarly parameter-free, although in this case there are no previous Fourier space measurements to compare with. Our analysis of N-body simulations explicitly tests the predictions of the excursion set peaks (ESP) formalism of Paranjape et al. (2013) for the scale dependence of bias; we find that the ESP predictions accurately describe our measurements. In addition, our measurements in Pinocchio serve as a useful, successful consistency check between Pinocchio and N-body simulations that is not accessible to traditional measurements.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "The Orange \"Data for Development\" (D4D) challenge is an open data challenge on anonymous call patterns of Orange's mobile phone users in Ivory Coast. The goal of the challenge is to help address society development questions in novel ways by contributing to the socio-economic development and well-being of the Ivory Coast population. Participants to the challenge are given access to four mobile phone datasets and the purpose of this paper is to describe the four datasets. The website http://www.d4d.orange.com contains more information about the participation rules. The datasets are based on anonymized Call Detail Records (CDR) of phone calls and SMS exchanges between five million of Orange's customers in Ivory Coast between December 1, 2011 and April 28, 2012. The datasets are: (a) antenna-to-antenna traffic on an hourly basis, (b) individual trajectories for 50,000 customers for two week time windows with antenna location information, (3) individual trajectories for 500,000 customers over the entire observation period with sub-prefecture location information, and (4) a sample of communication graphs for 5,000 customers\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "We study the spindown of isolated neutron stars from initially rapid rotation rates, driven by two factors: (i) gravitational wave emission due to r-modes and (ii) magnetic braking. In the context of isolated neutron stars, we present the first study including self-consistently the magnetic damping of r-modes in the spin evolution. We track the spin evolution employing the RNS code, which accounts for the rotating structure of neutron stars for various equations of state. We find that, despite the strong damping due to the magnetic field, r-modes alter the braking rate from pure magnetic braking for B<10^{13}G. For realistic values of the saturation amplitude, the r-mode can also decrease the time to reach the threshold central density for quark deconfinement. Within a phenomenological model, we assess the gravitational waveform that would result from r-mode driven spindown of a magnetized neutron star. To contrast with the persistent signal during the spindown phase, we also present a preliminary estimate of the transient gravitational wave signal from an explosive quark-hadron phase transition, which can be a signal for the deconfinement of quarks inside neutron stars.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "Due to hardware and computational constraints, wireless sensor networks (WSNs) normally do not take measurements of time-of-arrival or time-difference-of-arrival for rangebased localization. Instead, WSNs in some applications use rangefree localization for simple but less accurate determination of sensor positions. A well-known algorithm for this purpose is the centroid algorithm. This paper presents a range-free localization technique based on the radical line of intersecting circles. This technique provides greater accuracy than the centroid algorithm, at the expense of a slight increase in computational load. Simulation results show that for the scenarios studied, the radical line method can give an approximately 2 to 30% increase in accuracy over the centroid algorithm, depending on whether or not the anchors have identical ranges, and on the value of DOI.\n        \u25b3 Less", "author": "Vincent Chan"}, {"abstract": "The growing popularity of cloud-based machine learning raises a natural question about the privacy guarantees that can be provided in such a setting. Our work tackles this problem in the context where a client wishes to classify private images using a convolutional neural network (CNN) trained by a server. Our goal is to build efficient protocols whereby the client can acquire the classification result without revealing their input to the server, while guaranteeing the privacy of the server's neural network.\n  To this end, we design Gazelle, a scalable and low-latency system for secure neural network inference, using an intricate combination of homomorphic encryption and traditional two-party computation techniques (such as garbled circuits). Gazelle makes three contributions. First, we design the Gazelle homomorphic encryption library which provides fast algorithms for basic homomorphic operations such as SIMD (single instruction multiple data) addition, SIMD multiplication and ciphertext permutation. Second, we implement the Gazelle homomorphic linear algebra kernels which map neural network layers to optimized homomorphic matrix-vector multiplication and convolution routines. Third, we design optimized encryption switching protocols which seamlessly convert between homomorphic and garbled circuit encodings to enable implementation of complete neural network inference.\n  We evaluate our protocols on benchmark neural networks trained on the MNIST and CIFAR-10 datasets and show that Gazelle outperforms the best existing systems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint 2017/1164) by 30 times in online runtime. Similarly when compared with fully homomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders of magnitude faster online run-time.\n        \u25b3 Less", "author": "Anantha Chandrakasan"}, {"abstract": "Ultra-wideband (UWB) communication is an emerging wireless technology that promises high data rates over short distances and precise locationing. The large available bandwidth and the constraint of a maximum power spectral density drives a unique set of system challenges. This paper addresses these challenges using two UWB transceivers and a discrete prototype platform.\n        \u25b3 Less", "author": "Anantha Chandrakasan"}, {"abstract": "Wireless microsensor networks, which have been the topic of intensive research in recent years, are now emerging in industrial applications. An important milestone in this transition has been the release of the IEEE 802.15.4 standard that specifies interoperable wireless physical and medium access control layers targeted to sensor node radios. In this paper, we evaluate the potential of an 802.15.4 radio for use in an ultra low power sensor node operating in a dense network. Starting from measurements carried out on the off-the-shelf radio, effective radio activation and link adaptation policies are derived. It is shown that, in a typical sensor network scenario, the average power per node can be reduced down to 211m mm mW. Next, the energy consumption breakdown between the different phases of a packet transmission is presented, indicating which part of the transceiver architecture can most effectively be optimized in order to further reduce the radio power, enabling self-powered wireless microsensor networks.\n        \u25b3 Less", "author": "Anantha Chandrakasan"}, {"abstract": "Though many safety-critical software systems use floating point to represent real-world input and output, programmers usually have idealized versions in mind that compute with real numbers. Significant deviations from the ideal can cause errors and jeopardize safety. Some programming systems implement exact real arithmetic, which resolves this matter but complicates others, such as decision making. In these systems, it is impossible to compute (total and deterministic) discrete decisions based on connected spaces such as $\\mathbb{R}$. We present programming-language semantics based on constructive topology with variants allowing nondeterminism and/or partiality. Either nondeterminism or partiality suffices to allow computable decision making on connected spaces such as $\\mathbb{R}$. We then introduce pattern matching on spaces, a language construct for creating programs on spaces, generalizing pattern matching in functional programming, where patterns need not represent decidable predicates and also may overlap or be inexhaustive, giving rise to nondeterminism or partiality, respectively. Nondeterminism and/or partiality also yield formal logics for constructing approximate decision procedures. We implemented these constructs in the Marshall language for exact real arithmetic.\n        \u25b3 Less", "author": "Adam Chlipala"}, {"abstract": "It is a neat result from functional programming that libraries of parser combinators can support rapid construction of decoders for quite a range of formats. With a little more work, the same combinator program can denote both a decoder and an encoder. Unfortunately, the real world is full of gnarly formats, as with the packet formats that make up the standard Internet protocol stack. Most past parser-combinator approaches cannot handle these formats, and the few exceptions require redundancy -- one part of the natural grammar needs to be hand-translated into hints in multiple parts of a parser program. We show how to recover very natural and nonredundant format specifications, covering all popular network packet formats and generating both decoders and encoders automatically. The catch is that we use the Coq proof assistant to derive both kinds of artifacts using tactics, automatically, in a way that guarantees that they form inverses of each other. We used our approach to reimplement packet processing for a full Internet protocol stack, inserting our replacement into the OCaml-based MirageOS unikernel, resulting in minimal performance degradation.\n        \u25b3 Less", "author": "Adam Chlipala"}, {"abstract": "We describe our experience implementing a broad category-theory library in Coq. Category theory and computational performance are not usually mentioned in the same breath, but we have needed substantial engineering effort to teach Coq to cope with large categorical constructions without slowing proof script processing unacceptably. In this paper, we share the lessons we have learned about how to represent very abstract mathematical objects and arguments in Coq and how future proof assistants might be designed to better support such reasoning. One particular encoding trick to which we draw attention allows category-theoretic arguments involving duality to be internalized in Coq's logic with definitional equality. Ours may be the largest Coq development to date that uses the relatively new Coq version developed by homotopy type theorists, and we reflect on which new features were especially helpful.\n        \u25b3 Less", "author": "Adam Chlipala"}, {"abstract": "We describe a method for building composable and extensible verification procedures within the Coq proof assistant. Unlike traditional methods that rely on run-time generation and checking of proofs, we use verified-correct procedures with Coq soundness proofs. Though they are internalized in Coq's logic, our provers support sound extension by users with hints over new domains, enabling automated reasoning about user-defined abstract predicates. We maintain soundness by developing an architecture for modular packaging, construction, and composition of hint databases, which had previously only been implemented in Coq at the level of its dynamically typed, proof-generating tactic language. Our provers also include rich handling of unification variables, enabling integration with other tactic-based deduction steps within Coq. We have implemented our techniques in MirrorShard, an open-source framework for reflective verification. We demonstrate its applicability by instantiating it to separation logic in order to reason about imperative program verification.\n        \u25b3 Less", "author": "Adam Chlipala"}, {"abstract": "We report on the implementation of a certified compiler for a high-level hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs). Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq. The target language of the compiler corresponds to a synthesisable subset of Verilog or VHDL. A key aspect of our approach is that input programs to the compiler can be defined and proved correct inside Coq. Then, we use extraction and a Verilog back-end (written in OCaml) to get a certified version of a hardware design.\n        \u25b3 Less", "author": "Adam Chlipala"}, {"abstract": "While quantum devices rely on interactions between constituent subsystems and with their environment to operate, native interactions alone often fail to deliver targeted performance. Coherent pulsed control provides the ability to tailor effective interactions, known as Hamiltonian engineering. We propose a Hamiltonian engineering method that maximizes desired interactions while mitigating deleterious ones by conducting a pulse sequence search using constrained optimization. The optimization formulation incorporates pulse sequence length and cardinality penalties consistent with linear or integer programming. We apply the general technique to magnetometry with solid state spin ensembles in which inhomogeneous interactions between sensing spins limit coherence. Defining figures of merit for broadband Ramsey magnetometry, we present novel pulse sequences which outperform known techniques for homonuclear spin decoupling in both spin-1/2 and spin-1 systems. When applied to nitrogen vacancy (NV) centers in diamond, this scheme partially preserves the Zeeman interaction while zeroing dipolar coupling between negatively charged NV$^{\\text -}$ centers. Such a scheme is of interest for NV$^\\text{-}$ magnetometers which have reached the NV$^\\text{-}$-NV$^\\text{-}$ coupling limit. We discuss experimental implementation in NV ensembles, as well as applicability of the current approach to more general spin bath decoupling and superconducting qubit control.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Compared to humans, machine learning models generally require significantly more training examples and fail to extrapolate from experience to solve previously unseen challenges. To help close this performance gap, we augment single-task neural networks with a meta-recognition model which learns a succinct model code via its autoencoder structure, using just a few informative examples. The model code is then employed by a meta-generative model to construct parameters for the task-specific model. We demonstrate that for previously unseen tasks, without additional training, this Meta-Learning Autoencoder (MeLA) framework can build models that closely match the true underlying models, with loss significantly lower than given by fine-tuned baseline networks, and performance that compares favorably with state-of-the-art meta-learning algorithms. MeLA also adds the ability to identify influential training examples and predict which additional data will be most valuable to acquire to improve model prediction.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Each time a learner in a self-paced online course is trying to answer an assessment question, it takes some time to submit the answer, and if multiple attempts are allowed and the first answer was incorrect, it takes some time to submit the second attempt, and so on. Here we study the distribution of such \"response times\". We find that the log-normal statistical model for such times, previously suggested in the literature, holds for online courses qualitatively. Users who, according to this model, tend to take longer on submits are more likely to complete the course, have a higher level of engagement and achieve a higher grade. This finding can be the basis for designing interventions in online courses, such as MOOCs, which would encourage some users to slow down.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We present a novel set of reversible modular multipliers applicable to quantum computing, derived from three classical techniques: 1) traditional integer division, 2) Montgomery residue arithmetic, and 3) Barrett reduction. Each multiplier computes an exact result for all binary input values, while maintaining the asymptotic resource complexity of a single (non-modular) integer multiplier. We additionally conduct an empirical resource analysis of our designs in order to determine the total gate count and circuit depth of each fully constructed circuit, with inputs as large as 2048 bits. Our comparative analysis considers both circuit implementations which allow for arbitrary (controlled) rotation gates, as well as those restricted to a typical fault-tolerant gate set.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We establish a symmetry-operator framework for designing quantum error correcting~(QEC) codes based on fundamental properties of the underlying system dynamics. Based on this framework, we propose three hardware-efficient bosonic QEC codes that are suitable for $\u03c7^{(2)}$-interaction based quantum computation: the $\u03c7^{(2)}$ parity-check code, the $\u03c7^{(2)}$ embedded error-correcting code, and the $\u03c7^{(2)}$ binomial code, all of which detect photon-loss or photon-gain errors by means of photon-number parity measurements and then correct them via $\u03c7^{(2)}$ Hamiltonian evolutions and linear-optics transformations. Our symmetry-operator framework provides a systematic procedure for finding QEC codes that are not stabilizer codes. The $\u03c7^{(2)}$ binomial code is of special interest because, with $m\\le N$ identified from channel monitoring, it can correct $m$-photon loss errors, $m$-photon gain errors, and $(m-1)$th-order dephasing errors using logical qudits that are encoded in $O(N)$ photons. In comparison, other bosonic QEC codes require $O(N^2)$ photons to correct the same degree of bosonic errors. Such improved photon-efficiency underscores the additional error-correction power that can be provided by channel monitoring. We develop quantum Hamming bounds for photon-loss errors in the code subspaces associated with the $\u03c7^{(2)}$ parity-check code and the $\u03c7^{(2)}$ embedded error-correcting code, and we prove that these codes saturate their respective bounds. Our $\u03c7^{(2)}$ QEC codes exhibit hardware efficiency in that they address the principal error mechanisms and exploit the available physical interactions of the underlying hardware, thus reducing the physical resources required for implementing their encoding, decoding, and error-correction operations, and their universal encoded-basis gate sets.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "The exponential speedups promised by Hamiltonian simulation on a quantum computer depends crucially on structure in both the Hamiltonian $\\hat{H}$, and the quantum circuit $\\hat{U}$ that encodes its description. In the quest to better approximate time-evolution $e^{-i\\hat{H}t}$ with error $\u03b5$, we motivate a systematic approach to understanding and exploiting structure, in a setting where Hamiltonians are encoded as measurement operators of unitary circuits $\\hat{U}$ for generalized measurement. This allows us to define a \\emph{uniform spectral amplification} problem on this framework for expanding the spectrum of encoded Hamiltonian with exponentially small distortion. We present general solutions to uniform spectral amplification in a hierarchy where factoring $\\hat{U}$ into $n=1,2,3$ unitary oracles represents increasing structural knowledge of the encoding. Combined with structural knowledge of the Hamiltonian, specializing these results allow us simulate time-evolution by $d$-sparse Hamiltonians using $\\mathcal{O}\\left(t(d \\|\\hat H\\|_{\\text{max}}\\|\\hat H\\|_{1})^{1/2}\\log{(t\\|\\hat{H}\\|/\u03b5)}\\right)$ queries, where $\\|\\hat H\\|\\le \\|\\hat H\\|_1\\le d\\|\\hat H\\|_{\\text{max}}$. Up to logarithmic factors, this is a polynomial improvement upon prior art using $\\mathcal{O}\\left(td\\|\\hat H\\|_{\\text{max}}+\\frac{\\log{(1/\u03b5)}}{\\log\\log{(1/\u03b5)}}\\right)$ or $\\mathcal{O}(t^{3/2}(d \\|\\hat H\\|_{\\text{max}}\\|\\hat H\\|_{1}\\|\\hat H\\|/\u03b5)^{1/2})$ queries. In the process, we also prove a matching lower bound of $\u03a9(t(d\\|\\hat H\\|_{\\text{max}}\\|\\hat H\\|_{1})^{1/2})$ queries, present a distortion-free generalization of spectral gap amplification, and an amplitude amplification algorithm that performs multiplication on unknown state amplitudes.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "A non-Clifford gate is required for universal quantum computation, and, typically, this is the most error-prone and resource intensive logical operation on an error-correcting code. Small, single-qubit rotations are popular choices for this non-Clifford gate, but certain three-qubit gates, such as Toffoli or controlled-controlled-Z (CCZ), are equivalent options that are also more suited for implementing some quantum algorithms, for instance those with coherent classical subroutines. Here, we calculate error rates and resource overheads for implementing logical CCZ with pieceable fault-tolerance, a non-transversal method for implementing logical gates. We provide a comparison with a non-local magic-state scheme on a concatenated code and a local magic-state scheme on the surface code. We find the pieceable fault-tolerance scheme particularly advantaged over magic states on concatenated codes and in certain regimes over magic states on the surface code. Our results suggest that pieceable fault-tolerance is a promising candidate for fault-tolerance in a near-future quantum computer.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Noisy PN learning is the problem of binary classification when training examples may be mislabeled (flipped) uniformly with noise rate rho1 for positive examples and rho0 for negative examples. We propose Rank Pruning (RP) to solve noisy PN learning and the open problem of estimating the noise rates, i.e. the fraction of wrong positive and negative labels. Unlike prior solutions, RP is time-efficient and general, requiring O(T) for any unrestricted choice of probabilistic classifier with T fitting time. We prove RP has consistent noise estimation and equivalent expected risk as learning with uncorrupted labels in ideal conditions, and derive closed-form solutions when conditions are non-ideal. RP achieves state-of-the-art noise estimation and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the amount of noise and performs similarly impressively when a large portion of training examples are noise drawn from a third distribution. To highlight, RP with a CNN classifier can predict if an MNIST digit is a \"one\"or \"not\" with only 0.25% error, and 0.46 error across all digits, even when 50% of positive examples are mislabeled and 50% of observed positive labels are mislabeled negative examples.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We prove that universal quantum computation can be realized---using only linear optics and $\u03c7^{(2)}$ (three-wave mixing) interactions---in any $(n+1)$-dimensional qudit basis of the $n$-pump-photon subspace. First, we exhibit a strictly universal gate set for the qubit basis in the one-pump-photon subspace. Next, we demonstrate qutrit-basis universality by proving that $\u03c7^{(2)}$ Hamiltonians and photon-number operators generate the full $\\mathfrak{u}(3)$ Lie algebra in the two-pump-photon subspace, and showing how the qutrit controlled-$Z$ gate can be implemented with only linear optics and $\u03c7^{(2)}$ interactions. We then use proof by induction to obtain our general qudit result. Our induction proof relies on coherent photon injection/subtraction, a technique enabled by $\u03c7^{(2)}$ interaction between the encoding modes and ancillary modes. Finally, we show that coherent photon injection is more than a conceptual tool in that it offers a route to preparing high-photon-number Fock states from single-photon Fock states.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Given a Hermitian operator $\\hat{H}=\\langle G|\\hat{U}|G\\rangle$ that is the projection of an oracle $\\hat{U}$ by state $|G\\rangle$ created with oracle $\\hat{G}$, the problem of Hamiltonian simulation is approximating the time evolution operator $e^{-i\\hat{H}t}$ at time $t$ with error $\u03b5$. We show that this can be done with query complexity $\\mathcal{O}\\big(t+\\frac{\\log{(1/\u03b5)}}{\\log\\log{(1/\u03b5)}}\\big)$ to $\\hat{G},\\hat{U}$ that is optimal, not just in asymptotic limits, but for all values $t,\u03b5$. Furthermore, only $2$ additional ancilla qubits are required in total, together with $\\mathcal{O}(1)$ additional single and two-qubit gates per query. Our approach to Hamiltonian simulation subsumes important prior art considering Hamiltonians which are $d$-sparse or a linear combination of unitaries, leading to significant improvements in space complexity, as well as a quadratic speed-up for precision simulations. It also motivates useful new instances, such as where $\\langle G|\\hat{U}|G\\rangle$ is a density matrix. A key technical result is `qubitization' which uses controlled-$\\hat{U}$ and controlled-$\\hat{G}$ to embed $\\hat{H}$ in an invariant $\\text{SU}(2)$ subspace. A large class of operator functions of $\\hat{H}$ can then be computed with optimal query complexity, of which $e^{-i\\hat{H}t}$ is a special case.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Fixed-point quantum search algorithms succeed at finding one of $M$ target items among $N$ total items even when the run time of the algorithm is longer than necessary. While the famous Grover's algorithm can search quadratically faster than a classical computer, it lacks the fixed-point property --- the fraction of target items must be known precisely to know when to terminate the algorithm. Recently, Yoder, Low, and Chuang gave an optimal gate-model search algorithm with the fixed-point property. Meanwhile, it is known that an adiabatic quantum algorithm, operating by continuously varying a Hamiltonian, can reproduce the quadratic speedup of gate-model Grover search. We ask, can an adiabatic algorithm also reproduce the fixed-point property? We show that the answer depends on what interpolation schedule is used, so as in the gate model, there are both fixed-point and non-fixed-point versions of adiabatic search, only some of which attain the quadratic quantum speedup. Guided by geometric intuition on the Bloch sphere, we rigorously justify our claims with an explicit upper bound on the error in the adiabatic approximation. We also show that the fixed-point adiabatic search algorithm can be simulated in the gate model with neither loss of the quadratic Grover speedup nor of the fixed-point property. Finally, we discuss natural uses of fixed-point algorithms such as preparation of a relatively prime state and oblivious amplitude amplification.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "The physics of quantum mechanics is the inspiration for, and underlies, quantum computation. As such, one expects physical intuition to be highly influential in the understanding and design of many quantum algorithms, particularly simulation of physical systems. Surprisingly, this has been challenging, with current Hamiltonian simulation algorithms remaining abstract and often the result of sophisticated but unintuitive constructions. We contend that physical intuition can lead to optimal simulation methods by showing that a focus on simple single-qubit rotations elegantly furnishes an optimal algorithm for Hamiltonian simulation, a universal problem that encapsulates all the power of quantum computation. Specifically, we show that the query complexity of implementing time evolution by a $d$-sparse Hamiltonian $\\hat{H}$ for time-interval $t$ with error $\u03b5$ is $\\mathcal{O}(td\\|\\hat{H}\\|_{\\text{max}}+\\frac{\\log{(1/\u03b5)}}{\\log{\\log{(1/\u03b5)}}})$, which matches lower bounds in all parameters. This connection is made through general three-step \"quantum signal processing\" methodology, comprised of (1) transducing eigenvalues of $\\hat{H}$ into a single ancilla qubit, (2) transforming these eigenvalues through an optimal-length sequence of single-qubit rotations, and (3) projecting this ancilla with near unity success probability.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Classical imaging works by scattering photons from an object to be imaged, and achieves resolution scaling as $1/\\sqrt{t}$, with $t$ the imaging time. By contrast, the laws of quantum mechanics allow one to utilize quantum coherence to obtain imaging resolution that can scale as quickly as $1/t$ -- the so-called \"Heisenberg limit.\" However, ambiguities in the obtained signal often preclude taking full advantage of this quantum enhancement, while imaging techniques designed to be unambiguous often lose this optimal Heisenberg scaling. Here, we demonstrate an imaging technique which combines unambiguous detection of the target with Heisenberg scaling of the resolution. We also demonstrate a binary search algorithm which can efficiently locate a coherent target using the technique, resolving a target trapped ion to within 0.3% of the $1/e^2$ diameter of the excitation beam.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We report on a method for measuring the branching ratios of dipole transitions of trapped atomic ions by performing nested sequences of population inversions. This scheme is broadly applicable and does not use ultrafast pulsed or narrow linewidth lasers. It is simple to perform and insensitive to experimental variables such as laser and magnetic field noise as well as ion heating. To demonstrate its effectiveness, we make the most accurate measurements thus far of the branching ratios of both 5P1/2 and 5P3/2 states in 88Sr+ with sub-1% uncertainties. We measure 17.175(27) for the branching ratio of 5P1/2-5S1/2, 15.845(71) for 5P3/2-5S1/2, and 0.05609(21) for 5P3/2-4D5/2, ten- fold and thirty-fold improvements in precision for 5P1/2 and 5P3/2 branching ratios respectively over the best previous experimental values.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "The creation of composite quantum gates that implement quantum response functions $\\hat{U}(\u03b8)$ dependent on some parameter of interest $\u03b8$ is often more of an art than a science. Through inspired design, a sequence of $L$ primitive gates also depending on $\u03b8$ can engineer a highly nontrivial $\\hat{U}(\u03b8)$ that enables myriad precision metrology, spectroscopy, and control techniques. However, discovering new, useful examples of $\\hat{U}(\u03b8)$ requires great intuition to perceive the possibilities, and often brute-force to find optimal implementations. We present a systematic and efficient methodology for composite gate design of arbitrary length, where phase-controlled primitive gates all rotating by $\u03b8$ act on a single spin. We fully characterize the realizable family of $\\hat{U}(\u03b8)$, provide an efficient algorithm that decomposes a choice of $\\hat{U}(\u03b8)$ into its shortest sequence of gates, and show how to efficiently choose an achievable $\\hat{U}(\u03b8)$ that for fixed $L$, is an optimal approximation to objective functions on its quadratures. A strong connection is forged with \\emph{classical} discrete-time signal processing, allowing us to swiftly construct, as examples, compensated gates with optimal bandwidth that implement arbitrary single spin rotations with sub-wavelength spatial selectivity.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "It is an oft-cited fact that no quantum code can support a set of fault-tolerant logical gates that is both universal and transversal. This no-go theorem is generally responsible for the interest in alternative universality constructions including magic state distillation. Widely overlooked, however, is the possibility of non-transversal, yet still fault-tolerant, gates that work directly on small quantum codes. Here we demonstrate precisely the existence of such gates. In particular, we show how the limits of non-transversality can be overcome by performing rounds of intermediate error-correction to create logical gates on stabilizer codes that use no ancillas other than those required for syndrome measurement. Moreover, the logical gates we construct, the most prominent examples being Toffoli and controlled-controlled-Z, often complete universal gate sets on their codes. We detail such universal constructions for the smallest quantum codes, the 5-qubit and 7-qubit codes, and then proceed to generalize the approach. One remarkable result of this generalization is that any nondegenerate stabilizer code with a complete set of fault-tolerant single-qubit Clifford gates has a universal set of fault-tolerant gates. Another is the interaction of logical qubits across different stabilizer codes, which, for instance, implies a broadly applicable method of code switching.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We describe a cheating strategy enabled by the features of massive open online courses (MOOCs) and detectable by virtue of the sophisticated data systems that MOOCs provide. The strategy, Copying Answers using Multiple Existences Online (CAMEO), involves a user who gathers solutions to assessment questions using a \"harvester\" account and then submits correct answers using a separate \"master\" account. We use \"clickstream\" learner data to detect CAMEO use among 1.9 million course participants in 115 MOOCs from two universities. Using conservative thresholds, we estimate CAMEO prevalence at 1,237 certificates, accounting for 1.3% of the certificates in the 69 MOOCs with CAMEO users. Among earners of 20 or more certificates, 25% have used the CAMEO strategy. CAMEO users are more likely to be young, male, and international than other MOOC certificate earners. We identify preventive strategies that can decrease CAMEO rates and show evidence of their effectiveness in science courses.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Quantum computers are able to outperform classical algorithms. This was long recognized by the visionary Richard Feynman who pointed out in the 1980s that quantum mechanical problems were better solved with quantum machines. It was only in 1994 that Peter Shor came up with an algorithm that is able to calculate the prime factors of a large number vastly more efficiently than known possible with a classical computer. This paradigmatic algorithm stimulated the flourishing research in quantum information processing and the quest for an actual implementation of a quantum computer. Over the last fifteen years, using skillful optimizations, several instances of a Shor algorithm have been implemented on various platforms and clearly proved the feasibility of quantum factoring. For general scalability, though, a different approach has to be pursued. Here, we report the realization of a fully scalable Shor algorithm as proposed by Kitaev. For this, we demonstrate factoring the number fifteen by effectively employing and controlling seven qubits and four \"cache-qubits\", together with the implementation of generalized arithmetic operations, known as modular multipliers. The scalable algorithm has been realized with an ion-trap quantum computer exhibiting success probabilities in excess of 90%.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We study the vacuum-induced degradation of high-finesse optical cavities with mirror coatings composed of SiO$_2$-Ta$_{2}$O$_{5}$ dielectric stacks, and present methods to protect these coatings and to recover their initial quality factor. For separate coatings with reflectivities centered at 370 nm and 422 nm, a vacuum-induced continuous increase in optical loss occurs if the surface-layer coating is made of Ta$_{2}$O$_{5}$, while it does not occur if it is made of SiO$_2$. The incurred optical loss can be reversed by filling the vacuum chamber with oxygen at atmospheric pressure, and the recovery rate can be strongly accelerated by continuous laser illumination at 422 nm. Both the degradation and the recovery processes depend strongly on temperature. We find that a 1 nm-thick layer of SiO$_2$ passivating the Ta$_{2}$O$_{5}$ surface layer is sufficient to reduce the degradation rate by more than a factor of 10, strongly supporting surface oxygen depletion as the primary degradation mechanism.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Scaling-up from prototype systems to dense arrays of ions on chip, or vast networks of ions connected by photonic channels, will require developing entirely new technologies that combine miniaturized ion trapping systems with devices to capture, transmit and detect light, while refining how ions are confined and controlled. Building a cohesive ion system from such diverse parts involves many challenges, including navigating materials incompatibilities and undesired coupling between elements. Here, we review our recent efforts to create scalable ion systems incorporating unconventional materials such as graphene and indium tin oxide, integrating devices like optical fibers and mirrors, and exploring alternative ion loading and trapping techniques.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Conventional wisdom dictates that to image the position of fluorescent atoms or molecules, one should stimulate as much emission and collect as many photons as possible. That is, in this classical case, it has always been assumed that the coherence time of the system should be made short, and that the statistical scaling $\\sim1/\\sqrt{t}$ defines the resolution limit for imaging time $t$. However, here we show in contrast that given the same resources, a long coherence time permits a higher resolution image. In this quantum regime, we give a procedure for determining the position of a single two-level system, and demonstrate that the standard errors of our position estimates scale at the Heisenberg limit as $\\sim 1/t$, a quadratic, and notably optimal, improvement over the classical case.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Grover's quantum search and its generalization, quantum amplitude amplification, provide quadratic advantage over classical algorithms for a diverse set of tasks, but are tricky to use without knowing beforehand what fraction $\u03bb$ of the initial state is comprised of the target states. In contrast, fixed-point search algorithms need only a reliable lower bound on this fraction, but, as a consequence, lose the very quadratic advantage that makes Grover's algorithm so appealing. Here we provide the first version of amplitude amplification that achieves fixed-point behavior without sacrificing the quantum speedup. Our result incorporates an adjustable bound on the failure probability, and, for a given number of oracle queries, guarantees that this bound is satisfied over the broadest possible range of $\u03bb$.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Performing exact inference on Bayesian networks is known to be #P-hard. Typically approximate inference techniques are used instead to sample from the distribution on query variables given the values $e$ of evidence variables. Classically, a single unbiased sample is obtained from a Bayesian network on $n$ variables with at most $m$ parents per node in time $\\mathcal{O}(nmP(e)^{-1})$, depending critically on $P(e)$, the probability the evidence might occur in the first place. By implementing a quantum version of rejection sampling, we obtain a square-root speedup, taking $\\mathcal{O}(n2^mP(e)^{-\\frac12})$ time per sample. We exploit the Bayesian network's graph structure to efficiently construct a quantum state, a q-sample, representing the intended classical distribution, and also to efficiently apply amplitude amplification, the source of our speedup. Thus, our speedup is notable as it is unrelativized -- we count primitive operations and require no blackbox oracle queries.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Massive Open Online Courses are an exciting new avenue for instruction and research, yet they are full of unknowns. In the Spring of 2013, MITx released its first introductory physics MOOC through the edX platform, generating a total enrollment of 43,000 students from around the world. We describe the population of participants in terms of their age, gender, level of education, and country of origin, highlighting both the diversity of 8.02x enrollees as well as gender gap and retention. Using three midterm exams and the final as waypoints, we highlight performance by different demographic subpopulations and their retention rates. Our work is generally aimed at making a bridge between available MOOC data and topics associated with the Physics Education Research community.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Implementing a single qubit unitary is often hampered by imperfect control. Systematic amplitude errors $\u03b5$, caused by incorrect duration or strength of a pulse, are an especially common problem. But a sequence of imperfect pulses can provide a better implementation of a desired operation, as compared to a single primitive pulse. We find optimal pulse sequences consisting of $L$ primitive $\u03c0$ or $2\u03c0$ rotations that suppress such errors to arbitrary order $\\mathcal{O}(\u03b5^{n})$ on arbitrary initial states. Optimality is demonstrated by proving an $L=\\mathcal{O}(n)$ lower bound and saturating it with $L=2n$ solutions. Closed-form solutions for arbitrary rotation angles are given for $n=1,2,3,4$. Perturbative solutions for any $n$ are proven for small angles, while arbitrary angle solutions are obtained by analytic continuation up to $n=12$. The derivation proceeds by a novel algebraic and non-recursive approach, in which finding amplitude error correcting sequences can be reduced to solving polynomial equations.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We present a novel hybrid system where an optical cavity is integrated with a microfabricated planar-electrode ion trap. The trap electrodes produce a tunable periodic potential allowing the trapping of up to 50 separate ion chains spaced by 160 $\u03bc$m along the cavity axis. Each chain can contain up to 20 individually addressable Yb\\textsuperscript{+} ions coupled to the cavity mode. We demonstrate deterministic distribution of ions between the sites of the electrostatic periodic potential and control of the ion-cavity coupling. The measured strength of this coupling should allow access to the strong collective coupling regime with $\\lesssim$10 ions. The optical cavity could serve as a quantum information bus between ions or be used to generate a strong wavelength-scale periodic optical potential.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Fluorescence collection sets the efficiency of state detection and the rate of entanglement generation between remote trapped ion qubits. Despite efforts to improve light collection using various optical elements, solid angle capture is limited to ~10% for implementations that are scalable to many ions. We present an approach based on fluorescence detection through a transparent trap using an integrated photodetector, combining collection efficiency approaching 50% with scalability. We microfabricate transparent surface traps with indium tin oxide and verify stable trapping of single ions. The fluorescence from a cloud of ions is detected using a photodiode sandwiched with a transparent trap.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Fermions, as a major class of quantum particles, provide platforms for quantum information processing beyond the possibilities of spins or bosons which have been studied more extensively. One particularly interesting model to study, in view of recent progress in manipulating ultracold fermion gases, is the fermionic version of measurement-based quantum computation (MBQC), which implements full quantum computation with only single site measurements on a proper fermionic many-body resource state. However, it is not known which fermionic states can be used as the resource states for MBQC and how to find them. In this paper, we generalize the framework of spin MBQC to fermions. In particular, we provide a general formalism to construct many-body entangled fermion resource states for MBQC based on the fermionic projected entangled pair state representation. We give a specific fermionic state which enables universal MBQC and demonstrate that the non-locality inherent in fermion systems can be properly taken care of with suitable measurement schemes. Such a framework opens up possibilities of finding MBQC resource states which can be more readily realized in the lab.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Previous analyses of conditional \u03c6-phase gates for photonic qubits that treat cross-phase modulation (XPM) in a causal, multimode, quantum field setting suggest that a large (~\u03c0rad) nonlinear phase shift is always accompanied by fidelity-degrading noise [J. H. Shapiro, Phys. Rev. A 73, 062305 (2006); J. Gea-Banacloche, Phys. Rev. A 81, 043823 (2010)]. Using an atomic V-system to model an XPM medium, we present a conditional phase gate that, for sufficiently small nonzero \u03c6, has high fidelity. The gate is made cascadable by using using a special measurement, principal mode projection, to exploit the quantum Zeno effect and preclude the accumulation of fidelity-degrading departures from the principal-mode Hilbert space when both control and target photons illuminate the gate.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We model electric field noise from fluctuating patch potentials on conducting surfaces by taking into account the finite geometry of the ion trap electrodes to gain insight into the origin of anomalous heating in ion traps. The scaling of anomalous heating rates with surface distance, $d$, is obtained for several generic geometries of relevance to current ion trap designs, ranging from planar to spheroidal electrodes. The influence of patch size is studied both by solving Laplace's equation in terms of the appropriate Green's function as well as through an eigenfunction expansion. Scaling with surface distance is found to be highly dependent on the choice of geometry and the relative scale between the spatial extent of the electrode, the ion-electrode distance, and the patch size. Our model generally supports the $d^{-4}$ dependence currently found by most experiments and models, but also predicts geometry-driven deviations from this trend.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Electrical charging of metal surfaces due to photoelectric generation of carriers is of concern in trapped ion quantum computation systems, due to the high sensitivity of the ions' motional quantum states to deformation of the trapping potential. The charging induced by typical laser frequencies involved in doppler cooling and quantum control is studied here, with microfabricated surface electrode traps made of aluminum, copper, and gold, operated at 6 K with a single Sr$^+$ ion trapped 100 $\u03bc$m above the trap surface. The lasers used are at 370, 405, 460, and 674 nm, and the typical photon flux at the trap is 10$^{14}$ photons/cm$^2$/sec. Charging is detected by monitoring the ion's micromotion signal, which is related to the number of charges created on the trap. A wavelength and material dependence of the charging behavior is observed: lasers at lower wavelengths cause more charging, and aluminum exhibits more charging than copper or gold. We describe the charging dynamic based on a rate equation approach.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "An atomic ion is trapped at the tip of a single-mode optical fiber in a cryogenic (8 K) surface-electrode ion trap. The fiber serves as an integrated source of laser light, which drives the quadrupole qubit transition of $^{88}$Sr$^+$. Through \\emph{in situ} translation of the nodal point of the trapping field, the Gaussian beam profile of the fiber output is imaged, and the fiber-ion displacement, in units of the mode waist at the ion, is optimized to within $0.13\\pm0.10$ of the mode center despite an initial offset of $3.30\\pm0.10$. Fiber-induced charging at $125 \u03bc$W is observed to be ${\\sim}10$ V/m at an ion height of $670 \u03bc$m, with charging and discharging time constants of $1.6\\pm0.3$ s and $4.7\\pm0.6$ s respectively. This work is of importance to large-scale, ion-based quantum information processing, where optics integration in surface-electrode designs may be a crucial enabling technology.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "A novel approach to optics integration in ion traps is demonstrated based on a surface electrode ion trap that is microfabricated on top of a dielectric mirror. Additional optical losses due to fabrication are found to be as low as 80 ppm for light at 422 nm. The integrated mirror is used to demonstrate light collection from, and imaging of, a single 88 Sr+ ion trapped $169\\pm4 \u03bc$m above the mirror.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We fabricate superconducting ion traps with niobium and niobium nitride and trap single 88Sr ions at cryogenic temperatures. The superconducting transition is verified and characterized by measuring the resistance and critical current using a 4-wire measurement on the trap structure, and observing change in the rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz at 6 K and shows no significant change across the superconducting transition, suggesting that anomalous heating is primarily caused by noise sources on the surface. This demonstration of superconducting ion traps opens up possibilities for integrating trapped ions and molecular ions with superconducting devices.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "It is well known that the ground state energy of many-particle Hamiltonians involving only 2-body interactions can be obtained using constrained optimizations over density matrices which arise from reducing an N-particle state. While determining which 2-particle density matrices are \"N- representable\" is a computationally hard problem, all known extreme N-representable 2-particle reduced density matrices arise from a unique N-particle pre-image, satisfying a conjecture established in 1972. We present explicit counterexamples to this conjecture through giving Hamiltonians with 2-body interactions which have degenerate ground states that cannot be distinguished by any 2-body operator. We relate the existence of such counterexamples to quantum error correction codes and topologically ordered spin systems.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Two-dimensional crystals of trapped ions are a promising system with which to implement quantum simulations of challenging problems such as spin frustration. Here, we present a design for a surface-electrode elliptical ion trap which produces a 2-D ion crystal and is amenable to microfabrication, which would enable higher simulated coupling rates, as well as interactions based on magnetic forces generated by on-chip currents. Working in an 11 K cryogenic environment, we experimentally verify to within 5% a numerical model of the structure of ion crystals in the trap. We also explore the possibility of implementing quantum simulation using magnetic forces, and calculate J-coupling rates on the order of 10^3 / s for an ion crystal height of 10 microns, using a current of 1 A.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We present a model as well as experimental results for a surface electrode radio-frequency Paul trap that has a circular electrode geometry well-suited for trapping of single ions and two-dimensional planar ion crystals. The trap design is compatible with microfabrication and offers a simple method by which the height of the trapped ions above the surface may be changed \\emph{in situ}. We demonstrate trapping of single and few Sr+ ions over an ion height range of 200-1000 microns for several hours under Doppler laser cooling, and use these to characterize the trap, finding good agreement with our model.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "The tensor product representation of quantum states leads to a promising variational approach to study quantum phase and quantum phase transitions, especially topological ordered phases which are impossible to handle with conventional methods due to their long range entanglement. However, an important issue arises when we use tensor product states (TPS) as variational states to find the ground state of a Hamiltonian: can arbitrary variations in the tensors that represent ground state of a Hamiltonian be induced by local perturbations to the Hamiltonian? Starting from a tensor product state which is the exact ground state of a Hamiltonian with $\\mathbb{Z}_2$ topological order, we show that, surprisingly, not all variations of the tensors correspond to the variation of the ground state caused by local perturbations of the Hamiltonian. Even in the absence of any symmetry requirement of the perturbed Hamiltonian, one necessary condition for the variations of the tensors to be physical is that they respect certain $\\mathbb{Z}_2$ symmetry. We support this claim by calculating explicitly the change in topological entanglement entropy with different variations in the tensors. This finding will provide important guidance to numerical variational study of topological phase and phase transitions. It is also a crucial step in using TPS to study universal properties of a quantum phase and its topological order.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Entanglement, as studied in quantum information science, and non-local quantum correlations, as studied in condensed matter physics, are fundamentally akin to each other. However, their relationship is often hard to quantify due to the lack of a general approach to study both on the same footing. In particular, while entanglement and non-local correlations are properties of states, both arise from symmetries of global operators that commute with the system Hamiltonian. Here, we introduce a framework for completely classifying the local and non-local properties of all such global operators, given the Hamiltonian and a bi-partitioning of the system. This framework is limited to descriptions based on stabilizer quantum codes, but may be generalized. We illustrate the use of this framework to study entanglement and non-local correlations by analyzing global symmetries in topological order, distribution of entanglement and entanglement entropy.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We demonstrate quantum control techniques for a single trapped ion in a cryogenic, surface-electrode trap. A narrow optical transition of Sr+ along with the ground and first excited motional states of the harmonic trapping potential form a two-qubit system. The optical qubit transition is susceptible to magnetic field fluctuations, which we stabilize with a simple and compact method using superconducting rings. Decoherence of the motional qubit is suppressed by the cryogenic environment. AC Stark shift correction is accomplished by controlling the laser phase in the pulse sequencer, eliminating the need for an additional laser. Quantum process tomography is implemented on atomic and motional states using conditional pulse sequences. With these techniques we demonstrate a Cirac-Zoller Controlled-NOT gate in a single ion with a mean fidelity of 91(1)%.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Graphs are closely related to quantum error-correcting codes: every stabilizer code is locally equivalent to a graph code, and every codeword stabilized code can be described by a graph and a classical code. For the construction of good quantum codes of relatively large block length, concatenated quantum codes and their generalizations play an important role. We develop a systematic method for constructing concatenated quantum codes based on \"graph concatenation\", where graphs representing the inner and outer codes are concatenated via a simple graph operation called \"generalized local complementation.\" Our method applies to both binary and non-binary concatenated quantum codes as well as their generalizations.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We report a demonstration and quantitative characterization of one-dimensional cavity cooling of a single trapped 88Sr+ ion in the resolved sideband regime. We measure the spectrum of cavity transitions, the rates of cavity heating and cooling, and the steady-state cooling limit. The cavity cooling dynamics and cooling limit of 22.5(3) motional quanta, limited by the moderate coupling between the ion and the cavity, are consistent with a simple model [Phys. Rev. A 64, 033405] without any free parameters, validating the rate equation model for cavity cooling.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Many-body entangled quantum states studied in condensed matter physics can be primary resources for quantum information, allowing any quantum computation to be realized using measurements alone, on the state. Such a universal state would be remarkably valuable, if only it were thermodynamically stable and experimentally accessible, by virtue of being the unique ground state of a physically reasonable Hamiltonian made of two-body, nearest neighbor interactions. We introduce such a state, composed of six-state particles on a hexagonal lattice, and describe a general method for analyzing its properties based on its projected entangled pair state representation.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Dense array of ions in microfabricated traps represent one possible way to scale up ion trap quantum computing. The ability to address individual ions is an important component of such a scheme. We demonstrate individual addressing of trapped ions in a microfabricated surface-electrode trap using a magnetic field gradient generated on-chip. A frequency splitting of 310(2) kHz for two ions separated by 5 um is achieved. Selective single qubit operations are performed on one of two trapped ions with an average of 2.2+/-1.0% crosstalk. Coherence time as measured by the spin-echo technique is unaffected by the field gradient.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Quantum simulations of spin systems could enable the solution of problems which otherwise require infeasible classical resources. Such a simulation may be implemented using a well-controlled system of effective spins, such as a two-dimensional lattice of locally interacting ions. We propose here a layered planar rf trap design that can be used to create arbitrary two-dimensional lattices of ions. The design also leads naturally to ease of microfabrication. As a first experimental demonstration, we confine strontium-88 ions in a mm-scale lattice trap and verify numerical models of the trap by measuring the motional frequencies. We also confine 440 nm diameter charged microspheres and observe ion-ion repulsion between ions in neighboring lattice sites. Our design, when scaled to smaller ion-ion distances, is appropriate for quantum simulation schemes, e.g. that of Porras and Cirac (PRL 92 207901 (2004)). We note, however, that in practical realizations of the trap, an increase in the secular frequency with decreasing ion spacing may make a coupling rate that is large relative to the decoherence rate in such a trap difficult to achieve.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "The codeword stabilized (CWS) quantum codes formalism presents a unifying approach to both additive and nonadditive quantum error-correcting codes (arXiv:0708.1021 [quant-ph]), but only for binary states. Here we generalize the CWS framework to the nonbinary case (of both prime and nonprime dimension) and map the search for nonbinary quantum codes to a corresponding search problem for classical nonbinary codes with specific error patterns. We show that while the additivity properties of nonbinary CWS codes are similar to the binary case, the structural properties of the nonbinary codes differ substantially from the binary case, even for prime dimensions. In particular, we identify specific structure patterns of stabilizer groups, based on which efficient constructions might be possible for codes that encode more dimensions than any stabilizer codes of the same length and distance; similar methods cannot be applied in the binary case. Understanding of these structural properties can help prune the search space and facilitate the identification of good nonbinary CWS codes.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Electric field noise from fluctuating patch potentials is a significant problem for a broad range of precision experiments, including trapped ion quantum computation and single spin detection. Recent results demonstrated strong suppression of this noise by cryogenic cooling, suggesting an underlying thermal process. We present measurements characterizing the temperature and frequency dependence of the noise from 7 to 100 K, using a single Sr+ ion trapped 75 um above the surface of a gold plated surface electrode ion trap. The noise amplitude is observed to have an approximate 1/f spectrum around 1 MHz, and grows rapidly with temperature as T^beta for beta from 2 to 4. The data are consistent with microfabricated cantilever measurements of non-contact friction but do not extrapolate to the DC measurements with neutral atoms or contact potential probes.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "The codeword stabilized (\"CWS\") quantum codes formalism presents a unifying approach to both additive and nonadditive quantum error-correcting codes (arXiv:0708.1021). This formalism reduces the problem of constructing such quantum codes to finding a binary classical code correcting an error pattern induced by a graph state. Finding such a classical code can be very difficult. Here, we consider an algorithm which maps the search for CWS codes to a problem of identifying maximum cliques in a graph. While solving this problem is in general very hard, we prove three structure theorems which reduce the search space, specifying certain admissible and optimal ((n,K,d)) additive codes. In particular, we find there does not exist any ((7,3,3)) CWS code though the linear programming bound does not rule it out. The complexity of the CWS search algorithm is compared with the contrasting method introduced by Aggarwal and Calderbank (arXiv:cs/0610159).\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "A long-standing open problem in fault-tolerant quantum computation has been to find a universal set of transversal gates. As three of us proved in arXiv: 0706.1382, such a set does not exist for binary stabilizer codes. Here we generalize our work to show that for subsystem stabilizer codes in $d$ dimensional Hilbert space, such a universal set of transversal gates cannot exist for even one encoded qudit, for any dimension $d$, prime or nonprime. This result strongly supports the idea that other primitives, such as quantum teleportation, are necessary for universal fault-tolerant quantum computation, and may be an important factor for fault tolerance noise thresholds.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "Teleportation is a crucial element in fault-tolerant quantum computation and a complete understanding of its capacity is very important for the practical implementation of optimal fault-tolerant architectures. It is known that stabilizer codes support a natural set of gates that can be more easily implemented by teleportation than any other gates. These gates belong to the so called $\\mathcal{C}_k$ hierarchy introduced by Gottesman and Chuang (Nature \\textbf{402}, 390). Moreover, a subset of $\\mathcal{C}_k$ gates, called semi-Clifford operations, can be implemented by an even simpler architecture than the traditional teleportation setup (Phys. Rev. \\textbf{A62}, 052316). However, the precise set of gates in $\\mathcal{C}_k$ remains unknown, even for a fixed number of qubits $n$, which prevents us from knowing exactly what teleportation is capable of. In this paper we study the structure of $\\mathcal{C}_k$ in terms of semi-Clifford operations, which send by conjugation at least one maximal abelian subgroup of the $n$-qubit Pauli group into another one. We show that for $n=1,2$, all the $\\mathcal{C}_k$ gates are semi-Clifford, which is also true for $\\{n=3,k=3\\}$. However, this is no longer true for $\\{n>2,k>3\\}$. To measure the capability of this teleportation primitive, we introduce a quantity called `teleportation depth', which characterizes how many teleportation steps are necessary, on average, to implement a given gate. We calculate upper bounds for teleportation depth by decomposing gates into both semi-Clifford $\\mathcal{C}_k$ gates and those $\\mathcal{C}_k$ gates beyond semi-Clifford operations, and compare their efficiency.\n        \u25b3 Less", "author": "Isaac Chuang"}, {"abstract": "We define a set inner product to be a function on pairs of convex bodies which is symmetric, Minkowski linear in each dimension, positive definite, and satisfies the natural analogue of the Cauchy-Schwartz inequality (which is not implied by the other conditions). We show that any set inner product can be embedded into an inner product space on the associate support functions, thereby extending fundamental results of Hormander and Radstrom. The set inner product provides a geometry on the space of convex bodies. We explore some of the properties of that geometry, and discuss an application of these ideas to the reconstruction of ancestral ecological niches in evolutionary biology.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Two-dimensional semiconductors - atomic layers of materials with covalent intra-layer bonding and weak (van der Waals or quadrupole) coupling between the layers - are a new class of materials with great potential for optoelectronic applications. Among those, a special position is now being taken by post-transition metal chalcogenides (PTMC), InSe and GaSe. It has recently been found that the band gap in 2D crystals of InSe more than doubles in the monolayer compared to thick multilayer crystals, while the high mobility of conduction band electrons is promoted by their light in-plane mass. Here, we use Raman and PL measurements of encapsulated few layer samples, coupled with accurate atomic force and transmission electron microscope structural characterisation to reveal new optical properties of atomically thin GaSe preserved by hBN encapsulation. The band gaps we observe complement the spectral range provided by InSe films, so that optical activity of these two almost lattice-matched PTMC films and their heterostructures densely cover the spectrum of photons from violet to infrared. We demonstrate the realisation of the latter by the first observation of interlayer excitonic photoluminescence in few-layer InSe-GaSe heterostructures. The spatially indirect transition is direct in k-space and therefore is bright, while its energy can be tuned in a broad range by the number of layers.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "JINGLE is a new JCMT legacy survey designed to systematically study the cold interstellar medium of galaxies in the local Universe. As part of the survey we perform 850um continuum measurements with SCUBA-2 for a representative sample of 193 Herschel-selected galaxies with M*>10^9Msun, as well as integrated CO(2-1) line fluxes with RxA3m for a subset of 90 of these galaxies. The sample is selected from fields covered by the Herschel-ATLAS survey that are also targeted by the MaNGA optical integral-field spectroscopic survey. The new JCMT observations combined with the multi-wavelength ancillary data will allow for the robust characterization of the properties of dust in the nearby Universe, and the benchmarking of scaling relations between dust, gas, and global galaxy properties. In this paper we give an overview of the survey objectives and details about the sample selection and JCMT observations, present a consistent 30 band UV-to-FIR photometric catalog with derived properties, and introduce the JINGLE Main Data Release (MDR). Science highlights include the non-linearity of the relation between 850um luminosity and CO line luminosity, and the serendipitous discovery of candidate z>6 galaxies.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The galaxies found in optical surveys fall in two distinct regions of a diagram of optical colour versus absolute magnitude: the red sequence and the blue cloud with the green valley in between. We show that the galaxies found in a submillimetre survey have almost the opposite distribution in this diagram, forming a `green mountain'. We show that these distinctive distributions follow naturally from a single, continuous, curved Galaxy Sequence in a diagram of specific star-formation rate versus stellar mass without there being the need for a separate star-forming galaxy Main Sequence and region of passive galaxies. The cause of the red sequence and the blue cloud is the geometric mapping between stellar mass/specific star-formation rate and absolute magnitude/colour, which distorts a continuous Galaxy Sequence in the diagram of intrinsic properties into a bimodal distribution in the diagram of observed properties. The cause of the green mountain is Malmquist bias in the submillimetre waveband, with submillimetre surveys tending to select galaxies on the curve of the Galaxy Sequence, which have the highest ratios of submillimetre-to-optical luminosity. This effect, working in reverse, causes galaxies on the curve of the Galaxy Sequence to be underrepresented in optical samples, deepening the green valley. The green valley is therefore not evidence (1) for there being two distinct populations of galaxies, (2) for galaxies in this region evolving more quickly than galaxies in the blue cloud and the red sequence, (c) for rapid quenching processes in the galaxy population.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We identify an observable imprint of a first-order hadron-quark phase transition at supranuclear densities on the gravitational-wave (GW) emission of neutron star mergers. Specifically, we show that the dominant postmerger GW frequency f_peak may exhibit a significant deviation from an empirical relation between f_peak and the tidal deformability if a strong first-order phase transition leads to the formation of a gravitationally stable extended quark matter core in the postmerger remnant. A comparison of the GW signatures from a large, representative sample of microphysical, purely hadronic equations of state indicates that this imprint is only observed in those systems which undergo a strong first-order phase transition. Such a shift of the dominant postmerger GW frequency can be revealed by future GW observations, which would provide evidence for the existence of a strong first-order phase transition in the interior of neutron stars.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "In July 2018 an FRIB Theory Alliance program was held on the implications of GW170817 and its associated kilonova for r-process nucleosynthesis. Topics of discussion included the astrophysical and nuclear physics uncertainties in the interpretation of the GW170817 kilonova, what we can learn about the astrophysical site or sites of the r process from this event, and the advances in nuclear experiment and theory most crucial to pursue in light of the new data. Here we compile a selection of scientific contributions to the workshop, broadly representative of progress in r-process studies since the GW170817 event.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We present observations of supernova (SN) 2017ens, discovered by the ATLAS survey and identified as a hot blue object through the GREAT program. The redshift z=0.1086 implies a peak brightness of M_g=-21.1 mag, placing the object within the regime of superluminous supernovae. We observe dramatic spectral evolution, from initially being blue and featureless, to later developing features similar to those of the broad-lined Type Ic SN 1998bw, to eventually showing broad H-alpha and H-beta emission, together with relatively narrow emission (reminiscent of a SN IIn) in all periods. We also detect coronal lines, indicative of a dense circumstellar medium. We constrain the progenitor wind velocity to ~50-60 km s^-1 based on P-Cygni profiles, which is far slower than those present in Wolf-Rayet stars. This may suggest that the progenitor passed through a luminous blue variable phase, or that the wind is instead from a binary companion red supergiant star. At late times we see broad (~2000 km s^-1) and strong (~3x10^40 erg s^-1) H-alpha emission, perhaps indicative of additional mass loss at high velocity, suggesting that SN 2017ens was a pulsational pair-instability SN.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The challenges of high contrast imaging (HCI) for detecting exoplanets for both ground and space applications can be met with extreme adaptive optics (ExAO), a high-order adaptive optics system that performs wavefront sensing (WFS) and correction at high speed. We describe two ExAO optical system designs, one each for ground-based telescopes and space-based missions, and examine them using the angular spectrum Fresnel propagation module within the Physical Optics Propagation in Python (POPPY) package. We present an end-to-end (E2E) simulation of the MagAO-X instrument, an ExAO system capable of delivering 6$\\times10^{-5}$ visible-light raw contrast for static, noncommon path aberrations without atmosphere. We present a laser guidestar (LGS) companion spacecraft testbed demonstration, which uses a remote beacon to increase the signal available for WFS and control of the primary aperture segments of a future large space telescope, providing on order of a factor of ten factor improvement for relaxing observatory stability requirements. The LGS E2E simulation provides an easily adjustable model to explore parameters, limits, and trade-offs on testbed design and characterization.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Traditional program analysis analyses a program language, that is, all programs that can be written in the language. There is a difference, however, between all possible programs that can be written and the corpus of actual programs written in a language. We seek to exploit this difference: for a given program, we apply a bespoke program transformation Indexify to convert expressions that current SMT solvers do not, in general, handle, such as constraints on strings, into equisatisfiable expressions that they do handle. To this end, Indexify replaces operators in hard-to-handle expressions with homomorphic versions that behave the same on a finite subset of the domain of the original operator, and return bottom denoting unknown outside of that subset. By focusing on what literals and expressions are most useful for analysing a given program, Indexify constructs a small, finite theory that extends the power of a solver on the expressions a target program builds.\n  Indexify's bespoke nature necessarily means that its evaluation must be experimental, resting on a demonstration of its effectiveness in practice. We have developed Indexif}, a tool for Indexify. We demonstrate its utility and effectiveness by applying it to two real world benchmarks --- string expressions in coreutils and floats in fdlibm53. Indexify reduces time-to-completion on coreutils from Klee's 49.5m on average to 6.0m. It increases branch coverage on coreutils from 30.10% for Klee and 14.79% for Zesti to 66.83%. When indexifying floats in fdlibm53, Indexifyl increases branch coverage from 34.45% to 71.56% over Klee. For a restricted class of inputs, Indexify permits the symbolic execution of program paths unreachable with previous techniques: it covers more than twice as many branches in coreutils as Klee.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Exciton-polaritons are quasiparticles with mixed photon and exciton character that demonstrate rich quantum phenomena, novel optoelectronic devices and the potential to modify chemical properties of materials. Organic semiconductors are of current interest for their room-temperature polariton formation. However, within organic optoelectronic devices, it is often the 'dark' spin-1 triplet excitons that dominate operation. These triplets have been largely ignored in treatments of polariton physics. Here we demonstrate polariton population from the triplet manifold via triplet-triplet annihilation, leading to polariton emission that is longer-lived (>microseconds) even than exciton emission in bare films. This enhancement arises from spin-2 triplet-pair states, formed by singlet fission or triplet-triplet annihilation, feeding the polariton. This is possible due to state mixing, which -in the strong coupling regime- leads to sharing of photonic character with states that are formally non-emissive. Such 'photonic sharing' offers the enticing possibility of harvesting or manipulating even states that are formally dark.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The $\\textit{axial coupling of the nucleon}$, $g_A$, is the strength of its coupling to the $\\textit{weak}$ axial current of the Standard Model of particle physics, in much the same way as the electric charge is the strength of the coupling to the electromagnetic current. This axial coupling dictates the rate at which neutrons decay to protons, the strength of the attractive long-range force between nucleons and other features of nuclear physics. Precision tests of the Standard Model in nuclear environments require a quantitative understanding of nuclear physics rooted in Quantum Chromodynamics, a pillar of the Standard Model. The prominence of $g_A$ makes it a benchmark quantity to determine theoretically - a difficult task because quantum chromodynamics is non-perturbative, precluding known analytical methods. Lattice Quantum Chromodynamics provides a rigorous, non-perturbative definition of quantum chromodynamics that can be implemented numerically. It has been estimated that a precision of two percent would be possible by 2020 if two challenges are overcome: contamination of $g_A$ from excited states must be controlled in the calculations and statistical precision must be improved markedly. Here we report a calculation of $g_A^{QCD} = 1.271\\pm0.013$, using an unconventional method inspired by the Feynman-Hellmann theorem that overcomes these challenges.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The alpha/beta interface in Ti-6Al-2Sn-4Zr-6Mo (Ti-6246) is investigated via centre of symmetry analysis, both as-grown and after 10% cold work. Semi-coherent interface steps are observed at a spacing of 4.5 +/-1.13 atoms in the as-grown condition, in good agreement with theory prediction (4.37 atoms). Lattice accommodation is observed, with elongation along [-1 2 -1 0]alpha and contraction along [1 0 -1 0]alpha . Deformed alpha exhibited larger, less coherent steps with slip bands lying in {110}beta. This indicates dislocation pile-up at the grain boundary, a precursor to globularisation, offering insight into the effect of deformation processing on the interface, which is important for titanium alloy processing route design.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Neuromorphic architectures achieve low-power operation by using many simple spiking neurons in lieu of traditional hardware. Here, we develop methods for precise linear computations in spiking neural networks and use these methods to map the evolution of a linear dynamical system (LDS) onto an existing neuromorphic chip: IBM's TrueNorth. We analytically characterize, and numerically validate, the discrepancy between the spiking LDS state sequence and that of its non-spiking counterpart. These analytical results shed light on the multiway tradeoff between time, space, energy, and accuracy in neuromorphic computation. To demonstrate the utility of our work, we implemented a neuromorphic Kalman filter (KF) and used it for offline decoding of human vocal pitch from neural data. The neuromorphic KF could be used for low-power filtering in domains beyond neuroscience, such as navigation or robotics.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Observation of neutrinoless double beta decay, a lepton number violating process that has been proposed to clarify the nature of neutrino masses, has spawned an enormous world-wide experimental effort. Relating nuclear decay rates to high-energy, beyond the Standard Model (BSM) physics requires detailed knowledge of non-perturbative QCD effects. Using lattice QCD, we compute the necessary matrix elements of short-range operators, which arise due to heavy BSM mediators, that contribute to this decay via the leading order $\u03c0^- \\to \u03c0^+$ exchange diagrams. Utilizing our result and taking advantage of effective field theory methods will allow for model-independent calculations of the relevant two-nucleon decay, which may then be used as input for nuclear many-body calculations of the relevant experimental decays. Contributions from short-range operators may prove to be equally important to, or even more important than, those from long-range Majorana neutrino exchange.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The Kilodegree Extremely Little Telescope (KELT) project has been conducting a photometric survey for transiting planets orbiting bright stars for over ten years. The KELT images have a pixel scale of ~23\"/pixel---very similar to that of NASA's Transiting Exoplanet Survey Satellite (TESS)---as well as a large point spread function, and the KELT reduction pipeline uses a weighted photometric aperture with radius 3'. At this angular scale, multiple stars are typically blended in the photometric apertures. In order to identify false positives and confirm transiting exoplanets, we have assembled a follow-up network (KELT-FUN) to conduct imaging with higher spatial resolution, cadence, and photometric precision than the KELT telescopes, as well as spectroscopic observations of the candidate host stars. The KELT-FUN team has followed-up over 1,600 planet candidates since 2011, resulting in more than 20 planet discoveries. Excluding ~450 false alarms of non-astrophysical origin (i.e., instrumental noise or systematics), we present an all-sky catalog of the 1,128 bright stars (6<V<10) that show transit-like features in the KELT light curves, but which were subsequently determined to be astrophysical false positives (FPs) after photometric and/or spectroscopic follow-up observations. The KELT-FUN team continues to pursue KELT and other planet candidates and will eventually follow up certain classes of TESS candidates. The KELT FP catalog will help minimize the duplication of follow-up observations by current and future transit surveys such as TESS.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "During the shutdown of the CERN Large Hadron Collider in 2013-2014, an additional pixel layer was installed between the existing Pixel detector of the ATLAS experiment and a new, smaller radius beam pipe. The motivation for this new pixel layer, the Insertable B-Layer (IBL), was to maintain or improve the robustness and performance of the ATLAS tracking system, given the higher instantaneous and integrated luminosities realised following the shutdown. Because of the extreme radiation and collision rate environment, several new radiation-tolerant sensor and electronic technologies were utilised for this layer. This paper reports on the IBL construction and integration prior to its operation in the ATLAS detector.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The challenge of controlling magnetism using electric fields raises fundamental questions and addresses technological needs such as low-dissipation magnetic memory. The recently reported two-dimensional (2D) magnets provide a new system for studying this problem owing to their unique magnetic properties. For instance, bilayer chromium triiodide (CrI3) behaves as a layered antiferromagnet with a magnetic field-driven metamagnetic transition. Here, we demonstrate electrostatic gate control of magnetism in CrI3 bilayers, probed by magneto-optical Kerr effect (MOKE) microscopy. At fixed magnetic fields near the metamagnetic transition, we realize voltage-controlled switching between antiferromagnetic and ferromagnetic states. At zero magnetic field, we demonstrate a time-reversal pair of layered antiferromagnetic states which exhibit spin-layer locking, leading to a remarkable linear dependence of their MOKE signals on gate voltage with opposite slopes. Our results pave the way for exploring new magnetoelectric phenomena and van der Waals spintronics based on 2D materials.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "QMCPACK is an open source quantum Monte Carlo package for ab-initio electronic structure calculations. It supports calculations of metallic and insulating solids, molecules, atoms, and some model Hamiltonians. Implemented real space quantum Monte Carlo algorithms include variational, diffusion, and reptation Monte Carlo. QMCPACK uses Slater-Jastrow type trial wave functions in conjunction with a sophisticated optimizer capable of optimizing tens of thousands of parameters. The orbital space auxiliary field quantum Monte Carlo method is also implemented, enabling cross validation between different highly accurate methods. The code is specifically optimized for calculations with large numbers of electrons on the latest high performance computing architectures, including multicore central processing unit (CPU) and graphical processing unit (GPU) systems. We detail the program's capabilities, outline its structure, and give examples of its use in current research calculations. The package is available at http://www.qmcpack.org .\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Bitcoin users are directly or indirectly forced to deal with public key cryptography, which has a number of security and usability challenges that differ from the password-based authentication underlying most online banking services. Users must ensure that keys are simultaneously accessible, resistant to digital theft and resilient to loss. In this paper, we contribute an evaluation framework for comparing Bitcoin key management approaches, and conduct a broad usability evaluation of six representative Bitcoin clients. We find that Bitcoin shares many of the fundamental challenges of key management known from other domains, but that Bitcoin may present a unique opportunity to rethink key management for end users.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The rigid-intensity-shift model of differential phase contrast scanning transmission electron microscopy (DPC-STEM) imaging assumes that the phase gradient imposed on the probe by the sample causes the diffraction pattern intensity to shift rigidly by an amount proportional to that phase gradient. This behaviour is seldom realised exactly in practice. Through a combination of experimental results, analytical modelling and numerical calculations, we explore the breakdown of the rigid-intensity-shift behaviour and how this depends on the magnitude of the phase gradient and the relative scale of features in the phase profile and the probe size. We present guidelines as to when the rigid-intensity-shift model can be applied for quantitative phase reconstruction using segmented detectors, and propose probe-shaping strategies to further improve the accuracy.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We present a photometric detection of the first brightness dips of the unique variable star KIC 8462852 since the end of the Kepler space mission in 2013 May. Our regular photometric surveillance started in October 2015, and a sequence of dipping began in 2017 May continuing on through the end of 2017, when the star was no longer visible from Earth. We distinguish four main 1-2.5% dips, named \"Elsie,\" \"Celeste,\" \"Skara Brae,\" and \"Angkor\", which persist on timescales from several days to weeks. Our main results so far are: (i) there are no apparent changes of the stellar spectrum or polarization during the dips; (ii) the multiband photometry of the dips shows differential reddening favoring non-grey extinction. Therefore, our data are inconsistent with dip models that invoke optically thick material, but rather they are in-line with predictions for an occulter consisting primarily of ordinary dust, where much of the material must be optically thin with a size scale <<1um, and may also be consistent with models invoking variations intrinsic to the stellar photosphere. Notably, our data do not place constraints on the color of the longer-term \"secular\" dimming, which may be caused by independent processes, or probe different regimes of a single process.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Fermi has detected over 200 pulsars above 100 MeV. In a previous work, using 3 years of LAT data (1FHL catalog) we reported that 28 of these pulsars show emission above 10 GeV; only three of these, however, were millisecond pulsars (MSPs). The recently-released Third Catalog of Hard Fermi-LAT Sources (3FHL) contains over 1500 sources showing emission above 10 GeV, 17 of which are associated with gamma-ray MSPs. Using three times as much data as in our previous study (1FHL), we report on a systematic analysis of these pulsars to determine the highest energy (pulsed) emission fromMSPs and discuss the best possible candidates for follow-up observations with ground-based TeV instruments (H.E.S.S., MAGIC, VERITAS, and the upcoming CTA).\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We present Symbolic Quick Error Detection (Symbolic QED), a structured approach for logic bug detection and localization which can be used both during pre-silicon design verification as well as post-silicon validation and debug. This new methodology leverages prior work on Quick Error Detection (QED) which has been demonstrated to drastically reduce the latency, in terms of the number of clock cycles, of error detection following the activation of a logic (or electrical) bug. QED works through software transformations, including redundant execution and control flow checking, of the applied tests. Symbolic QED combines these error-detecting QED transformations with bounded model checking-based formal analysis to generate minimal-length bug activation traces that detect and localize any logic bugs in the design. We demonstrate the practicality and effectiveness of Symbolic QED using the OpenSPARC T2, a 500-million-transistor open-source multicore System-on-Chip (SoC) design, and using \"difficult\" logic bug scenarios observed in various state-of-the-art commercial multicore SoCs. Our results show that Symbolic QED: (i) is fully automatic, unlike manual techniques in use today that can be extremely time-consuming and expensive; (ii) requires only a few hours in contrast to manual approaches that might take days (or even months) or formal techniques that often take days or fail completely for large designs; and (iii) generates counter-examples (for activating and detecting logic bugs) that are up to 6 orders of magnitude shorter than those produced by traditional techniques. Significantly, this new approach does not require any additional hardware.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We present a study of comet C/2017 K2 (PANSTARRS) using prediscovery archival data taken from 2013 to 2017. Our measurements show that the comet has been marginally increasing in activity since at least 2013 May (heliocentric distance of $r_{\\mathrm{H}} = 23.7$ AU pre-perihelion). We estimate the mass-loss rate during the period 2013--2017 as $\\overline{\\dot{M}} \\approx \\left(2.4 \\pm 1.1 \\right) \\times 10^{2}$ kg s$^{-1}$, which requires a minimum active surface area of $\\sim$10--10$^2$ km$^{2}$ for sublimation of supervolatiles such as CO and CO$_2$, by assuming a nominal cometary albedo $p_V = 0.04 \\pm 0.02$. The corresponding lower limit to the nucleus radius is a few kilometers. Our Monte Carlo dust simulations show that dust grains in the coma are $\\gtrsim0.5$ mm in radius, with ejection speeds from $\\sim$1--3 m s$^{-1}$, and have been emitted in a protracted manner since 2013, confirming estimates by Jewitt et al. (2017). The current heliocentric orbit is hyperbolic. Our N-body backward dynamical integration of the orbit suggests that the comet is most likely (with a probability of $\\sim$98\\%) from the Oort spike. The calculated median reciprocal of the semimajor axis 1 Myr ago was $a_{\\mathrm{med}}^{-1} = \\left( 3.61 \\pm 1.71 \\right) \\times 10^{-5}$ AU$^{-1}$ (in a reference system of the solar-system barycentre).\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "On June 8, 2017 at 02:01:16.49 UTC, a gravitational-wave signal from the merger of two stellar-mass black holes was observed by the two Advanced LIGO detectors with a network signal-to-noise ratio of 13. This system is the lightest black hole binary so far observed, with component masses $12^{+7}_{-2}\\,M_\\odot$ and $7^{+2}_{-2}\\,M_\\odot$ (90% credible intervals). These lie in the range of measured black hole masses in low-mass X-ray binaries, thus allowing us to compare black holes detected through gravitational waves with electromagnetic observations. The source's luminosity distance is $340^{+140}_{-140}$ Mpc, corresponding to redshift $0.07^{+0.03}_{-0.03}$. We verify that the signal waveform is consistent with the predictions of general relativity.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "A host of new low-frequency radio telescopes seek to measure the 21-cm transition of neutral hydrogen from the early universe. These telescopes have the potential to directly probe star and galaxy formation at redshifts $20 \\gtrsim z \\gtrsim 7$, but are limited by the dynamic range they can achieve against foreground sources of low-frequency radio emission. Consequently, there is a growing demand for modern, high-fidelity maps of the sky at frequencies below 200 MHz for use in foreground modeling and removal. We describe a new widefield imaging technique for drift-scanning interferometers, Tikhonov-regularized $m$-mode analysis imaging. This technique constructs images of the entire sky in a single synthesis imaging step with exact treatment of widefield effects. We describe how the CLEAN algorithm can be adapted to deconvolve maps generated by $m$-mode analysis imaging. We demonstrate Tikhonov-regularized $m$-mode analysis imaging using the Owens Valley Long Wavelength Array (OVRO-LWA) by generating 8 new maps of the sky north of $\u03b4=-30^\\circ$ with 15 arcmin angular resolution, at frequencies evenly spaced between 36.528 MHz and 73.152 MHz, and $\\sim$800 mJy/beam thermal noise. These maps are a 10-fold improvement in angular resolution over existing full-sky maps at comparable frequencies, which have angular resolutions $\\ge 2^\\circ$. Each map is constructed exclusively from interferometric observations and does not represent the globally averaged sky brightness. Future improvements will incorporate total power radiometry, improved thermal noise, and improved angular resolution -- due to the planned expansion of the OVRO-LWA to 2.6 km baselines. These maps serve as a first step on the path to the use of more sophisticated foreground filters in 21-cm cosmology incorporating the measured angular and frequency structure of all foreground contaminants.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The first observation of a binary neutron star coalescence by the Advanced LIGO and Advanced Virgo gravitational-wave detectors offers an unprecedented opportunity to study matter under the most extreme conditions. After such a merger, a compact remnant is left over whose nature depends primarily on the masses of the inspiralling objects and on the equation of state of nuclear matter. This could be either a black hole or a neutron star (NS), with the latter being either long-lived or too massive for stability implying delayed collapse to a black hole. Here, we present a search for gravitational waves from the remnant of the binary neutron star merger GW170817 using data from Advanced LIGO and Advanced Virgo. We search for short ($\\lesssim1$ s) and intermediate-duration ($\\lesssim 500$ s) signals, which includes gravitational-wave emission from a hypermassive NS or supramassive NS, respectively. We find no signal from the post-merger remnant. Our derived strain upper limits are more than an order of magnitude larger than those predicted by most models. For short signals, our best upper limit on the root-sum-square of the gravitational-wave strain emitted from 1--4 kHz is $h_{\\rm rss}^{50\\%}=2.1\\times 10^{-22}$ Hz$^{-1/2}$ at 50% detection efficiency. For intermediate-duration signals, our best upper limit at 50% detection efficiency is $h_{\\rm rss}^{50\\%}=8.4\\times 10^{-22}$ Hz$^{-1/2}$ for a millisecond magnetar model, and $h_{\\rm rss}^{50\\%}=5.9\\times 10^{-22}$ Hz$^{-1/2}$ for a bar-mode model. These results indicate that post-merger emission from a similar event may be detectable when advanced detectors reach design sensitivity or with next-generation detectors.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We present state-of-the-art results from a lattice QCD calculation of the nucleon axial coupling, $g_A$, using M\u00f6bius Domain-Wall fermions solved on the dynamical $N_f = 2 + 1 + 1$ HISQ ensembles after they are smeared using the gradient-flow algorithm. Relevant three-point correlation functions are calculated using a method inspired by the Feynman-Hellmann theorem, and demonstrate significant improvement in signal for fixed stochastic samples. The calculation is performed at five pion masses of $m_\u03c0\\sim \\{400, 350, 310, 220, 130\\}$~MeV, three lattice spacings of $a\\sim\\{0.15, 0.12, 0.09\\}$~fm, and we do a dedicated volume study with $m_\u03c0L\\sim\\{3.22, 4.29, 5.36\\}$. Control over all relevant sources of systematic uncertainty are demonstrated and quantified. We achieve a preliminary value of $g_A = 1.285(17)$, with a relative uncertainty of 1.33\\%.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The Advanced LIGO and Advanced Virgo observatories recently discovered gravitational waves from a binary neutron star inspiral. A short gamma-ray burst (GRB) that followed the merger of this binary was also recorded by the Fermi Gamma-ray Burst Monitor (Fermi-GBM), and the Anticoincidence Shield for the Spectrometer for the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), indicating particle acceleration by the source. The precise location of the event was determined by optical detections of emission following the merger. We searched for high-energy neutrinos from the merger in the GeV--EeV energy range using the ANTARES, IceCube, and Pierre Auger Observatories. No neutrinos directionally coincident with the source were detected within $\\pm500$ s around the merger time. Additionally, no MeV neutrino burst signal was detected coincident with the merger. We further carried out an extended search in the direction of the source for high-energy neutrinos within the 14-day period following the merger, but found no evidence of emission. We used these results to probe dissipation mechanisms in relativistic outflows driven by the binary neutron star merger. The non-detection is consistent with model predictions of short GRBs observed at a large off-axis angle.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "On 2017 August 17 the merger of two compact objects with masses consistent with two neutron stars was discovered through gravitational-wave (GW170817), gamma-ray (GRB 170817A), and optical (SSS17a/AT 2017gfo) observations. The optical source was associated with the early-type galaxy NGC 4993 at a distance of just $\\sim$40 Mpc, consistent with the gravitational-wave measurement, and the merger was localized to be at a projected distance of $\\sim$2 kpc away from the galaxy's center. We use this minimal set of facts and the mass posteriors of the two neutron stars to derive the first constraints on the progenitor of GW170817 at the time of the second supernova (SN). We generate simulated progenitor populations and follow the three-dimensional kinematic evolution from the binary neutron star (BNS) birth to the merger time, accounting for pre-SN galactic motion, for considerably different input distributions of the progenitor mass, pre-SN semimajor axis, and SN-kick velocity. Though not considerably tight, we find these constraints to be comparable to those for Galactic BNS progenitors. The derived constraints are very strongly influenced by the requirement of keeping the binary bound after the second SN and having the merger occur relatively close to the center of the galaxy. These constraints are insensitive to the galaxy's star formation history, provided the stellar populations are older than 1 Gyr.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The LIGO Scientific and Virgo Collaborations have announced the first detection of gravitational waves from the coalescence of two neutron stars. The merger rate of binary neutron stars estimated from this event suggests that distant, unresolvable binary neutron stars create a significant astrophysical stochastic gravitational-wave background. The binary neutron star background will add to the background from binary black holes, increasing the amplitude of the total astrophysical background relative to previous expectations. In the Advanced LIGO-Virgo frequency band most sensitive to stochastic backgrounds (near 25 Hz), we predict a total astrophysical background with amplitude $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.8_{-1.3}^{+2.7} \\times 10^{-9}$ with $90\\%$ confidence, compared with $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.1_{-0.7}^{+1.2} \\times 10^{-9}$ from binary black holes alone. Assuming the most probable rate for compact binary mergers, we find that the total background may be detectable with a signal-to-noise-ratio of 3 after 40 months of total observation time, based on the expected timeline for Advanced LIGO and Virgo to reach their design sensitivity.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The source of the gravitational-wave signal GW170817, very likely a binary neutron star merger, was also observed electromagnetically, providing the first multi-messenger observations of this type. The two week long electromagnetic counterpart had a signature indicative of an r-process-induced optical transient known as a kilonova. This Letter examines how the mass of the dynamical ejecta can be estimated without a direct electromagnetic observation of the kilonova, using gravitational-wave measurements and a phenomenological model calibrated to numerical simulations of mergers with dynamical ejecta. Specifically, we apply the model to the binary masses inferred from the gravitational-wave measurements, and use the resulting mass of the dynamical ejecta to estimate its contribution (without the effects of wind ejecta) to the corresponding kilonova light curves from various models. The distributions of dynamical ejecta mass range between $M_{ej} = 10^{-3} - 10^{-2} M_{\\odot}$ for various equations of state, assuming the neutron stars are rotating slowly. In addition, we use our estimates of the dynamical ejecta mass and the neutron star merger rates inferred from GW170817 to constrain the contribution of events like this to the r-process element abundance in the Galaxy when ejecta mass from post-merger winds is neglected. We find that if $\\gtrsim10\\%$ of the matter dynamically ejected from BNS mergers is converted to r-process elements, GW170817-like BNS mergers could fully account for the amount of r-process material observed in the Milky Way.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The detection of GW170817 in both gravitational waves and electromagnetic waves heralds the age of gravitational-wave multi-messenger astronomy. On 17 August 2017 the Advanced LIGO and Virgo detectors observed GW170817, a strong signal from the merger of a binary neutron-star system. Less than 2 seconds after the merger, a gamma-ray burst (GRB 170817A) was detected within a region of the sky consistent with the LIGO-Virgo-derived location of the gravitational-wave source. This sky region was subsequently observed by optical astronomy facilities, resulting in the identification of an optical transient signal within $\\sim 10$ arcsec of the galaxy NGC 4993. These multi-messenger observations allow us to use GW170817 as a standard siren, the gravitational-wave analog of an astronomical standard candle, to measure the Hubble constant. This quantity, which represents the local expansion rate of the Universe, sets the overall scale of the Universe and is of fundamental importance to cosmology. Our measurement combines the distance to the source inferred purely from the gravitational-wave signal with the recession velocity inferred from measurements of the redshift using electromagnetic data. This approach does not require any form of cosmic \"distance ladder;\" the gravitational wave analysis can be used to estimate the luminosity distance out to cosmological scales directly, without the use of intermediate astronomical distance measurements. We determine the Hubble constant to be $70.0^{+12.0}_{-8.0} \\, \\mathrm{km} \\, \\mathrm{s}^{-1} \\, \\mathrm{Mpc}^{-1}$ (maximum a posteriori and 68% credible interval). This is consistent with existing measurements, while being completely independent of them. Additional standard-siren measurements from future gravitational-wave sources will provide precision constraints of this important cosmological parameter.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Strong disorder in interacting quantum systems can give rise to the phenomenon of Many-Body Localization (MBL), which defies thermalization due to the formation of an extensive number of quasi local integrals of motion. The one particle operator content of these integrals of motion is related to the one particle orbitals of the one particle density matrix and shows a strong signature across the MBL transition as recently pointed out by Bera et al. [Phys. Rev. Lett. 115, 046603 (2015); Ann. Phys. 529, 1600356 (2017)]. We study the properties of the one particle orbitals of many-body eigenstates of an MBL system in one dimension. Using shift-and-invert MPS (SIMPS), a matrix product state method to target highly excited many-body eigenstates introduced in [Phys. Rev. Lett. 118, 017201 (2017)], we are able to obtain accurate results for large systems of sizes up to L = 64. We find that the one particle orbitals drawn from eigenstates at different energy densities have high overlap and their occupations are correlated with the energy of the eigenstates. Moreover, the standard deviation of the inverse participation ratio of these orbitals is maximal at the nose of the mobility edge. Also, the one particle orbitals decay exponentially in real space, with a correlation length that increases at low disorder. In addition, we find a 1/f distribution of the coupling constants of a certain range of the number operators of the OPOs, which is related to their exponential decay.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We synthesized the liquid crystal dimer and trimer members of a series of flexible linear oligomers and characterized their microscopic and nanoscopic properties using resonant soft x-ray scattering and a number of other experimental techniques. On the microscopic scale, the twist-bend phases of the dimer and trimer appear essentially identical. However, while the liquid crystal dimer exhibits a temperature-dependent variation of its twist-bend helical pitch varying from 100 - 170 \u00c5 on heating, the trimer exhibits an essentially temperature-independent pitch of 66 \u00c5, significantly shorter than those reported for other twist-bend forming materials in the literature. We attribute this to a specific combination of intrinsic conformational bend of the trimer molecules and a sterically favorable intercalation of the trimers over a commensurate fraction (two-thirds) of the molecular length. We develop a geometric model of the twist-bend phase for these materials with the molecules arranging into helical chain structures, and we fully determine their respective geometric parameters.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken. We propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We present results from the first directed search for nontensorial gravitational waves. While general relativity allows for tensorial (plus and cross) modes only, a generic metric theory may, in principle, predict waves with up to six different polarizations. This analysis is sensitive to continuous signals of scalar, vector or tensor polarizations, and does not rely on any specific theory of gravity. After searching data from the first observation run of the advanced LIGO detectors for signals at twice the rotational frequency of 200 known pulsars, we find no evidence of gravitational waves of any polarization. We report the first upper limits for scalar and vector strains, finding values comparable in magnitude to previously-published limits for tensor strain. Our results may be translated into constraints on specific alternative theories of gravity.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Autonomous vehicles are highly complex systems, required to function reliably in a wide variety of situations. Manually crafting software controllers for these vehicles is difficult, but there has been some success in using deep neural networks generated using machine-learning. However, deep neural networks are opaque to human engineers, rendering their correctness very difficult to prove manually; and existing automated techniques, which were not designed to operate on neural networks, fail to scale to large systems. This paper focuses on proving the adversarial robustness of deep neural networks, i.e. proving that small perturbations to a correctly-classified input to the network cannot cause it to be misclassified. We describe some of our recent and ongoing work on verifying the adversarial robustness of networks, and discuss some of the open questions we have encountered and how they might be addressed.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "This white paper summarizes the workshop \"U.S. Cosmic Visions: New Ideas in Dark Matter\" held at University of Maryland on March 23-25, 2017.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We report results of a deep all-sky search for periodic gravitational waves from isolated neutron stars in data from the first Advanced LIGO observing run. This search investigates the low frequency range of Advanced LIGO data, between 20 and 100 Hz, much of which was not explored in initial LIGO. The search was made possible by the computing power provided by the volunteers of the Einstein@Home project. We find no significant signal candidate and set the most stringent upper limits to date on the amplitude of gravitational wave signals from the target population, corresponding to a sensitivity depth of 48.7 [1/$\\sqrt{\\textrm{Hz}}$]. At the frequency of best strain sensitivity, near 100 Hz, we set 90% confidence upper limits of $1.8 \\times 10^{-25}$. At the low end of our frequency range, 20 Hz, we achieve upper limits of $3.9 \\times 10^{-24}$. At 55 Hz we can exclude sources with ellipticities greater than $10^{-5}$ within 100 pc of Earth with fiducial value of the principal moment of inertia of $10^{38} \\textrm{kg m}^2$.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We report on an all-sky search for periodic gravitational waves in the frequency band 20-475 Hz and with a frequency time derivative in the range of [-1.0, +0.1]e-8 Hz/s. Such a signal could be produced by a nearby spinning and slightly non-axisymmetric isolated neutron star in our galaxy. This search uses the data from Advanced LIGO's first observational run, O1. No periodic gravitational wave signals were observed, and upper limits were placed on their strengths. The lowest upper limits on worst-case (linearly polarized) strain amplitude h0 are 4e-25 near 170 Hz. For a circularly polarized source (most favorable orientation), the smallest upper limits obtained are 1.5e-25. These upper limits refer to all sky locations and the entire range of frequency derivative values. For a population-averaged ensemble of sky locations and stellar orientations, the lowest upper limits obtained for the strain amplitude are 2.5e-25.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We present the results of a semicoherent search for continuous gravitational waves from the low-mass X-ray binary Scorpius X-1, using data from the first Advanced LIGO observing run. The search method uses details of the modelled, parametrized continuous signal to combine coherently data separated by less than a specified coherence time, which can be adjusted to trade off sensitivity against computational cost. A search was conducted over the frequency range from 25 Hz to 2000 Hz, spanning the current observationally-constrained range of the binary orbital parameters. No significant detection candidates were found, and frequency-dependent upper limits were set using a combination of sensitivity estimates and simulated signal injections. The most stringent upper limit was set at 175 Hz, with comparable limits set across the most sensitive frequency range from 100 Hz to 200 Hz. At this frequency, the 95 pct upper limit on signal amplitude h0 is 2.3e-25 marginalized over the unknown inclination angle of the neutron star's spin, and 8.03e-26 assuming the best orientation (which results in circularly polarized gravitational waves). These limits are a factor of 3-4 stronger than those set by other analyses of the same data, and a factor of about 7 stronger than the best upper limits set using initial LIGO data. In the vicinity of 100 Hz, the limits are a factor of between 1.2 and 3.5 above the predictions of the torque balance model, depending on inclination angle, if the most likely inclination angle of 44 degrees is assumed, they are within a factor of 1.7.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We describe the observation of GW170104, a gravitational-wave signal produced by the coalescence of a pair of stellar-mass black holes. The signal was measured on January 4, 2017 at 10:11:58.6 UTC by the twin advanced detectors of the Laser Interferometer Gravitational-Wave Observatory during their second observing run, with a network signal-to-noise ratio of 13 and a false alarm rate less than 1 in 70,000 years. The inferred component black hole masses are $31.2^{+8.4}_{-6.0}\\,M_\\odot$ and $19.4^{+5.3}_{-5.9}\\,M_\\odot$ (at the 90% credible level). The black hole spins are best constrained through measurement of the effective inspiral spin parameter, a mass-weighted combination of the spin components perpendicular to the orbital plane, $\u03c7_\\mathrm{eff} = -0.12^{+0.21}_{-0.30}.$ This result implies that spin configurations with both component spins positively aligned with the orbital angular momentum are disfavored. The source luminosity distance is $880^{+450}_{-390}~\\mathrm{Mpc}$ corresponding to a redshift of $z = 0.18^{+0.08}_{-0.07}$. We constrain the magnitude of modifications to the gravitational-wave dispersion relation and perform null tests of general relativity. Assuming that gravitons are dispersed in vacuum like massive particles, we bound the graviton mass to $m_g \\le 7.7 \\times 10^{-23}~\\mathrm{eV}/c^2$. In all cases, we find that GW170104 is consistent with general relativity.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X - 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "During their first observational run, the two Advanced LIGO detectors attained an unprecedented sensitivity, resulting in the first direct detections of gravitational-wave signals and GW151226, produced by stellar-mass binary black hole systems. This paper reports on an all-sky search for gravitational waves (GWs) from merging intermediate mass black hole binaries (IMBHBs). The combined results from two independent search techniques were used in this study: the first employs a matched-filter algorithm that uses a bank of filters covering the GW signal parameter space, while the second is a generic search for GW transients (bursts). No GWs from IMBHBs were detected, therefore, we constrain the rate of several classes of IMBHB mergers. The most stringent limit is obtained for black holes of individual mass $100\\,M_\\odot$, with spins aligned with the binary orbital angular momentum. For such systems, the merger rate is constrained to be less than $0.93~\\mathrm{Gpc^{-3}\\,yr}^{-1}$ in comoving units at the $90\\%$ confidence level, an improvement of nearly 2 orders of magnitude over previous upper limits.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Results are presented from a semi-coherent search for continuous gravitational waves from the brightest low-mass X-ray binary, Scorpius X-1, using data collected during the first Advanced LIGO observing run (O1). The search combines a frequency domain matched filter (Bessel-weighted $\\mathcal{F}$-statistic) with a hidden Markov model to track wandering of the neutron star spin frequency. No evidence of gravitational waves is found in the frequency range 60-650 Hz. Frequentist 95% confidence strain upper limits, $h_0^{95\\%} = 4.0\\times10^{-25}$, $8.3\\times10^{-25}$, and $3.0\\times10^{-25}$ for electromagnetically restricted source orientation, unknown polarization, and circular polarization, respectively, are reported at 106 Hz. They are $\\leq 10$ times higher than the theoretical torque-balance limit at 106 Hz.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "We report on a lattice QCD calculation of the nucleon axial charge, $g_A$, using M\u00f6bius Domain-Wall fermions solved on the dynamical $N_f=2+1+1$ HISQ ensembles after they are smeared using the gradient-flow algorithm. The calculation is performed with three pion masses, $m_\u03c0\\sim\\{310,220,130\\}$ MeV. Three lattice spacings ($a\\sim\\{0.15,0.12,0.09\\}$ fm) are used with the heaviest pion mass, while the coarsest two spacings are used on the middle pion mass and only the coarsest spacing is used with the near physical pion mass. On the $m_\u03c0\\sim220$ MeV, $a\\sim0.12$ fm point, a dedicated volume study is performed with $m_\u03c0L \\sim \\{3.22,4.29,5.36\\}$. Using a new strategy motivated by the Feynman-Hellmann Theorem, we achieve a precise determination of $g_A$ with relatively low statistics, and demonstrable control over the excited state, continuum, infinite volume and chiral extrapolation systematic uncertainties, the latter of which remains the dominant uncertainty. Our final determination at 2.6\\% total uncertainty is $g_A = 1.278(21)(26)$, with the first uncertainty including statistical and systematic uncertainties from fitting and the second including model selection systematics related to the chiral and continuum extrapolation. The largest reduction of the second uncertainty will come from a greater number of pion mass points as well as more precise lattice QCD results near the physical pion mass.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "The twist-bend nematic liquid crystal phase is a three-dimensional fluid in which achiral bent molecules spontaneously form an orientationally ordered macroscopically chiral heliconical winding of molecular scale pitch, in absence of positional ordering. Here we characterize the structure of the ground state of the twist-bend phase of the bent dimer CB7CB and its mixtures with 5CB over a wide range of concentrations and temperatures, showing that the contour length along the molecular direction for a single turn of the helix is approximately equal to 2\u03c0Rmol, where Rmol is the radius of bend curvature of a single all-trans CB7CB molecule. This relation emerges from a model which simply relates the macroscopic characteristics of the helical structure, which is mostly biaxial twist and has little bend, to the bent molecular shape. This connection comes about through the presence in the fluid of self-assembled oligomer-like correlations of interlocking molecules, arising from the nanosegregation of rigid and flexible molecular subcomponents, forming a brickwork tiling of pairs of molecular strands into a duplex double-helical chain.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Since the celebrated discovery of graphene, the family of two-dimensional (2D) materials has grown to encompass a broad range of electronic properties. Recent additions include spin-valley coupled semiconductors, Ising superconductors that can be tuned into a quantum metal, possible Mott insulators with tunable charge-density waves, and topological semi-metals with edge transport. Despite this progress, there is still no 2D crystal with intrinsic magnetism, which would be useful for many technologies such as sensing, information, and data storage. Theoretically, magnetic order is prohibited in the 2D isotropic Heisenberg model at finite temperatures by the Mermin-Wagner theorem. However, magnetic anisotropy removes this restriction and enables, for instance, the occurrence of 2D Ising ferromagnetism. Here, we use magneto-optical Kerr effect (MOKE) microscopy to demonstrate that monolayer chromium triiodide (CrI3) is an Ising ferromagnet with out-of-plane spin orientation. Its Curie temperature of 45 K is only slightly lower than the 61 K of the bulk crystal, consistent with a weak interlayer coupling. Moreover, our studies suggest a layer-dependent magnetic phase transition, showcasing the hallmark thickness-dependent physical properties typical of van der Waals crystals. Remarkably, bilayer CrI3 displays suppressed magnetization with a metamagnetic effect, while in trilayer the interlayer ferromagnetism observed in the bulk crystal is restored. Our work creates opportunities for studying magnetism by harnessing the unique features of atomically-thin materials, such as electrical control for realizing magnetoelectronics, and van der Waals engineering for novel interface phenomena.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Despite the more than one order of magnitude difference between the measured dipole moments in $^{144}$Ba and $^{146}$Ba, the strength of the octupole correlations in $^{146}$Ba are found to be as strong as those in $^{144}$Ba with a similarly large value of $B(E3;3^- \\rightarrow 0^+)$ determined as 48($^{+21}_{-29}$) W.u. The new results not only establish unambiguously the presence of a region of octupole deformation centered on these neutron-rich Ba isotopes, but also manifest the dependence of the electric dipole moments on the occupancy of different neutron orbitals in nuclei with enhanced octupole strength, as revealed by fully microscopic calculations.\n        \u25b3 Less", "author": "David Clark"}, {"abstract": "Spurred by the growth of transportation network companies and increasing data capabilities, vehicle routing and ride-matching algorithms can improve the efficiency of private transportation services. However, existing routing solutions do not address where drivers should travel after dropping off a passenger and before receiving the next passenger ride request, i.e., during the between-ride period. We address this problem by developing an efficient algorithm to find the optimal policy for drivers between rides in order to maximize driver profits. We model the road network as a graph, and we show that the between-ride routing problem is equivalent to a stochastic shortest path problem, an infinite dynamic program with no discounting. We prove under reasonable assumptions that an optimal routing policy exists that avoids cycles; policies of this type can be efficiently found. We present an iterative approach to find an optimal routing policy. Our approach can account for various factors, including the frequency of passenger ride requests at different locations, traffic conditions, and surge pricing. We demonstrate the effectiveness of the approach by implementing it on road network data from Boston and New York City.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "This paper addresses two fundamental problems in the context of jump linear systems (JLS). The first problem is concerned with characterizing the minimal state space dimension solely from input--output pairs and without any knowledge of the number of mode switches. The second problem is concerned with characterizing the number of discrete modes of the JLS. For the first problem, we develop a linear system theory based approach and construct an appropriate Hankel--like matrix. The rank of this matrix gives us the state space dimension. For the second problem we show that minimal number of modes corresponds to the minimal rank of a positive semi--definite matrix obtained via a non--convex formulation.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "In this work, we aim to create a data marketplace; a robust real-time matching mechanism to efficiently buy and sell training data for Machine Learning tasks. While the monetization of data and pre-trained models is an essential focus of industry today, there does not exist a market mechanism to price training data and match buyers to vendors while still addressing the associated (computational and other) complexity. The challenge in creating such a market stems from the very nature of data as an asset: (i) it is freely replicable; (ii) its value is inherently combinatorial due to correlation with signal in other data; (iii) prediction tasks and the value of accuracy vary widely; (iv) usefulness of training data is difficult to verify a priori without first applying it to a prediction task. As our main contributions we: (i) propose a mathematical model for a two-sided data market and formally define the key associated challenges; (ii) construct algorithms for such a market to function and rigorously prove how they meet the challenges defined. We highlight two technical contributions: (i) a new notion of \"fairness\" required for cooperative games with freely replicable goods; (ii) a truthful, zero regret mechanism for auctioning a particular class of combinatorial goods based on utilizing Myerson's payment function and the Multiplicative Weights algorithm. These might be of independent interest.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We consider the problem of robustness in large consensus networks that occur in many areas such as distributed optimization. Robustness, in this context, is the scaling of performance measures, e.g. H2-norm, as a function of network dimension. We provide a formal framework to quantify the relation between such performance scaling and the convergence speed of the network. Specifically, we provide upper and lower bounds for the convergence speed in terms of robustness and discuss how these bounds scale with the network topology. The main contribution of this work is that we obtain tight bounds, that hold regardless of network topology. The work here also encompasses some results in convergence time analysis in previous literature.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "This paper examines the dependence of network performance measures on network size and considers scaling results for large networks. We connect two performance measures that are well studied, but appear to be unrelated. The first measure is concerned with energy metrics, namely the $\\Hcal_2$--norm of a network, which arises in control theory applications. The second measure is concerned with the notion of \"tail risk\" which arises in economic and financial networks. We study the question of why such performance measures may deteriorate at a faster rate than the growth rate of the network. We first focus on the energy metric and its well known connection to controllability Gramian of the underlying dynamical system. We show that undirected networks exhibit the most graceful energy growth rates as network size grows. This rate is quantified completely by the proximity of spectral radius to unity or distance to instability. In contrast, we show that the simple characterization of energy in terms of network spectrum does not exist for directed networks. We demonstrate that, for any fixed distance to instability, energy of a directed network can grow at an exponentially faster rate. We provide general methods for manipulating networks to reduce energy. In particular, we prove that certain operations that increase the symmetry in a network cannot increase energy (in an order sense). Secondly, we focus on tail risk in economic and financial networks. In contrast to $\\Hcal_2$--norm which arises from computing the expectation of energy in the network, tail risk focuses on tail probability behavior of network variables. Although the two measures differ substantially we show that they are precisely connected through the system Gramian. This surprising result explains why topology considerations rather than specific performance measures dictate the large scale behavior of networks.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "In coalitional games, traditional coalitional game theory does not apply if different participants hold different opinions about the payoff function that corresponds to each subset of the coalition. In this paper, we propose a framework in which players can exchange opinions about their views of payoff functions and then decide the distribution of the value of the grand coalition. When all players are truth-telling, the problem of opinion consensus is decoupled from the coalitional game, but interesting dynamics will arise when players are strategic in the consensus phase. Assuming that all players are rational, the model implies that, if influential players are risk-averse, an efficient fusion of the distributed data is achieved at pure strategy Nash equilibrium, meaning that the average opinion will not drift. Also, without the assumption that all players are rational, each player can use an algorithmic R-learning process, which gives the same result as the pure strategy Nash equilibrium with rational players.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "This paper analyzes the impact of peer effects on electricity consumption of a network of rational, utility-maximizing users. Users derive utility from consuming electricity as well as consuming less energy than their neighbors. However, a disutility is incurred for consuming more than their neighbors. To maximize the profit of the load-serving entity that provides electricity to such users, we develop a two-stage game-theoretic model, where the entity sets the prices in the first stage. In the second stage, consumers decide on their demand in response to the observed price set in the first stage so as to maximize their utility. To this end, we derive theoretical statements under which such peer effects reduce aggregate user consumption. Further, we obtain expressions for the resulting electricity consumption and profit of the load serving entity for the case of perfect price discrimination and a single price under complete information, and approximations under incomplete information. Simulations suggest that exposing only a selected subset of all users to peer effects maximizes the entity's profit.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Load-serving entities which procure electricity from the wholesale electricity market to service end-users face significant quantity and price risks due to the volatile nature of electricity demand and quasi-fixed residential tariffs at which electricity is sold. This paper investigates strategies for load serving entities to hedge against such price risks. Specifically, we compute profit-maximizing portfolios of forward contract and call options as a function of the uncertain aggregate user demand. We compare the profit to the case of Demand Response, where users are offered monetary incentives to temporarily reduce their consumption during periods of supply shortages. Using smart meter data of residential customers in California, we simulate optimal portfolios and derive conditions under which Demand Response outperforms call options and forward contracts.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Residential Demand Response has emerged as a viable tool to alleviate supply and demand imbalances of electricity, particularly during times when the electric grid is strained due a shortage of supply. Demand Response providers bid reduction capacity into the wholesale electricity market by asking their customers under contract to temporarily reduce their consumption in exchange for a monetary incentive. To contribute to the analysis of consumer behavior in response to such incentives, this paper formulates Demand Response as a Mechanism Design problem, where a Demand Response Provider elicits private information of its rational, profit-maximizing customers who derive positive expected utility by participating in reduction events. By designing an incentive compatible and individually rational mechanism to collect users' price elasticities of demand, the Demand Response provider can target the most susceptible users to incentives. We measure reductions by comparing the materialized consumption to the projected consumption, which we model as the \"10-in-10\"-baseline, the regulatory standard set by the California Independent System Operator. Due to the suboptimal performance of this baseline, we show, using consumption data of residential customers in California, that Demand Response Providers receive payments for \"virtual reductions\", which exist due to the inaccuracies of the baseline rather than actual reductions. Improving the accuracy of the baseline diminishes the contribution of these virtual reductions.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We investigate the ability of a homogeneous collection of deferrable energy loads to behave as a battery; that is, to absorb and release energy in a controllable fashion up to fixed and predetermined limits on volume, charge rate and discharge rate. We derive explicit bounds on the battery capacity that can be offered, and show that there is a fundamental trade-off between the abilities of collective load to absorb and release energy at high aggregate rates. Finally, we introduce a new class of dynamic priority-driven feedback policies that balance these abilities, and characterize the batteries that they can emulate.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We investigate the ability of a homogeneous collection of deferrable energy loads to behave as a battery; that is, to absorb and release energy in a controllable fashion up to fixed and predetermined limits on volume, charge rate and discharge rate. We derive bounds on the battery capacity that can be realized and show that there are fundamental trade-offs between battery parameters. By characterizing the state trajectories under scheduling policies that emulate two illustrative batteries, we show that the trade-offs occur because the states that allow the loads to absorb and release energy at high aggregate rates are conflicting.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "In this paper, we investigate the use of variable speed limits for resilient operation of transportation networks, which are modeled as dynamical flow networks under local routing decisions. In such systems, some external inflow is injected to the so-called origin nodes of the network. The total inflow arriving at each node is routed to its operational outgoing links based on their current particle densities. The density on each link has first order dynamics driven by the difference of its incoming and outgoing flows. A link irreversibly fails if it reaches its jam density. Such failures may propagate in the network and cause a systemic failure. We show that larger link capacities do not necessarily help in preventing systemic failures under local routing. Accordingly, we propose the use of variable speed limits to operate the links below their capacities, when necessary, to compensate for the lack of global information and coordination in routing decisions. Our main result shows that systemic failures under feasible external inflows can always be averted through a proper selection of speed limits if the routing decisions are sufficiently responsive to local congestion and the network is initially uncongested. This is an attractive feature as it is much easier in practice to adjust the speed limits than to build more physical capacity or to alter routing decisions that are determined by social behavior.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "This paper analyzes stability conditions for wholesale electricity markets under real-time retail pricing and realistic consumption models with memory, which explicitly take into account previous electricity prices and consumption levels. By passing on the current retail price of electricity from supplier to consumer and feeding the observed consumption back to the supplier, a closed-loop dynamical system for electricity prices and consumption arises whose stability is to be investigated. Under mild assumptions on the generation cost of electricity and consumers' backlog disutility functions, we show that, for consumer models with price memory only, market stability is achieved if the ratio between the consumers' marginal backlog disutility and the suppliers' marginal cost of supply remains below a fixed threshold. Further, consumer models with price and consumption memory can result in greater stability regions and faster convergence to the equilibrium compared to models with price memory alone, if consumption deviations from nominal demand are adequately penalized.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "In this paper, we are concerned with the resilience of locally routed network flows with finite link capacities. In this setting, an external inflow is injected to the so-called origin nodes. The total inflow arriving at each node is routed locally such that none of the outgoing links are overloaded unless the node receives an inflow greater than its total outgoing capacity. A link irreversibly fails if it is overloaded or if there is no operational link in its immediate downstream to carry its flow. For such systems, resilience is defined as the minimum amount of reduction in the link capacities that would result in the failure of all the outgoing links of an origin node. We show that such networks do not necessarily become more resilient as additional capacity is built in the network. Moreover, when the external inflow does not exceed the network capacity, selective reductions of capacity at certain links can actually help averting the cascading failures, without requiring any change in the local routing policies. This is an attractive feature as it is often easier in practice to reduce the available capacity of some critical links than to add physical capacity or to alter routing policies, e.g., when such policies are determined by social behavior, as in the case of road traffic networks. The results can thus be used for real-time monitoring of distance-to-failure in such networks and devising a feasible course of actions to avert systemic failures.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Voltage control plays an important role in the operation of electricity distribution networks, especially when there is a large penetration of renewable energy resources. In this paper, we focus on voltage control through reactive power compensation and study how different information structures affect the control performance. In particular, we first show that using only voltage measurements to determine reactive power compensation is insufficient to maintain voltage in the acceptable range. Then we propose two fully decentralized and robust algorithms by adding additional information, which can stabilize the voltage in the acceptable range. The one with higher complexity can further minimize a cost of reactive power compensation in a particular form. Both of the two algorithms use only local measurements and local variables and require no communication. In addition, the two algorithms are robust against heterogeneous update rates and delays.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We introduce a new class of (dynamical) systems that inherently capture cascading effects (viewed as consequential effects) and are naturally amenable to combinations. We develop an axiomatic general theory around those systems, and guide the endeavor towards an understanding of cascading failure. The theory evolves as an interplay of lattices and fixed points, and its results may be instantiated to commonly studied models of cascade effects.\n  We characterize the systems through their fixed points, and equip them with two operators. We uncover properties of the operators, and express global systems through combinations of local systems. We enhance the theory with a notion of failure, and understand the class of shocks inducing a system to failure. We develop a notion of mu-rank to capture the energy of a system, and understand the minimal amount of effort required to fail a system, termed resilience. We deduce a dual notion of fragility and show that the combination of systems sets a limit on the amount of fragility inherited.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Consider a stationary discrete random process with alphabet size d, which is assumed to be the output process of an unknown stationary Hidden Markov Model (HMM). Given the joint probabilities of finite length strings of the process, we are interested in finding a finite state generative model to describe the entire process. In particular, we focus on two classes of models: HMMs and quasi-HMMs, which is a strictly larger class of models containing HMMs. In the main theorem, we show that if the random process is generated by an HMM of order less or equal than k, and whose transition and observation probability matrix are in general position, namely almost everywhere on the parameter space, both the minimal quasi-HMM realization and the minimal HMM realization can be efficiently computed based on the joint probabilities of all the length N strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to compare and connect the two lines of literature: realization theory of HMMs, and the recent development in learning latent variable models with tensor decomposition techniques.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We propose a dynamical model for cascading failures in single-commodity network flows. In the proposed model, the network state consists of flows and activation status of the links. Network dynamics is determined by a, possibly state-dependent and adversarial, disturbance process that reduces flow capacity on the links, and routing policies at the nodes that have access to the network state, but are oblivious to the presence of disturbance. Under the proposed dynamics, a link becomes irreversibly inactive either due to overload condition on itself or on all of its immediate downstream links. The coupling between link activation and flow dynamics implies that links to become inactive successively are not necessarily adjacent to each other, and hence the pattern of cascading failure under our model is qualitatively different than standard cascade models. The magnitude of a disturbance process is defined as the sum of cumulative capacity reductions across time and links of the network, and the margin of resilience of the network is defined as the infimum over the magnitude of all disturbance processes under which the links at the origin node become inactive. We propose an algorithm to compute an upper bound on the margin of resilience for the setting where the routing policy only has access to information about the local state of the network. For the limiting case when the routing policies update their action as fast as network dynamics, we identify sufficient conditions on network parameters under which the upper bound is tight under an appropriate routing policy. Our analysis relies on making connections between network parameters and monotonicity in network state evolution under proposed dynamics.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "The primary concerns of this paper are twofold: to understand the economic value of storage in the presence of ramp constraints and exogenous electricity prices, and to understand the implications of the associated optimal storage management policy on qualitative and quantitative characteristics of storage response to real-time prices. We present an analytic characterization of the optimal policy, along with the associated finite-horizon time-averaged value of storage. We also derive an analytical upperbound on the infinite-horizon time-averaged value of storage. This bound is valid for any achievable realization of prices when the support of the distribution is fixed, and highlights the dependence of the value of storage on ramp constraints and storage capacity. While the value of storage is a non-decreasing function of price volatility, due to the finite ramp rate, the value of storage saturates quickly as the capacity increases, regardless of volatility. To study the implications of the optimal policy, we first present computational experiments that suggest that optimal utilization of storage can, in expectation, induce a considerable amount of price elasticity near the average price, but little or no elasticity far from it. We then present a computational framework for understanding the behavior of storage as a function of price and the amount of stored energy, and for characterization of the buy/sell phase transition region in the price-state plane. Finally, we study the impact of market-based operation of storage on the required reserves, and show that the reserves may need to be expanded to accommodate market-based storage.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We study a model for cascade effects over finite networks based on a deterministic binary linear threshold model. Our starting point is a networked coordination game where each agent's payoff is the sum of the payoffs coming from pairwise interactions with each of the neighbors. We first establish that the best response dynamics in this networked game is equivalent to the linear threshold dynamics with heterogeneous thresholds over the agents. While the previous literature has studied such linear threshold models under the assumption that each agent may change actions at most once, a study of best response dynamics in such networked games necessitates an analysis that allows for multiple switches in actions. In this paper, we develop such an analysis and construct a combinatorial framework to understand the behavior of the model. To this end, we establish that the agents behavior cycles among different actions in the limit and provide three sets of results.\n  We first characterize the limiting behavioral properties of the dynamics. We determine the length of the limit cycles and reveal bounds on the time steps required to reach such cycles for different network structures. We then study the complexity of decision/counting problems that arise within the context. Specifically, we consider the tractability of counting the number of limit cycles and fixed-points, and deciding the reachability of action profiles. We finally propose a measure of network resilience that captures the nature of the involved dynamics. We prove bounds and investigate the resilience of different network structures under this measure.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "In this paper, we examine in an abstract framework, how a tradeoff between efficiency and robustness arises in different dynamic oligopolistic market architectures. We consider a market in which there is a monopolistic resource provider and agents that enter and exit the market following a random process. Self-interested and fully rational agents dynamically update their resource consumption decisions over a finite time horizon, under the constraint that the total resource consumption requirements are met before each individual's deadline. We then compare the statistics of the stationary aggregate demand processes induced by the non-cooperative and cooperative load scheduling schemes. We show that although the non-cooperative load scheduling scheme leads to an efficiency loss - widely known as the \"price of anarchy\" - the stationary distribution of the corresponding aggregate demand process has a smaller tail. This tail, which corresponds to rare and undesirable demand spikes, is important in many applications of interest. On the other hand, when the agents can cooperate with each other in optimizing their total cost, a higher market efficiency is achieved at the cost of a higher probability of demand spikes. We thus posit that the origins of endogenous risk in such systems may lie in the market architecture, which is an inherent characteristic of the system.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Robustness of routing policies for networks is a central problem which is gaining increased attention with a growing awareness to safeguard critical infrastructure networks against natural and man-induced disruptions. Routing under limited information and the possibility of cascades through the network adds serious challenges to this problem. This abstract considers the framework of dynamical networks introduced in our earlier work [1,2], where the network is modeled by a system of ordinary differential equations derived from mass conservation laws on directed acyclic graphs with a single origin-destination pair and a constant inflow at the origin. The rate of change of the particle density on each link of the network equals the difference between the inflow and the outflow on that link. The latter is modeled to depend on the current particle density on that link through a flow function. The novel modeling element in this paper is that every link is assumed to have finite capacity for particle density and that the flow function is modeled to be strictly increasing as density increases from zero up to the maximum density capacity, and is discontinuous at the maximum density capacity, with the flow function value being zero at that point. This feature, in particular, allows for the possibility of spill-backs in our model. In this paper, we present our results on resilience of such networks under distributed routing, towards perturbations that reduce link-wise flow functions.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "This paper examines the value of storage in securing reliability of a system with uncertain supply and demand, and supply friction. The storage is frictionless as a supply source, but once used, it cannot be filled up instantaneously. The focus application is a power supply network in which the base supply and demand are assumed to match perfectly, while deviations from the base are modeled as random shocks with stochastic arrivals. Due to friction, the random surge shocks cannot be tracked by the main supply sources. Storage, when available, can be used to compensate, fully or partially, for the surge in demand or loss of supply. The problem of optimal utilization of storage with the objective of maximizing system reliability is formulated as minimization of the expected discounted cost of blackouts over an infinite horizon. It is shown that when the stage cost is linear in the size of the blackout, the optimal policy is myopic in the sense that all shocks are compensated by storage up to the available level of storage. However, when the stage cost is strictly convex, it may be optimal to curtail some of the demand and allow a small current blackout in the interest of maintaining a higher level of reserve to avoid a large blackout in the future. The value of storage capacity in improving system's reliability, as well as the effects of the associated optimal policies under different stage costs on the probability distribution of blackouts are examined.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We propose a general methodology for performing statistical inference within a `rare-events regime' that was recently suggested by Wagner, Viswanath and Kulkarni. Our approach allows one to easily establish consistent estimators for a very large class of canonical estimation problems, in a large alphabet setting. These include the problems studied in the original paper, such as entropy and probability estimation, in addition to many other interesting ones. We particularly illustrate this approach by consistently estimating the size of the alphabet and the range of the probabilities. We start by proposing an abstract methodology based on constructing a probability measure with the desired asymptotic properties. We then demonstrate two concrete constructions by casting the Good-Turing estimator as a pseudo-empirical measure, and by using the theory of mixture model estimation.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "The paper proposes a framework for modeling and analysis of the dynamics of supply, demand, and clearing prices in power system with real-time retail pricing and information asymmetry. Real-time retail pricing is characterized by passing on the real-time wholesale electricity prices to the end consumers, and is shown to create a closed-loop feedback system between the physical layer and the market layer of the power system. In the absence of a carefully designed control law, such direct feedback between the two layers could increase volatility and lower the system's robustness to uncertainty in demand and generation. A new notion of generalized price-elasticity is introduced, and it is shown that price volatility can be characterized in terms of the system's maximal relative price elasticity, defined as the maximal ratio of the generalized price-elasticity of consumers to that of the producers. As this ratio increases, the system becomes more volatile, and eventually, unstable. As new demand response technologies and distributed storage increase the price-elasticity of demand, the architecture under examination is likely to lead to increased volatility and possibly instability. This highlights the need for assessing architecture systematically and in advance, in order to optimally strike the trade-offs between volatility, economic efficiency, and system reliability.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Strong resilience properties of dynamical flow networks are analyzed for distributed routing policies. The latter are characterized by the property that the way the inflow at a non-destination node gets split among its outgoing links is allowed to depend only on local information about the current particle densities on the outgoing links. The strong resilience of the network is defined as the infimum sum of link-wise flow capacity reductions under which the network cannot maintain the asymptotic total inflow to the destination node to be equal to the inflow at the origin. A class of distributed routing policies that are locally responsive to local information is shown to yield the maximum possible strong resilience under such local information constraints for an acyclic dynamical flow network with a single origin-destination pair. The maximal strong resilience achievable is shown to be equal to the minimum node residual capacity of the network. The latter depends on the limit flow of the unperturbed network and is defined as the minimum, among all the non-destination nodes, of the sum, over all the links outgoing from the node, of the differences between the maximum flow capacity and the limit flow of the unperturbed network. We propose a simple convex optimization problem to solve for equilibrium limit flows of the unperturbed network that minimize average delay subject to strong resilience guarantees, and discuss the use of tolls to induce such an equilibrium limit flow in transportation networks. Finally, we present illustrative simulations to discuss the connection between cascaded failures and the resilience properties of the network.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Robustness of distributed routing policies is studied for dynamical flow networks, with respect to adversarial disturbances that reduce the link flow capacities. A dynamical flow network is modeled as a system of ordinary differential equations derived from mass conservation laws on a directed acyclic graph with a single origin-destination pair and a constant inflow at the origin. Routing policies regulate the way the inflow at a non-destination node gets split among its outgoing links as a function of the current particle density, while the outflow of a link is modeled to depend on the current particle density on that link through a flow function. The dynamical flow network is called partially transferring if the total inflow at the destination node is asymptotically bounded away from zero, and its weak resilience is measured as the minimum sum of the link-wise magnitude of all disturbances that make it not partially transferring. The weak resilience of a dynamical flow network with arbitrary routing policy is shown to be upper-bounded by the network's min-cut capacity, independently of the initial flow conditions. Moreover, a class of distributed routing policies that rely exclusively on local information on the particle densities, and are locally responsive to that, is shown to yield such maximal weak resilience. These results imply that locality constraints on the information available to the routing policies do not cause loss of weak resilience. Some fundamental properties of dynamical flow networks driven by locally responsive distributed policies are analyzed in detail, including global convergence to a unique limit flow.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Stability of Wardrop equilibria is analyzed for dynamical transportation networks in which the drivers' route choices are influenced by information at multiple temporal and spatial scales. The considered model involves a continuum of indistinguishable drivers commuting between a common origin/destination pair in an acyclic transportation network. The drivers' route choices are affected by their, relatively infrequent, perturbed best responses to global information about the current network congestion levels, as well as their instantaneous local observation of the immediate surroundings as they transit through the network. A novel model is proposed for the drivers' route choice behavior, exhibiting local consistency with their preference toward globally less congested paths as well as myopic decisions in favor of locally less congested paths. The simultaneous evolution of the traffic congestion on the network and of the aggregate path preference is modeled by a system of coupled ordinary differential equations. The main result shows that, if the frequency of updates of path preferences is sufficiently small as compared to the frequency of the traffic flow dynamics, then the state of the transportation network ultimately approaches a neighborhood of the Wardrop equilibrium. The presented results may be read as a further evidence in support of Wardrop's postulate of equilibrium, showing robustness of it with respect to non-persistent perturbations. The proposed analysis combines techniques from singular perturbation theory, evolutionary game theory, and cooperative dynamical systems.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We demonstrate the use of a new, control-oriented notion of finite state approximation for a particular class of hybrid systems. Specifically, we consider the problem of designing a stabilizing binary output feedback switching controller for a pair of unstable homogeneous second order systems. The constructive approach presented in this note, in addition to yielding an explicit construction of a deterministic finite state approximate model of the hybrid plant, allows us to efficiently establish a useable upper bound on the quality of approximation, and leads to a discrete optimization problem whose solution immediately provides a certifiably correct-by-design controller for the original system. The resulting controller consists of a finite state observer for the plant and a corresponding full state feedback switching control law.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "A set of N independent Gaussian linear time invariant systems is observed by M sensors whose task is to provide the best possible steady-state causal minimum mean square estimate of the state of the systems, in addition to minimizing a steady-state measurement cost. The sensors can switch between systems instantaneously, and there are additional resource constraints, for example on the number of sensors which can observe a given system simultaneously. We first derive a tractable relaxation of the problem, which provides a bound on the achievable performance. This bound can be computed by solving a convex program involving linear matrix inequalities. Exploiting the additional structure of the sites evolving independently, we can decompose this program into coupled smaller dimensional problems. In the scalar case with identical sensors, we give an analytical expression of an index policy proposed in a more general context by Whittle. In the general case, we develop open-loop periodic switching policies whose performance matches the bound arbitrarily closely.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "We extend a relaxation technique due to Bertsimas and Nino-Mora for the restless bandit problem to the case where arbitrary costs penalize switching between the bandits. We also construct a one-step lookahead policy using the solution of the relaxation. Computational experiments and a bound for approximate dynamic programming provide some empirical support for the heuristic.\n        \u25b3 Less", "author": "Munther Dahleh"}, {"abstract": "Verifying robustness of neural network classifiers has attracted great interests and attention due to the success of deep neural networks and their unexpected vulnerability to adversarial perturbations. Although finding minimum adversarial distortion of neural networks (with ReLU activations) has been shown to be an NP-complete problem, obtaining a non-trivial lower bound of minimum distortion as a provable robustness guarantee is possible. However, most previous works only focused on simple fully-connected layers (multilayer perceptrons) and were limited to ReLU activations. This motivates us to propose a general and efficient framework, CNN-Cert, that is capable of certifying robustness on general convolutional neural networks. Our framework is general -- we can handle various architectures including convolutional layers, max-pooling layers, batch normalization layer, residual blocks, as well as general activation functions; our approach is efficient -- by exploiting the special structure of convolutional layers, we achieve up to 17 and 11 times of speed-up compared to the state-of-the-art certification algorithms (e.g. Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach while our algorithm obtains similar or even better verification bounds. In addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and CROWN. We demonstrate by extensive experiments that our method outperforms state-of-the-art lower-bound-based certification algorithms in terms of both bound quality and speed.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The transition from monolayers to multilayered structures in bacterial colonies is a fundamental step in biofilm development. Observed across different morphotypes and species, this transition is triggered within freely growing bacterial microcolonies comprising a few hundred cells. Using a combination of numerical simulations and analytical modeling, here we demonstrate that this transition originates from the competition between growth-induced in-plane active stresses and vertical restoring forces, due to the cell-substrate interactions. Using a simple chain-like colony of laterally confined cells, we show that the transition is triggered by the mechanical instability of individual cells, thus it is localized and mechanically deterministic. Asynchronous cell division renders the process stochastic, so that all the critical parameters that control the onset of the transition are continuously distributed random variables. Upon modeling cell division as a Poisson process, we can approximately calculate the probability distribution function of the position and time associated with the first extrusion. The rate of such a Poisson process can be identified as the order parameter of the transition, thus highlighting its mixed deterministic/stochastic nature.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The electron-phonon coupling strength in the spin-split valence band maximum of single-layer MoS$_2$ is studied using angle-resolved photoemission spectroscopy and density functional theory-based calculations. Values of the electron-phonon coupling parameter $\u03bb$ are obtained by measuring the linewidth of the spin-split bands as a function of temperature and fitting the data points using a Debye model. The experimental values of $\u03bb$ for the upper and lower spin-split bands at K are found to be 0.05 and 0.32, respectively, in excellent agreement with the calculated values for a free-standing single-layer MoS$_2$. The results are discussed in the context of spin and phase-space restricted scattering channels, as reported earlier for single-layer WS$_2$ on Au(111). The fact that the absolute valence band maximum in single-layer MoS$_2$ at K is almost degenerate with the local valence band maximum at $\u0393$ can potentially be used to tune the strength of the electron-phonon interaction in this material.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Recent observational and analytical studies suggested that a new regime of kinetic turbulence may exist in plasma environments with low electron beta (Chen and Boldyrev, 2017). Such a regime, termed inertial kinetic-Alfv\u00e9n turbulence, is relevant for the solar corona, Earth's magnetosheath, and other astrophysical systems where the electron and ion plasma beta parameters satisfy the condition $\u03b2_e\\ll \u03b2_i\\lesssim 1$. In this paper we present kinetic numerical simulations that confirm existence of the iKAW regime. Specifically, the simulations demonstrate a transition at scales below electron inertial length $d_e$ when $\u03b2_e\\ll \u03b2_i\\lesssim 1$. Spectral slopes and other statistical properties of turbulence at sub-$d_e$ scales are consistent with the phenomenological theory of inertial kinetic-Alfv\u00e9n turbulence proposed by Chen and Boldyrev (2017) and with the recent observations in the Earth's magnetosheath.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "CLEVER (Cross-Lipschitz Extreme Value for nEtwork Robustness) is an Extreme Value Theory (EVT) based robustness score for large-scale deep neural networks (DNNs). In this paper, we propose two extensions on this robustness score. First, we provide a new formal robustness guarantee for classifier functions that are twice differentiable. We apply extreme value theory on the new formal robustness guarantee and the estimated robustness is called second-order CLEVER score. Second, we discuss how to handle gradient masking, a common defensive technique, using CLEVER with Backward Pass Differentiable Approximation (BPDA). With BPDA applied, CLEVER can evaluate the intrinsic robustness of neural networks of a broader class -- networks with non-differentiable input transformations. We demonstrate the effectiveness of CLEVER with BPDA in experiments on a 121-layer Densenet model trained on the ImageNet dataset.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Sterile neutrinos are a minimal extension of the Standard Model of Particle Physics. If their mass is in the kilo-electron-volt regime, they are viable dark matter candidates. One way to search for sterile neutrinos in a laboratory-based experiment is via tritium-beta decay, where the new neutrino mass eigenstate would manifest itself as a kink-like distortion of the $\u03b2$-decay spectrum. The objective of the TRISTAN project is to extend the KATRIN setup with a new multi-pixel silicon drift detector system to search for a keV-scale sterile neutrino signal. In this paper we describe the requirements of such a new detector, and present first characterization measurement results obtained with a 7-pixel prototype system.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Electromagnetic fields coupled with mechanical degrees of freedom have recently shown exceptional and innovative applications, ultimately leading to mesoscopic optomechanical devices operating in the quantum regime of motion. Simultaneously, micromechanical elements have provided new ways to enhance and manipulate the optical properties of passive photonic elements. Following this concept, in this article we show how combining a chiral metasurface with a GaAs suspended micromembrane can offer new scenarios for controlling the polarization state of near-infrared light beams. Starting from the uncommon properties of chiral metasurface to statically realize target polarization states and circular and linear dichroism, we report mechanically induced, ~300 kHz polarization modulation, which favorably compares, in terms of speed, with liquid-crystals commercial devices. Moreover, we demonstrate how the mechanical resonance can be non-trivially affected by the input light polarization (and chiral state) via a thermoelastic effect triggered by intracavity photons. This work inaugurates the field of Polarization Optomechanics, which could pave the way to fast polarimetric devices, polarization modulators and dynamically tunable chiral state generators and detectors, as well as giving access to new form of polarization nonlinearities and control.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Background: HIV treatment prescription is a complex process; clinical decision support systems (CDSS) can assist clinicians to choose optimal treatments. These support systems are based on clinical trials and expert knowledge; however, the amount of data available to these systems is limited. For this reason, CDSSs could be significantly improved by using the knowledge obtained by treating HIV patients. This knowledge is mainly contained in patient records, whose usage is restricted due to privacy and confidentiality constraints.\n  Methods: A treatment effectiveness measure, containing valuable information for HIV treatment prescription, was defined and a method to extract this measure from patient records was developed. This method uses an advanced cryptographic technology, known as secure Multiparty Computation (henceforth referred to as MPC), to preserve the privacy of the patient records and the confidentiality of the clinicians' decisions.\n  Results: Our solution enables to compute the effectiveness measure of an HIV treatment based on patient records, while preserving privacy. Moreover, clinicians are not burdened with the computational and communication costs introduced by the privacy-preserving techniques that are used. Our system is able to compute the effectiveness of 100 treatments for a specific patient in less than 24 minutes, querying a database containing 20,000 patient records.\n  Conclusion: This paper presents a novel and efficient HIV clinical decision support system, that harnesses the potential and insights acquired from treatment data, while preserving the privacy of patient records and the confidentiality of clinician decisions.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The chemical composition of an exoplanet is a key ingredient in constraining its formation history. Iron is the most abundant transition metal, but has never been directly detected in an exoplanet due to its highly refractory nature. KELT-9b (HD 195689b) is the archetype of the class of ultra-hot Jupiters that straddle the transition between stars and gas-giant exoplanets and serve as distinctive laboratories for studying atmospheric chemistry, because of its high equilibrium temperature of 4050 +/- 180 K. These properties imply that its atmosphere is a tightly constrained chemical system that is expected to be nearly in chemical equilibrium and cloud-free. It was previously predicted that the spectral lines of iron will be detectable in the visible range of wavelengths. At these high temperatures, iron and several other transition metals are not sequestered in molecules or cloud particles and exist solely in their atomic forms. Here, we report the direct detection of atomic neutral and singly-ionized iron (Fe and Fe+), and singly-ionized titanium (Ti+) in KELT-9b via the cross-correlation technique applied to high-resolution spectra obtained during the primary transit of the exoplanet.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The spin structure of the valence and conduction bands at the $\\overline{\\text{K}}$ and $\\overline{\\text{K}}$' valleys of single-layer WS$_2$ on Au(111) is determined by spin- and angle-resolved photoemission and inverse photoemission. The bands confining the direct band gap of 1.98 eV are out-of-plane spin polarized with spin-dependent energy splittings of 417 meV in the valence band and 16 meV in the conduction band. The sequence of the spin-split bands is the same in the valence and in the conduction bands and opposite at the $\\overline{\\text{K}}$ and the $\\overline{\\text{K}}$' high-symmetry points. The first observation explains \"dark\" excitons discussed in optical experiments, the latter points to coupled spin and valley physics in electron transport. The experimentally observed band dispersions are discussed along with band structure calculations for a freestanding single layer and for a single layer on Au(111).\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The X-ray Integral Field Unit (X-IFU) is the high resolution X-ray spectrometer of the ESA Athena X-ray observatory. Over a field of view of 5' equivalent diameter, it will deliver X-ray spectra from 0.2 to 12 keV with a spectral resolution of 2.5 eV up to 7 keV on ~5 arcsecond pixels. The X-IFU is based on a large format array of super-conducting molybdenum-gold Transition Edge Sensors cooled at about 90 mK, each coupled with an absorber made of gold and bismuth with a pitch of 249 microns. A cryogenic anti-coincidence detector located underneath the prime TES array enables the non X-ray background to be reduced. A bath temperature of about 50 mK is obtained by a series of mechanical coolers combining 15K Pulse Tubes, 4K and 2K Joule-Thomson coolers which pre-cool a sub Kelvin cooler made of a 3He sorption cooler coupled with an Adiabatic Demagnetization Refrigerator. Frequency domain multiplexing enables to read out 40 pixels in one single channel. A photon interacting with an absorber leads to a current pulse, amplified by the readout electronics and whose shape is reconstructed on board to recover its energy with high accuracy. The defocusing capability offered by the Athena movable mirror assembly enables the X-IFU to observe the brightest X-ray sources of the sky (up to Crab-like intensities) by spreading the telescope point spread function over hundreds of pixels. Thus the X-IFU delivers low pile-up, high throughput (>50%), and typically 10 eV spectral resolution at 1 Crab intensities, i.e. a factor of 10 or more better than Silicon based X-ray detectors. In this paper, the current X-IFU baseline is presented, together with an assessment of its anticipated performance in terms of spectral resolution, background, and count rate capability. The X-IFU baseline configuration will be subject to a preliminary requirement review that is scheduled at the end of 2018.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Neuronal activity in the brain generates synchronous oscillations of the Local Field Potential (LFP). The traditional analyses of the LFPs are based on decomposing the signal into simpler components, such as sinusoidal harmonics. However, a common drawback of such methods is that the decomposition primitives are usually presumed from the onset, which may bias our understanding of the signal's structure. Here, we introduce an alternative approach that allows an impartial, high resolution, hands-off decomposition of the brain waves into a small number of discrete, frequency-modulated oscillatory processes, which we call oscillons. In particular, we demonstrate that mouse hippocampal LFP contain a single oscillon that occupies the $\u03b8$-frequency band and a couple of $\u03b3$-oscillons that correspond, respectively, to slow and fast $\u03b3$-waves. Since the oscillons were identified empirically, they may represent the actual, physical structure of synchronous oscillations in neuronal ensembles, whereas Fourier-defined \"brain waves\" are nothing but poorly resolved oscillons.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We present a complete characterisation at the nanoscale of the growth and structure of single-layer tungsten disulfide (WS$_2$) epitaxially grown on Au(111). Following the growth process in real time with fast x-ray photoelectron spectroscopy, we obtain a singly-oriented layer by choosing the proper W evaporation rate and substrate temperature during the growth. Information about the morphology, size and layer stacking of the WS$_2$ layer were achieved by employing x-ray photoelectron diffraction and low-energy electron microscopy. The strong spin splitting in the valence band of WS$_2$ coupled with the single-orientation character of the layer make this material the ideal candidate for the exploitation of the spin and valley degrees of freedom.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We report on a search for Weakly Interacting Massive Particles (WIMPs) using 278.8 days of data collected with the XENON1T experiment at LNGS. XENON1T utilizes a liquid xenon time projection chamber with a fiducial mass of $(1.30 \\pm 0.01)$ t, resulting in a 1.0 t$\\times$yr exposure. The energy region of interest, [1.4, 10.6] $\\mathrm{keV_{ee}}$ ([4.9, 40.9] $\\mathrm{keV_{nr}}$), exhibits an ultra-low electron recoil background rate of $(82\\substack{+5 \\\\ -3}\\textrm{ (sys)}\\pm3\\textrm{ (stat)})$ events/$(\\mathrm{t}\\times\\mathrm{yr}\\times\\mathrm{keV_{ee}})$. No significant excess over background is found and a profile likelihood analysis parameterized in spatial and energy dimensions excludes new parameter space for the WIMP-nucleon spin-independent elastic scatter cross-section for WIMP masses above 6 GeV/c${}^2$, with a minimum of $4.1\\times10^{-47}$ cm$^2$ at 30 GeV/c${}^2$ and 90% confidence level.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer CAV17]. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or delivering low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms Fast-Lin and Fast-Lip that are able to certify non-trivial lower bounds of minimum distortions, by bounding the ReLU units with appropriate linear functions Fast-Lin, or by bounding the local Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods deliver bounds close to (the gap is 2-3X) exact minimum distortion found by Reluplex in small MNIST networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35% and usually around 10%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core.\n  In addition, we show that, in fact, there is no polynomial time algorithm that can approximately find the minimum $\\ell_1$ adversarial distortion of a ReLU network with a $0.99\\ln n$ approximation ratio unless $\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "GALAH is a large-scale magnitude-limited southern stellar spectroscopic survey. Its second data release (GALAH DR2) provides values of stellar parameters and abundances of 23 elements for 342,682 stars (Buder et al.). Here we add a description of the public release of radial velocities with a typical accuracy of 0.1 km/s for 336,215 of these stars, achievable due to the large wavelength coverage, high resolving power and good signal to noise ratio of the observed spectra, but also because convective motions in stellar atmosphere and gravitational redshift from the star to the observer are taken into account. In the process we derive medians of observed spectra which are nearly noiseless, as they are obtained from between 100 and 1116 observed spectra belonging to the same bin with a width of 50 K in temperature, 0.2 dex in gravity, and 0.1 dex in metallicity. Publicly released 1181 median spectra have a resolving power of 28,000 and trace the well-populated stellar types with metallicities between -0.6 and +0.3. Note that radial velocities from GALAH are an excellent match to the accuracy of velocity components along the sky plane derived by Gaia for the same stars. The level of accuracy achieved here is adequate for studies of dynamics within stellar clusters, associations and streams in the Galaxy. So it may be relevant for studies of the distribution of dark matter.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We present gate-controlled single, double, and triple dot operation in electrostatically gapped bilayer graphene. Thanks to the recent advancements in sample fabrication, which include the encapsulation of bilayer graphene in hexagonal boron nitride and the use of graphite gates, it has become possible to electrostatically confine carriers in bilayer graphene and to completely pinch-off current through quantum dot devices. Here, we discuss the operation and characterization of electron-hole double dots. We show a remarkable degree of control of our device, which allows the implementation of two different gate-defined electron-hole double-dot systems with very similar energy scales. In the single dot regime, we extract excited state energies and investigate their evolution in a parallel magnetic field, which is in agreement with a Zeeman-spin-splitting expected for a g-factor of two.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "VS2 is a challenging material to prepare stoichiometrically in the bulk, and the single layer has not been successfully isolated before now. Here we report the first realization of single-layer VS2, which we have prepared epitaxially with high quality on Au(111) in the octahedral (1T) structure. We find that we can deplete the VS2 lattice of S by annealing in vacuum so as to create an entirely new two-dimensional compound that has no bulk analogue. The transition is reversible upon annealing in an H2S gas atmosphere. We report the structural properties of both the stoichiometric and S-depleted compounds on the basis of low-energy electron diffraction, X-ray photoelectron spectroscopy and diffraction, and scanning tunneling microscopy experiments.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Autoreactive B cells have a central role in the pathogenesis of rheumatoid arthritis (RA), and recent findings have proposed that anti-citrullinated protein autoantibodies (ACPA) may be directly pathogenic. Herein, we demonstrate the frequency of variable-region glycosylation in single-cell cloned mAbs. A total of 14 ACPA mAbs were evaluated for predicted N-linked glycosylation motifs in silico and compared to 452 highly-mutated mAbs from RA patients and controls. Variable region N-linked motifs (N-X-S/T) were strikingly prevalent within ACPA (100%) compared to somatically hypermutated (SHM) RA bone marrow plasma cells (21%), and synovial plasma cells from seropositive (39%) and seronegative RA (7%). When normalized for SHM, ACPA still had significantly higher frequency of N-linked motifs compared to all studied mAbs including highly-mutated HIV broadly-neutralizing and malaria-associated mAbs. The Fab glycans of ACPA-mAbs were highly sialylated, contributed to altered charge, but did not influence antigen binding. The analysis revealed evidence of unusual B-cell selection pressure and SHM-mediated decreased in surface charge and isoelectric point in ACPA. It is still unknown how these distinct features of anti-citrulline immunity may have an impact on pathogenesis. However, it is evident that they offer selective advantages for ACPA+ B cells, possibly also through non-antigen driven mechanisms.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Quantum teleportation establishes a correspondence between an entangled state shared by two separate par- ties that can communicate classically and the presence of a quantum channel connecting the two parties. The standard benchmark for quantum teleportation, based on the average fidelity between the input and output states, indicates that some entangled states do not lead to channels which can be certified to be quantum. It was re- cently shown that if one considers a finer-tuned witness, then all entangled states can be certified to produce a non-classical teleportation channel. Here we experimentally demonstrate a complete characterization of a new family of such witnesses, of the type proposed in Phys. Rev. Lett. 119, 110501 (2017) under different con- ditions of noise. Furthermore, we show non-classical teleportation using quantum states that can not achieve average teleportation fidelity above the classical limit. Our results have fundamental implications in quantum information protocols and may also lead to new applications and quality certification of quantum technologies.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Using GALAH survey data of nearby stars, we look at how structure in the planar (u,v) velocity distribution depends on metallicity and on viewing direction within the Galaxy. In nearby stars, with distance d < 1 kpc, the Hercules stream is most strongly seen in higher metallicity stars [Fe/H] > 0.2. The Hercules stream peak v value depends on viewed galactic longitude, which we interpret as due to the gap between the stellar stream and more circular orbits being associated with a specific angular momentum value of about 1640 km/s kpc. The association of the gap with a particular angular momentum value supports a bar resonant model for the Hercules stream.\n  Moving groups previously identified in Hipparcos observations are easiest to see in stars nearer than 250 pc, and their visibility and peak velocities in the velocity distributions depends on both viewing direction (galactic longitude and hemisphere) and metallicity. We infer that there is fine structure in local velocity distributions that varies over distances of a few hundred pc in the Galaxy.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We propose an evolution of the Mu2e experiment, called Mu2e-II, that would leverage advances in detector technology and utilize the increased proton intensity provided by the Fermilab PIP-II upgrade to improve the sensitivity for neutrinoless muon-to-electron conversion by one order of magnitude beyond the Mu2e experiment, providing the deepest probe of charged lepton flavor violation in the foreseeable future. Mu2e-II will use as much of the Mu2e infrastructure as possible, providing, where required, improvements to the Mu2e apparatus to accommodate the increased beam intensity and cope with the accompanying increase in backgrounds.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We present a study on the growth and characterization of high-quality single-layer MoS$_2$ with a single orientation, i.e. without the presence of mirror domains. This single orientation of the MoS$_2$ layer is established by means of x-ray photoelectron diffraction. The high quality is evidenced by combining scanning tunneling microscopy with x-ray photoelectron spectroscopy measurements. Spin- and angle-resolved photoemission experiments performed on the sample revealed complete spin-polarization of the valence band states near the K and -K points of the Brillouin zone. These findings open up the possibility to exploit the spin and valley degrees of freedom for encoding and processing information in devices that are based on epitaxially grown materials.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We study the formation of epitaxial graphene on Ru(0001) using fast x-ray photoelectron spectroscopy during the growth process. The assignment of different C 1s and Ru 3d core level components and their evolution during the growth process gives a detailed insight into the graphene formation and the strongly varying graphene-Ru interaction strength within the large moire unit cell. Subsequent intercalation of oxygen can be achieved at elevated temperature and the core level spectra show a conversion of the strongly corrugated to quasi free-standing graphene, characterised by a single narrow C 1s component. This conversion and the accompanying flattening of the graphene layer is also confirmed by x-ray photoelectron diffraction. The effect of oxygen intercalation on the electronic structure is studied using angle-resolved photoemission of the valence band states. For graphene/Ru(0001), the strong graphene-substrate hybridisation disrupts the \u03c0-band dispersion but oxygen intercalation fully restores the \u03c0-band with a strong p-doping that shifts the Dirac point 785 meV above the Fermi level. The doping of the system is highly tunable, as the additional exposure to rubidium can convert the carrier filling to n-type with the Dirac point 970 meV below the Fermi level.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The robustness of neural networks to adversarial examples has received great attention due to security implications. Despite various attack approaches to crafting visually imperceptible adversarial examples, little has been developed towards a comprehensive measure of robustness. In this paper, we provide a theoretical justification for converting robustness analysis into a local Lipschitz constant estimation problem, and propose to use the Extreme Value Theory for efficient evaluation. Our analysis yields a novel robustness metric called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork Robustness. The proposed CLEVER score is attack-agnostic and computationally feasible for large neural networks. Experimental results on various networks, including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned with the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms of adversarial examples from powerful attacks, and (ii) defended networks using defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To the best of our knowledge, CLEVER is the first attack-independent robustness metric that can be applied to any neural network classifier.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The KATRIN (Karlsruhe Tritium Neutrino) experiment investigates the energetic endpoint of the tritium $\u03b2$-decay spectrum to determine the effective mass of the electron anti-neutrino with a precision of $200\\,\\mathrm{meV}$ ($90\\,\\%$ C.L.) after an effective data taking time of three years.\n  The TRISTAN (tritium $\u03b2$-decay to search for sterile neutrinos) group aims to detect a sterile neutrino signature by measuring the entire tritium $\u03b2$-decay spectrum with an upgraded KATRIN system. One of the greatest challenges is to handle the high signal rates generated by the strong activity of the KATRIN tritium source. Therefore, a novel multi-pixel silicon drift detector is being designed, which is able to handle rates up to $10^{8}\\,\\mathrm{cps}$ with an excellent energy resolution of $<200\\,\\mathrm{eV}$ (FWHM) at $10\\,\\mathrm{keV}$.\n  This work gives an overview of the ongoing detector development and test results of the first seven pixel prototype detectors.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted by the Numediart Institute of Creative Technologies of the University of Mons from August 10th to September 2015. During the four weeks, students and researchers from all over the world came together in the Numediart Institute of the University of Mons to work on eight selected projects structured around intelligent interfaces. Eight projects were selected and their reports are shown here.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "This work aims to design the uplink (UL) of a cellular network for maximal energy efficiency (EE). Each base station (BS) is randomly deployed within a given area and is equipped with $M$ antennas to serve $K$ user equipments (UEs). A multislope (distance-dependent) path loss model is considered and linear processing is used, under the assumption that channel state information is acquired by using pilot sequences (reused across the network). Within this setting, a lower bound on the UL spectral efficiency and a realistic circuit power consumption model are used to evaluate the network EE. Numerical results are first used to compute the optimal BS density and pilot reuse factor for a Massive MIMO network with three different detection schemes, namely, maximum ratio combining, zero-forcing (ZF) and multicell minimum mean-squared error. The numerical analysis shows that the EE is a unimodal function of BS density and achieves its maximum for a relatively small density of BS, irrespective of the employed detection scheme. This is in contrast to the single-slope (distance-independent) path loss model, for which the EE is a monotonic non-decreasing function of BS density. Then, we concentrate on ZF and use stochastic geometry to compute a new lower bound on the spectral efficiency, which is then used to optimize, for a given BS density, the pilot reuse factor, number of BS antennas and UEs. Closed- form expressions are computed from which valuable insights into the interplay between optimization variables, hardware characteristics, and propagation environment are obtained.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R&D activities required to prepare for this software upgrade.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Accurate and precise radius estimates of transiting exoplanets are critical for understanding their compositions and formation mechanisms. To know the planet, we must know the host star in as much detail as possible. We present first results from the K2-HERMES project, which uses the HERMES multi-object spectrograph on the Anglo-Australian Telescope to obtain R$\\sim$28,000 spectra of up to 360 stars in one exposure. This ongoing project aims to derive self-consistent spectroscopic parameters for about half of K2 target stars. We present complete stellar parameters and isochrone-derived masses and radii for 46 stars hosting 57 K2 candidate planets in Campaigns 1-3. Our revised host-star radii cast severe doubt on three candidate planets: EPIC\\,201407812.01, EPIC\\,203070421.01, and EPIC\\,202843107.01, all of which now have inferred radii well in excess of the largest known inflated Jovian planets.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We investigate the effect of the formation of metal droplets on the growth dynamics of InGaN by Plasma-Assisted Molecular Beam Epitaxy (PAMBE) at low temperatures (T = 450\u00b0C). We find that the presence of droplets on the growth surface strongly affects the adatom incorporation dynamics, making the growth rate a decreasing function of the overall metal flux impinging on the surface as soon as the metal dose exceeds the critical amount required for the nucleation of droplets. We explain this phenomenon via a model that takes into account droplet effects on the incorporation of metal adatoms into the crystal. A relevant role is played by the vapor-liquid-solid growth mode that takes place under the droplets due to nitrogen molecules directly impinging on the droplets.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Solving inverse problems is central to geosciences and remote sensing. Radiative transfer models (RTMs) represent mathematically the physical laws which govern the phenomena in remote sensing applications (forward models). The numerical inversion of the RTM equations is a challenging and computationally demanding problem, and for this reason, often the application of a nonlinear statistical regression is preferred. In general, regression models predict the biophysical parameter of interest from the corresponding received radiance. However, this approach does not employ the physical information encoded in the RTMs. An alternative strategy, which attempts to include the physical knowledge, consists in learning a regression model trained using data simulated by an RTM code. In this work, we introduce a nonlinear nonparametric regression model which combines the benefits of the two aforementioned approaches. The inversion is performed taking into account jointly both real observations and RTM-simulated data. The proposed Joint Gaussian Process (JGP) provides a solid framework for exploiting the regularities between the two types of data. The JGP automatically detects the relative quality of the simulated and real data, and combines them accordingly. This occurs by learning an additional hyper-parameter w.r.t. a standard GP model, and fitting parameters through maximizing the pseudo-likelihood of the real observations. The resulting scheme is both simple and robust, i.e., capable of adapting to different scenarios. The advantages of the JGP method compared to benchmark strategies are shown considering RTM-simulated and real observations in different experiments. Specifically, we consider leaf area index (LAI) retrieval from Landsat data combined with simulated data generated by the PROSAIL model.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Several semiconductor quantum dot techniques have been investigated for the generation of entangled photon pairs. Among the other techniques, droplet epitaxy enables the control of the shape, size, density, and emission wavelength of the quantum emitters. However, the fraction of the entanglement-ready quantum dots that can be fabricated with this method is still limited to around 5%, and matching the energy of the entangled photons to atomic transitions (a promising route towards quantum networking) remains an outstanding challenge.\n  Here, we overcome these obstacles by introducing a modified approach to droplet epitaxy on a high symmetry (111)A substrate, where the fundamental crystallization step is performed at a significantly higher temperature as compared to previous reports. Our method drastically improves the yield of entanglement-ready photon sources near the emission wavelength of interest, which can be as high as 95% due to the low values of fine structure splitting and radiative lifetime, together with the reduced exciton dephasing offered by the choice of GaAs/AlGaAs materials. The quantum dots are designed to emit in the operating spectral region of Rb-based slow-light media, providing a viable technology for quantum repeater stations.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The Cherenkov Telescope Array, CTA, will be the major global observatory for very high energy gamma-ray astronomy over the next decade and beyond. The scientific potential of CTA is extremely broad: from understanding the role of relativistic cosmic particles to the search for dark matter. CTA is an explorer of the extreme universe, probing environments from the immediate neighbourhood of black holes to cosmic voids on the largest scales. Covering a huge range in photon energy from 20 GeV to 300 TeV, CTA will improve on all aspects of performance with respect to current instruments.\n  The observatory will operate arrays on sites in both hemispheres to provide full sky coverage and will hence maximize the potential for the rarest phenomena such as very nearby supernovae, gamma-ray bursts or gravitational wave transients. With 99 telescopes on the southern site and 19 telescopes on the northern site, flexible operation will be possible, with sub-arrays available for specific tasks. CTA will have important synergies with many of the new generation of major astronomical and astroparticle observatories. Multi-wavelength and multi-messenger approaches combining CTA data with those from other instruments will lead to a deeper understanding of the broad-band non-thermal properties of target sources.\n  The CTA Observatory will be operated as an open, proposal-driven observatory, with all data available on a public archive after a pre-defined proprietary period. Scientists from institutions worldwide have combined together to form the CTA Consortium. This Consortium has prepared a proposal for a Core Programme of highly motivated observations. The programme, encompassing approximately 40% of the available observing time over the first ten years of CTA operation, is made up of individual Key Science Projects (KSPs), which are presented in this document.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Compressed bitmap indexes are used in systems such as Git or Oracle to accelerate queries. They represent sets and often support operations such as unions, intersections, differences, and symmetric differences. Several important systems such as Elasticsearch, Apache Spark, Netflix's Atlas, LinkedIn's Pivot, Metamarkets' Druid, Pilosa, Apache Hive, Apache Tez, Microsoft Visual Studio Team Services and Apache Kylin rely on a specific type of compressed bitmap index called Roaring. We present an optimized software library written in C implementing Roaring bitmaps: CRoaring. It benefits from several algorithms designed for the single-instruction-multiple-data (SIMD) instructions available on commodity processors. In particular, we present vectorized algorithms to compute the intersection, union, difference and symmetric difference between arrays. We benchmark the library against a wide range of competitive alternatives, identifying weaknesses and strengths in our software. Our work is available under a liberal open-source license.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Next generation wireless networks aim at providing substantial improvements in spectral efficiency (SE) and energy efficiency (EE). Massive MIMO has been proved to be a viable technology to achieve these goals by spatially multiplexing several users using many base station (BS) antennas. A potential limitation of Massive MIMO in multicell systems is pilot contamination, which arises in the channel estimation process from the interference caused by reusing pilots in neighboring cells. A standard method to reduce pilot contamination, known as regular pilot (RP), is to adjust the length of pilot sequences while transmitting data and pilot symbols disjointly. An alternative method, called superimposed pilot (SP), sends a superposition of pilot and data symbols. This allows to use longer pilots which, in turn, reduces pilot contamination. We consider the uplink of a multicell Massive MIMO network using maximum ratio combining detection and compare RP and SP in terms of SE and EE. To this end, we derive rigorous closed-form achievable rates with SP under a practical random BS deployment. We prove that the reduction of pilot contamination with SP is outweighed by the additional coherent and non-coherent interference. Numerical results show that when both methods are optimized, RP achieves comparable SE and EE to SP in practical scenarios.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "This work aims to design a cellular network for maximal energy efficiency (EE). In particular, we consider the uplink with multi-antenna base stations and assume that zero- forcing (ZF) combining is used for data detection with imperfect channel state information. Using stochastic geometry and a new lower bound on the average per-user spectral efficiency of the network, we optimize the pilot reuse factor, number of antennas and users per base station. Closed-form expressions are computed from which valuable insights into the interplay between the optimization variables, hardware characteristics, and propagation environment are obtained. Numerical results are used to validate the analysis and make comparisons with a network using maximum ratio (MR) combining. The results show that a Massive MIMO setup arises as the EE-optimal network configuration. In addition, ZF provides higher EE than MR while allowing a smaller pilot reuse factor and a more dense network deployment.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "List of contributions from the Cherenkov Telescope Array Consortium presented at the 35th International Cosmic Ray Conference, July 12-20 2017, Busan, Korea.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The encapsulation of graphene based Hall sensors on foil is shown to be an effective method for improving the performance in terms of higher sensitivity for magnetic field detection. Two types of encapsulation were investigated: a simple encapsulation of graphene with polymethyl methacrylate (PMMA) as a proof of concept and an encapsulation with mechanically exfoliated hexagonal boron nitride (hBN). The Hall sensor with PMMA encapsulation already shows higher sensitivity compared to the one without encapsulation. However, the Hall sensor with graphene encapsulated between two stacks of hBN shows a current and a voltage normalized sensitivity of up to 2270 V/AT and 0.68 V/VT respectively, which are the highest reported sensitivity values for Hall sensors on foil so far.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We propose a new algorithm for the computation of a singular value decomposition (SVD) low-rank approximation of a matrix in the Matrix Product Operator (MPO) format, also called the Tensor Train Matrix format. Our tensor network randomized SVD (TNrSVD) algorithm is an MPO implementation of the randomized SVD algorithm that is able to compute dominant singular values and their corresponding singular vectors. In contrast to the state-of-the-art tensor-based alternating least squares SVD (ALS-SVD) and modified alternating least squares SVD (MALS-SVD) matrix approximation methods, TNrSVD can be up to 17 times faster while achieving the same accuracy. In addition, our TNrSVD algorithm also produces accurate approximations in particular cases where both ALS-SVD and MALS-SVD fail to converge. We also propose a new algorithm for the fast conversion of a sparse matrix into its corresponding MPO form, which is up to 509 times faster than the standard Tensor Train SVD (TT-SVD) method while achieving machine precision accuracy. The efficiency and accuracy of both algorithms are demonstrated in numerical experiments.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "This white paper summarizes the workshop \"U.S. Cosmic Visions: New Ideas in Dark Matter\" held at University of Maryland on March 23-25, 2017.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Lensing of the CMB is now a well-developed probe of large-scale clustering over a broad range of redshifts. By exploiting the non-Gaussian imprints of lensing in the polarization of the CMB, the CORE mission can produce a clean map of the lensing deflections over nearly the full-sky. The number of high-S/N modes in this map will exceed current CMB lensing maps by a factor of 40, and the measurement will be sample-variance limited on all scales where linear theory is valid. Here, we summarise this mission product and discuss the science that it will enable. For example, the summed mass of neutrinos will be determined to an accuracy of 17 meV combining CORE lensing and CMB two-point information with contemporaneous BAO measurements, three times smaller than the minimum total mass allowed by neutrino oscillations. In the search for B-mode polarization from primordial gravitational waves with CORE, lens-induced B-modes will dominate over instrument noise, limiting constraints on the gravitational wave power spectrum amplitude. With lensing reconstructed by CORE, one can \"delens\" the observed polarization internally, reducing the lensing B-mode power by 60%. This improves to 70% by combining lensing and CIB measurements from CORE, reducing the error on the gravitational wave amplitude by 2.5 compared to no delensing (in the null hypothesis). Lensing measurements from CORE will allow calibration of the halo masses of the 40000 galaxy clusters that it will find, with constraints dominated by the clean polarization-based estimators. CORE can accurately remove Galactic emission from CMB maps with its 19 frequency channels. We present initial findings that show that residual Galactic foreground contamination will not be a significant source of bias for lensing power spectrum measurements with CORE. [abridged]\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Realistic oxide materials are often semiconductors, in particular at elevated temperatures, and their surfaces contain undercoordiated atoms at structural defects such as steps and corners. Using hybrid density-functional theory and ab initio atomistic thermodynamics, we investigate the interplay of bond-making, bond-breaking, and charge-carrier trapping at the corner defects at the (100) surface of a p-doped MgO in thermodynamic equilibrium with an O2 atmosphere. We show that by manipulating the coordination of surface atoms one can drastically change and even reverse the order of stability of reduced versus oxidized surface sites.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Photonic interference is a key quantum resource for optical quantum computation, and in particular for so-called boson sampling machines. In interferometers with certain symmetries, genuine multiphoton quantum interference effectively suppresses certain sets of events, as in the original Hong-Ou-Mandel effect. Recently, it was shown that some classical and semi-classical models could be ruled out by identifying such suppressions in Fourier interferometers. Here we propose a suppression law suitable for random-input experiments in multimode Sylvester interferometers, and verify it experimentally using 4- and 8-mode integrated interferometers. The observed suppression is stronger than what is observed in Fourier interferometers of the same size, and could be relevant to certification of boson sampling machines and other experiments relying on bosonic interference.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Injuries have a great impact on professional soccer, due to their large influence on team performance and the considerable costs of rehabilitation for players. Existing studies in the literature provide just a preliminary understanding of which factors mostly affect injury risk, while an evaluation of the potential of statistical models in forecasting injuries is still missing. In this paper, we propose a multi-dimensional approach to injury forecasting in professional soccer that is based on GPS measurements and machine learning. By using GPS tracking technology, we collect data describing the training workload of players in a professional soccer club during a season. We then construct an injury forecaster and show that it is both accurate and interpretable by providing a set of case studies of interest to soccer practitioners. Our approach opens a novel perspective on injury prevention, providing a set of simple and practical rules for evaluating and interpreting the complex relations between injury risk and training performance in professional soccer.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We present a new method to locate the starting points in time of an arbitrary number of (damped) delayed signals. For a finite data sequence, the method permits to first locate the starting point of the component with the longest delay, and then --by iteration-- all the preceding ones. Numerical examples are given and noise sensitivity is tested for weak noise.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Bacterial colonies are abundant on living and nonliving surfaces and are known to mediate a broad range of processes in ecology, medicine, and industry. Although extensively researched, from single cells to demographic scales, a comprehensive biomechanical picture, highlighting the cell-to-colony dynamics, is still lacking. Here, using molecular dynamics simulations and continuous modeling, we investigate the geometrical and mechanical properties of a bacterial colony growing on a substrate with a free boundary and demonstrate that such an expanding colony self-organizes into a \"mosaic\" of microdomains consisting of highly aligned cells. The emergence of microdomains is mediated by two competing forces: the steric forces between neighboring cells, which favor cell alignment, and the extensile stresses due to cell growth that tend to reduce the local orientational order and thereby distort the system. This interplay results in an exponential distribution of the domain areas and sets a characteristic length scale proportional to the square root of the ratio between the system orientational stiffness and the magnitude of the extensile active stress. Our theoretical predictions are finally compared with experiments with freely growing E. coli microcolonies, finding quantitative agreement.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "We forecast the scientific capabilities to improve our understanding of cosmic inflation of CORE, a proposed CMB space satellite submitted in response to the ESA fifth call for a medium-size mission opportunity. The CORE satellite will map the CMB anisotropies in temperature and polarization in 19 frequency channels spanning the range 60-600 GHz. CORE will have an aggregate noise sensitivity of $1.7 \u03bc$K$\\cdot \\,$arcmin and an angular resolution of 5' at 200 GHz. We explore the impact of telescope size and noise sensitivity on the inflation science return by making forecasts for several instrumental configurations. This study assumes that the lower and higher frequency channels suffice to remove foreground contaminations and complements other related studies of component separation and systematic effects, which will be reported in other papers of the series \"Exploring Cosmic Origins with CORE.\" We forecast the capability to determine key inflationary parameters, to lower the detection limit for the tensor-to-scalar ratio down to the $10^{-3}$ level, to chart the landscape of single field slow-roll inflationary models, to constrain the epoch of reheating, thus connecting inflation to the standard radiation-matter dominated Big Bang era, to reconstruct the primordial power spectrum, to constrain the contribution from isocurvature perturbations to the $10^{-3}$ level, to improve constraints on the cosmic string tension to a level below the presumptive GUT scale, and to improve the current measurements of primordial non-Gaussianities down to the $f_{NL}^{\\rm local} < 1$ level. For all the models explored, CORE alone will improve significantly on the present constraints on the physics of inflation. Its capabilities will be further enhanced by combining with complementary future cosmological observations.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "The advent of space-based missions like $Kepler$ has revolutionized the study of solar-type stars, particularly through the measurement and modeling of their resonant modes of oscillation. Here we analyze a sample of 66 $Kepler$ main-sequence stars showing solar-like oscillations as part of the $Kepler$ seismic LEGACY project. We use $Kepler$ short-cadence data, of which each star has at least 12 months, to create frequency power spectra optimized for asteroseismology. For each star we identify its modes of oscillation and extract parameters such as frequency, amplitude, and line width using a Bayesian Markov chain Monte Carlo `peak-bagging' approach. We report the extracted mode parameters for all 66 stars, as well as derived quantities such as frequency difference ratios, the large and small separations $\u0394\u03bd$ and $\u03b4\u03bd_{02}$; the behavior of line widths with frequency and line widths at $\u03bd_{\\rm max}$ with $T_{\\rm eff}$, for which we derive parametrizations; and behavior of mode visibilities. These average properties can be applied in future peak-bagging exercises to better constrain the parameters of the stellar oscillation spectra. The frequencies and frequency ratios can tightly constrain the fundamental parameters of these solar-type stars, and mode line widths and amplitudes can test models of mode damping and excitation.\n        \u25b3 Less", "author": "Luca Daniel"}, {"abstract": "Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an accurate method for estimating probabilities of events on a small number of variables of a graphical model satisfying Dobrushin's condition~\\cite{DeSaOR16}. We investigate whether it can be used to accurately estimate expectations of functions of {\\em all the variables} of the model. Under the same condition, we show that the synchronous (sequential) and asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between their (multivariate) samples remains bounded by $O(\u03c4\\log n),$ where $n$ is the number of variables in the graphical model, and $\u03c4$ is a measure of the asynchronicity. A similar bound holds for any constant power of the Hamming distance. Hence, the expectation of any function that is Lipschitz with respect to a power of the Hamming distance, can be estimated with a bias that grows logarithmically in $n$. Going beyond Lipschitz functions, we consider the bias arising from asynchronicity in estimating the expectation of polynomial functions of all variables in the model. Using recent concentration of measure results, we show that the bias introduced by the asynchronicity is of smaller order than the standard deviation of the function value already present in the true model. We perform experiments on a multi-processor machine to empirically illustrate our theoretical findings.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon [BCMV14] but allows for a wider range of perturbation models, including discrete ones. We give an application to recovering assemblies of neurons.\n  Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies. This suggests that an animal's memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data. Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their $\\ell$-wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide an efficient algorithm for the classical problem, going back to Galton, Pearson, and Fisher, of estimating, with arbitrary accuracy the parameters of a multivariate normal distribution from truncated samples. Truncated samples from a $d$-variate normal ${\\cal N}(\\mathbf\u03bc,\\mathbf\u03a3)$ means a samples is only revealed if it falls in some subset $S \\subseteq \\mathbb{R}^d$; otherwise the samples are hidden and their count in proportion to the revealed samples is also hidden. We show that the mean $\\mathbf\u03bc$ and covariance matrix $\\mathbf\u03a3$ can be estimated with arbitrary accuracy in polynomial-time, as long as we have oracle access to $S$, and $S$ has non-trivial measure under the unknown $d$-variate normal distribution. Additionally we show that without oracle access to $S$, any non-trivial estimation is impossible.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Motivated by applications in Game Theory, Optimization, and Generative Adversarial Networks, recent work of Daskalakis et al~\\cite{DISZ17} and follow-up work of Liang and Stokes~\\cite{LiangS18} have established that a variant of the widely used Gradient Descent/Ascent procedure, called \"Optimistic Gradient Descent/Ascent (OGDA)\", exhibits last-iterate convergence to saddle points in {\\em unconstrained} convex-concave min-max optimization problems. We show that the same holds true in the more general problem of {\\em constrained} min-max optimization under a variant of the no-regret Multiplicative-Weights-Update method called \"Optimistic Multiplicative-Weights Update (OMWU)\". This answers an open question of Syrgkanis et al~\\cite{SALS15}.\n  The proof of our result requires fundamentally different techniques from those that exist in no-regret learning literature and the aforementioned papers. We show that OMWU monotonically improves the Kullback-Leibler divergence of the current iterate to the (appropriately normalized) min-max solution until it enters a neighborhood of the solution. Inside that neighborhood we show that OMWU becomes a contracting map converging to the exact solution. We believe that our techniques will be useful in the analysis of the last iterate of other learning algorithms.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods in min-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA). We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of \\{OGDA\\}-stable critical points is a superset of \\{GDA\\}-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We consider testing and learning problems on causal Bayesian networks as defined by Pearl (Pearl, 2009). Given a causal Bayesian network $\\mathcal{M}$ on a graph with $n$ discrete variables and bounded in-degree and bounded `confounded components', we show that $O(\\log n)$ interventions on an unknown causal Bayesian network $\\mathcal{X}$ on the same graph, and $\\tilde{O}(n/\u03b5^2)$ samples per intervention, suffice to efficiently distinguish whether $\\mathcal{X}=\\mathcal{M}$ or whether there exists some intervention under which $\\mathcal{X}$ and $\\mathcal{M}$ are farther than $\u03b5$ in total variation distance. We also obtain sample/time/intervention efficient algorithms for: (i) testing the identity of two unknown causal Bayesian networks on the same graph; and (ii) learning a causal Bayesian network on a given graph. Although our algorithms are non-adaptive, we show that adaptivity does not help in general: $\u03a9(\\log n)$ interventions are necessary for testing the identity of two unknown causal Bayesian networks on the same graph, even adaptively. Our algorithms are enabled by a new subadditivity inequality for the squared Hellinger distance between two causal Bayesian networks.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We study revenue optimization in a repeated auction between a single seller and a single buyer. Traditionally, the design of repeated auctions requires strong modeling assumptions about the bidder behavior, such as it being myopic, infinite lookahead, or some specific form of learning behavior. Is it possible to design mechanisms which are simultaneously optimal against a multitude of possible buyer behaviors? We answer this question by designing a simple state-based mechanism that is simultaneously approximately optimal against a $k$-lookahead buyer for all $k$, a buyer who is a no-regret learner, and a buyer who is a policy-regret learner. Against each type of buyer our mechanism attains a constant fraction of the optimal revenue attainable against that type of buyer. We complement our positive results with almost tight impossibility results, showing that the revenue approximation tradeoffs achieved by our mechanism for different lookahead attitudes are near-optimal.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Deep neural networks are demonstrating excellent performance on several classical vision problems. However, these networks are vulnerable to adversarial examples, minutely modified images that induce arbitrary attacker-chosen output from the network. We propose a mechanism to protect against these adversarial inputs based on a generative model of the data. We introduce a pre-processing step that projects on the range of a generative model using gradient descent before feeding an input into a classifier. We show that this step provides the classifier with robustness against first-order, substitute model, and combined adversarial attacks. Using a min-max formulation, we show that there may exist adversarial examples even in the range of the generator, natural-looking images extremely close to the decision boundary for which the classifier has unjustifiedly high confidence. We show that adversarial training on the generative manifold can be used to make a classifier that is robust to these attacks.\n  Finally, we show how our method can be applied even without a pre-trained generative model using a recent method called the deep image prior. We evaluate our method on MNIST, CelebA and Imagenet and show robustness against the current state of the art attacks.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We address the issue of limit cycling behavior in training Generative Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for training Wasserstein GANs. Recent theoretical results have shown that optimistic mirror decent (OMD) can enjoy faster regret rates in the context of zero-sum games. WGANs is exactly a context of solving a zero-sum game with simultaneous no-regret dynamics. Moreover, we show that optimistic mirror decent addresses the limit cycling problem in training WGANs. We formally show that in the case of bi-linear zero-sum games the last iterate of OMD dynamics converges to an equilibrium, in contrast to GD dynamics which are bound to cycle. We also portray the huge qualitative difference between GD and OMD dynamics with toy examples, even when GD is modified with many adaptations proposed in the recent literature, such as gradient penalty or momentum. We apply OMD WGAN training to a bioinformatics problem of generating DNA sequences. We observe that models trained with OMD achieve consistently smaller KL divergence with respect to the true underlying distribution, than models trained with GD variants. Finally, we introduce a new algorithm, Optimistic Adam, which is an optimistic variant of Adam. We apply it to WGAN training on CIFAR10 and observe improved performance in terms of inception score as compared to Adam.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We prove near-tight concentration of measure for polynomial functions of the Ising model under high temperature. For any degree $d$, we show that a degree-$d$ polynomial of a $n$-spin Ising model exhibits exponential tails that scale as $\\exp(-r^{2/d})$ at radius $r=\\tilde\u03a9_d(n^{d/2})$. Our concentration radius is optimal up to logarithmic factors for constant $d$, improving known results by polynomial factors in the number of spins. We demonstrate the efficacy of polynomial functions as statistics for testing the strength of interactions in social networks in both synthetic and real world data.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide algorithms that learn simple auctions whose revenue is approximately optimal in multi-item multi-bidder settings, for a wide range of valuations including unit-demand, additive, constrained additive, XOS, and subadditive. We obtain our learning results in two settings. The first is the commonly studied setting where sample access to the bidders' distributions over valuations is given, for both regular distributions and arbitrary distributions with bounded support. Our algorithms require polynomially many samples in the number of items and bidders. The second is a more general max-min learning setting that we introduce, where we are given \"approximate distributions,\" and we seek to compute an auction whose revenue is approximately optimal simultaneously for all \"true distributions\" that are close to the given ones. These results are more general in that they imply the sample-based results, and are also applicable in settings where we have no sample access to the underlying distributions but have estimated them indirectly via market research or by observation of previously run, potentially non-truthful auctions.\n  Our results hold for valuation distributions satisfying the standard (and necessary) independence-across-items property. They also generalize and improve upon recent works, which have provided algorithms that learn approximately optimal auctions in more restricted settings with additive, subadditive and unit-demand valuations using sample access to distributions. We generalize these results to the complete unit-demand, additive, and XOS setting, to i.i.d. subadditive bidders, and to the max-min setting.\n  Our results are enabled by new uniform convergence bounds for hypotheses classes under product measures. Our bounds result in exponential savings in sample complexity compared to bounds derived by bounding the VC dimension, and are of independent interest.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Given samples from an unknown distribution $p$ and a description of a distribution $q$, are $p$ and $q$ close or far? This question of \"identity testing\" has received significant attention in the case of testing whether $p$ and $q$ are equal or far in total variation distance. However, in recent work, the following questions have been been critical to solving problems at the frontiers of distribution testing:\n  -Alternative Distances: Can we test whether $p$ and $q$ are far in other distances, say Hellinger?\n  -Tolerance: Can we test when $p$ and $q$ are close, rather than equal? And if so, close in which distances?\n  Motivated by these questions, we characterize the complexity of distribution testing under a variety of distances, including total variation, $\\ell_2$, Hellinger, Kullback-Leibler, and $\u03c7^2$. For each pair of distances $d_1$ and $d_2$, we study the complexity of testing if $p$ and $q$ are close in $d_1$ versus far in $d_2$, with a focus on identifying which problems allow strongly sublinear testers (i.e., those with complexity $O(n^{1 - \u03b3})$ for some $\u03b3> 0$ where $n$ is the size of the support of the distributions $p$ and $q$). We provide matching upper and lower bounds for each case. We also study these questions in the case where we only have samples from $q$ (equivalence testing), showing qualitative differences from identity testing in terms of when tolerance can be achieved. Our algorithms fall into the classical paradigm of $\u03c7^2$-statistics, but require crucial changes to handle the challenges introduced by each distance we consider. Finally, we survey other recent results in an attempt to serve as a reference for the complexity of various distribution testing problems.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Classical distribution testing assumes access to i.i.d. samples from the distribution that is being tested. We initiate the study of Markov chain testing, assuming access to a single trajectory of a Markov Chain. In particular, we observe a single trajectory X0,...,Xt,... of an unknown, symmetric, and finite state Markov Chain M. We do not control the starting state X0, and we cannot restart the chain. Given our single trajectory, the goal is to test whether M is identical to a model Markov Chain M0 , or far from it under an appropriate notion of difference. We propose a measure of difference between two Markov chains, motivated by the early work of Kazakos [Kaz78], which captures the scaling behavior of the total variation distance between trajectories sampled from the Markov chains as the length of these trajectories grows. We provide efficient testers and information-theoretic lower bounds for testing identity of symmetric Markov chains under our proposed measure of difference, which are tight up to logarithmic factors if the hitting times of the model chain M0 is O(n) in the size of the state space n.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We develop differentially private hypothesis testing methods for the small sample regime. Given a sample $\\cal D$ from a categorical distribution $p$ over some domain $\u03a3$, an explicitly described distribution $q$ over $\u03a3$, some privacy parameter $\\varepsilon$, accuracy parameter $\u03b1$, and requirements $\u03b2_{\\rm I}$ and $\u03b2_{\\rm II}$ for the type I and type II errors of our test, the goal is to distinguish between $p=q$ and $d_{\\rm{TV}}(p,q) \\geq \u03b1$.\n  We provide theoretical bounds for the sample size $|{\\cal D}|$ so that our method both satisfies $(\\varepsilon,0)$-differential privacy, and guarantees $\u03b2_{\\rm I}$ and $\u03b2_{\\rm II}$ type I and type II errors. We show that differential privacy may come for free in some regimes of parameters, and we always beat the sample complexity resulting from running the $\u03c7^2$-test with noisy counts, or standard approaches such as repetition for endowing non-private $\u03c7^2$-style statistics with differential privacy guarantees. We experimentally compare the sample complexity of our method to that of recently proposed methods for private hypothesis testing.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Banach's fixed point theorem for contraction maps has been widely used to analyze the convergence of iterative methods in non-convex problems. It is a common experience, however, that iterative maps fail to be globally contracting under the natural metric in their domain, making the applicability of Banach's theorem limited. We explore how generally we can apply Banach's fixed point theorem to establish the convergence of iterative methods when pairing it with carefully designed metrics.\n  Our first result is a strong converse of Banach's theorem, showing that it is a universal analysis tool for establishing global convergence of iterative methods to unique fixed points, and for bounding their convergence rate. In other words, we show that, whenever an iterative map globally converges to a unique fixed point, there exists a metric under which the iterative map is contracting and which can be used to bound the number of iterations until convergence. We illustrate our approach in the widely used power method, providing a new way of bounding its convergence rate through contraction arguments.\n  We next consider the computational complexity of Banach's fixed point theorem. Making the proof of our converse theorem constructive, we show that computing a fixed point whose existence is guaranteed by Banach's fixed point theorem is CLS-complete. We thus provide the first natural complete problem for the class CLS, which was defined in [Daskalakis, Papadimitriou 2011] to capture the complexity of problems such as P-matrix LCP, computing KKT-points, and finding mixed Nash equilibria in congestion and network coordination games.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We show that the square Hellinger distance between two Bayesian networks on the same directed graph, $G$, is subadditive with respect to the neighborhoods of $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two Bayesian networks on the same DAG, our inequality states that the square Hellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the sum, $\\sum_v H^2(P_{\\{v\\} \\cup \u03a0_v}, Q_{\\{v\\} \\cup \u03a0_v})$, of the square Hellinger distances between the marginals of $P$ and $Q$ on every node $v$ and its parents $\u03a0_v$ in the DAG. Importantly, our bound does not involve the conditionals but the marginals of $P$ and $Q$. We derive a similar inequality for more general Markov Random Fields.\n  As an application of our inequality, we show that distinguishing whether two Bayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy $P=Q$ vs $d_{\\rm TV}(P,Q)>\u03b5$ can be performed from $\\tilde{O}(|\u03a3|^{3/4(d+1)} \\cdot n/\u03b5^2)$ samples, where $d$ is the maximum in-degree of the DAG and $\u03a3$ the domain of each variable of the Bayesian networks. If $P$ and $Q$ are defined on potentially different and potentially unknown trees, the sample complexity becomes $\\tilde{O}(|\u03a3|^{4.5} n/\u03b5^2)$, whose dependence on $n, \u03b5$ is optimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product distributions over $\\{0,1\\}^n$ and $Q$ is known, the sample complexity becomes $O(\\sqrt{n}/\u03b5^2)$, which is optimal up to constant factors.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Given samples from an unknown multivariate distribution $p$, is it possible to distinguish whether $p$ is the product of its marginals versus $p$ being far from every product distribution? Similarly, is it possible to distinguish whether $p$ equals a given distribution $q$ versus $p$ and $q$ being far from each other? These problems of testing independence and goodness-of-fit have received enormous attention in statistics, information theory, and theoretical computer science, with sample-optimal algorithms known in several interesting regimes of parameters. Unfortunately, it has also been understood that these problems become intractable in large dimensions, necessitating exponential sample complexity.\n  Motivated by the exponential lower bounds for general distributions as well as the ubiquity of Markov Random Fields (MRFs) in the modeling of high-dimensional distributions, we initiate the study of distribution testing on structured multivariate distributions, and in particular the prototypical example of MRFs: the Ising Model. We demonstrate that, in this structured setting, we can avoid the curse of dimensionality, obtaining sample and time efficient testers for independence and goodness-of-fit. One of the key technical challenges we face along the way is bounding the variance of functions of the Ising model.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "The Expectation-Maximization (EM) algorithm is a widely used method for maximum likelihood estimation in models with latent variables. For estimating mixtures of Gaussians, its iteration can be viewed as a soft version of the k-means clustering algorithm. Despite its wide use and applications, there are essentially no known convergence guarantees for this method. We provide global convergence guarantees for mixtures of two Gaussians with known covariance matrices. We show that the population version of EM, where the algorithm is given access to infinitely many samples from the mixture, converges geometrically to the correct mean vectors, and provide simple, closed-form expressions for the convergence rate. As a simple illustration, we show that, in one dimension, ten steps of the EM algorithm initialized at infinity result in less than 1\\% error estimation of the means. In the finite sample regime, we show that, under a random initialization, $\\tilde{O}(d/\u03b5^2)$ samples suffice to compute the unknown vectors to within $\u03b5$ in Mahalanobis distance, where $d$ is the dimension. In particular, the error rate of the EM based estimator is $\\tilde{O}\\left(\\sqrt{d \\over n}\\right)$ where $n$ is the number of samples, which is optimal up to logarithmic factors.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We consider the problem of a revenue-maximizing seller with m items for sale to n additive bidders with hard budget constraints, assuming that the seller has some prior distribution over bidder values and budgets. The prior may be correlated across items and budgets of the same bidder, but is assumed independent across bidders. We target mechanisms that are Bayesian Incentive Compatible, but that are ex-post Individually Rational and ex-post budget respecting. Virtually no such mechanisms are known that satisfy all these conditions and guarantee any revenue approximation, even with just a single item. We provide a computationally efficient mechanism that is a $3$-approximation with respect to all BIC, ex-post IR, and ex-post budget respecting mechanisms. Note that the problem is NP-hard to approximate better than a factor of 16/15, even in the case where the prior is a point mass \\cite{ChakrabartyGoel}. We further characterize the optimal mechanism in this setting, showing that it can be interpreted as a distribution over virtual welfare maximizers.\n  We prove our results by making use of a black-box reduction from mechanism to algorithm design developed by \\cite{CaiDW13b}. Our main technical contribution is a computationally efficient $3$-approximation algorithm for the algorithmic problem that results by an application of their framework to this problem. The algorithmic problem has a mixed-sign objective and is NP-hard to optimize exactly, so it is surprising that a computationally efficient approximation is possible at all. In the case of a single item ($m=1$), the algorithmic problem can be solved exactly via exhaustive search, leading to a computationally efficient exact algorithm and a stronger characterization of the optimal mechanism as a distribution over virtual value maximizers.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide a characterization of revenue-optimal dynamic mechanisms in settings where a monopolist sells k items over k periods to a buyer who realizes his value for item i in the beginning of period i. We require that the mechanism satisfies a strong individual rationality constraint, requiring that the stage utility of each agent be positive during each period. We show that the optimum mechanism can be computed by solving a nested sequence of static (single-period) mechanisms that optimize a tradeoff between the surplus of the allocation and the buyer's utility. We also provide a simple dynamic mechanism that obtains at least half of the optimal revenue. The mechanism either ignores history and posts the optimal monopoly price in each period, or allocates with a probability that is independent of the current report of the agent and is based only on previous reports. Our characterization extends to multi-agent auctions. We also formulate a discounted infinite horizon version of the problem, where we study the performance of \"Markov mechanisms.\"\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the sum of $n$ independent random vectors supported on the set ${\\cal B}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We show that any $(n,k)$-PMD is ${\\rm poly}\\left({k\\over \u03c3}\\right)$-close in total variation distance to the (appropriately discretized) multi-dimensional Gaussian with the same first two moments, removing the dependence on $n$ from the Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT is obtained by bootstrapping the Valiant-Valiant CLT itself through the structural characterization of PMDs shown in recent work by Daskalakis, Kamath, and Tzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTAS for approximate Nash equilibria in anonymous games, significantly improving the state of the art, and matching qualitatively the running time dependence on $n$ and $1/\\varepsilon$ of the best known algorithm for two-strategy anonymous games. Our new CLT also enables the construction of covers for the set of $(n,k)$-PMDs, which are proper and whose size is shown to be essentially optimal. Our cover construction combines our CLT with the Shapley-Folkman theorem and recent sparsification results for Laplacian matrices by Batson, Spielman, and Srivastava. Our cover size lower bound is based on an algebraic geometric construction. Finally, leveraging the structural properties of the Fourier spectrum of PMDs we show that these distributions can be learned from $O_k(1/\\varepsilon^2)$ samples in ${\\rm poly}_k(1/\\varepsilon)$-time, removing the quasi-polynomial dependence of the running time on $1/\\varepsilon$ from the algorithm of Daskalakis, Kamath, and Tzamos.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "A line of recent work provides welfare guarantees of simple combinatorial auction formats, such as selling m items via simultaneous second price auctions (SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et al. 2013). These guarantees hold even when the auctions are repeatedly executed and players use no-regret learning algorithms. Unfortunately, off-the-shelf no-regret algorithms for these auctions are computationally inefficient as the number of actions is exponential. We show that this obstacle is insurmountable: there are no polynomial-time no-regret algorithms for SiSPAs, unless RP$\\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises the question of how good outcomes polynomially-bounded bidders may discover in such auctions.\n  To answer this question, we propose a novel concept of learning in auctions, termed \"no-envy learning.\" This notion is founded upon Walrasian equilibrium, and we show that it is both efficiently implementable and results in approximately optimal welfare, even when the bidders have fractionally subadditive (XOS) valuations (assuming demand oracles) or coverage valuations (without demand oracles). No-envy learning outcomes are a relaxation of no-regret outcomes, which maintain their approximate welfare optimality while endowing them with computational tractability. Our results extend to other auction formats that have been studied in the literature via the smoothness paradigm.\n  Our results for XOS valuations are enabled by a novel Follow-The-Perturbed-Leader algorithm for settings where the number of experts is infinite, and the payoff function of the learner is non-linear. This algorithm has applications outside of auction settings, such as in security games. Our result for coverage valuations is based on a novel use of convex rounding schemes and a reduction to online convex optimization.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Reconstructing the tree of life from molecular sequences is a fundamental problem in computational biology. Modern data sets often contain a large number of genes, which can complicate the reconstruction problem due to the fact that different genes may undergo different evolutionary histories. This is the case in particular in the presence of horizontal genetic transfer (HGT), where a gene is inherited from a distant species rather than an immediate ancestor. Such an event produces a gene tree which is distinct from, but related to, the species phylogeny.\n  In previous work, a natural stochastic models of HGT was introduced and studied. It was shown, both in simulation and theoretical studies, that a species phylogeny can be reconstructed from gene trees despite surprisingly high rates of HGT under this model. Rigorous lower and upper bounds on this achievable rate were also obtained, but a large gap remained. Here we close this gap, up to a constant. Specifically we show that a species phylogeny can be reconstructed correctly from gene trees even when, on each gene, each edge of the species tree has a constant probability of being the location of an HGT event. Our new reconstruction algorithm, which relies only on unrooted gene tree topologies, builds the tree recursively from the leaves and runs in polynomial time.\n  We also provide a matching bound in the negative direction (up to a constant) and extend our results to some cases where gene trees are not perfectly known.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Given samples from an unknown distribution $p$, is it possible to distinguish whether $p$ belongs to some class of distributions $\\mathcal{C}$ versus $p$ being far from every distribution in $\\mathcal{C}$? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, and more recently in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of distributions such as monotonicity, log-concavity, unimodality, independence, and monotone-hazard rate, the optimal sample complexity is unknown.\n  We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution $p$, and a known distribution $q$, are $p$ and $q$ close in $\u03c7^2$-distance, or far in total variation distance?\n  The optimality of our testers is established by providing matching lower bounds with respect to both $n$ and $\\varepsilon$. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave and monotone hazard rate distributions.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the sum of $n$ independent random vectors supported on the set ${\\cal B}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We prove a structural characterization of these distributions, showing that, for all $\\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is $\\varepsilon$-close, in total variation distance, to the sum of a discretized multidimensional Gaussian and an independent $(\\text{poly}(k/\\varepsilon), k)$-Poisson multinomial random vector. Our structural characterization extends the multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to all approximation requirements $\\varepsilon$. In particular, it overcomes factors depending on $\\log n$ and, importantly, the minimum eigenvalue of the PMD's covariance matrix from the distance to a multidimensional Gaussian random variable.\n  We use our structural characterization to obtain an $\\varepsilon$-cover, in total variation distance, of the set of all $(n, k)$-PMDs, significantly improving the cover size of Daskalakis and Papadimitriou, and obtaining the same qualitative dependence of the cover size on $n$ and $\\varepsilon$ as the $k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structure to show that $(n,k)$-PMDs can be learned to within $\\varepsilon$ in total variation distance from $\\tilde{O}_k(1/\\varepsilon^2)$ samples, which is near-optimal in terms of dependence on $\\varepsilon$ and independent of $n$. In particular, our result generalizes the single-dimensional result of Daskalakis, Diakonikolas, and Servedio for Poisson Binomials to arbitrary dimension.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We show that computing the revenue-optimal deterministic auction in unit-demand single-buyer Bayesian settings, i.e. the optimal item-pricing, is computationally hard even in single-item settings where the buyer's value distribution is a sum of independently distributed attributes, or multi-item settings where the buyer's values for the items are independent. We also show that it is intractable to optimally price the grand bundle of multiple items for an additive bidder whose values for the items are independent. These difficulties stem from implicit definitions of a value distribution. We provide three instances of how different properties of implicit distributions can lead to intractability: the first is a #P-hardness proof, while the remaining two are reductions from the SQRT-SUM problem of Garey, Graham, and Johnson. While simple pricing schemes can oftentimes approximate the best scheme in revenue, they can have drastically different underlying structure. We argue therefore that either the specification of the input distribution must be highly restricted in format, or it is necessary for the goal to be mere approximation to the optimal scheme's revenue instead of computing properties of the scheme itself.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Optimal mechanisms have been provided in quite general multi-item settings, as long as each bidder's type distribution is given explicitly by listing every type in the support along with its associated probability. In the implicit setting, e.g. when the bidders have additive valuations with independent and/or continuous values for the items, these results do not apply, and it was recently shown that exact revenue optimization is intractable, even when there is only one bidder. Even for item distributions with special structure, optimal mechanisms have been surprisingly rare and the problem is challenging even in the two-item case. In this paper, we provide a framework for designing optimal mechanisms using optimal transport theory and duality theory. We instantiate our framework to obtain conditions under which only pricing the grand bundle is optimal in multi-item settings (complementing the work of [Manelli and Vincent 2006], as well as to characterize optimal two-item mechanisms. We use our results to derive closed-form descriptions of the optimal mechanism in several two-item settings, exhibiting also a setting where a continuum of lotteries is necessary for revenue optimization but a closed-form representation of the mechanism can still be found efficiently using our framework.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Fictitious play is a natural dynamic for equilibrium play in zero-sum games, proposed by [Brown 1949], and shown to converge by [Robinson 1951]. Samuel Karlin conjectured in 1959 that fictitious play converges at rate $O(1/\\sqrt{t})$ with the number of steps $t$. We disprove this conjecture showing that, when the payoff matrix of the row player is the $n \\times n$ identity matrix, fictitious play may converge with rate as slow as $\u03a9(t^{-1/n})$.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "A Poisson Binomial distribution over $n$ variables is the distribution of the sum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm for testing whether a distribution $P$ supported on $\\{0,...,n\\}$ to which we have sample access is a Poisson Binomial distribution, or far from all Poisson Binomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$ to which we provide a matching lower bound. We note that our sample complexity improves quadratically upon that of the naive \"learn followed by tolerant-test\" approach, while instance optimal identity testing [VV14] is not applicable since we are looking to simultaneously test against a whole family of distributions.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We characterize optimal mechanisms for the multiple-good monopoly problem and provide a framework to find them. We show that a mechanism is optimal if and only if a measure $\u03bc$ derived from the buyer's type distribution satisfies certain stochastic dominance conditions. This measure expresses the marginal change in the seller's revenue under marginal changes in the rent paid to subsets of buyer types. As a corollary, we characterize the optimality of grand-bundling mechanisms, strengthening several results in the literature, where only sufficient optimality conditions have been derived. As an application, we show that the optimal mechanism for $n$ independent uniform items each supported on $[c,c+1]$ is a grand-bundling mechanism, as long as $c$ is sufficiently large, extending Pavlov's result for $2$ items [Pavlov'11]. At the same time, our characterization also implies that, for all $c$ and for all sufficiently large $n$, the optimal mechanism for $n$ independent uniform items supported on $[c,c+1]$ is not a grand bundling mechanism.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We propose an optimum mechanism for providing monetary incentives to the data sources of a statistical estimator such as linear regression, so that high quality data is provided at low cost, in the sense that the sum of payments and estimation error is minimized. The mechanism applies to a broad range of estimators, including linear and polynomial regression, kernel regression, and, under some additional assumptions, ridge regression. It also generalizes to several objectives, including minimizing estimation error subject to budget constraints. Besides our concrete results for regression problems, we contribute a mechanism design framework through which to design and analyze statistical estimators whose examples are supplied by workers with cost for labeling said examples.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide polynomial-time approximately optimal Bayesian mechanisms for makespan minimization on unrelated machines as well as for max-min fair allocations of indivisible goods, with approximation factors of $2$ and $\\min\\{m-k+1, \\tilde{O}(\\sqrt{k})\\}$ respectively, matching the approximation ratios of best known polynomial-time \\emph{algorithms} (for max-min fairness, the latter claim is true for certain ratios of the number of goods $m$ to people $k$). Our mechanisms are obtained by establishing a polynomial-time approximation-sensitive reduction from the problem of designing approximately optimal {\\em mechanisms} for some arbitrary objective ${\\cal O}$ to that of designing bi-criterion approximation {\\em algorithms} for the same objective ${\\cal O}$ plus a linear allocation cost term. Our reduction is itself enabled by extending the celebrated \"equivalence of separation and optimization\"[GLSS81,KP80] to also accommodate bi-criterion approximations. Moreover, to apply the reduction to the specific problems of makespan and max-min fairness we develop polynomial-time bi-criterion approximation algorithms for makespan minimization with costs and max-min fairness with costs, adapting the algorithms of [ST93], [BD05] and [AS07] to the type of bi-criterion approximation that is required by the reduction.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide an algorithm for properly learning mixtures of two single-dimensional Gaussians without any separability assumptions. Given $\\tilde{O}(1/\\varepsilon^2)$ samples from an unknown mixture, our algorithm outputs a mixture that is $\\varepsilon$-close in total variation distance, in time $\\tilde{O}(1/\\varepsilon^5)$. Our sample complexity is optimal up to logarithmic factors, and significantly improves upon both Kalai et al., whose algorithm has a prohibitive dependence on $1/\\varepsilon$, and Feldman et al., whose algorithm requires bounds on the mixture parameters and depends pseudo-polynomially in these parameters.\n  One of our main contributions is an improved and generalized algorithm for selecting a good candidate distribution from among competing hypotheses. Namely, given a collection of $N$ hypotheses containing at least one candidate that is $\\varepsilon$-close to an unknown distribution, our algorithm outputs a candidate which is $O(\\varepsilon)$-close to the distribution. The algorithm requires ${O}(\\log{N}/\\varepsilon^2)$ samples from the unknown distribution and ${O}(N \\log N/\\varepsilon^2)$ time, which improves previous such results (such as the Scheff\u00e9 estimator) from a quadratic dependence of the running time on $N$ to quasilinear. Given the wide use of such results for the purpose of hypothesis selection, our improved algorithm implies immediate improvements to any such use.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "The Chord algorithm is a popular, simple method for the succinct approximation of curves, which is widely used, under different names, in a variety of areas, such as, multiobjective and parametric optimization, computational geometry, and graphics. We analyze the performance of the Chord algorithm, as compared to the optimal approximation that achieves a desired accuracy with the minimum number of points. We prove sharp upper and lower bounds, both in the worst case and average case setting.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We consider a problem which has received considerable attention in systems literature because of its applications to routing in delay tolerant networks and replica placement in distributed storage systems. In abstract terms the problem can be stated as follows: Given a random variable $X$ generated by a known product distribution over $\\{0,1\\}^n$ and a target value $0 \\leq \u03b8\\leq 1$, output a non-negative vector $w$, with $\\|w\\|_1 \\le 1$, which maximizes the probability of the event $w \\cdot X \\ge \u03b8$. This is a challenging non-convex optimization problem for which even computing the value $\\Pr[w \\cdot X \\ge \u03b8]$ of a proposed solution vector $w$ is #P-hard.\n  We provide an additive EPTAS for this problem which, for constant-bounded product distributions, runs in $ \\poly(n) \\cdot 2^{\\poly(1/\\eps)}$ time and outputs an $\\eps$-approximately optimal solution vector $w$ for this problem. Our approach is inspired by, and extends, recent structural results from the complexity-theoretic study of linear threshold functions. Furthermore, in spite of the objective function being non-smooth, we give a \\emph{unicriterion} PTAS while previous work for such objective functions has typically led to a \\emph{bicriterion} PTAS. We believe our techniques may be applicable to get unicriterion PTAS for other non-smooth objective functions.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "For all $n, \u03b5>0$, we show that the set of Poisson Binomial distributions on $n$ variables admits a proper $\u03b5$-cover in total variation distance of size $n^2+n \\cdot (1/\u03b5)^{O(\\log^2 (1/\u03b5))}$, which can also be computed in polynomial time. We discuss the implications of our construction for approximation algorithms and the computation of approximate Nash equilibria in anonymous games.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide a computationally efficient black-box reduction from mechanism design to algorithm design in very general settings. Specifically, we give an approximation-preserving reduction from truthfully maximizing \\emph{any} objective under \\emph{arbitrary} feasibility constraints with \\emph{arbitrary} bidder types to (not necessarily truthfully) maximizing the same objective plus virtual welfare (under the same feasibility constraints). Our reduction is based on a fundamentally new approach: we describe a mechanism's behavior indirectly only in terms of the expected value it awards bidders for certain behavior, and never directly access the allocation rule at all.\n  Applying our new approach to revenue, we exhibit settings where our reduction holds \\emph{both ways}. That is, we also provide an approximation-sensitive reduction from (non-truthfully) maximizing virtual welfare to (truthfully) maximizing revenue, and therefore the two problems are computationally equivalent. With this equivalence in hand, we show that both problems are NP-hard to approximate within any polynomial factor, even for a single monotone submodular bidder.\n  We further demonstrate the applicability of our reduction by providing a truthful mechanism maximizing fractional max-min fairness. This is the first instance of a truthful mechanism that optimizes a non-linear objective.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "It was recently shown in [http://arxiv.org/abs/1207.5518] that revenue optimization can be computationally efficiently reduced to welfare optimization in all multi-dimensional Bayesian auction problems with arbitrary (possibly combinatorial) feasibility constraints and independent additive bidders with arbitrary (possibly combinatorial) demand constraints. This reduction provides a poly-time solution to the optimal mechanism design problem in all auction settings where welfare optimization can be solved efficiently, but it is fragile to approximation and cannot provide solutions to settings where welfare maximization can only be tractably approximated. In this paper, we extend the reduction to accommodate approximation algorithms, providing an approximation preserving reduction from (truthful) revenue maximization to (not necessarily truthful) welfare maximization. The mechanisms output by our reduction choose allocations via black-box calls to welfare approximation on randomly selected inputs, thereby generalizing also our earlier structural results on optimal multi-dimensional mechanisms to approximately optimal mechanisms. Unlike [http://arxiv.org/abs/1207.5518], our results here are obtained through novel uses of the Ellipsoid algorithm and other optimization techniques over {\\em non-convex regions}.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Myerson's seminal work provides a computationally efficient revenue-optimal auction for selling one item to multiple bidders. Generalizing this work to selling multiple items at once has been a central question in economics and algorithmic game theory, but its complexity has remained poorly understood. We answer this question by showing that a revenue-optimal auction in multi-item settings cannot be found and implemented computationally efficiently, unless ZPP contains P^#P. This is true even for a single additive bidder whose values for the items are independently distributed on two rational numbers with rational probabilities. Our result is very general: we show that it is hard to compute any encoding of an optimal auction of any format (direct or indirect, truthful or non-truthful) that can be implemented in expected polynomial time. In particular, under well-believed complexity-theoretic assumptions, revenue-optimization in very simple multi-item settings can only be tractably approximated.\n  We note that our hardness result applies to randomized mechanisms in a very simple setting, and is not an artifact of introducing combinatorial structure to the problem by allowing correlation among item values, introducing combinatorial valuations, or requiring the mechanism to be deterministic (whose structure is readily combinatorial). Our proof is enabled by a flow-interpretation of the solutions of an exponential-size linear program for revenue maximization with an additional supermodularity constraint.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide a reduction from revenue maximization to welfare maximization in multi-dimensional Bayesian auctions with arbitrary (possibly combinatorial) feasibility constraints and independent bidders with arbitrary (possibly combinatorial) demand constraints, appropriately extending Myerson's result to this setting. We also show that every feasible Bayesian auction can be implemented as a distribution over virtual VCG allocation rules. A virtual VCG allocation rule has the following simple form: Every bidder's type t_i is transformed into a virtual type f_i(t_i), via a bidder-specific function. Then, the allocation maximizing virtual welfare is chosen. Using this characterization, we show how to find and run the revenue-optimal auction given only black box access to an implementation of the VCG allocation rule. We generalize this result to arbitrarily correlated bidders, introducing the notion of a second-order VCG allocation rule.\n  We obtain our reduction from revenue to welfare optimization via two algorithmic results on reduced forms in settings with arbitrary feasibility and demand constraints. First, we provide a separation oracle for determining feasibility of a reduced form. Second, we provide a geometric algorithm to decompose any feasible reduced form into a distribution over virtual VCG allocation rules. In addition, we show how to execute both algorithms given only black box access to an implementation of the VCG allocation rule.\n  Our results are computationally efficient for all multi-dimensional settings where the bidders are additive. In this case, our mechanisms run in time polynomial in the total number of bidder types, but not type profiles. For generic correlated distributions, this is the natural description complexity of the problem. The runtime can be further improved to poly(#items, #bidders) in item-symmetric settings by making use of recent techniques.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We give highly efficient algorithms, and almost matching lower bounds, for a range of basic statistical problems that involve testing and estimating the L_1 distance between two k-modal distributions $p$ and $q$ over the discrete domain $\\{1,\\dots,n\\}$. More precisely, we consider the following four problems: given sample access to an unknown k-modal distribution $p$,\n  Testing identity to a known or unknown distribution:\n  1. Determine whether $p = q$ (for an explicitly given k-modal distribution $q$) versus $p$ is $\\eps$-far from $q$;\n  2. Determine whether $p=q$ (where $q$ is available via sample access) versus $p$ is $\\eps$-far from $q$;\n  Estimating $L_1$ distance (\"tolerant testing'') against a known or unknown distribution:\n  3. Approximate $d_{TV}(p,q)$ to within additive $\\eps$ where $q$ is an explicitly given k-modal distribution $q$;\n  4. Approximate $d_{TV}(p,q)$ to within additive $\\eps$ where $q$ is available via sample access.\n  For each of these four problems we give sub-logarithmic sample algorithms, that we show are tight up to additive $\\poly(k)$ and multiplicative $\\polylog\\log n+\\polylog k$ factors. Thus our bounds significantly improve the previous results of \\cite{BKR:04}, which were for testing identity of distributions (items (1) and (2) above) in the special cases k=0 (monotone distributions) and k=1 (unimodal distributions) and required $O((\\log n)^3)$ samples.\n  As our main conceptual contribution, we introduce a new reduction-based approach for distribution-testing problems that lets us obtain all the above results in a unified way. Roughly speaking, this approach enables us to transform various distribution testing problems for k-modal distributions over $\\{1,\\dots,n\\}$ to the corresponding distribution testing problems for unrestricted distributions over a much smaller domain $\\{1,\\dots,\\ell\\}$ where $\\ell = O(k \\log n).$\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide a constructive proof of Border's theorem [Bor91, HR15a] and its generalization to reduced-form auctions with asymmetric bidders [Bor07, MV10, CKM13]. Given a reduced form, we identify a subset of Border constraints that are necessary and sufficient to determine its feasibility. Importantly, the number of these constraints is linear in the total number of bidder types. In addition, we provide a characterization result showing that every feasible reduced form can be induced by an ex-post allocation rule that is a distribution over ironings of the same total ordering of the union of all bidders' types.\n  We show how to leverage our results for single-item reduced forms to design auctions with heterogeneous items and asymmetric bidders with valuations that are additive over items. Appealing to our constructive Border's theorem, we obtain polynomial-time algorithms for computing the revenue-optimal auction. Appealing to our characterization of feasible reduced forms, we characterize feasible multi-item allocation rules.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We efficiently solve the optimal multi-dimensional mechanism design problem for independent bidders with arbitrary demand constraints when either the number of bidders is a constant or the number of items is a constant. In the first setting, we need that each bidder's values for the items are sampled from a possibly correlated, item-symmetric distribution, allowing different distributions for each bidder. In the second setting, we allow the values of each bidder for the items to be arbitrarily correlated, but assume that the distribution of bidder types is bidder-symmetric.\n  For all eps>0, we obtain an additive eps-approximation, when the value distributions are bounded, or a multiplicative (1-eps)-approximation when the value distributions are unbounded, but satisfy the Monotone Hazard Rate condition, covering a widely studied class of distributions in Economics. Our runtime is polynomial in max{#items,#bidders}, and not the size of the support of the joint distribution of all bidders' values for all items, which is typically exponential in both the number of items and the number of bidders. Our mechanisms are randomized, explicitly price bundles, and can sometimes accommodate budget constraints.\n  Our results are enabled by establishing several new tools and structural properties of Bayesian mechanisms. We provide a symmetrization technique turning any truthful mechanism into one that has the same revenue and respects all symmetries in the underlying value distributions. We also prove that item-symmetric mechanisms satisfy a natural monotonicity property which, unlike cyclic-monotonicity, can be harnessed algorithmically. Finally, we provide a technique that turns any given eps-BIC mechanism (i.e. one where incentive constraints are violated by eps) into a truly-BIC mechanism at the cost of O(sqrt{eps}) revenue. We expect our tools to be used beyond the settings we consider here.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We present an efficient phylogenetic reconstruction algorithm allowing insertions and deletions which provably achieves a sequence-length requirement (or sample complexity) growing polynomially in the number of taxa. Our algorithm is distance-based, that is, it relies on pairwise sequence comparisons. More importantly, our approach largely bypasses the difficult problem of multiple sequence alignment.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We consider a basic problem in unsupervised learning: learning an unknown \\emph{Poisson Binomial Distribution}. A Poisson Binomial Distribution (PBD) over $\\{0,1,\\dots,n\\}$ is the distribution of a sum of $n$ independent Bernoulli random variables which may have arbitrary, potentially non-equal, expectations. These distributions were first studied by S. Poisson in 1837 \\cite{Poisson:37} and are a natural $n$-parameter generalization of the familiar Binomial Distribution. Surprisingly, prior to our work this basic learning problem was poorly understood, and known results for it were far from optimal.\n  We essentially settle the complexity of the learning problem for this basic class of distributions. As our first main result we give a highly efficient algorithm which learns to $\\eps$-accuracy (with respect to the total variation distance) using $\\tilde{O}(1/\\eps^3)$ samples \\emph{independent of $n$}. The running time of the algorithm is \\emph{quasilinear} in the size of its input data, i.e., $\\tilde{O}(\\log(n)/\\eps^3)$ bit-operations. (Observe that each draw from the distribution is a $\\log(n)$-bit string.) Our second main result is a {\\em proper} learning algorithm that learns to $\\eps$-accuracy using $\\tilde{O}(1/\\eps^2)$ samples, and runs in time $(1/\\eps)^{\\poly (\\log (1/\\eps))} \\cdot \\log n$. This is nearly optimal, since any algorithm {for this problem} must use $\u03a9(1/\\eps^2)$ samples. We also give positive and negative results for some extensions of this learning problem to weighted sums of independent Bernoulli random variables.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "A $k$-modal probability distribution over the discrete domain $\\{1,...,n\\}$ is one whose histogram has at most $k$ \"peaks\" and \"valleys.\" Such distributions are natural generalizations of monotone ($k=0$) and unimodal ($k=1$) probability distributions, which have been intensively studied in probability theory and statistics.\n  In this paper we consider the problem of \\emph{learning} (i.e., performing density estimation of) an unknown $k$-modal distribution with respect to the $L_1$ distance. The learning algorithm is given access to independent samples drawn from an unknown $k$-modal distribution $p$, and it must output a hypothesis distribution $\\widehat{p}$ such that with high probability the total variation distance between $p$ and $\\widehat{p}$ is at most $\u03b5.$ Our main goal is to obtain \\emph{computationally efficient} algorithms for this problem that use (close to) an information-theoretically optimal number of samples.\n  We give an efficient algorithm for this problem that runs in time $\\mathrm{poly}(k,\\log(n),1/\u03b5)$. For $k \\leq \\tilde{O}(\\log n)$, the number of samples used by our algorithm is very close (within an $\\tilde{O}(\\log(1/\u03b5))$ factor) to being information-theoretically optimal. Prior to this work computationally efficient algorithms were known only for the cases $k=0,1$ \\cite{Birge:87b,Birge:97}.\n  A novel feature of our approach is that our learning algorithm crucially uses a new algorithm for \\emph{property testing of probability distributions} as a key subroutine. The learning algorithm uses the property tester to efficiently decompose the $k$-modal distribution into $k$ (near-)monotone distributions, which are easier to learn.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We provide a near-optimal, computationally efficient algorithm for the unit-demand pricing problem, where a seller wants to price n items to optimize revenue against a unit-demand buyer whose values for the items are independently drawn from known distributions. For any chosen accuracy eps>0 and item values bounded in [0,1], our algorithm achieves revenue that is optimal up to an additive error of at most eps, in polynomial time. For values sampled from Monotone Hazard Rate (MHR) distributions, we achieve a (1-eps)-fraction of the optimal revenue in polynomial time, while for values sampled from regular distributions the same revenue guarantees are achieved in quasi-polynomial time.\n  Our algorithm for bounded distributions applies probabilistic techniques to understand the statistical properties of revenue distributions, obtaining a reduction in the search space of the algorithm via dynamic programming. Adapting this approach to MHR and regular distributions requires the proof of novel extreme value theorems for such distributions.\n  As a byproduct, our techniques establish structural properties of approximately-optimal and near-optimal solutions. We show that, for values independently distributed according to MHR distributions, pricing all items at the same price achieves a constant fraction of the optimal revenue. Moreover, for all eps >0, g(1/eps) distinct prices suffice to obtain a (1-eps)-fraction of the optimal revenue, where g(1/eps) is quadratic in 1/eps and independent of n. Similarly, for all eps>0 and n>0, at most g(1/(eps log n)) distinct prices suffice if the values are independently distributed according to regular distributions, where g() is a polynomial function. Finally, when the values are i.i.d. from some MHR distribution, we show that, if n is a sufficiently large function of 1/eps, a single price suffices to achieve a (1-eps)-fraction of the optimal revenue.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "We consider the problem of learning an unknown product distribution $X$ over $\\{0,1\\}^n$ using samples $f(X)$ where $f$ is a \\emph{known} transformation function. Each choice of a transformation function $f$ specifies a learning problem in this framework.\n  Information-theoretic arguments show that for every transformation function $f$ the corresponding learning problem can be solved to accuracy $\\eps$, using $\\tilde{O}(n/\\eps^2)$ examples, by a generic algorithm whose running time may be exponential in $n.$ We show that this learning problem can be computationally intractable even for constant $\\eps$ and rather simple transformation functions. Moreover, the above sample complexity bound is nearly optimal for the general problem, as we give a simple explicit linear transformation function $f(x)=w \\cdot x$ with integer weights $w_i \\leq n$ and prove that the corresponding learning problem requires $\u03a9(n)$ samples.\n  As our main positive result we give a highly efficient algorithm for learning a sum of independent unknown Bernoulli random variables, corresponding to the transformation function $f(x)= \\sum_{i=1}^n x_i$. Our algorithm learns to $\\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\\eps)$ number of samples that is independent of $n.$ We also give an efficient algorithm that uses $\\log n \\cdot \\poly(1/\\eps)$ samples but has running time that is only $\\poly(\\log n, 1/\\eps).$\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "If a game has a Nash equilibrium with probability values that are either zero or Omega(1) then this equilibrium can be found exhaustively in polynomial time. Somewhat surprisingly, we show that there is a PTAS for the games whose equilibria are guaranteed to have small-O(1/n)-values, and therefore large-Omega(n)-supports. We also point out that there is a PTAS for games with sparse payoff matrices, which are known to be PPAD-complete to solve exactly. Both algorithms are of a special kind that we call oblivious: The algorithm just samples a fixed distribution on pairs of mixed strategies, and the game is only used to determine whether the sampled strategies comprise an eps-Nash equilibrium; the answer is yes with inverse polynomial probability. These results bring about the question: Is there an oblivious PTAS for Nash equilibrium in general games? We answer this question in the negative; our lower bound comes close to the quasi-polynomial upper bound of [Lipton, Markakis, Mehta 2003].\n  Another recent PTAS for anonymous games is also oblivious in a weaker sense appropriate for this class of games (it samples from a fixed distribution on unordered collections of mixed strategies), but its runtime is exponential in 1/eps. We prove that any oblivious PTAS for anonymous games with two strategies and three player types must have 1/eps^c in the exponent of the running time for some c>1/3, rendering the algorithm in [Daskalakis 2008] essentially optimal within oblivious algorithms. In contrast, we devise a poly(n) (1/eps)^O(log^2(1/eps)) non-oblivious PTAS for anonymous games with 2 strategies and any bounded number of player types.\n  Our algorithm is based on the construction of a sparse (and efficiently computable) eps-cover of the set of all possible sums of n independent indicators, under the total variation distance. The size of the cover is poly(n) (1/ eps^{O(log^2 (1/eps))}.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Molecular phylogenetic techniques do not generally account for such common evolutionary events as site insertions and deletions (known as indels). Instead tree building algorithms and ancestral state inference procedures typically rely on substitution-only models of sequence evolution. In practice these methods are extended beyond this simplified setting with the use of heuristics that produce global alignments of the input sequences--an important problem which has no rigorous model-based solution. In this paper we consider a new version of the multiple sequence alignment in the context of stochastic indel models. More precisely, we introduce the following {\\em trace reconstruction problem on a tree} (TRPT): a binary sequence is broadcast through a tree channel where we allow substitutions, deletions, and insertions; we seek to reconstruct the original sequence from the sequences received at the leaves of the tree. We give a recursive procedure for this problem with strong reconstruction guarantees at low mutation rates, providing also an alignment of the sequences at the leaves of the tree. The TRPT problem without indels has been studied in previous work (Mossel 2004, Daskalakis et al. 2006) as a bootstrapping step towards obtaining optimal phylogenetic reconstruction methods. The present work sets up a framework for extending these works to evolutionary models with indels.\n        \u25b3 Less", "author": "Constantinos Daskalakis"}, {"abstract": "Adaptive mesh refinement (AMR) is often used when solving time-dependent partial differential equations using numerical methods. It enables time-varying regions of much higher resolution, which can be used to track discontinuities in the solution by selectively refining around those areas. The open source Clawpack software implements block-structured AMR to refine around propagating waves in the AMRClaw package. For problems where the solution must be computed over a large domain but is only of interest in a small area this approach often refines waves that will not impact the target area. We seek a method that enables the identification and refinement of only the waves that will influence the target area.\n  Here we show that solving the time-dependent adjoint equation and using a suitable inner product allows for a more precise refinement of the relevant waves. We present the adjoint methodology in general, and give details on how this method has been implemented in AMRClaw. Examples for linear acoustics equations are presented, and a computational performance analysis is conducted. The adjoint method is compared to AMR methods already available in the AMRClaw software, and the resulting advantages and disadvantages are discussed. The code for the examples presented is archived on Github.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "We present ALMA and MUSE observations of the Brightest Cluster Galaxy in Abell 2597, a nearby (z=0.0821) cool core cluster of galaxies. The data map the kinematics of a three billion solar mass filamentary nebula that spans the innermost 30 kpc of the galaxy's core. Its warm ionized and cold molecular components are both cospatial and comoving, consistent with the hypothesis that the optical nebula traces the warm envelopes of many cold molecular clouds that drift in the velocity field of the hot X-ray atmosphere. The clouds are not in dynamical equilibrium, and instead show evidence for inflow toward the central supermassive black hole, outflow along the jets it launches, and uplift by the buoyant hot bubbles those jets inflate. The entire scenario is therefore consistent with a galaxy-spanning \"fountain\", wherein cold gas clouds drain into the black hole accretion reservoir, powering jets and bubbles that uplift a cooling plume of low-entropy multiphase gas, which may stimulate additional cooling and accretion as part of a self-regulating feedback loop. All velocities are below the escape speed from the galaxy, and so these clouds should rain back toward the galaxy center from which they came, keeping the fountain long-lived. The data are consistent with major predictions of chaotic cold accretion, precipitation, and stimulated feedback models, and may trace processes fundamental to galaxy evolution at effectively all mass scales.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "Recent observations of interplanetary medium (IPM) atomic hydrogen Lyman-\u03b1 (Ly\u03b1) emission in the outer solar system, made with the Alice ultraviolet spectrograph on New Horizons (NH), are presented. The observations include regularly spaced great-circle scans of the sky and pointed observations near the downstream and upstream flow directions of interstellar H atoms. The NH Alice data agree very well with the much earlier Voyager UVS results, after these are reduced by a factor of 2.4 in brightness, in accordance with recent re-analyses. In particular, the falloff of IPM Ly\u03b1 brightness in the upstream-looking direction as a function of spacecraft distance from the Sun is well-matched by an expected 1/r dependence, but with an added constant brightness of ~40 Rayleighs. This additional brightness is a possible signature of the hydrogen wall at the heliopause or of a more distant background. Ongoing observations are planned at a cadence of roughly twice per year.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "The Alice instrument on NASA's New Horizons spacecraft observed an ultraviolet solar occultation by Pluto's atmosphere on 2015 July 14. The transmission vs. altitude was sensitive to the presence of N2, CH4, C2H2, C2H4, C2H6, and haze. We derived line-of-sight abundances and local number densities for the 5 molecular species, and line-of-sight optical depth and extinction coefficients for the haze. We found the following major conclusions: 1) We confirmed temperatures in Pluto's upper atmosphere that were colder than expected before the New Horizons flyby, with upper atmospheric temperatures near 65-68 K. The inferred enhanced Jeans escape rates were (3e22-7e22) N2/s and (4e25-8e25) CH4/s at the exobase (at a radius of ~2900 km, or an altitude of ~1710 km). 2) We measured CH4 abundances from 80 to 1200 km above the surface. A joint analysis of the Alice CH4 and Alice and REX N2 measurements implied a very stable lower atmosphere with a small eddy diffusion coefficient, most likely between 550 and 4000 cm2/s. Such a small eddy diffusion coefficient placed the homopause within 12 km of the surface, giving Pluto a small planetary boundary layer. The inferred CH4 surface mixing ratio was ~0.28-0.35%. 3) The abundance profiles of the C2Hx hydrocarbons (C2H2, C2H4, C2H6) were not simply exponential with altitude. We detected local maxima in line-of-sight abundance near 410 km altitude for C2H4, near 320 km for C2H2, and an inflection point or the suggestion of a local maximum at 260 km for C2H6. We also detected local minima near 200 km altitude for C2H4, near 170 km for C2H2, and an inflection point or minimum near 170-200 km for C2H6. These compared favorably with models for hydrocarbon production near 300-400 km and haze condensation near 200 km, especially for C2H2 and C2H4 (Wong et al. 2017). 4) We found haze that had an extinction coefficient approximately proportional to N2 density.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "From Harry Potter to American Horror Story, fanfiction is extremely popular among young people. Sites such as Fanfiction.net host millions of stories, with thousands more posted each day. Enthusiasts are sharing their writing and reading stories written by others. Exactly how does a generation known more for videogame expertise than long-form writing become so engaged in reading and writing in these communities? Via a nine-month ethnographic investigation of fanfiction communities that included participant observation, interviews, a thematic analysis of 4,500 reader reviews and an in-depth case study of a discussion group, we found that members of fanfiction communities spontaneously mentor each other in open forums, and that this mentoring builds upon previous interactions in a way that is distinct from traditional forms of mentoring and made possible by the affordances of networked publics. This work extends and develops the theory of distributed mentoring. Our findings illustrate how distributed mentoring supports fanfiction authors as they work to develop their writing skills. We believe distributed mentoring holds potential for supporting learning in a variety of formal and informal learning environments.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular neuropsychological screening tool for cognitive conditions. The Digital Clock Drawing Test (dCDT) uses novel software to analyze data from a digitizing ballpoint pen that reports its position with considerable spatial and temporal precision, making possible the analysis of both the drawing process and final product. We developed methodology to analyze pen stroke data from these drawings, and computed a large collection of features which were then analyzed with a variety of machine learning techniques. The resulting scoring systems were designed to be more accurate than the systems currently used by clinicians, but just as interpretable and easy to use. The systems also allow us to quantify the tradeoff between accuracy and interpretability. We created automated versions of the CDT scoring systems currently used by clinicians, allowing us to benchmark our models, which indicated that our machine learning models substantially outperformed the existing scoring systems.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "One difficulty in developing numerical methods for tsunami modeling is the fact that solutions contain regions where much higher resolution is required than elsewhere in the domain, particularly since the solution may contain discontinuities or other localized features. The Clawpack software deals with this issue by using block-structured adaptive mesh refinement to selectively refine around propagating waves. For problems where only a target area of the total solution is of interest (e.g. one coastal community), a method that allows identifying and refining the grid only in regions that influence this target area would significantly reduce the computational cost of finding a solution.\n  In this work, we show that solving the time-dependent adjoint equation and using a suitable inner product with the forward solution allows more precise refinement of the relevant waves. We present examples solving the shallow water equations in one and two dimensions. To perform these simulations, the use of the adjoint method has been integrated into the adaptive mesh refinement strategy of the open source GeoClaw software. We also present results that show that the accuracy of the solution is maintained and the computational time required is significantly reduced through the integration of the adjoint method into adaptive mesh refinement.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "We describe a sketch interpretation system that detects and classifies clock numerals created by subjects taking the Clock Drawing Test, a clinical tool widely used to screen for cognitive impairments (e.g., dementia). We describe how it balances appearance and context, and document its performance on some 2,000 drawings (about 24K clock numerals) produced by a wide spectrum of patients. We calibrate the utility of different forms of context, describing experiments with Conditional Random Fields trained and tested using a variety of features. We identify context that contributes to interpreting otherwise ambiguous or incomprehensible strokes. We describe ST-slices, a novel representation that enables \"unpeeling\" the layers of ink that result when people overwrite, which often produces ink impossible to analyze if only the final drawing is examined. We characterize when ST-slices work, calibrate their impact on performance, and consider their breadth of applicability.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "One difficulty in developing numerical methods for hyperbolic systems of conservation laws is the fact that solutions often contain regions where much higher resolution is required than elsewhere in the domain, particularly since the solution may contain discontinuities or other localized features. The Clawpack software deals with this issue by using block-structured adaptive mesh refinement to selectively refine around propagating waves. For problems where only a target area of the total solution is of interest, a method that allows identifying and refining the grid only in regions that influence this target area would significantly reduce the computational cost of finding a solution.\n  In this work, we show that solving the time-dependent adjoint equation and using a suitable inner product with the forward solution allows more precise refinement of the relevant waves. We present acoustics examples in one and two dimensions and a tsunami propagation example. To perform these simulations, the use of the adjoint method has been integrated into the adaptive mesh refinement strategy of the open source Clawpack and GeoClaw software. We also present results that show that the accuracy of the solution is maintained and the computational time required is significantly reduced through the integration of the adjoint method into AMR.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "Young people worldwide are participating in ever-increasing numbers in online fan communities. Far from mere shallow repositories of pop culture, these sites are accumulating significant evidence that sophisticated informal learning is taking place online in novel and unexpected ways. In order to understand and analyze in more detail how learning might be occurring, we conducted an in-depth nine-month ethnographic investigation of online fanfiction communities, including participant observation and fanfiction author interviews. Our observations led to the development of a theory we term distributed mentoring, which we present in detail in this paper. Distributed mentoring exemplifies one instance of how networked technology affords new extensions of behaviors that were previously bounded by time and space. Distributed mentoring holds potential for application beyond the spontaneous mentoring observed in this investigation and may help students receive diverse, thoughtful feedback in formal learning environments as well.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "A major goal of the Atacama Large Millimeter/submillimeter Array (ALMA) is to make accurate images with resolutions of tens of milliarcseconds, which at submillimeter (submm) wavelengths requires baselines up to ~15 km. To develop and test this capability, a Long Baseline Campaign (LBC) was carried out from September to late November 2014, culminating in end-to-end observations, calibrations, and imaging of selected Science Verification (SV) targets. This paper presents an overview of the campaign and its main results, including an investigation of the short-term coherence properties and systematic phase errors over the long baselines at the ALMA site, a summary of the SV targets and observations, and recommendations for science observing strategies at long baselines. Deep ALMA images of the quasar 3C138 at 97 and 241 GHz are also compared to VLA 43 GHz results, demonstrating an agreement at a level of a few percent. As a result of the extensive program of LBC testing, the highly successful SV imaging at long baselines achieved angular resolutions as fine as 19 mas at ~350 GHz. Observing with ALMA on baselines of up to 15 km is now possible, and opens up new parameter space for submm astronomy.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "The Allen Telescope Array (ATA) is a cm-wave interferometer in California, comprising 42 antenna elements with 6-m diameter dishes. We characterize the antenna optical accuracy using two-antenna interferometry and radio holography. The distortion of each telescope relative to the average is small, with RMS differences of 1 percent of beam peak value. Holography provides images of dish illumination pattern, allowing characterization of as-built mirror surfaces. The ATA dishes can experience mm-scale distortions across -2 meter lengths due to mounting stresses or solar radiation. Experimental RMS errors are 0.7 mm at night and 3 mm under worst case solar illumination. For frequencies 4, 10, and 15 GHz, the nighttime values indicate sensitivity losses of 1, 10 and 20 percent, respectively. The ATA.s exceptional wide-bandwidth permits observations over a continuous range 0.5 to 11.2 GHz, and future retrofits may increase this range to 15 GHz. Beam patterns show a slowly varying focus frequency dependence. We probe the antenna optical gain and beam pattern stability as a function of focus and observation frequency, concluding that ATA can produce high fidelity images over a decade of simultaneous observation frequencies. In the day, the antenna sensitivity and pointing accuracy are affected. We find that at frequencies greater than 5 GHz, daytime observations greater than 5 GHz will suffer some sensitivity loss and it may be necessary to make antenna pointing corrections on a 1 to 2 hourly basis.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "We report observations of the lunar helium exosphere made between December 29, 2011, and January 26, 2012, with the Lyman Alpha Mapping Project (LAMP) ultraviolet spectrograph on NASA's Lunar Reconnaissance Orbiter Mission (LRO). The observations were made of resonantly scattered He I 584 from illuminated atmosphere against the dark lunar surface on the dawn side of the terminator. We find no or little variation of the derived surface He density with latitude but day-to-day variations that likely reflect variations in the solar wind alpha flux. The 5-day passage of the Moon through the Earth's magnetotail results in a factor of two decrease in surface density, which is well explained by model simulations.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "The Pi GHz Sky Survey (PiGSS) is a key project of the Allen Telescope Array. PiGSS is a 3.1 GHz survey of radio continuum emission in the extragalactic sky with an emphasis on synoptic observations that measure the static and time-variable properties of the sky. During the 2.5-year campaign, PiGSS will twice observe ~250,000 radio sources in the 10,000 deg^2 region of the sky with b > 30 deg to an rms sensitivity of ~1 mJy. Additionally, sub-regions of the sky will be observed multiple times to characterize variability on time scales of days to years. We present here observations of a 10 deg^2 region in the Bootes constellation overlapping the NOAO Deep Wide Field Survey field. The PiGSS image was constructed from 75 daily observations distributed over a 4-month period and has an rms flux density between 200 and 250 microJy. This represents a deeper image by a factor of 4 to 8 than we will achieve over the entire 10,000 deg^2. We provide flux densities, source sizes, and spectral indices for the 425 sources detected in the image. We identify ~100$ new flat spectrum radio sources; we project that when completed PiGSS will identify 10^4 flat spectrum sources. We identify one source that is a possible transient radio source. This survey provides new limits on faint radio transients and variables with characteristic durations of months.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "We present the Allen Telescope Array Twenty-centimeter Survey (ATATS), a multi-epoch (12 visits), 690 square degree radio image and catalog at 1.4GHz. The survey is designed to detect rare, very bright transients as well as to verify the capabilities of the ATA to form large mosaics. The combined image using data from all 12 ATATS epochs has RMS noise sigma = 3.94mJy / beam and dynamic range 180, with a circular beam of 150 arcsec FWHM. It contains 4408 sources to a limiting sensitivity of S = 20 mJy / beam. We compare the catalog generated from this 12-epoch combined image to the NRAO VLA Sky Survey (NVSS), a legacy survey at the same frequency, and find that we can measure source positions to better than ~20 arcsec. For sources above the ATATS completeness limit, the median flux density is 97% of the median value for matched NVSS sources, indicative of an accurate overall flux calibration. We examine the effects of source confusion due to the effects of differing resolution between ATATS and NVSS on our ability to compare flux densities. We detect no transients at flux densities greater than 40 mJy in comparison with NVSS, and place a 2-sigma upper limit on the transient rate for such sources of 0.004 per square degree. These results suggest that the > 1 Jy transients reported by Matsumura et al. (2009) may not be true transients, but rather variable sources at their flux density threshold.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "Low-mass pre-main sequence (PMS) stars are strong and variable X-ray emitters, as has been well established by EINSTEIN and ROSAT observatories. It was originally believed that this emission was of thermal nature and primarily originated from coronal activity (magnetically confined loops, in analogy with Solar activity) on contracting young stars. Broadband spectral analysis showed that the emission was not isothermal and that elemental abundances were non-Solar. The resolving power of the Chandra and XMM X-ray gratings spectrometers have provided the first, tantalizing details concerning the physical conditions such as temperatures, densities, and abundances that characterize the X-ray emitting regions of young star. These existing high resolution spectrometers, however, simply do not have the effective area to measure diagnostic lines for a large number of PMS stars over required to answer global questions such as: how does magnetic activity in PMS stars differ from that of main sequence stars, how do they evolve, what determines the population structure and activity in stellar clusters, and how does the activity influence the evolution of protostellar disks. Highly resolved (R>3000) X-ray spectroscopy at orders of magnitude greater efficiency than currently available will provide major advances in answering these questions. This requires the ability to resolve the key diagnostic emission lines with a precision of better than 100 km/s.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "Understanding the origins and distribution of matter in the Universe is one of the most important quests in physics and astronomy. Themes range from astro-particle physics to chemical evolution in the Galaxy to cosmic nucleosynthesis and chemistry in an anticipation of a full account of matter in the Universe. Studies of chemical evolution in the early Universe will answer questions about when and where the majority of metals were formed, how they spread and why they appar today as they are. The evolution of matter in our Universe cannot be characterized as a simple path of development. In fact the state of matter today tells us that mass and matter is under constant reformation through on-going star formation, nucleosynthesis and mass loss on stellar and galactic scales. X-ray absorption studies have evolved in recent years into powerful means to probe the various phases of interstellar and intergalactic media. Future observatories such as IXO and Gen-X will provide vast new opportunities to study structure and distribution of matter with high resolution X-ray spectra. Specifically the capabilities of the soft energy gratings with a resolution of R=3000 onboard IXO will provide ground breaking determinations of element abundance, ionization structure, and dispersion velocities of the interstellar and intergalactic media of our Galaxy and the Local Group\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "The first 42 elements of the Allen Telescope Array (ATA-42) are beginning to deliver data at the Hat Creek Radio Observatory in Northern California. Scientists and engineers are actively exploiting all of the flexibility designed into this innovative instrument for simultaneously conducting surveys of the astrophysical sky and conducting searches for distant technological civilizations. This paper summarizes the design elements of the ATA, the cost savings made possible by the use of COTS components, and the cost/performance trades that eventually enabled this first snapshot radio camera. The fundamental scientific program of this new telescope is varied and exciting; some of the first astronomical results will be discussed.\n        \u25b3 Less", "author": "Randall Davis"}, {"abstract": "A simulation methodology for ultra-scaled InAs quantum well field effect transistors (QWFETs) is presented and used to provide design guidelines and a path to improve device performance. A multiscale modeling approach is adopted, where strain is computed in an atomistic valence-force-field method, an atomistic sp3d5s* tight-binding model is used to compute channel effective masses, and a 2-D real-space effective mass based ballistic quantum transport model is employed to simulate three terminal current-voltage characteristics including gate leakage. The simulation methodology is first benchmarked against experimental I-V data obtained from devices with gate lengths ranging from 30 to 50 nm. A good quantitative match is obtained. The calibrated simulation methodology is subsequently applied to optimize the design of a 20 nm gate length device. Two critical parameters have been identified to control the gate leakage magnitude of the QWFETs, (i) the geometry of the gate contact (curved or square) and (ii) the gate metal work function. In addition to pushing the threshold voltage towards an enhancement mode operation, a higher gate metal work function can help suppress the gate leakage and allow for much aggressive insulator scaling.\n        \u25b3 Less", "author": "Jes\u00fas del Alamo"}, {"abstract": "Cookie Clicker is a popular online incremental game where the goal of the game is to generate as many cookies as possible. In the game you start with an initial cookie generation rate, and you can use cookies as currency to purchase various items that increase your cookie generation rate. In this paper, we analyze strategies for playing Cookie Clicker optimally. While simple to state, the game gives rise to interesting analysis involving ideas from NP-hardness, approximation algorithms, and dynamic programming.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "An oritatami system (OS) is a theoretical model of self-assembly via co-transcriptional folding. It consists of a growing chain of beads which can form bonds with each other as they are transcribed. During the transcription process, the $\u03b4$ most recently produced beads dynamically fold so as to maximize the number of bonds formed, self-assemblying into a shape incrementally. The parameter $\u03b4$ is called the delay and is related to the transcription rate in nature.\n  This article initiates the study of shape self-assembly using oritatami. A shape is a connected set of points in the triangular lattice. We first show that oritatami systems differ fundamentally from tile-assembly systems by exhibiting a family of infinite shapes that can be tile-assembled but cannot be folded by any OS. As it is NP-hard in general to determine whether there is an OS that folds into (self-assembles) a given finite shape, we explore the folding of upscaled versions of finite shapes. We show that any shape can be folded from a constant size seed, at any scale n >= 3, by an OS with delay 1. We also show that any shape can be folded at the smaller scale 2 by an OS with unbounded delay. This leads us to investigate the influence of delay and to prove that, for all \u03b4 > 2, there are shapes that can be folded (at scale 1) with delay \u03b4 but not with delay \u03b4'<\u03b4. These results serve as a foundation for the study of shape-building in this new model of self-assembly, and have the potential to provide better understanding of cotranscriptional folding in biology, as well as improved abilities of experimentalists to design artificial systems that self-assemble via this complex dynamical process.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We prove computational intractability of variants of checkers: (1) deciding whether there is a move that forces the other player to win in one move is NP-complete; (2) checkers where players must always be able to jump on their turn is PSPACE-complete; and (3) cooperative versions of (1) and (2) are NP-complete. We also give cooperative checkers puzzles whose solutions are the letters of the alphabet.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We initiate a general theory for analyzing the complexity of motion planning of a single robot through a graph of \"gadgets\", each with their own state, set of locations, and allowed traversals between locations that can depend on and change the state. This type of setup is common to many robot motion planning hardness proofs. We characterize the complexity for a natural simple case: each gadget connects up to four locations in a perfect matching (but each direction can be traversable or not in the current state), has one or two states, every gadget traversal is immediately undoable, and that gadget locations are connected by an always-traversable forest, possibly restricted to avoid crossings in the plane. Specifically, we show that any single nontrivial four-location two-state gadget type is enough for motion planning to become PSPACE-complete, while any set of simpler gadgets (effectively two-location or one-state) has a polynomial-time motion planning algorithm. As a sample application, our results show that motion planning games with \"spinners\" are PSPACE-complete, establishing a new hard aspect of Zelda: Oracle of Seasons.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We develop a new framework for generalizing approximation algorithms from the structural graph algorithm literature so that they apply to graphs somewhat close to that class (a scenario we expect is common when working with real-world networks) while still guaranteeing approximation ratios. The idea is to $\\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph into an algorithmically tractable class, apply known approximation algorithms for that class, and then $\\textit{lift}$ the solution to apply to the original graph. We give a general characterization of when an optimization problem is amenable to this approach, and show that it includes many well-studied graph problems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum Maximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, and Edge ($\\ell$-)Dominating Set.\n  To enable this framework, we develop new editing algorithms that find the approximately fewest edits required to bring a given graph into one of several important graph classes (in some cases, also approximating the target parameter of the family). For bounded degeneracy, we obtain a bicriteria $(4,4)$-approximation which also extends to a smoother bicriteria trade-off. For bounded treewidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log w}))$-approximation, and for bounded pathwidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log w} \\cdot \\log n))$-approximation. For treedepth $2$ (also related to bounded expansion), we obtain a $4$-approximation. We also prove complementary hardness-of-approximation results assuming $\\mathrm{P} \\neq \\mathrm{NP}$: in particular, all of the problems just stated have a log-factor inapproximability, except the last which is not approximable below some constant factor ($2$ assuming UGC).\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We consider the computational complexity of reconfiguration problems, in which one is given two combinatorial configurations satisfying some constraints, and is asked to transform one into the other using elementary transformations, while satisfying the constraints at all times. Such problems appear naturally in many contexts, such as model checking, motion planning, enumeration and sampling, and recreational mathematics. We provide hardness results for problems in this family, in which the constraints and operations are particularly simple. More precisely, we prove the PSPACE-completeness of the following decision problems:\n  $\\bullet$ Given two satisfying assignments to a planar monotone instance of Not-All-Equal 3-SAT, can one assignment be transformed into the other by single variable `flips' (assignment changes), preserving satisfiability at every step?\n  $\\bullet$ Given two subsets of a set S of integers with the same sum, can one subset be transformed into the other by adding or removing at most three elements of S at a time, such that the intermediate subsets also have the same sum?\n  $\\bullet$ Given two points in $\\{0,1\\}^n$ contained in a polytope P specified by a constant number of linear inequalities, is there a path in the n-hypercube connecting the two points and contained in P?\n  These problems can be interpreted as reconfiguration analogues of standard problems in NP. Interestingly, the instances of the NP problems that appear as input to the reconfiguration problems in our reductions can be shown to lie in P. In particular, the elements of S and the coefficients of the inequalities defining P can be restricted to have logarithmic bit-length.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We analyze the computational complexity of the many types of pencil-and-paper-style puzzles featured in the 2016 puzzle video game The Witness. In all puzzles, the goal is to draw a path in a rectangular grid graph from a start vertex to a destination vertex. The different puzzle types place different constraints on the path: preventing some edges from being visited (broken edges); forcing some edges or vertices to be visited (hexagons); forcing some cells to have certain numbers of incident path edges (triangles); or forcing the regions formed by the path to be partially monochromatic (squares), have exactly two special cells (stars), or be singly covered by given shapes (polyominoes) and/or negatively counting shapes (antipolyominoes). We show that any one of these clue types (except the first) is enough to make path finding NP-complete (\"witnesses exist but are hard to find\"), even for rectangular boards. Furthermore, we show that a final clue type (antibody), which necessarily \"cancels\" the effect of another clue in the same region, makes path finding $\u03a3_2$-complete (\"witnesses do not exist\"), even with a single antibody (combined with many anti/polyominoes), and the problem gets no harder with many antibodies.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Since the introduction of retroactive data structures at SODA 2004, a major unsolved problem has been to bound the gap between the best partially retroactive data structure (where changes can be made to the past, but only the present can be queried) and the best fully retroactive data structure (where the past can also be queried) for any problem. It was proved in 2004 that any partially retroactive data structure with operation time $T(n,m)$ can be transformed into a fully retroactive data structure with operation time $O(\\sqrt{m} \\cdot T(n,m))$, where $n$ is the size of the data structure and $m$ is the number of operations in the timeline [Demaine 2004], but it has been open for 14 years whether such a gap is necessary.\n  In this paper, we prove nearly matching upper and lower bounds on this gap for all $n$ and $m$. We improve the upper bound for $n \\ll \\sqrt m$ by showing a new transformation with multiplicative overhead $n \\log m$. We then prove a lower bound of $\\min\\{n \\log m, \\sqrt m\\}^{1-o(1)}$ assuming any of the following conjectures:\n  - Conjecture I: Circuit SAT requires $2^{n - o(n)}$ time on $n$-input circuits of size $2^{o(n)}$. (Far weaker than the well-believed SETH conjecture, which asserts that CNF SAT with $n$ variables and $O(n)$ clauses already requires $2^{n-o(n)}$ time.)\n  - Conjecture II: Online $(\\min,+)$ product between an integer $n\\times n$ matrix and $n$ vectors requires $n^{3 - o(1)}$ time.\n  - Conjecture III (3-SUM Conjecture): Given three sets $A,B,C$ of integers, each of size $n$, deciding whether there exist $a \\in A, b \\in B, c \\in C$ such that $a + b + c = 0$ requires $n^{2 - o(1)}$ time.\n  Our lower bound construction illustrates an interesting power of fully retroactive queries: they can be used to quickly solve batched pair evaluation. We believe this technique can prove useful for other data structure lower bounds, especially dynamic ones.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We analyze the computational complexity of optimally playing the two-player board game Push Fight, generalized to an arbitrary board and number of pieces. We prove that the game is PSPACE-hard to decide who will win from a given position, even for simple (almost rectangular) hole-free boards. We also analyze the mate-in-1 problem: can the player win in a single turn? One turn in Push Fight consists of up to two \"moves\" followed by a mandatory \"push\". With these rules, or generalizing the number of allowed moves to any constant, we show mate-in-1 can be solved in polynomial time. If, however, the number of moves per turn is part of the input, the problem becomes NP-complete. On the other hand, without any limit on the number of moves per turn, the problem becomes polynomially solvable again.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We prove that path puzzles with complete row and column information--or equivalently, 2D orthogonal discrete tomography with Hamiltonicity constraint--are strongly NP-complete, ASP-complete, and #P-complete. Along the way, we newly establish ASP-completeness and #P-completeness for 3-Dimensional Matching and Numerical 3-Dimensional Matching.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We prove that two polygons $A$ and $B$ have a reversible hinged dissection (a chain hinged dissection that reverses inside and outside boundaries when folding between $A$ and $B$) if and only if $A$ and $B$ are two non-crossing nets of a common polyhedron. Furthermore, monotone hinged dissections (where all hinges rotate in the same direction when changing from $A$ to $B$ correspond exactly to non-crossing nets of a common convex polyhedron. By envelope/parcel magic, it becomes easy to design many hinged dissections.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We present a number of breakthroughs for coordinated motion planning, in which the objective is to reconfigure a swarm of labeled convex objects by a combination of parallel, continuous, collision-free translations into a given target arrangement. Problems of this type can be traced back to the classic work of Schwartz and Sharir (1983), who gave a method for deciding the existence of a coordinated motion for a set of disks between obstacles; their approach is polynomial in the complexity of the obstacles, but exponential in the number of disks. Other previous work has largely focused on {\\em sequential} schedules, in which one robot moves at a time.\n  We provide constant-factor approximation algorithms for minimizing the execution time of a coordinated, {\\em parallel} motion plan for a swarm of robots in the absence of obstacles, provided some amount of separability.\n  Our algorithm achieves {\\em constant stretch factor}: If all robots are at most $d$ units from their respective starting positions, the total duration of the overall schedule is $O(d)$. Extensions include unlabeled robots and different classes of robots. We also prove that finding a plan with minimal execution time is NP-hard, even for a grid arrangement without any stationary obstacles. On the other hand, we show that for densely packed disks that cannot be well separated, a stretch factor $\u03a9(N^{1/4})$ may be required. On the positive side, we establish a stretch factor of $O(N^{1/2})$ even in this case.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We study the problem of folding a polyomino $P$ into a polycube $Q$, allowing faces of $Q$ to be covered multiple times. First, we define a variety of folding models according to whether the folds (a) must be along grid lines of $P$ or can divide squares in half (diagonally and/or orthogonally), (b) must be mountain or can be both mountain and valley, (c) can remain flat (forming an angle of $180^\\circ$), and (d) must lie on just the polycube surface or can have interior faces as well. Second, we give all the inclusion relations among all models that fold on the grid lines of $P$. Third, we characterize all polyominoes that can fold into a unit cube, in some models. Fourth, we give a linear-time dynamic programming algorithm to fold a tree-shaped polyomino into a constant-size polycube, in some models. Finally, we consider the triangular version of the problem, characterizing which polyiamonds fold into a regular tetrahedron.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We investigate algorithmic control of a large swarm of mobile particles (such as robots, sensors, or building material) that move in a 2D workspace using a global input signal (such as gravity or a magnetic field). We show that a maze of obstacles to the environment can be used to create complex systems. We provide a wide range of results for a wide range of questions. These can be subdivided into external algorithmic problems, in which particle configurations serve as input for computations that are performed elsewhere, and internal logic problems, in which the particle configurations themselves are used for carrying out computations. For external algorithms, we give both negative and positive results. If we are given a set of stationary obstacles, we prove that it is NP-hard to decide whether a given initial configuration of unit-sized particles can be transformed into a desired target configuration. Moreover, we show that finding a control sequence of minimum length is PSPACE-complete. We also work on the inverse problem, providing constructive algorithms to design workspaces that efficiently implement arbitrary permutations between different configurations. For internal logic, we investigate how arbitrary computations can be implemented. We demonstrate how to encode dual-rail logic to build a universal logic gate that concurrently evaluates and, nand, nor, and or operations. Using many of these gates and appropriate interconnects, we can evaluate any logical expression. However, we establish that simulating the full range of complex interactions present in arbitrary digital circuits encounters a fundamental difficulty: a fan-out gate cannot be generated. We resolve this missing component with the help of 2x1 particles, which can create fan-out gates that produce multiple copies of the inputs. Using these gates we provide rules for replicating arbitrary digital circuits.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "This paper initiates the study of I/O algorithms (minimizing cache misses) from the perspective of fine-grained complexity (conditional polynomial lower bounds). Specifically, we aim to answer why sparse graph problems are so hard, and why the Longest Common Subsequence problem gets a savings of a factor of the size of cache times the length of a cache line, but no more. We take the reductions and techniques from complexity and fine-grained complexity and apply them to the I/O model to generate new (conditional) lower bounds as well as faster algorithms. We also prove the existence of a time hierarchy for the I/O model, which motivates the fine-grained reductions.\n  Using fine-grained reductions, we give an algorithm for distinguishing 2 vs. 3 diameter and radius that runs in $O(|E|^2/(MB))$ cache misses, which for sparse graphs improves over the previous $O(|V|^2/B)$ running time. We give new reductions from radius and diameter to Wiener index and median. We show meaningful reductions between problems that have linear-time solutions in the RAM model. The reductions use low I/O complexity (typically $O(n/B)$), and thus help to finely capture the relationship between \"I/O linear time\" $\u0398(n/B)$ and RAM linear time $\u0398(n)$. We generate new I/O assumptions based on the difficulty of improving sparse graph problem running times in the I/O model. We create conjectures that the current best known algorithms for Single Source Shortest Paths (SSSP), diameter, and radius are optimal. From these I/O-model assumptions, we show that many of the known reductions in the word-RAM model can naturally extend to hold in the I/O model as well (e.g., a lower bound on the I/O complexity of Longest Common Subsequence that matches the best known running time). Finally, we prove an analog of the Time Hierarchy Theorem in the I/O model.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "This paper proves that push-pull block puzzles in 3D are PSPACE-complete to solve, and push-pull block puzzles in 2D with thin walls are NP-hard to solve, settling an open question by Zubaran and Ritt. Push-pull block puzzles are a type of recreational motion planning problem, similar to Sokoban, that involve moving a `robot' on a square grid with $1 \\times 1$ obstacles. The obstacles cannot be traversed by the robot, but some can be pushed and pulled by the robot into adjacent squares. Thin walls prevent movement between two adjacent squares. This work follows in a long line of algorithms and complexity work on similar problems. The 2D push-pull block puzzle shows up in the video games Pukoban as well as The Legend of Zelda: A Link to the Past, giving another proof of hardness for the latter. This variant of block-pushing puzzles is of particular interest because of its connections to reversibility, since any action (e.g., push or pull) can be inverted by another valid action (e.g., pull or push).\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We analyze a directed variation of the book embedding problem when the page partition is prespecified and the nodes on the spine must be in topological order (upward book embedding). Given a directed acyclic graph and a partition of its edges into $k$ pages, can we linearly order the vertices such that the drawing is upward (a topological sort) and each page avoids crossings? We prove that the problem is NP-complete for $k\\ge 3$, and for $k\\ge 4$ even in the special case when each page is a matching. By contrast, the problem can be solved in linear time for $k=2$ pages when pages are restricted to matchings. The problem comes from Jack Edmonds (1997), motivated as a generalization of the map folding problem from computational origami.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Pebble games are single-player games on DAGs involving placing and moving pebbles on nodes of the graph according to a certain set of rules. The goal is to pebble a set of target nodes using a minimum number of pebbles. In this paper, we present a possibly simpler proof of the result in [CLNV15] and strengthen the result to show that it is PSPACE-hard to determine the minimum number of pebbles to an additive $n^{1/3-\u03b5}$ term for all $\u03b5> 0$, which improves upon the currently known additive constant hardness of approximation [CLNV15] in the standard pebble game. We also introduce a family of explicit, constant indegree graphs with $n$ nodes where there exists a graph in the family such that using constant $k$ pebbles requires $\u03a9(n^k)$ moves to pebble in both the standard and black-white pebble games. This independently answers an open question summarized in [Nor15] of whether a family of DAGs exists that meets the upper bound of $O(n^k)$ moves using constant $k$ pebbles with a different construction than that presented in [AdRNV17].\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "The 15 puzzle is a classic reconfiguration puzzle with fifteen uniquely labeled unit squares within a $4 \\times 4$ board in which the goal is to slide the squares (without ever overlapping) into a target configuration. By generalizing the puzzle to an $n \\times n$ board with $n^2-1$ squares, we can study the computational complexity of problems related to the puzzle; in particular, we consider the problem of determining whether a given end configuration can be reached from a given start configuration via at most a given number of moves. This problem was shown NP-complete in Ratner and Warmuth (1990). We provide an alternative simpler proof of this fact by reduction from the rectilinear Steiner tree problem.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "In 2007, Arkin et al. initiated a systematic study of the complexity of the Hamiltonian cycle problem on square, triangular, or hexagonal grid graphs, restricted to polygonal, thin, superthin, degree-bounded, or solid grid graphs. They solved many combinations of these problems, proving them either polynomially solvable or NP-complete, but left three combinations open. In this paper, we prove two of these unsolved combinations to be NP-complete: Hamiltonicity of Square Polygonal Grid Graphs and Hamiltonicity of Hexagonal Thin Grid Graphs. We also consider a new restriction, where the grid graph is both thin and polygonal, and prove that Hamiltonicity then becomes polynomially solvable for square, triangular, and hexagonal grid graphs.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "In this paper, we introduce a new problem called Tree-Residue Vertex-Breaking (TRVB): given a multigraph $G$ some of whose vertices are marked \"breakable,\" is it possible to convert $G$ into a tree via a sequence of \"vertex-breaking\" operations (replacing a degree-$k$ breakable vertex by $k$ degree-$1$ vertices, disconnecting the $k$ incident edges)?\n  We characterize the computational complexity of TRVB with any combination of the following additional constraints: $G$ must be planar, $G$ must be a simple graph, the degree of every breakable vertex must belong to an allowed list $B$, and the degree of every unbreakable vertex must belong to an allowed list $U$. The two results which we expect to be most generally applicable are that (1) TRVB is polynomially solvable when breakable vertices are restricted to have degree at most $3$; and (2) for any $k \\ge 4$, TRVB is NP-complete when the given multigraph is restricted to be planar and to consist entirely of degree-$k$ breakable vertices. To demonstrate the use of TRVB, we give a simple proof of the known result that Hamiltonicity in max-degree-$3$ square grid graphs is NP-hard.\n  We also demonstrate a connection between TRVB and the Hypergraph Spanning Tree problem. This connection allows us to show that the Hypergraph Spanning Tree problem in $k$-uniform $2$-regular hypergraphs is NP-complete for any $k \\ge 4$, even when the incidence graph of the hypergraph is planar.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "In this paper, we prove that optimally solving an $n \\times n \\times n$ Rubik's Cube is NP-complete by reducing from the Hamiltonian Cycle problem in square grid graphs. This improves the previous result that optimally solving an $n \\times n \\times n$ Rubik's Cube with missing stickers is NP-complete. We prove this result first for the simpler case of the Rubik's Square---an $n \\times n \\times 1$ generalization of the Rubik's Cube---and then proceed with a similar but more complicated proof for the Rubik's Cube case.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "This paper addresses the problem of finding minimum forcing sets in origami. The origami material folds flat along straight lines called creases that can be labeled as mountains or valleys. A forcing set is a subset of creases that force all the other creases to fold according to their labels. The result is a flat folding of the origami material. In this paper we develop a linear time algorithm that finds minimum forcing sets in one dimensional origami.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We study the complexity of symmetric assembly puzzles: given a collection of simple polygons, can we translate, rotate, and possibly flip them so that their interior-disjoint union is line symmetric? On the negative side, we show that the problem is strongly NP-complete even if the pieces are all polyominos. On the positive side, we show that the problem can be solved in polynomial time if the number of pieces is a fixed constant.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "A conflict-free k-coloring of a graph assigns one of k different colors to some of the vertices such that, for every vertex v, there is a color that is assigned to exactly one vertex among v and v's neighbors. Such colorings have applications in wireless networking, robotics, and geometry, and are well-studied in graph theory. Here we study the natural problem of the conflict-free chromatic number chi_CF(G) (the smallest k for which conflict-free k-colorings exist). We provide results both for closed neighborhoods N[v], for which a vertex v is a member of its neighborhood, and for open neighborhoods N(v), for which vertex v is not a member of its neighborhood.\n  For closed neighborhoods, we prove the conflict-free variant of the famous Hadwiger Conjecture: If an arbitrary graph G does not contain K_{k+1} as a minor, then chi_CF(G) <= k. For planar graphs, we obtain a tight worst-case bound: three colors are sometimes necessary and always sufficient. We also give a complete characterization of the computational complexity of conflict-free coloring. Deciding whether chi_CF(G)<= 1 is NP-complete for planar graphs G, but polynomial for outerplanar graphs. Furthermore, deciding whether chi_CF(G)<= 2 is NP-complete for planar graphs G, but always true for outerplanar graphs. For the bicriteria problem of minimizing the number of colored vertices subject to a given bound k on the number of colors, we give a full algorithmic characterization in terms of complexity and approximation for outerplanar and planar graphs.\n  For open neighborhoods, we show that every planar bipartite graph has a conflict-free coloring with at most four colors; on the other hand, we prove that for k in {1,2,3}, it is NP-complete to decide whether a planar bipartite graph has a conflict-free k-coloring. Moreover, we establish that any general} planar graph has a conflict-free coloring with at most eight colors.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We prove the computational intractability of rotating and placing $n$ square tiles into a $1 \\times n$ array such that adjacent tiles are compatible--either equal edge colors, as in edge-matching puzzles, or matching tab/pocket shapes, as in jigsaw puzzles. Beyond basic NP-hardness, we prove that it is NP-hard even to approximately maximize the number of placed tiles (allowing blanks), while satisfying the compatibility constraint between nonblank tiles, within a factor of 0.9999999851. (On the other hand, there is an easy $1 \\over 2$-approximation.) This is the first (correct) proof of inapproximability for edge-matching and jigsaw puzzles. Along the way, we prove NP-hardness of distinguishing, for a directed graph on $n$ nodes, between having a Hamiltonian path (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a vertex-disjoint union of paths. We use this gap hardness and gap-preserving reductions to establish similar gap hardness for $1 \\times n$ jigsaw and edge-matching puzzles.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We classify the computational complexity of the popular video games Portal and Portal 2. We isolate individual mechanics of the game and prove NP-hardness, PSPACE-completeness, or (pseudo)polynomiality depending on the specific game mechanics allowed. One of our proofs generalizes to prove NP-hardness of many other video games such as Half-Life 2, Halo, Doom, Elder Scrolls, Fallout, Grand Theft Auto, Left 4 Dead, Mass Effect, Deus Ex, Metal Gear Solid, and Resident Evil.\n  These results build on the established literature on the complexity of video games.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We present two universal hinge patterns that enable a strip of material to fold into any connected surface made up of unit squares on the 3D cube grid--for example, the surface of any polycube. The folding is efficient: for target surfaces topologically equivalent to a sphere, the strip needs to have only twice the target surface area, and the folding stacks at most two layers of material anywhere. These geometric results offer a new way to build programmable matter that is substantially more efficient than what is possible with a square $N \\times N$ sheet of material, which can fold into all polycubes only of surface area $O(N)$ and may stack $\u0398(N^2)$ layers at one point. We also show how our strip foldings can be executed by a rigid motion without collisions, which is not possible in general with 2D sheet folding.\n  To achieve these results, we develop new approximation algorithms for milling the surface of a grid polyhedron, which simultaneously give a 2-approximation in tour length and an 8/3-approximation in the number of turns. Both length and turns consume area when folding a strip, so we build on past approximation algorithms for these two objectives from 2D milling.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We show that every orthogonal polyhedron of genus at most 2 can be unfolded without overlap while using only a linear number of orthogonal cuts (parallel to the polyhedron edges). This is the first result on unfolding general orthogonal polyhedra beyond genus-0. Our unfolding algorithm relies on the existence of at most 2 special leaves in what we call the \"unfolding tree\" (which ties back to the genus), so unfolding polyhedra of genus 3 and beyond requires new techniques.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We show how to design a universal shape replicator in a self-assembly system with both attractive and repulsive forces. More precisely, we show that there is a universal set of constant-size objects that, when added to any unknown hole-free polyomino shape, produces an unbounded number of copies of that shape (plus constant-size garbage objects). The constant-size objects can be easily constructed from a constant number of individual tile types using a constant number of preprocessing self-assembly steps. Our construction uses the well-studied 2-Handed Assembly Model (2HAM) of tile self-assembly, in the simple model where glues interact only with identical glues, allowing glue strengths that are either positive (attractive) or negative (repulsive), and constant temperature (required glue strength for parts to hold together). We also require that the given shape has specified glue types on its surface, and that the feature size (smallest distance between nonincident edges) is bounded below by a constant. Shape replication necessarily requires a self-assembly model where parts can both attach and detach, and this construction is the first to do so using the natural model of negative/repulsive glues (also studied before for other problems such as fuel-efficient computation); previous replication constructions require more powerful global operations such as an \"enzyme\" that destroys a subset of the tile types.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "This paper proves that arrangement of music is NP-hard when subject to various constraints: avoiding musical dissonance, limiting how many notes can be played simultaneously, and limiting transition speed between chords. These results imply the computational complexity of related musical problems, including musical choreography and rhythm games.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We study the computational complexity of the Buttons \\& Scissors game and obtain sharp thresholds with respect to several parameters. Specifically we show that the game is NP-complete for $C = 2$ colors but polytime solvable for $C = 1$. Similarly the game is NP-complete if every color is used by at most $F = 4$ buttons but polytime solvable for $F \\leq 3$. We also consider restrictions on the board size, cut directions, and cut sizes. Finally, we introduce several natural two-player versions of the game and show that they are PSPACE-complete.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We introduce a new programming language for expressing reversibility, Energy-Efficient Language (Eel), geared toward algorithm design and implementation. Eel is the first language to take advantage of a partially reversible computation model, where programs can be composed of both reversible and irreversible operations. In this model, irreversible operations cost energy for every bit of information created or destroyed. To handle programs of varying degrees of reversibility, Eel supports a log stack to automatically trade energy costs for space costs, and introduces many powerful control logic operators including protected conditional, general conditional, protected loops, and general loops. In this paper, we present the design and compiler for the three language levels of Eel along with an interpreter to simulate and annotate incurred energy costs of a program.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We initiate the systematic study of the energy complexity of algorithms (in addition to time and space complexity) based on Landauer's Principle in physics, which gives a lower bound on the amount of energy a system must dissipate if it destroys information. We propose energy-aware variations of three standard models of computation: circuit RAM, word RAM, and transdichotomous RAM. On top of these models, we build familiar high-level primitives such as control logic, memory allocation, and garbage collection with zero energy complexity and only constant-factor overheads in space and time complexity, enabling simple expression of energy-efficient algorithms. We analyze several classic algorithms in our models and develop low-energy variations: comparison sort, insertion sort, counting sort, breadth-first search, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL trees, binary heaps, and dynamic arrays. We explore the time/space/energy trade-off and develop several general techniques for analyzing algorithms and reducing their energy complexity. These results lay a theoretical foundation for a new field of semi-reversible computing and provide a new framework for the investigation of algorithms.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Modeling folding surfaces with nonzero thickness is of practical interest for mechanical engineering. There are many existing approaches that account for material thickness in folding applications. We propose a new systematic and broadly applicable algorithm to transform certain flat-foldable crease patterns into new crease patterns with similar folded structure but with a facet-separated folded state. We provide conditions on input crease patterns for the algorithm to produce a thickened crease pattern avoiding local self intersection, and provide bounds for the maximum thickness that the algorithm can produce for a given input. We demonstrate these results in parameterized numerical simulations and physical models.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Inspired by the Japanese game Pachinko, we study simple (perfectly \"inelastic\" collisions) dynamics of a unit ball falling amidst point obstacles (pins) in the plane. A classic example is that a checkerboard grid of pins produces the binomial distribution, but what probability distributions result from different pin placements? In the 50-50 model, where the pins form a subset of this grid, not all probability distributions are possible, but surprisingly the uniform distribution is possible for $\\{1,2,4,8,16\\}$ possible drop locations. Furthermore, every probability distribution can be approximated arbitrarily closely, and every dyadic probability distribution can be divided by a suitable power of $2$ and then constructed exactly (along with extra \"junk\" outputs). In a more general model, if a ball hits a pin off center, it falls left or right accordingly. Then we prove a universality result: any distribution of $n$ dyadic probabilities, each specified by $k$ bits, can be constructed using $O(n k^2)$ pins, which is close to the information-theoretic lower bound of $\u03a9(n k)$.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We prove that it is NP-hard to dissect one simple orthogonal polygon into another using a given number of pieces, as is approximating the fewest pieces to within a factor of $1+1/1080-\\varepsilon$.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We develop an intrinsic necessary and sufficient condition for single-vertex origami crease patterns to be able to fold rigidly. We classify such patterns in the case where the creases are pre-assigned to be mountains and valleys as well as in the unassigned case. We also illustrate the utility of this result by applying it to the new concept of minimal forcing sets for rigid origami models, which are the smallest collection of creases that, when folded, will force all the other creases to fold in a prescribed way.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We prove that the classic 1994 Taito video game, known as Puzzle Bobble or Bust-a-Move, is NP-complete. Our proof applies to the perfect-information version where the bubble sequence is known in advance, and it uses just three bubble colors.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We consider staged self-assembly systems, in which square-shaped tiles can be added to bins in several stages. Within these bins, the tiles may connect to each other, depending on the glue types of their edges. Previous work by Demaine et al. showed that a relatively small number of tile types suffices to produce arbitrary shapes in this model. However, these constructions were only based on a spanning tree of the geometric shape, so they did not produce full connectivity of the underlying grid graph in the case of shapes with holes; designing fully connected assemblies with a polylogarithmic number of stages was left as a major open problem. We resolve this challenge by presenting new systems for staged assembly that produce fully connected polyominoes in O(log^2 n) stages, for various scale factors and temperature \u03c4 = 2 as well as \u03c4 = 1. Our constructions work even for shapes with holes and uses only a constant number of glues and tiles. Moreover, the underlying approach is more geometric in nature, implying that it promised to be more feasible for shapes with compact geometric description.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "We describe a general family of curved-crease folding tessellations consisting of a repeating \"lens\" motif formed by two convex curved arcs. The third author invented the first such design in 1992, when he made both a sketch of the crease pattern and a vinyl model (pictured below). Curve fitting suggests that this initial design used circular arcs. We show that in fact the curve can be chosen to be any smooth convex curve without inflection point. We identify the ruling configuration through qualitative properties that a curved folding satisfies, and prove that the folded form exists with no additional creases, through the use of differential geometry.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "In this paper, we study how to fold a specified origami crease pattern in order to minimize the impact of paper thickness. Specifically, origami designs are often expressed by a mountain-valley pattern (plane graph of creases with relative fold orientations), but in general this specification is consistent with exponentially many possible folded states. We analyze the complexity of finding the best consistent folded state according to two metrics: minimizing the total number of layers in the folded state (so that a \"flat folding\" is indeed close to flat), and minimizing the total amount of paper required to execute the folding (where \"thicker\" creases consume more paper). We prove both problems strongly NP-complete even for 1D folding. On the other hand, we prove the first problem fixed-parameter tractable in 1D with respect to the number of layers.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Given a sheet of paper and a prescribed folding of its boundary, is there a way to fold the paper's interior without stretching so that the boundary lines up with the prescribed boundary folding? For polygonal boundaries nonexpansively folded at finitely many points, we prove that a consistent isometric mapping of the polygon interior always exists and is computable in polynomial time.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "When can $t$ terminal pairs in an $m \\times n$ grid be connected by $t$ vertex-disjoint paths that cover all vertices of the grid? We prove that this problem is NP-complete. Our hardness result can be compared to two previous NP-hardness proofs: Lynch's 1975 proof without the ``cover all vertices'' constraint, and Kotsuma and Takenaga's 2010 proof when the paths are restricted to have the fewest possible corners within their homotopy class. The latter restriction is a common form of the famous Nikoli puzzle \\emph{Numberlink}; our problem is another common form of Numberlink, sometimes called \\emph{Zig-Zag Numberlink} and popularized by the smartphone app \\emph{Flow Free}.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "When can a plane graph with prescribed edge lengths and prescribed angles (from among $\\{0,180^\\circ, 360^\\circ$\\}) be folded flat to lie in an infinitesimally thin line, without crossings? This problem generalizes the classic theory of single-vertex flat origami with prescribed mountain-valley assignment, which corresponds to the case of a cycle graph. We characterize such flat-foldable plane graphs by two obviously necessary but also sufficient conditions, proving a conjecture made in 2001: the angles at each vertex should sum to $360^\\circ$, and every face of the graph must itself be flat foldable. This characterization leads to a linear-time algorithm for testing flat foldability of plane graphs with prescribed edge lengths and angles, and a polynomial-time algorithm for counting the number of distinct folded states.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Suppose that we are given two independent sets $I_b$ and $I_r$ of a graph such that $|I_b|=|I_r|$, and imagine that a token is placed on each vertex in $I_b$. Then, the sliding token problem is to determine whether there exists a sequence of independent sets which transforms $I_b$ into $I_r$ so that each independent set in the sequence results from the previous one by sliding exactly one token along an edge in the graph. This problem is known to be PSPACE-complete even for planar graphs, and also for bounded treewidth graphs. In this paper, we thus study the problem restricted to trees, and give the following three results: (1) the decision problem is solvable in linear time; (2) for a yes-instance, we can find in quadratic time an actual sequence of independent sets between $I_b$ and $I_r$ whose length (i.e., the number of token-slides) is quadratic; and (3) there exists an infinite family of instances on paths for which any sequence requires quadratic length.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "This research establishes that many real-world networks exhibit bounded expansion, a strong notion of structural sparsity, and demonstrates that it can be leveraged to design efficient algorithms for network analysis. We analyze several common network models regarding their structural sparsity. We show that, with high probability, (1) graphs sampled with a prescribed s parse degree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block models with small probabilities; result in graphs of bounded expansion.\n  In contrast, we show that the Kleinberg and the Barabasi-Albert model have unbounded expansion. We support our findings with empirical measurements on a corpus of real-world networks.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "In this paper, we study the problem of fast dynamic pointer following: given a directed graph $G$ where each vertex has outdegree $1$, efficiently support the operations of i) changing the outgoing edge of any vertex, and ii) find the vertex $k$ vertices `after' a given vertex. We exhibit a solution to this problem based on link-cut trees that requires $O(\\lg n)$ time per operation, and prove that this is optimal in the cell-probe complexity model.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Can folding a piece of paper flat make it larger? We explore whether a shape $S$ must be scaled to cover a flat-folded copy of itself. We consider both single folds and arbitrary folds (continuous piecewise isometries $S\\rightarrow R^2$). The underlying problem is motivated by computational origami, and is related to other covering and fixturing problems, such as Lebesgue's universal cover problem and force closure grasps. In addition to considering special shapes (squares, equilateral triangles, polygons and disks), we give upper and lower bounds on scale factors for single folds of convex objects and arbitrary folds of simply connected objects.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "Over the past decade, we have designed six typefaces based on mathematical theorems and open problems, specifically computational geometry. These typefaces expose the general public in a unique way to intriguing results and hard problems in hinged dissections, geometric tours, origami design, computer-aided glass design, physical simulation, and protein folding. In particular, most of these typefaces include puzzle fonts, where reading the intended message requires solving a series of puzzles which illustrate the challenge of the underlying algorithmic problem.\n        \u25b3 Less", "author": "Erik Demaine"}, {"abstract": "The Super Cryogenic Dark Matter Search experiment (SuperCDMS) at the Soudan Underground Laboratory studied energy loss associated with Frenkel defect formation in germanium crystals at mK temperatures using in situ $^{210}$Pb sources. We examine the spectrum of $^{206}$Pb nuclear recoils near its expected 103 keV endpoint energy and determine an energy loss of $\\left(6.08\\pm0.18\\right)$ %, which we attribute to defect formation. From this result and using TRIM simulations, we extract the first experimentally determined average displacement threshold energy of $\\left(19.7^{+0.6}_{-0.5}\\right)$ eV for germanium. This has implications for the analysis thresholds of future germanium-based dark matter searches.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Solar flare emission at X-ray and extreme ultraviolet (EUV) energies can cause substantial enhancements in the electron density in the Earth's lower ionosphere. It is now become clear that flares exhibit quasi-periodic pulsations with timescales of minutes at X-ray energies, but to date, it has not been known if the ionosphere is sensitive to this variability. Here, using a combination of Very Low Frequency (24 kHz) measurement together with space-based X-ray and EUV observations, we report pulsations of the ionospheric D-region, which are synchronized with a set of pulsating flare loops. Modeling of the ionosphere show that the D-region electron density varies by up to an order of magnitude over the timescale of the pulsations ($\\sim$20 mins). Our results reveal that the Earth's ionosphere is more sensitive to small-scale changes in solar soft X-ray flux than previously thought, and implies that planetary ionospheres are closely coupled to small-scale changes in solar/stellar activity.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "In this Letter we report the detection of chromospheric 3-minute oscillations in disk-integrated EUV irradiance observations during a solar flare. A wavelet analysis of detrended Lyman-alpha (from GOES/EUVS) and Lyman continuum (from SDO/EVE) emission from the 2011 February 15 X-class flare (SOL2011-02-15T01:56) revealed a $\\sim$3-minute period present during the flare's main phase. The formation temperature of this emission locates this radiation to the flare's chromospheric footpoints, and similar behaviour is found in the SDO/AIA 1600\u00c5 and 1700\u00c5 channels, which are dominated by chromospheric continuum. The implication is that the chromosphere responds dynamically at its acoustic cutoff frequency to an impulsive injection of energy. Since the 3-minute period was not found at hard X-ray energies (50-100 keV) in RHESSI data we can state that this 3-minute oscillation does not depend on the rate of energization of non-thermal electrons. However, a second period of 120 s found in both hard X-ray and chromospheric emission is consistent with episodic electron energization on 2-minute timescales. Our finding on the 3-minute oscillation suggests that chromospheric mechanical energy should be included in the flare energy budget, and the fluctuations in the Lyman-alpha line may influence the composition and dynamics of planetary atmospheres during periods of high activity.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Quasi-periodic pulsations (QPP) seen in the time derivative of the GOES soft X-ray light curves are analyzed for the near-limb X3.2 event on 14 May 2013. The pulsations are apparent for a total of at least two hours from the impulsive phase to well into the decay phase, with a total of 163 distinct pulses evident to the naked eye. A wavelet analysis shows that the characteristic time scale of these pulsations increases systematically from $\\sim$25 s at 01:10 UT, the time of the GOES peak, to $\\sim$100 s at 02:00 UT. A second ridge in the wavelet power spectrum, most likely associated with flaring emission from a different active region, shows an increase from $\\sim$40 s at 01:40 UT to $\\sim$100 s at 03:10 UT. We assume that the QPP that produced the first ridge result from vertical kink-mode oscillations of the newly formed loops following magnetic reconnection in the coronal current sheet. This allows us to estimate the magnetic field strength as a function of altitude given the density, loop length, and QPP time scale as functions of time determined from the GOES light curves and RHESSI images. The calculated magnetic field strength of the newly formed loops ranges from about $\\sim$500 G at an altitude of 24 Mm to a low value of $\\sim$10 G at 60 Mm, in general agreement with the expected values at these altitudes. Fast sausage mode oscillations are also discussed and cannot be ruled out as an alternate mechanism for producing the QPP.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We present the solar S-index record of cycle 24, obtained by the TIGRE robotic telescope facility and its high-resolution spectrograph HEROS (R$\\approx$20,000), which measures the solar chromospheric Ca II H&K line emission by using moonlight. Our calibration process uses the same set of standard stars as introduced by the Mt. Wilson team, thus giving us a direct comparison with their huge body of observations taken between 1966 and 1992, as well as with other cool stars. Carrington cycle 24 activity started from the unusually deep and long minimum 2008/09, with an S-index average of only 0.154, by 0.015 deeper than the one of 1986 (<S>=0.169). In this respect,the chromospheric radiative losses differ remarkably from the variation of the coronal radio flux F10.7cm and the sunspot numbers. In addition, the cycle 24 S-amplitude remained small, 0.022 (cycles 21 and 22 averaged: 0.024), and so resulted a very low 2014 maximum of <S>=0.176 (cycles 21 and 22 averaged: 0.193). We argue that this find is significant, since the Ca II H&K line emission is a good proxy for the solar far-UV flux, which plays an important role in the heating of the Earth's stratosphere, and we further argue that the solar far-UV flux changes change s with solar activity much more strongly than the total solar output.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We present results of our dense spectroscopic monitoring of Nova V5668 Sgr. Starting on March 19 in 2015, only a few days after discovery, we have obtained a series of spectra with the TIGRE telescope and its HEROS echelle spectrograph which offers a resolution of R = 20,000 and covers the optical wavelength range from 3800 to 8800 \u00c5. We performed a line identification of the discernible features for four spectra which are representative for the respective phases in the light curve evolution of that nova. By simultaneously analysing the variations in the visual light curve and the corresponding spectra of Nova V5668 Sgr, we found that during the declining phases of the nova the absorption features in all hydrogen and many other lines had shifted to higher expansion velocities of -2000 km s^-1. Conversely, during the rise towards the following maximum, these observed absorption features had returned to lower expansion velocities.We found that the absorption features of some Fe II lines displayed the same behaviour, but in addition disappeared for a few days during some declining phases. Features of several N I lines also disappeared while new N II lines appeared in emission for a few days during some of the declining phases of the light curve of Nova V5668 Sgr. The shape of the emission features is changing during the evolution and shows a clear double peak structure after the deep minimum.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "How impulsive magnetic energy release leads to solar eruptions and how those eruptions are energized and evolve are vital unsolved problems in Heliophysics. The standard model for solar eruptions summarizes our current understanding of these events. Magnetic energy in the corona is released through drastic restructuring of the magnetic field via reconnection. Electrons and ions are then accelerated by poorly understood processes. Theories include contracting loops, merging magnetic islands, stochastic acceleration, and turbulence at shocks, among others. Although this basic model is well established, the fundamental physics is poorly understood. HXR observations using grazing-incidence focusing optics can now probe all of the key regions of the standard model. These include two above-the-looptop (ALT) sources which bookend the reconnection region and are likely the sites of particle acceleration and direct heating. The science achievable by a direct HXR imaging instrument can be summarized by the following science questions and objectives which are some of the most outstanding issues in solar physics (1) How are particles accelerated at the Sun? (1a) Where are electrons accelerated and on what time scales? (1b) What fraction of electrons is accelerated out of the ambient medium? (2) How does magnetic energy release on the Sun lead to flares and eruptions? A Focusing Optics X-ray Solar Imager (FOXSI) instrument, which can be built now using proven technology and at modest cost, would enable revolutionary advancements in our understanding of impulsive magnetic energy release and particle acceleration, a process which is known to occur at the Sun but also throughout the Universe.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "One of the most debated subjects in Astronomy since the discovery of exoplanets is how can we distinguish the most massive of such objects from very-low mass stars like Brown Dwarfs (BDs)? We have been looking for evidences of a difference in physical characteristics that could be related to different formation processes. Using a new diagnostic diagram that compares the baryonic gravitational potential (BGP) with the distances from their host stars, we have classified a sample of 355 well-studied exoplanets according to their possible structures. We have then compared the exoplanets to a sample of 87 confirmed BDs, identifying a range in BGP that could be common to both objects. By analyzing the mass-radius relations (MRR) of the exoplanets and BDs in those different BGP ranges, we were able to distinguish different characteristic behaviors. By comparing with models in the literature, our results suggest that BDs and massive exoplanets might have similar structures dominated by liquid metallic hydrogen (LMH).\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Quasi-periodic pulsations (QPP) are often observed in X-ray emission from solar flares. To date, it is unclear what their physical origins are. Here, we present a multi-instrument investigation of the nature of QPP during the impulsive and decay phases of the X1.0 flare of 28 October 2013. We focus on the character of the fine structure pulsations evident in the soft X-ray time derivatives and compare this variability with structure across multiple wavelengths including hard X-ray and microwave emission. We find that during the impulsive phase of the flare, high correlations between pulsations in the thermal and non-thermal emissions are seen. A characteristic timescale of ~20s is observed in all channels and a second timescale of ~55s is observed in the non-thermal emissions. Soft X-ray pulsations are seen to persist into the decay phase of this flare, up to 20 minutes after the non-thermal emission has ceased. We find that these decay phase thermal pulsations have very small amplitude and show an increase in characteristic timescale from ~40s up to ~70s. We interpret the bursty nature of the co-existing multi-wavelength QPP during the impulsive phase in terms of episodic particle acceleration and plasma heating. The persistent thermal decay phase QPP are most likely connected with compressive MHD processes in the post-flare loops such as the fast sausage mode or the vertical kink mode.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We present a time series of high resolution spectra of the Type Ia supernova 2014J, which exploded in the nearby galaxy M82. The spectra were obtained with the HEROS echelle spectrograph installed at the 1.2 m TIGRE telescope. We present a series of 33 spectra with a resolution of R = 20, 000, which covers the important bright phases in the evolution of SN 2014J during the period from January 24 to April 1 of 2014. The spectral evolution of SN 2014J is derived empirically. The expansion velocities of the Si II P-Cygni features were measured and show the expected decreasing behaviour, beginning with a high velocity of 14,000 km/s on January 24. The Ca II infrared triplet feature shows a high velocity component with expansion velocities of > 20, 000 km/s during the early evolution apart from the normal component showing similar velocities as Si II. Further broad P-Cygni profiles are exhibited by the principal lines of Ca II, Mg II and Fe II. The TIGRE SN 2014J spectra also resolve several very sharp Na I D doublet absorption components. Our analysis suggests interesting substructures in the interstellar medium of the host galaxy M82, as well as in our Milky Way, confirming other work on this SN. We were able to identify the interstellar absorption of M82 in the lines of Ca II H & K at 3933 and 3968 A as well as K I at 7664 and 7698 A. Furthermore, we confirm several Diffuse Interstellar Bands, at wavelengths of 6196, 6283, 6376, 6379 and 6613 A and give their measured equivalent widths.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We obtained a time series of spectra covering the secondary maximum in the I-band of the bright Type Ia supernova 2014J in M82 with the TIGRE telescope. Comparing the observations with theoretical models calculated with the time dependent extension of the PHOENIX code, we identify the feature that causes the secondary maximum in the I-band light curve. Fe II 3d6(3D)4s-3d6(5D)4p and similar high excitation transitions produce a blended feature at 7500 \u00c5, which causes the rise of the light curve towards the secondary maximum. The series of observed spectra of SN 2014J and archival data of SN 2011fe confirm this conclusion. We further studied the plateau phase of the Rband light curve of SN 2014J and searched for features which contribute to the flux. The theoretical models do not clearly indicate a new feature that may cause the Rband plateau phase. However, Co II features in the range of 6500 - 7000 \u00c5 and the Fe II feature of the I-band are clearly seen in the theoretical spectra, but do not appear to provide all of the flux necessary for the R-band plateau.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "This paper presents measurements of the energy radiated by the lower solar atmosphere, at optical, UV, and EUV wavelengths, during an X-class solar flare (SOL2011-02-15T01:56) in response to an injection of energy assumed to be in the form of nonthermal electrons. Hard X-ray observations from RHESSI were used to track the evolution of the parameters of the nonthermal electron distribution to reveal the total power contained in flare accelerated electrons. By integrating over the duration of the impulsive phase, the total energy contained in the nonthermal electrons was found to be $>2\\times10^{31}$ erg. The response of the lower solar atmosphere was measured in the free-bound EUV continua of H I (Lyman), He I, and He II, plus the emission lines of He II at 304\u00c5 and H I (Ly$\u03b1$) at 1216\u00c5 by SDO/EVE, the UV continua at 1600\u00c5 and 1700\u00c5 by SDO/AIA, and the WL continuum at 4504\u00c5, 5550\u00c5, and 6684\u00c5, along with the Ca II H line at 3968\u00c5 using Hinode/SOT. The summed energy detected by these instruments amounted to $\\sim3\\times10^{30}$ erg; about 15% of the total nonthermal energy. The Ly$\u03b1$ line was found to dominate the measured radiative losses. Parameters of both the driving electron distribution and the resulting chromospheric response are presented in detail to encourage the numerical modelling of flare heating for this event, to determine the depth of the solar atmosphere at which these line and continuum processes originate, and the mechanism(s) responsible for their generation.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Since the discovery of the first exoplanet we have known that other planetary systems can look quite unlike our own. However, until recently we have only been able to probe the upper range of the planet size distribution. The high precision of the Kepler space telescope has allowed us to detect planets that are the size of Earth and somewhat smaller, but no previous planets have been found that are smaller than those we see in our own Solar System. Here we report the discovery of a planet significantly smaller than Mercury. This tiny planet is the innermost of three planets that orbit the Sun-like host star, which we have designated Kepler-37. Owing to its extremely small size, similar to that of Earth's Moon, and highly irradiated surface, Kepler-37b is probably a rocky planet with no atmosphere or water, similar to Mercury.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We compare four different methods of calculating uncertainty estimates in fitting parameterized models to RHESSI X-ray spectra, considering only statistical sources of error. Three of the four methods are based on estimating the scale-size of the minimum in a hypersurface formed by the weighted sum of the squares of the differences between the model fit and the data as a function of the fit parameters, and are implemented as commonly practiced. The fourth method uses Bayesian data analysis and Markov chain Monte Carlo (MCMC) techniques to calculate an uncertainty estimate. Two flare spectra are modeled: one from the GOES X1.3 class flare of 19 January 2005, and the other from the X4.8 flare of 23 July 2002. The four methods give approximately the same uncertainty estimates for the 19 January 2005 spectral fit parameters, but lead to very different uncertainty estimates for the 23 July 2002 spectral fit. This is because each method implements different analyses of the hypersurface, yielding method-dependent results that differ greatly depending on the shape of the hypersurface. For the 23 July 2002 flare data, there is a 95% probability that the low energy cutoff lies below approximately 40 keV, and a 68% probability that it lies in the range 7-36 keV. The low-energy cutoff for the 19 January 2005 flare is more tightly constrained to 107+/-4 keV (68% probability). Using the Bayesian/MCMC approach, we also estimate for the first time probability density functions (PDFs) for the total number of flare accelerated electrons and the energy they carry. For the 23 July 2002 event, these PDFs are asymmetric with long tails orders of magnitude higher than the most probable value, caused by the poorly constrained value of the low-energy cutoff. For the 19 January 2005 flare spectrum, the PDFs for the total number of flare accelerated electrons and their energy are much more symmetric and narrow.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "There is compelling evidence that the peak brightness of a Type Ia supernova is affected by the electron fraction Ye at the time of the explosion. The electron fraction is set by the aboriginal composition of the white dwarf and the reactions that occur during the pre explosive convective burning. To date, determining the makeup of the white dwarf progenitor has relied on indirect proxies, such as the average metallicity of the host stellar population. In this paper, we present analytical calculations supporting the idea that the electron fraction of the progenitor systematically influences the nucleosynthesis of silicon group ejecta in Type Ia supernovae. In particular, we suggest the abundances generated in quasi nuclear statistical equilibrium are preserved during the subsequent freezeout. This allows one to potential recovery of Ye at explosion from the abundances recovered from an observed spectra. We show that measurement of 28Si, 32S, 40Ca, and 54Fe abundances can be used to construct Ye in the silicon rich regions of the supernovae. If these four abundances are determined exactly, they are sufficient to recover Ye to 6 percent. This is because these isotopes dominate the composition of silicon-rich material and iron rich material in quasi nuclear statistical equilibrium. Analytical analysis shows that the 28Si abundance is insensitive to Ye, the 32S abundance has a nearly linear trend with Ye, and the 40Ca abundance has a nearly quadratic trend with Ye. We verify these trends with post-processing of 1D models and show that these trends are reflected in model synthetic spectra.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We have used asteroseismology to determine fundamental properties for 66 Kepler planet-candidate host stars, with typical uncertainties of 3% and 7% in radius and mass, respectively. The results include new asteroseismic solutions for four host stars with confirmed planets (Kepler-4, Kepler-14, Kepler-23 and Kepler-25) and increase the total number of Kepler host stars with asteroseismic solutions to 77. A comparison with stellar properties in the planet-candidate catalog by Batalha et al. shows that radii for subgiants and giants obtained from spectroscopic follow-up are systematically too low by up to a factor of 1.5, while the properties for unevolved stars are in good agreement. We furthermore apply asteroseismology to confirm that a large majority of cool main-sequence hosts are indeed dwarfs and not misclassified giants. Using the revised stellar properties, we recalculate the radii for 107 planet candidates in our sample, and comment on candidates for which the radii change from a previously giant-planet/brown-dwarf/stellar regime to a sub-Jupiter size, or vice versa. A comparison of stellar densities from asteroseismology with densities derived from transit models in Batalha et al. assuming circular orbits shows significant disagreement for more than half of the sample due to systematics in the modeled impact parameters, or due to planet candidates which may be in eccentric orbits. Finally, we investigate tentative correlations between host-star masses and planet candidate radii, orbital periods, and multiplicity, but caution that these results may be influenced by the small sample size and detection biases.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Context. Time-dependent, 3D radiation transfer calculations are important for the modeling of a variety of objects, from supernovae and novae to simulations of stellar variability and activity. Furthermore, time-dependent calculations can be used to obtain a 3D radiative equilibrium model structure via relaxation in time. Aims. We extend our 3D radiative transfer framework to include direct time dependence of the radiation field; i.e., the $\\partial I/ \\partial t$ terms are fully considered in the solution of radiative transfer problems. Methods. We build on the framework that we have described in previous papers in this series and develop a subvoxel method for the $\\partial I/\\partial t$ terms. Results. We test the implementation by comparing the 3D results to our well tested 1D time dependent radiative transfer code in spherical symmetry. A simple 3D test model is also presented. Conclusions. The 3D time dependent radiative transfer method is now included in our 3D RT framework and in PHOENIX/3D.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "In the Solar system the planets' compositions vary with orbital distance, with rocky planets in close orbits and lower-density gas giants in wider orbits. The detection of close-in giant planets around other stars was the first clue that this pattern is not universal, and that planets' orbits can change substantially after their formation. Here we report another violation of the orbit-composition pattern: two planets orbiting the same star with orbital distances differing by only 10%, and densities differing by a factor of 8. One planet is likely a rocky `super-Earth', whereas the other is more akin to Neptune. These planets are thirty times more closely spaced--and have a larger density contrast--than any adjacent pair of planets in the Solar system.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We present a method for attaining sub-arcsecond pointing stability during sub- orbital balloon flights, as designed for in the High Altitude Lensing Observatory (HALO) concept. The pointing method presented here has the potential to perform near-space quality optical astronomical imaging at 1-2% of the cost of space-based missions. We also discuss an architecture that can achieve sufficient thermomechanical stability to match the pointing stability. This concept is motivated by advances in the development and testing of Ultra Long Duration Balloon (ULDB) flights which promise to allow observation campaigns lasting more than three months. The design incorporates a multi-stage pointing architecture comprising: a gondola coarse azimuth control system, a multi-axis nested gimbal frame structure with arcsecond stability, a telescope de-rotator to eliminate field rotation, and a fine guidance stage consisting of both a telescope mounted angular rate sensor and guide CCDs in the focal plane to drive a fast-steering mirror. We discuss the results of pointing tests together with a preliminary thermo-mechanical analysis required for sub-arcsecond pointing at high altitude. Possible future applications in the areas of wide-field surveys and exoplanet searches are also discussed.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Aims. With our time-dependent model atmosphere code PHOENIX, our goal is to simulate light curves and spectra of hydrodynamical models of all types of supernovae. In this work, we simulate near-infrared light curves of SNe Ia and confirm the cause of the secondary maximum. Methods. We apply a simple energy solver to compute the evolution of an SN Ia envelope during the free expansion phase. Included in the solver are energy changes due to expansion, the energy deposition of \u03b3-rays and interaction of radiation with the material. Results. We computed theoretical light curves of several SN Ia hydrodynamical models in the I, J, H, and K bands and compared them to the observed SN Ia light curves of SN 1999ee and SN 2002bo. By changing a line scattering parameter in time, we obtained quite reasonable fits to the observed near-infrared light curves. This is a strong hint that detailed NLTE effects in IR lines have to be modeled, which will be a future focus of our work. Conclusions. We found that IR line scattering is very important for the near-infrared SN Ia light curve modeling. In addition, the recombination of Fe III to Fe II and of Co III to Co II is responsible for the secondary maximum in the near-infrared bands. For future work the consideration of NLTE for all lines (including the IR subordinate lines) will be crucial.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "A search of the time-series photometry from NASA's Kepler spacecraft reveals a transiting planet candidate orbiting the 11th magnitude G5 dwarf KIC 10593626 with a period of 290 days. The characteristics of the host star are well constrained by high-resolution spectroscopy combined with an asteroseismic analysis of the Kepler photometry, leading to an estimated mass and radius of 0.970 +/- 0.060 MSun and 0.979 +/- 0.020 RSun. The depth of 492 +/- 10ppm for the three observed transits yields a radius of 2.38 +/- 0.13 REarth for the planet. The system passes a battery of tests for false positives, including reconnaissance spectroscopy, high-resolution imaging, and centroid motion. A full BLENDER analysis provides further validation of the planet interpretation by showing that contamination of the target by an eclipsing system would rarely mimic the observed shape of the transits. The final validation of the planet is provided by 16 radial velocities obtained with HIRES on Keck 1 over a one year span. Although the velocities do not lead to a reliable orbit and mass determination, they are able to constrain the mass to a 3\u03c3 upper limit of 124 MEarth, safely in the regime of planetary masses, thus earning the designation Kepler-22b. The radiative equilibrium temperature is 262K for a planet in Kepler-22b's orbit. Although there is no evidence that Kepler-22b is a rocky planet, it is the first confirmed planet with a measured radius to orbit in the Habitable Zone of any star other than the Sun.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We develop a physiologically-based lattice model for the transport and metabolism of drugs in the functional unit of the liver, called the lobule. In contrast to earlier studies, we have emphasized the dominant role of convection in well-vascularized tissue with a given structure. Estimates of convective, diffusive and reaction contributions are given. We have compared drug concentration levels observed exiting the lobule with their predicted detailed distribution inside the lobule, assuming that most often the former is accessible information while the latter is not.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We present ultraviolet (UV) spectroscopy and photometry of four Type Ia supernovae (SNe 2004dt, 2004ef, 2005M, and 2005cf) obtained with the UV prism of the Advanced Camera for Surveys on the Hubble Space Telescope. This dataset provides unique spectral time series down to 2000 Angstrom. Significant diversity is seen in the near maximum-light spectra (~ 2000--3500 Angstrom) for this small sample. The corresponding photometric data, together with archival data from Swift Ultraviolet/Optical Telescope observations, provide further evidence of increased dispersion in the UV emission with respect to the optical. The peak luminosities measured in uvw1/F250W are found to correlate with the B-band light-curve shape parameter dm15(B), but with much larger scatter relative to the correlation in the broad-band B band (e.g., ~0.4 mag versus ~0.2 mag for those with 0.8 < dm15 < 1.7 mag). SN 2004dt is found as an outlier of this correlation (at > 3 sigma), being brighter than normal SNe Ia such as SN 2005cf by ~0.9 mag and ~2.0 mag in the uvw1/F250W and uvm2/F220W filters, respectively. We show that different progenitor metallicity or line-expansion velocities alone cannot explain such a large discrepancy. Viewing-angle effects, such as due to an asymmetric explosion, may have a significant influence on the flux emitted in the UV region. Detailed modeling is needed to disentangle and quantify the above effects.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Aims. We present the first theoretical SN Ia light curves calculated with the time-dependent version of the general purpose model atmosphere code PHOENIX. Our goal is to produce light curves and spectra of hydro models of all types of supernovae. Methods. We extend our model atmosphere code PHOENIX to calculate type Ia supernovae light curves. A simple solver was imple- mented which keeps track of energy conservation in the atmosphere during the free expansion phase. Results. The correct operation of the new additions to PHOENIX were verified in test calculations. Furthermore, we calculated theo- retical light curves and compared them to the observed SN Ia light curves of SN 1999ee and SN 2002bo. We obtained LTE as well as NLTE model light curves. Conclusions. We have verified the correct operation of our extension into the time domain. We have calculated the first SN Ia model light curves using PHOENIX in both LTE and NLTE. For future work the infrared model light curves need to be further investigated.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "We report the entanglement of topological features, namely, isolated, linked optical vortex loops in the light from spontaneous parametric down-conversion (SPDC). In three dimensions, optical vortices are lines of phase singularity and vortices of energy flow which percolate through all optical fields. This example of entanglement is between features that extend over macroscopic and finite volumes, furthermore, topological features are robust to perturbation . The entanglement of photons in complex three-dimensional(3D) topological states suggests the possibility of entanglement of similar structures in other quantum systems describable by complex scalar functions, such as superconductors, superfluids and Bose-Einstein condensates.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Aims. We present first results and tests of a time-dependent extension to the general purpose model atmosphere code PHOENIX. We aim to produce light curves and spectra of hydro models for all types of supernovae. Methods. We extend our model atmosphere code PHOENIX to solve time-dependent non-grey, NLTE, radiative transfer in a special relativistic framework. A simple hydrodynamics solver was implemented to keep track of the energy conservation of the atmosphere during free expansion. Results. The correct operation of the new additions to PHOENIX were verified in test calculations. Conclusions. We have shown the correct operation of our extension to time-dependent radiative transfer and will be able to calculate supernova light curves and spectra in future work.\n        \u25b3 Less", "author": "Jack Dennis"}, {"abstract": "Sulfur has been observed to be severely depleted in dense clouds leading to uncertainty in the molecules that contain it and the chemistry behind their evolution. Here, we aim to shed light on the sulfur chemistry in young stellar objects (YSOs) by using high-resolution infrared spectroscopy of absorption by the $\u03bd_3$ rovibrational band of SO$_2$ obtained with the Echelon-Cross-Echelle Spectrograph on the Stratospheric Observatory for Infrared Astronomy. Using local thermodynamic equilibrium models we derive physical parameters for the SO$_2$ gas in the massive YSO MonR2 IRS3. This yields a SO$_2$/$\\mathrm{H}$ abundance lower limit of $5.6\\pm0.5\\times10^{-7}$, or $>\\!4\\%$ of the cosmic sulfur budget, and an intrinsic line width (Doppler parameter) of $b<3.20\\;\\mathrm{km\\;s}^{-1}$. The small line widths and high temperature ($T_\\mathrm{ex}=234\\pm15\\;\\mathrm{K}$) locate the gas in a relatively quiescent region near the YSO, presumably in the hot core where ices have evaporated. This sublimation unlocks a volatile sulfur reservoir (e.g., sulfur allotropes as detected abundantly in comet 67P/Churyumov--Gerasimenko), which is followed by SO$_2$ formation by warm, dense gas-phase chemistry. The narrowness of the lines makes formation of SO$_2$ from sulfur sputtered off grains in shocks less likely toward MonR2 IRS3.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "We have performed a 5-8 $\u03bc$m spectral line survey of the hot molecular core associated with the massive protostar AFGL 2591, using the Echelon-Cross-Echelle Spectrograph (EXES) on the Stratospheric Observatory for Infrared Astronomy (SOFIA). We have supplemented these data with a ground based study in the atmospheric M band around 4.5 $\u03bc$m using the iSHELL instrument on the Infrared Telescope Facility (IRTF), and the full N band window from 8-13 $\u03bc$m using the Texas Echelon Cross Echelle Spectrograph (TEXES) on the IRTF.\n  Here we present the first detection of ro-vibrational transitions of CS in this source. The absorption lines are centred on average around -10 kms$^{-1}$ and the line widths of CS compare well with the hot component of $^{13}$CO (around 10 kms$^{-1}$). Temperatures for CS, hot $^{13}$CO and $^{12}$CO v=1-2 agree well and are around 700 K. We derive a CS abundance of 8$\\times$10$^{-3}$ and 2$\\times$10$^{-6}$ with respect to CO and H$_2$ respectively. This enhanced CS abundance with respect to the surrounding cloud (1$\\times$10$^{-8}$) may reflect sublimation of H$_2$S ice followed by gas-phase reactions to form CS.\n  Transitions are in LTE and we derive a density of $>$10$^7$ cm$^{-3}$, which corresponds to an absorbing region of $<$0.04$''$. EXES observations of CS are likely to probe deeply into the hot core, to the base of the outflow. Submillimeter and infrared observations trace different components of the hot core as revealed by the difference in systemic velocities, line widths and temperatures, as well as the CS abundance.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "Mapping road networks is currently both expensive and labor-intensive. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct. We propose RoadTracer, a new method to automatically construct accurate road network maps from aerial images. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We compare our approach with a segmentation method on fifteen cities, and find that at a 5% error rate, RoadTracer correctly captures 45% more junctions across these cities.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "In this new era of \"big data\", traditional DBMSs are under attack from two sides. At one end of the spectrum, the use of document store NoSQL systems (e.g. MongoDB) threatens to move modern Web 2.0 applications away from traditional RDBMSs. At the other end of the spectrum, big data DSS analytics that used to be the domain of parallel RDBMSs is now under attack by another class of NoSQL data analytics systems, such as Hive on Hadoop. So, are the traditional RDBMSs, aka \"big elephants\", doomed as they are challenged from both ends of this \"big data\" spectrum? In this paper, we compare one representative NoSQL system from each end of this spectrum with SQL Server, and analyze the performance and scalability aspects of each of these approaches (NoSQL vs. SQL) on two workloads (decision support analysis and interactive data-serving) that represent the two ends of the application spectrum. We present insights from this evaluation and speculate on potential trends for the future.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "This paper introduces the CondorJ2 cluster management system. Traditionally, cluster management systems such as Condor employ a process-oriented approach with little or no use of modern database system technology. In contrast, CondorJ2 employs a data-centric, 3-tier web-application architecture for all system functions (e.g., job submission, monitoring and scheduling; node configuration, monitoring and management, etc.) except for job execution. Employing a data-oriented approach allows the core challenge (i.e., managing and coordinating a large set of distributed computing resources) to be transformed from a relatively low-level systems problem into a more abstract, higher-level data management problem. Preliminary results suggest that CondorJ2's use of standard 3-tier software represents a significant step forward to the design and implementation of large clusters (1,000 to 10,000 nodes).\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "We report the detection of the first extrasolar planet, ET-1 (HD 102195b), using the Exoplanet Tracker (ET), a new generation Doppler instrument. The planet orbits HD 102195, a young star with solar metallicity that may be part of the local association. The planet imparts radial velocity variability to the star with a semiamplitude of $63.4\\pm2.0$ m s$^{-1}$ and a period of 4.11 days. The planetary minimum mass ($m \\sin i$) is $0.488\\pm0.015$ $M_J$.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "This is a thought piece on data-intensive science requirements for databases and science centers. It argues that peta-scale datasets will be housed by science centers that provide substantial storage and processing for scientists who access the data via smart notebooks. Next-generation science instruments and simulations will generate these peta-scale datasets. The need to publish and share data and the need for generic analysis and visualization tools will finally create a convergence on common metadata standards. Database systems will be judged by their support of these metadata standards and by their ability to manage and access peta-scale datasets. The procedural stream-of-bytes-file-centric approach to data analysis is both too cumbersome and too serial for such large datasets. Non-procedural query and analysis of schematized self-describing data is both easier to use and allows much more parallelism.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "A group of senior database researchers gathers every few years to assess the state of database research and to point out problem areas that deserve additional focus. This report summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6, 2003 in Lowell, Mass. It observes that information management continues to be a critical component of most complex software systems. It recommends that database researchers increase focus on: integration of text, data, code, and streams; fusion of information from heterogeneous data sources; reasoning about uncertain data; unsupervised data mining for interesting correlations; information privacy; and self-adaptation and repair.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda -- broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.\n        \u25b3 Less", "author": "David DeWitt"}, {"abstract": "Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades. We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design. For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation. Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "Winograd-based convolution has quickly gained traction as a preferred approach to implement convolutional neural networks (ConvNet) on various hardware platforms because it requires fewer floating point operations than FFT-based or direct convolutions.\n  This paper compares three highly optimized implementations (regular FFT--, Gauss--FFT--, and Winograd--based convolutions) on modern multi-- and many--core CPUs. Although all three implementations employed the same optimizations for modern CPUs, our experimental results with two popular ConvNets (VGG and AlexNet) show that the FFT--based implementations generally outperform the Winograd--based approach, contrary to the popular belief.\n  To understand the results, we use a Roofline performance model to analyze the three implementations in detail, by looking at each of their computation phases and by considering not only the number of floating point operations, but also the memory bandwidth and the cache sizes. The performance analysis explains why, and under what conditions, the FFT--based implementations outperform the Winograd--based one, on modern CPUs.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "Widely used in news, business, and educational media, infographics are handcrafted to effectively communicate messages about complex and often abstract topics including `ways to conserve the environment' and `understanding the financial crisis'. Composed of stylistically and semantically diverse visual and textual elements, infographics pose new challenges for computer vision. While automatic text extraction works well on infographics, computer vision approaches trained on natural images fail to identify the stand-alone visual elements in infographics, or `icons'. To bridge this representation gap, we propose a synthetic data generation strategy: we augment background patches in infographics from our Visually29K dataset with Internet-scraped icons which we use as training data for an icon proposal mechanism. On a test set of 1K annotated infographics, icons are located with 38% precision and 34% recall (the best model trained with natural images achieves 14% precision and 7% recall). Combining our icon proposals with icon classification and text extraction, we present a multi-modal summarization application. Our application takes an infographic as input and automatically produces text tags and visual hashtags that are textually and visually representative of the infographic's topics respectively.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "We address the computational problem of novel human pose synthesis. Given an image of a person and a desired pose, we produce a depiction of that person in that pose, retaining the appearance of both the person and background. We present a modular generative neural network that synthesizes unseen poses using training pairs of images and poses taken from human action videos. Our network separates a scene into different body part and background layers, moves body parts to new locations and refines their appearances, and composites the new foreground with a hole-filled background. These subtasks, implemented with separate modules, are trained jointly using only a single target image as a supervised label. We use an adversarial discriminator to force our network to synthesize realistic details conditioned on pose. We demonstrate image synthesis results on three action classes: golf, yoga/workouts and tennis, and show that our method produces accurate results within action classes as well as across action classes. Given a sequence of desired poses, we also produce coherent videos of actions.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "Video motion magnification techniques allow us to see small motions previously invisible to the naked eyes, such as those of vibrating airplane wings, or swaying buildings under the influence of the wind. Because the motion is small, the magnification results are prone to noise or excessive blurring. The state of the art relies on hand-designed filters to extract representations that may not be optimal. In this paper, we seek to learn the filters directly from examples using deep convolutional neural networks. To make training tractable, we carefully design a synthetic dataset that captures small motion well, and use two-frame input for training. We show that the learned filters achieve high-quality results on real videos, with less ringing artifacts and better noise characteristics than previous methods. While our model is not trained with temporal filters, we found that the temporal filters can be used with our extracted representations up to a moderate magnification, enabling a frequency-based motion selection. Finally, we analyze the learned filters and show that they behave similarly to the derivative filters used in previous works. Our code, trained model, and datasets will be available online.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "We introduce the problem of visual hashtag discovery for infographics: extracting visual elements from an infographic that are diagnostic of its topic. Given an infographic as input, our computational approach automatically outputs textual and visual elements predicted to be representative of the infographic content. Concretely, from a curated dataset of 29K large infographic images sampled across 26 categories and 391 tags, we present an automated two step approach. First, we extract the text from an infographic and use it to predict text tags indicative of the infographic content. And second, we use these predicted text tags as a supervisory signal to localize the most diagnostic visual elements from within the infographic i.e. visual hashtags. We report performances on a categorization and multi-label tag prediction problem and compare our proposed visual hashtags to human annotations.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "Performance is a critical challenge in mobile image processing. Given a reference imaging pipeline, or even human-adjusted pairs of images, we seek to reproduce the enhancements and enable real-time evaluation. For this, we introduce a new neural network architecture inspired by bilateral grid processing and local affine color transforms. Using pairs of input/output images, we train a convolutional neural network to predict the coefficients of a locally-affine model in bilateral space. Our architecture learns to make local, global, and content-dependent decisions to approximate the desired image transformation. At runtime, the neural network consumes a low-resolution version of the input image, produces a set of affine transformations in bilateral space, upsamples those transformations in an edge-preserving fashion using a new slicing node, and then applies those upsampled transformations to the full-resolution image. Our algorithm processes high-resolution images on a smartphone in milliseconds, provides a real-time viewfinder at 1080p resolution, and matches the quality of state-of-the-art approximation techniques on a large class of image operators. Unlike previous work, our model is trained off-line from data and therefore does not require access to the original operator at runtime. This allows our model to learn complex, scene-dependent transformations for which no reference implementation is available, such as the photographic edits of a human retoucher.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "In this paper, we present BubbleView, an alternative methodology for eye tracking using discrete mouse clicks to measure which information people consciously choose to examine. BubbleView is a mouse-contingent, moving-window interface in which participants are presented with a series of blurred images and click to reveal \"bubbles\" - small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. Across 10 experiments with 28 different parameter combinations, we evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView clicks can both (i) successfully approximate eye fixations on different images, and (ii) be used to rank image and design elements by importance. BubbleView is designed to collect clicks on static images, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. BubbleView data is cleaner and more consistent than related methodologies that use continuous mouse movements. Our analyses validate the use of mouse-contingent, moving-window methodologies as approximating eye fixations for different image and task types.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "For many movement disorders, such as Parkinson's disease and ataxia, disease progression is visually assessed by a clinician using a numerical disease rating scale. These tests are subjective, time-consuming, and must be administered by a professional. This can be problematic where specialists are not available, or when a patient is not consistently evaluated by the same clinician. We present an automated method for quantifying the severity of motion impairment in patients with ataxia, using only video recordings. We consider videos of the finger-to-nose test, a common movement task used as part of the assessment of ataxia progression during the course of routine clinical checkups.\n  Our method uses neural network-based pose estimation and optical flow techniques to track the motion of the patient's hand in a video recording. We extract features that describe qualities of the motion such as speed and variation in performance. Using labels provided by an expert clinician, we train a supervised learning model that predicts severity according to the Brief Ataxia Rating Scale (BARS). The performance of our system is comparable to that of a group of ataxia specialists in terms of mean error and correlation, and our system's predictions were consistently within the range of inter-rater variability. This work demonstrates the feasibility of using computer vision and machine learning to produce consistent and clinically useful measures of motor impairment.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "The inverse diffusion curve problem focuses on automatic creation of diffusion curve images that resemble user provided color fields. This problem is challenging since the 1D curves have a nonlinear and global impact on resulting color fields via a partial differential equation (PDE). We introduce a new approach complementary to previous methods by optimizing curve geometry. In particular, we propose a novel iterative algorithm based on the theory of shape derivatives. The resulting diffusion curves are clean and well-shaped, and the final image closely approximates the input. Our method provides a user-controlled parameter to regularize curve complexity, and generalizes to handle input color fields represented in a variety of formats.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "How best to evaluate a saliency model's ability to predict where humans look in images is an open research question. The choice of evaluation metric depends on how saliency is defined and how the ground truth is represented. Metrics differ in how they rank saliency models, and this results from how false positives and false negatives are treated, whether viewing biases are accounted for, whether spatial deviations are factored in, and how the saliency maps are pre-processed. In this paper, we provide an analysis of 8 different evaluation metrics and their properties. With the help of systematic experiments and visualizations of metric computations, we add interpretability to saliency scores and more transparency to the evaluation of saliency models. Building off the differences in metric properties and behaviors, we make recommendations for metric selections under specific assumptions and for specific applications.\n        \u25b3 Less", "author": "Fredo Durand"}, {"abstract": "The design of DNNs has increasingly focused on reducing the computational complexity in addition to improving accuracy. While emerging DNNs tend to have fewer weights and operations, they also reduce the amount of data reuse with more widely varying layer shapes and sizes. This leads to a diverse set of DNNs, ranging from large ones with high reuse (e.g., AlexNet) to compact ones with high bandwidth requirements (e.g., MobileNet). However, many existing DNN processors depend on certain DNN properties, e.g., a large number of channels, to achieve high performance and energy efficiency and do not have sufficient flexibility to efficiently process a diverse set of DNNs. In this work, we present Eyexam, a performance analysis framework that quantitatively identifies the sources of performance loss in DNN processors. It highlights two architectural bottlenecks in many existing designs. First, their dataflows are not flexible enough to adapt to the varying layer shapes and sizes of different DNNs. Second, their network-on-chip (NoC) can't adapt to support both high data reuse and high bandwidth scenarios. Based on this analysis, we present Eyeriss v2, a high-performance DNN accelerator that adapts to a wide range of DNNs. Eyeriss v2 has a new dataflow, called Row-Stationary Plus (RS+), that enables the spatial tiling of data from all dimensions to fully utilize the parallelism for high performance. To support RS+, it has a low-cost and scalable NoC design, called hierarchical mesh, that connects the high-bandwidth global buffer to the array of processing elements (PEs) in a two-level hierarchy. This enables high-bandwidth data delivery while still being able to harness any available data reuse. Compared with Eyeriss, Eyeriss v2 has a performance increase of 10.4x-17.9x for 256 PEs, 37.7x-71.5x for 1024 PEs, and 448.8x-1086.7x for 16384 PEs on DNNs with widely varying amounts of data reuse.\n        \u25b3 Less", "author": "Joel Emer"}, {"abstract": "Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs in a wide range of situations, especially mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator applied during inference. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to the multiplier array, where they are extensively reused. In addition, the accumulation of multiplication products are performed in a novel accumulator array. Our results show that on contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7x and 2.3x, respectively, over a comparably provisioned dense CNN accelerator.\n        \u25b3 Less", "author": "Joel Emer"}, {"abstract": "Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems.\n  This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry.\n  The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.\n        \u25b3 Less", "author": "Joel Emer"}, {"abstract": "Computer vision enables a wide range of applications in robotics/drones, self-driving cars, smart Internet of Things, and portable/wearable electronics. For many of these applications, local embedded processing is preferred due to privacy and/or latency concerns. Accordingly, energy-efficient embedded vision hardware delivering real-time and robust performance is crucial. While deep learning is gaining popularity in several computer vision algorithms, a significant energy consumption difference exists compared to traditional hand-crafted approaches. In this paper, we provide an in-depth analysis of the computation, energy and accuracy trade-offs between learned features such as deep Convolutional Neural Networks (CNN) and hand-crafted features such as Histogram of Oriented Gradients (HOG). This analysis is supported by measurements from two chips that implement these algorithms. Our goal is to understand the source of the energy discrepancy between the two approaches and to provide insight about the potential areas where CNNs can be improved and eventually approach the energy-efficiency of HOG while maintaining its outstanding performance accuracy.\n        \u25b3 Less", "author": "Joel Emer"}, {"abstract": "Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).\n        \u25b3 Less", "author": "Joel Emer"}, {"abstract": "The nitrogen vacancy (NV) center in diamond has emerged as a leading solid-state quantum sensor for applications including magnetometry, electrometry, thermometry, and chemical sensing. However, an outstanding challenge for practical applications is that existing NV-based sensing techniques require bulky and discrete instruments for spin control and detection. Here, we address this challenge by integrating NV based quantum sensing with complementary metal-oxide-semiconductor (CMOS) technology. Through tailored CMOS-integrated microwave generation and photodetection, this work dramatically reduces the instrumentation footprint for quantum magnetometry and thermometry. This hybrid diamond-CMOS integration enables an ultra-compact and scalable platform for quantum sensing and quantum information processing.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Physically motivated quantum algorithms for specific near-term quantum hardware will likely be the next frontier in quantum information science. Here, we show how many of the features of neural networks for machine learning can naturally be mapped into the quantum optical domain by introducing the quantum optical neural network (QONN). Through numerical simulation and analysis we train the QONN to perform a range of quantum information processing tasks, including newly developed protocols for quantum optical state compression, reinforcement learning, and black-box quantum simulation. We consistently demonstrate our system can generalize from only a small set of training data onto states for which it has not been trained. Our results indicate QONNs are a powerful design tool for quantum optical systems and, leveraging advances in integrated quantum photonics, a promising architecture for next generation quantum processors.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We report on quantum emission from Pb-related color centers in diamond following ion implantation and high temperature vacuum annealing. First-principles calculations predict a negatively-charged Pb-vacancy center in a split-vacancy configuration, with a zero-phonon transition around 2.3 eV. Cryogenic photoluminescence measurements performed on emitters in nanofabricated pillars reveal several transitions, including a prominent doublet near 520 nm. The splitting of this doublet, 2 THz, exceeds that reported for other group-IV centers. These observations are consistent with the PbV center, which is expected to have the combination of narrow optical transitions and stable spin states, making it a promising system for quantum network nodes.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Physical annealing systems provide heuristic approaches to solving NP-hard Ising optimization problems. Here, we study the performance of two types of annealing machines--a commercially available quantum annealer built by D-Wave Systems, and measurement-feedback coherent Ising machines (CIMs) based on optical parametric oscillator networks--on two classes of problems, the Sherrington-Kirkpatrick (SK) model and MAX-CUT. The D-Wave quantum annealer outperforms the CIMs on MAX-CUT on regular graphs of degree 3. On denser problems, however, we observe an exponential penalty for the quantum annealer ($\\exp(-\u03b1_\\textrm{DW} N^2)$) relative to CIMs ($\\exp(-\u03b1_\\textrm{CIM} N)$) for fixed anneal times, on both the SK model and on 50%-edge-density MAX-CUT, where the coefficients $\u03b1_\\textrm{CIM}$ and $\u03b1_\\textrm{DW}$ are problem-class-dependent. On instances with over $50$ vertices, a several-orders-of-magnitude time-to-solution difference exists between CIMs and the D-Wave annealer. An optimal-annealing-time analysis is also consistent with a significant projected performance difference. The difference in performance between the sparsely connected D-Wave machine and the measurement-feedback facilitated all-to-all connectivity of the CIMs provides strong experimental support for efforts to increase the connectivity of quantum annealers.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We present an S-band tunable loop gap resonator (LGR) providing strong, homogeneous, and directionally uniform broadband microwave (MW) drive for nitrogen-vacancy (NV) ensembles. With 42 dBm of input power, the composite device provides drive field amplitudes approaching 5 G over a circular area $\\gtrsim\\!50$ mm$^2$ or cylindrical volume $\\gtrsim\\!250$ mm$^3$. The wide 80 MHz device bandwidth allows driving all eight NV Zeeman resonances for bias magnetic fields below 20 G. For pulsed applications the device realizes percent-scale microwave drive inhomogeneity; we measure a fractional root-mean-square inhomogeneity $\u03c3_\\text{rms}\\!=\\! 1.6\\%$ and a peak-to-peak variation $\u03c3_\\text{pp}\\!=\\! 3\\%$ over a circular area of 11 mm$^2$, and $\u03c3_\\text{rms} \\!=\\! 3.2\\%$ and $\u03c3_\\text{pp}\\! =\\! 10.5\\%$ over a larger 32 mm$^2$ circular area. We demonstrate incident MW power coupling to the LGR using multiple methodologies: a PCB-fabricated exciter antenna for deployed compact bulk sensors and an inductive coupling coil suitable for microscope-style imaging. The inductive coupling coil allows for approximately $2\u03c0$ steradian combined optical access above and below the device, ideal for envisioned and existing NV imaging and bulk sensing applications.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "The ability to confine light into tiny spatial dimensions is important for applications such as microscopy, sensing and nanoscale lasers. While plasmons offer an appealing avenue to confine light, Landau damping in metals imposes a trade-off between optical field confinement and losses. We show that a graphene-insulator-metal heterostructure can overcome that trade-off, and demonstrate plasmon confinement down to the ultimate limit of the lengthscale of one atom. This is achieved by far-field excitation of plasmon modes squeezed into an atomically thin hexagonal boron nitride dielectric h-BN spacer between graphene and metal rods. A theoretical model which takes into account the non-local optical response of both graphene and metal is used to describe the results. These ultra-confined plasmonic modes, addressed with far-field light excitation, enables a route to new regimes of ultra-strong light-matter interactions.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Optical random scattering is generally considered to be a nuisance of microscopy that limits imaging depth and spatial resolution. Wavefront shaping techniques have recently enabled optical imaging at unprecedented depth, but a remaining problem is also to attain super-resolution within complex media. To address this challenge, we introduce a new technique to focus inside of complex media by the use of a quantum reference beacon (QRB), consisting of solid-state quantum emitters with spin-dependent fluorescence. This QRB provides subwavelength guidestar feedback for wavefront shaping to achieve an optical focus below the microscope's diffraction limit. We implement the QRB-guided imaging approach using nitrogen-vacancy centers in diamond nanocrystals, which enable optical focusing with a subdiffraction resolution below 186 nm ($\\approx \u03bb/3.5\\mbox{NA}$), where the microscope's NA=0.8. This QRB-assisted wavefront shaping paves the way for a range of new applications, including deep-tissue quantum enhanced sensing and individual optical excitation of magnetically-coupled spin ensembles for applications in quantum information processing.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Coincidence detection of single photons is crucial in numerous quantum technologies and usually requires multiple time-resolved single-photon detectors. However, the electronic readout becomes a major challenge when the measurement basis scales to large numbers of spatial modes. Here, we address this problem by introducing a two-terminal coincidence detector that enables scalable readout of an array of detector segments based on superconducting nanowire microstrip transmission line. Exploiting timing logic, we demonstrate a 16-element detector that resolves all 136 possible single-photon and two-photon coincidence events. We further explore the pulse shapes of the detector output and resolve up to four-photon coincidence events in a 4-element device, giving the detector photon-number-resolving capability. This new detector architecture and operating scheme will be particularly useful for multi-photon coincidence detection in large-scale photonic integrated circuits.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Ultrafast electrically driven nanoscale light sources are critical components in nanophotonics. Compound semiconductor-based light sources for the nanophotonic platforms have been extensively investigated over the past decades. However, monolithic ultrafast light sources with a small footprint remain a challenge. Here, we demonstrate electrically driven ultrafast graphene light emitters that achieve light pulse generation with up to 10 GHz bandwidth, across a broad spectral range from the visible to the near-infrared. The fast response results from ultrafast charge carrier dynamics in graphene, and weak electron-acoustic phonon-mediated coupling between the electronic and lattice degrees of freedom. We also find that encapsulating graphene with hexagonal boron nitride (hBN) layers strongly modifies the emission spectrum by changing the local optical density of states, thus providing up to 460 % enhancement compared to the grey-body thermal radiation for a broad peak centered at 720 nm. Furthermore, the hBN encapsulation layers permit stable and bright visible thermal radiation with electronic temperatures up to 2,000 K under ambient conditions, as well as efficient ultrafast electronic cooling via near-field coupling to hybrid polaritonic modes. These high-speed graphene light emitters provide a promising path for on-chip light sources for optical communications and other optoelectronic applications.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "The practical implementation of many quantum technologies relies on the development of robust and bright single photon sources that operate at room temperature. The negatively charged silicon-vacancy (SiV-) color center in diamond is a possible candidate for such a single photon source. However, due to the high refraction index mismatch to air, color centers in diamond typically exhibit low photon out-coupling. An additional shortcoming is due to the random localization of native defects in the diamond sample. Here we demonstrate deterministic implantation of Si ions with high conversion efficiency to single SiV- centers, targeted to fabricated nanowires. The co-localization of single SiV- centers with the nanostructures yields a ten times higher light coupling efficiency than for single SiV- centers in bulk diamond. This enhanced photon out-coupling, together with the intrinsic scalability of the SiV- creation method, enables a new class of devices for integrated photonics and quantum science.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Scalable quantum photonic systems require efficient single photon sources coupled to integrated photonic devices. Solid-state quantum emitters can generate single photons with high efficiency, while silicon photonic circuits can manipulate them in an integrated device structure. Combining these two material platforms could, therefore, significantly increase the complexity of integrated quantum photonic devices. Here, we demonstrate hybrid integration of solid-state quantum emitters to a silicon photonic device. We develop a pick-and-place technique that can position epitaxially grown InAs/InP quantum dots emitting at telecom wavelengths on a silicon photonic chip deterministically with nanoscale precision. We employ an adiabatic tapering approach to transfer the emission from the quantum dots to the waveguide with high efficiency. We also incorporate an on-chip silicon-photonic beamsplitter to perform a Hanbury-Brown and Twiss measurement. Our approach could enable integration of pre-characterized III-V quantum photonic devices into large-scale photonic structures to enable complex devices composed of many emitters and photons.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Photonic integrated circuits (PICs) provide a compact and stable platform for quantum photonics. Here we demonstrate a silicon photonics quantum key distribution (QKD) transmitter in the first high-speed polarization-based QKD field tests. The systems reach composable secret key rates of 950 kbps in a local test (on a 103.6-m fiber with a total emulated loss of 9.2 dB) and 106 kbps in an intercity metropolitan test (on a 43-km fiber with 16.4 dB loss). Our results represent the highest secret key generation rate for polarization-based QKD experiments at a standard telecom wavelength and demonstrate PICs as a promising, scalable resource for future formation of metropolitan quantum-secure communications networks.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We demonstrate a spin-based, all-dielectric electrometer based on an ensemble of nitrogen-vacancy (NV$^-$) defects in diamond. An applied electric field causes energy level shifts symmetrically away from the NV$^-$'s degenerate triplet states via the Stark effect; this symmetry provides immunity to temperature fluctuations allowing for shot-noise-limited detection. Using an ensemble of NV$^-$s, we demonstrate shot-noise limited sensitivities approaching 1 V/cm/$\\sqrt{\\text{Hz}}$ under ambient conditions, at low frequencies ($<$10 Hz), and over a large dynamic range (20 dB). A theoretical model for the ensemble of NV$^-$s fits well with measurements of the ground-state electric susceptibility parameter, $\\langle k_\\perp\\rangle$. Implications of spin-based, dielectric sensors for micron-scale electric-field sensing are discussed.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We investigate the origin of imperfections in the fidelity of a two-photon controlled-phase gate based on two-level-emitter non-linearities. We focus on a passive system that operates without external modulations to enhance its performance. We demonstrate that the fidelity of the gate is limited by opposing requirements on the input pulse width for one- and two-photon scattering events. For one-photon scattering, the spectral pulse width must be narrow compared to the emitter linewidth, while two-photon scattering processes require the pulse width and emitter linewidth to be comparable. We find that these opposing requirements limit the maximum fidelity of the two-photon controlled-phase gate for Gaussian photon pulses to 84%.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Quantum key distribution (QKD) exploits the quantum nature of light to share provably secure keys, allowing secure communication in the presence of an eavesdropper. The first QKD schemes used photons encoded in two states, such as polarization. Recently, much effort has turned to large-alphabet QKD schemes, which encode photons in high-dimensional basis states. Compared to binary-encoded QKD, large-alphabet schemes can encode more secure information per detected photon, boosting secure communication rates, and also provide increased resilience to noise and loss. High-dimensional encoding may also improve the efficiency of other quantum information processing tasks, such as performing Bell tests and implementing quantum gates. Here, we demonstrate a large-alphabet QKD protocol based on high-dimensional temporal encoding. We achieve record secret-key rates and perform the first field demonstration of large-alphabet QKD. This demonstrates a new, practical way to optimize secret-key rates and marks an important step towards transmission of high-dimensional quantum states in deployed networks.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "The controlled creation of defect center---nanocavity systems is one of the outstanding challenges for efficiently interfacing spin quantum memories with photons for photon-based entanglement operations in a quantum network. Here, we demonstrate direct, maskless creation of atom-like single silicon-vacancy (SiV) centers in diamond nanostructures via focused ion beam implantation with $\\sim 32$ nm lateral precision and $< 50$ nm positioning accuracy relative to a nanocavity. Moreover, we determine the Si+ ion to SiV center conversion yield to $\\sim 2.5\\%$ and observe a 10-fold conversion yield increase by additional electron irradiation. We extract inhomogeneously broadened ensemble emission linewidths of $\\sim 51$ GHz, and close to lifetime-limited single-emitter transition linewidths down to $126 \\pm13$ MHz corresponding to $\\sim 1.4$-times the natural linewidth. This demonstration of deterministic creation of optically coherent solid-state single quantum systems is an important step towards development of scalable quantum optical devices.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "In the effort to make 2D materials-based devices smaller, faster, and more efficient, it is important to control charge carrier at lengths approaching the nanometer scale. Traditional gating techniques based on capacitive coupling through a gate dielectric cannot generate strong and uniform electric fields at this scale due to divergence of the fields in dielectrics. This field divergence limits the gating strength, boundary sharpness, and pitch size of periodic structures, and restricts possible geometries of local gates (due to wire packaging), precluding certain device concepts, such as plasmonics and transformation optics based on metamaterials. Here we present a new gating concept based on a dielectric-free self-aligned electrolyte technique that allows spatially modulating charges with nanometer resolution. We employ a combination of a solid-polymer electrolyte gate and an ion-impenetrable e-beam-defined resist mask to locally create excess charges on top of the gated surface. Electrostatic simulations indicate high carrier density variations of $\u0394n =10^{14}\\text{cm}^{-2}$ across a length of 10 nm at the mask boundaries on the surface of a 2D conductor, resulting in a sharp depletion region and a strong in-plane electric field of $6\\times10^8 \\text{Vm}^{-1}$ across the so-created junction. We apply this technique to the 2D material graphene to demonstrate the creation of tunable p-n junctions for optoelectronic applications. We also demonstrate the spatial versatility and self-aligned properties of this technique by introducing a novel graphene thermopile photodetector.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Hexagonal boron nitride (hBN) is an emerging two dimensional material for quantum photonics owing to its large bandgap and hyperbolic properties. Here we report a broad range of multicolor room temperature single photon emissions across the visible and the near infrared spectral ranges from point defects in hBN multilayers. We show that the emitters can be categorized into two general groups, but most likely possess similar crystallographic structure. We further show two approaches for engineering of the emitters using either electron beam irradiation or annealing, and characterize their photophysical properties. The emitters exhibit narrow line widths of sub 10 nm at room temperature, and a short excited state lifetime with high brightness. Remarkably, the emitters are extremely robust and withstand aggressive annealing treatments in oxidizing and reducing environments. Our results constitute the first step towards deterministic engineering of single emitters in 2D materials and hold great promise for the use of defects in boron nitride as sources for quantum information processing and nanophotonics.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Single photon sources are of paramount importance in quantum communication, quantum computation, and quantum metrology. In particular, there is great interest to realize scalable solid state platforms that can emit triggered photons on demand to achieve scalable nanophotonic networks. We report on a visible-spectrum single photon emitter in 4H-silicon carbide (SiC). The emitter is photostable at room- and low-temperature enabling photon counts per second (cps) in excess of 2$\\times$10$^6$ from unpatterned, bulk SiC. It exists in two orthogonally polarized states, which have parallel absorption and emission dipole orientations. Low temperature measurements reveal a narrow zero phonon line (linewidth $<0.1~$nm) that accounts for $>30~$% of the total photoluminescence spectrum.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Environmental noise and disorder play a critical role in quantum particle and wave transport in complex media, including solid-state and biological systems. Recent work has predicted that coupling between noisy environments and disordered systems, in which coherent transport has been arrested due to localization effects, could actually enhance transport. Photonic integrated circuits are promising platforms for studying such effects, with a central goal being the development of large systems providing low-loss, high-fidelity control over all parameters of the transport problem. Here, we fully map out the role of static and dynamic disorder in quantum transport using a low-loss, phase-stable, nanophotonic processor consisting of a mesh of 56 generalized beamsplitters programmable on microsecond timescales. Over 85,600 transport experiments, we observe several distinct transport regimes, including environment-enhanced transport in strong, statically disordered systems. Low loss and programmability make this nanophotonic processor a promising platform for many-boson quantum simulation experiments.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Graphene and other two-dimensional (2D) materials have emerged as promising materials for broadband and ultrafast photodetection and optical modulation. These optoelectronic capabilities can augment complementary metal-oxide-semiconductor (CMOS) devices for high-speed and low-power optical interconnects. Here, we demonstrate an on-chip ultrafast photodetector based on a two-dimensional heterostructure consisting of high-quality graphene encapsulated in hexagonal boron nitride. Coupled to the optical mode of a silicon waveguide, this 2D heterostructure-based photodetector exhibits a maximum responsivity of 0.36 A/W and high-speed operation with a 3 dB cut-off at 42 GHz. From photocurrent measurements as a function of the top-gate and source-drain voltages, we conclude that the photoresponse is consistent with hot electron mediated effects. At moderate peak powers above 50 mW, we observe a saturating photocurrent consistent with the mechanisms of electron-phonon supercollision cooling. This nonlinear photoresponse enables optical on-chip autocorrelation measurements with picosecond-scale timing resolution and exceptionally low peak powers.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "A central goal in quantum information science is to efficiently interface photons with single optical modes for quantum networking and distributed quantum computing. Here, we introduce and experimentally demonstrate a compact and efficient method for the low-loss coupling of a solid-state qubit, the nitrogen vacancy (NV) centre in diamond, with a single-mode optical fibre. In this approach, single-mode tapered diamond waveguides containing exactly one high quality NV memory are selected and integrated on tapered silica fibres. Numerical optimization of an adiabatic coupler indicates that near-unity-efficiency photon transfer is possible between the two modes. Experimentally, we find an overall collection efficiency between 18-40 % and observe a raw single photon count rate above 700 kHz. This integrated system enables robust, alignment-free, and efficient interfacing of single-mode optical fibres with single photon emitters and quantum memories in solids.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We present a scheme for implementing high-fidelity quantum logic gates using the quantum walk of a few interacting bosons on a one-dimensional lattice. The gate operation is carried out by a single compact lattice described by a one-dimensional Bose-Hubbard model with only nearest-neighbor hopping and on-site interactions. We find high-fidelity deterministic logic operations for a gate set (including the CNOT gate) that is universal for quantum information processing. We discuss the applicability of this scheme in light of recent developments in controlling and monitoring cold-atoms in optical lattices, as well as an implementation with realistic nonlinear quantum photonic devices.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Nanoscale and power-efficient electro-optic (EO) modulators are essential components for optical interconnects that are beginning to replace electrical wiring for intra- and inter-chip communications. Silicon-based EO modulators show sufficient figures of merits regarding device footprint, speed, power consumption and modulation depth. However, the weak electro-optic effect of silicon still sets a technical bottleneck for these devices, motivating the development of modulators based on new materials. Graphene, a two-dimensional carbon allotrope, has emerged as an alternative active material for optoelectronic applications owing to its exceptional optical and electronic properties. Here, we demonstrate a high-speed graphene electro-optic modulator based on a graphene-boron nitride (BN) heterostructure integrated with a silicon photonic crystal nanocavity. Strongly enhanced light-matter interaction of graphene in a submicron cavity enables efficient electrical tuning of the cavity reflection. We observe a modulation depth of 3.2 dB and a cut-off frequency of 1.2 GHz.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "A scalable approach for integrated photonic networks in single-crystal diamond using triangular etching of bulk samples is presented. We describe designs of high quality factor (Q=2.51x10^6) photonic crystal cavities with low mode volume (Vm=1.062x(\u03bb/n)^3), which are connected via waveguides supported by suspension structures with predicted transmission loss of only 0.05 dB. We demonstrate the fabrication of these structures using transferred single-crystal silicon hard masks and angular dry etching, yielding photonic crystal cavities in the visible spectrum with measured quality factors in excess of Q=3x103.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "High-dimensional quantum key distribution (HD-QKD) allows two parties to generate multiple secure bits of information per detected photon. In this work, we show that decoy state protocols can be practically implemented for HD-QKD using only one or two decoy states. HD-QKD with two decoy states, under realistic experimental constraints, can generate multiple secure bits per coincidence at distances over 200 km and at rates similar to those achieved by a protocol with infinite decoy states. Furthermore, HD-QKD with only one decoy state is practical at short distances, where it is almost as secure as a protocol with two decoy states. HD-QKD with only one or two decoy states can therefore be implemented to optimize the rate of secure quantum communications.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Optical spectroscopy is a fundamental tool in numerous areas of science and technology. Much effort has focused on miniaturizing spectrometers, but thus far at the cost of high spectral resolution and broad operating range. Here, we describe a compact spectrometer without this trade-off. The device relies on imaging multi-mode interference from leaky modes along a highly multimode tapered optical fiber, resulting in spectrally distinguishable images that form a basis for reconstructing an incident light spectrum. This tapered fiber multimode interference spectrometer enables the acquisition of broadband spectra in a single camera exposure with a measured resolution of 40 pm in the visible spectrum and 10 pm in the infrared spectrum, which are comparable to the performance of grating spectrometers. Spectroscopy from 500 nm to 1600 nm is demonstrated, though operation across the entire transparency window of silica fibers is possible. Multimode interference spectroscopy of leaky modes is suitable in a variety of device geometries, including planar waveguides in a broad range of transparent materials. We anticipate that this technique will greatly expand the availability of high-performance spectroscopy in a wide range of applications.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We propose and analyze the design of a programmable photonic integrated circuit for high-fidelity quantum computation and simulation. We demonstrate that the reconfigurability of our design allows us to overcome two major impediments to quantum optics on a chip: it removes the need for a full fabrication cycle for each experiment and allows for compensation of fabrication errors using numerical optimization techniques. Under a pessimistic fabrication model for the silicon-on-insulator process, we demonstrate a dramatic fidelity improvement for the linear optics CNOT and CPHASE gates and, showing the scalability of this approach, the iterative phase estimation algorithm built from individually optimized gates. We also propose and simulate a novel experiment that the programmability of our system would enable: a statistically robust study of the evolution of entangled photons in disordered quantum walks. Overall, our results suggest that existing fabrication processes are sufficient to build a quantum photonic processor capable of high fidelity operation.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We demonstrate the controlled enhancement of photoresponsivity in a graphene photodetector by coupling to slow light modes in a long photonic crystal linear defect cavity. Near the Brillouin zone (BZ) boundary, spectral coupling of multiple cavity modes results in broad-band photocurrent enhancement from 1530 nm to 1540 nm. Away from the BZ boundary, individual cavity resonances enhance the photocurrent eight-fold in narrow resonant peaks. Optimization of the photocurrent via critical coupling of the incident field with the graphene-cavity system is discussed. The enhanced photocurrent demonstrates the feasibility of a wavelength-scale graphene photodetector for efficient photodetection with high spectral selectivity and broadband response.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We report on controlling the spontaneous emission (SE) rate of a molybdenum disulfide (MoS$_2$) monolayer coupled with a planar photonic crystal (PPC) nanocavity. Spatially resolved photoluminescence (PL) mapping shows strong variations of emission when the MoS$_2$ monolayer is on the PPC cavity, on the PPC lattice, on the air gap, and on the unpatterned gallium phosphide substrate. Polarization dependences of the cavity-coupled MoS$_2$ emission show a more than 5 times stronger extracted PL intensity than the un-coupled emission, which indicates an underlying cavity mode Purcell enhancement of MoS$_2$ SE rate exceeding a factor of 70.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "Near-infrared Hong-Ou-Mandel quantum interference is observed in silicon nanophotonic directional couplers with raw visibilities on-chip at 90.5%. Spectrally-bright 1557-nm two-photon states are generated in a periodically-poled KTiOPO4 waveguide chip, serving as the entangled photon source and pumped with a self-injection locked laser, for the photon statistical measurements. Efficient four-port coupling in the communications C-band and in the high-index-contrast silicon photonics platform is demonstrated, with matching theoretical predictions of the quantum interference visibility. Constituents for the residual quantum visibility imperfection are examined, supported with theoretical analysis of the sequentially-triggered multipair biphoton contribution and techniques for visibility compensation, towards scalable high-bitrate quantum information processing and communications.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We demonstrate a high-contrast electro-optic modulation of a photonic crystal nanocavity integrated with an electrically gated monolayer graphene. A high quality (Q) factor air-slot nanocavity design is employed for high overlap between the optical field and graphene sheet. Tuning of graphene's Fermi level up to 0.8 eV enables efficient control of its complex dielectric constant, which allows modulation of the cavity reflection in excess of 10 dB for a swing voltage of only 1.5 V. We also observe a controllable resonance wavelength shift close to 2 nm around a wavelength of 1570 nm and a Q factor modulation in excess of three. These observations allow cavity-enhanced measurements of the graphene complex dielectric constant under different chemical potentials, in agreement with a theoretical model of the graphene dielectric constant under gating. This graphene-based nanocavity modulation demonstrates the feasibility of high-contrast, low-power frequency-selective electro-optic nanocavity modulators in graphene-integrated silicon photonic chips.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "We describe surface patterning strategies that permit high photon-collection efficiency together with high carrier-collection efficiency in an ultra-thin planar heterojunction organic photovoltaic cell. Optimized designs reach up to 50% photon collection efficiency in a P3HT layer of only 30 nm, representing a 3- to 5-fold improvement over an unpatterned cell of the same thickness. We compare the enhancement of light confinement in the active layer with an ITO top layer for TE and TM polarized light, and demonstrate that the light absorption can increase by a factor of 2 due to a gap-plasmon mode in the active layer.\n        \u25b3 Less", "author": "Dirk R. Englund"}, {"abstract": "This paper is primarily a semi-tutorial introduction to elementary algebraic topology and its applications to Ising-type models of statistical physics, using graphical models of linear and group codes. It contains new material on systematic (n,k) group codes and their information sets; normal realizations of (co)homology spaces; dual and hybrid models; and connections with system-theoretic concepts such as observability, controllability and input/output realizations.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "The Conti-Boston factorization theorem (CBFT) for linear tail-biting trellis realizations is extended to group realizations with a new and simpler proof, based on a controller granule decomposition of the behavior and known controllability results for group realizations. Further controllability results are given; e.g., a trellis realization is controllable if and only if its top (controllability) granule is trivial.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "This paper develops a fundamental theory of realizations of linear and group codes on general graphs using elementary group theory, including basic group duality theory. Principal new and extended results include: normal realization duality; analysis of systems-theoretic properties of fragments of realizations and their connections; \"minimal = trim and proper\" theorem for cycle-free codes; results showing that all constraint codes except interface nodes may be assumed to be trim and proper, and that the interesting part of a cyclic realization is its \"2-core;\" notions of observability and controllability for fragments, and related tests; relations between state-trimness and controllability, and dual state-trimness and observability.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "This paper investigates tail-biting trellis realizations for linear block codes. Intrinsic trellis properties are used to characterize irreducibility on given intervals of the time axis. It proves beneficial to always consider the trellis and its dual simultaneously. A major role is played by trellis properties that amount to observability and controllability for fragments of the trellis of various lengths. For fragments of length less than the minimum span length of the code it is shown that fragment observability and fragment controllability are equivalent to irreducibility. For reducible trellises, a constructive reduction procedure is presented. The considerations also lead to a characterization for when the dual of a trellis allows a product factorization into elementary (\"atomic\") trellises.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "This paper investigates properties of realizations of linear or group codes on general graphs that lead to local reducibility.\n  Trimness and properness are dual properties of constraint codes. A linear or group realization with a constraint code that is not both trim and proper is locally reducible. A linear or group realization on a finite cycle-free graph is minimal if and only if every local constraint code is trim and proper.\n  A realization is called observable if there is a one-to-one correspondence between codewords and configurations, and controllable if it has independent constraints. A linear or group realization is observable if and only if its dual is controllable. A simple counting test for controllability is given. An unobservable or uncontrollable realization is locally reducible. Parity-check realizations are controllable if and only if they have independent parity checks. In an uncontrollable tail-biting trellis realization, the behavior partitions into disconnected subbehaviors, but this property does not hold for non-trellis realizations. On a general graph, the support of an unobservable configuration is a generalized cycle.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "It is shown that a trellis realization can be locally reduced if it is not state-trim, branch-trim, proper, observable, and controllable. These conditions are not sufficient for local irreducibility. Making use of notions that amount to \"almost unobservability/uncontrollability\", a necessary and sufficient criterion of local irreducibility for tail-biting trellises is presented.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "This paper is concerned with the local reducibility properties of linear realizations of codes on finite graphs.\n  Trimness and properness are dual properties of constraint codes. A linear realization is locally reducible if any constraint code is not both trim and proper. On a finite cycle-free graph, a linear realization is minimal if and only if every constraint code is both trim and proper.\n  A linear realization is called observable if it is one-to-one, and controllable if all constraints are independent. Observability and controllability are dual properties. An unobservable or uncontrollable realization is locally reducible. A parity-check realization is uncontrollable if and only if it has redundant parity checks. A tail-biting trellis realization is uncontrollable if and only if its trajectories partition into disconnected subrealizations. General graphical realizations do not share this property.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "One of the most common types of functions in mathematics, physics, and engineering is a sum of products, sometimes called a partition function. After \"normalization,\" a sum of products has a natural graphical representation, called a normal factor graph (NFG), in which vertices represent factors, edges represent internal variables, and half-edges represent the external variables of the partition function. In physics, so-called trace diagrams share similar features. We believe that the conceptual framework of representing sums of products as partition functions of NFGs is an important and intuitive paradigm that, surprisingly, does not seem to have been introduced explicitly in the previous factor graph literature. Of particular interest are NFG modifications that leave the partition function invariant. A simple subclass of such NFG modifications offers a unifying view of the Fourier transform, tree-based reparameterization, loop calculus, and the Legendre transform.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "Shearer and McEliece [1977] showed that there is no MacWilliams identity for the free distance spectra of orthogonal linear convolutional codes. We show that on the other hand there does exist a MacWilliams identity between the generating functions of the weight distributions per unit time of a linear convolutional code C and its orthogonal code C^\\perp, and that this distribution is as useful as the free distance spectrum for estimating code performance. These observations are similar to those made recently by Bocharova, Hug, Johannesson and Kudryashov; however, we focus on terminating by tail-biting rather than by truncation.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "A conceptual framework involving partition functions of normal factor graphs is introduced, paralleling a similar recent development by Al-Bashabsheh and Mao. The partition functions of dual normal factor graphs are shown to be a Fourier transform pair, whether or not the graphs have cycles. The original normal graph duality theorem follows as a corollary. Within this framework, MacWilliams identities are found for various local and global weight generating functions of general group or linear codes on graphs; this generalizes and provides a concise proof of the MacWilliams identity for linear time-invariant convolutional codes that was recently found by Gluesing-Luerssen and Schneider. Further MacWilliams identities are developed for terminated convolutional codes, particularly for tail-biting codes, similar to those studied recently by Bocharova, Hug, Johannesson and Kudryashov.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "Given a controllable discrete-time linear system C, a shortest basis for C is a set of linearly independent generators for C with the least possible lengths. A basis B is a shortest basis if and only if it has the predictable span property (i.e., has the predictable delay and degree properties, and is non-catastrophic), or alternatively if and only if it has the subsystem basis property (for any interval J, the generators in B whose span is in J is a basis for the subsystem C_J). The dimensions of the minimal state spaces and minimal transition spaces of C are simply the numbers of generators in a shortest basis B that are active at any given state or symbol time, respectively. A minimal linear realization for C in controller canonical form follows directly from a shortest basis for C, and a minimal linear realization for C in observer canonical form follows directly from a shortest basis for the orthogonal system C^\\perp. This approach seems conceptually simpler than that of classical minimal realization theory.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "The MacWilliams identity for linear time-invariant convolutional codes that has recently been found by Gluesing-Luerssen and Schneider is proved concisely, and generalized to arbitrary group codes on graphs. A similar development yields a short, transparent proof of the dual sum-product update rule.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "Starting from Shannon's celebrated 1948 channel coding theorem, we trace the evolution of channel coding from Hamming codes to capacity-approaching codes. We focus on the contributions that have led to the most significant improvements in performance vs. complexity for practical applications, particularly on the additive white Gaussian noise (AWGN) channel. We discuss algebraic block codes, and why they did not prove to be the way to get to the Shannon limit. We trace the antecedents of today's capacity-approaching codes: convolutional codes, concatenated codes, and other probabilistic coding schemes. Finally, we sketch some of the practical applications of these codes.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "Rate-(n-2)/n unrestricted and CSS-type quantum convolutional codes with up to 4096 states and minimum distances up to 10 are constructed as stabilizer codes from classical self-orthogonal rate-1/n F_4-linear and binary linear convolutional codes, respectively. These codes generally have higher rate and less decoding complexity than comparable quantum block codes or previous quantum convolutional codes. Rate-(n-2)/n block stabilizer codes with the same rate and error-correction capability and essentially the same decoding algorithms are derived from these convolutional codes via tail-biting.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "The story of the Viterbi algorithm (VA) is told from a personal perspective. Applications both within and beyond communications are discussed. In brief summary, the VA has proved to be an extremely important algorithm in a surprising variety of fields.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "Simple rate-1/3 single-error-correcting unrestricted and CSS-type quantum convolutional codes are constructed from classical self-orthogonal $\\F_4$-linear and $\\F_2$-linear convolutional codes, respectively. These quantum convolutional codes have higher rate than comparable quantum block codes or previous quantum convolutional codes, and are simple to decode. A block single-error-correcting [9, 3, 3] tail-biting code is derived from the unrestricted convolutional code, and similarly a [15, 5, 3] CSS-type block code from the CSS-type convolutional code.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "We discuss why MMSE estimation arises in lattice-based schemes for approaching the capacity of linear Gaussian channels, and comment on its properties.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "We continue to discuss why MMSE estimation arises in coding schemes that approach the capacity of linear Gaussian channels. Here we consider schemes that involve successive decoding, such as decision-feedback equalization or successive cancellation.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "Fundamental results concerning the dynamics of abelian group codes (behaviors) and their duals are developed. Duals of sequence spaces over locally compact abelian groups may be defined via Pontryagin duality; dual group codes are orthogonal subgroups of dual sequence spaces. The dual of a complete code or system is finite, and the dual of a Laurent code or system is (anti-)Laurent. If C and C^\\perp are dual codes, then the state spaces of C act as the character groups of the state spaces of C^\\perp. The controllability properties of C are the observability properties of C^\\perp. In particular, C is (strongly) controllable if and only if C^\\perp is (strongly) observable, and the controller memory of C is the observer memory of C^\\perp. The controller granules of C act as the character groups of the observer granules of C^\\perp. Examples of minimal observer-form encoder and syndrome-former constructions are given. Finally, every observer granule of C is an \"end-around\" controller granule of C.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "Tight frames and rank-one quantum measurements are shown to be intimately related. In fact, the family of normalized tight frames for the space in which a quantum mechanical system lies is precisely the family of rank-one generalized quantum measurements (POVMs) on that space. Using this relationship, frame-theoretical analogues of various quantum-mechanical concepts and results are developed.\n  The analogue of a least-squares quantum measurement is a tight frame that is closest in a least-squares sense to a given set of vectors. The least-squares tight frame is found for both the case in which the scaling of the frame is specified (constrained least-squares frame (CLSF)) and the case in which the scaling is free (unconstrained least-squares frame (ULSF)). The well-known canonical frame is shown to be proportional to the ULSF and to coincide with the CLSF with a certain scaling.\n  Finally, the canonical frame vectors corresponding to a geometrically uniform vector set are shown to be geometrically uniform and to have the same symmetries as the original vector set.\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "In this paper we consider the problem of constructing measurements optimized to distinguish between a collection of possibly non-orthogonal quantum states. We consider a collection of pure states and seek a positive operator-valued measure (POVM) consisting of rank-one operators with measurement vectors closest in squared norm to the given states. We compare our results to previous measurements suggested by Peres and Wootters [Phys. Rev. Lett. 66, 1119 (1991)] and Hausladen et al. [Phys. Rev. A 54, 1869 (1996)], where we refer to the latter as the square-root measurement (SRM). We obtain a new characterization of the SRM, and prove that it is optimal in a least-squares sense. In addition, we show that for a geometrically uniform state set the SRM minimizes the probability of a detection error. This generalizes a similar result of Ban et al. [Int. J. Theor. Phys. 36, 1269 (1997)].\n        \u25b3 Less", "author": "David Forney"}, {"abstract": "The DUNE IDR describes the proposed physics program and technical designs of the DUNE far detector modules in preparation for the full TDR to be published in 2019. It is intended as an intermediate milestone on the path to a full TDR, justifying the technical choices that flow down from the high-level physics goals through requirements at all levels of the Project. These design choices will enable the DUNE experiment to make the ground-breaking discoveries that will help to answer fundamental physics questions. Volume 3 describes the dual-phase module's subsystems, the technical coordination required for its design, construction, installation, and integration, and its organizational structure.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "The DUNE IDR describes the proposed physics program and technical designs of the DUNE Far Detector modules in preparation for the full TDR to be published in 2019. It is intended as an intermediate milestone on the path to a full TDR, justifying the technical choices that flow down from the high-level physics goals through requirements at all levels of the Project. These design choices will enable the DUNE experiment to make the ground-breaking discoveries that will help to answer fundamental physics questions. Volume 1 contains an executive summary that describes the general aims of this document. The remainder of this first volume provides a more detailed description of the DUNE physics program that drives the choice of detector technologies. It also includes concise outlines of two overarching systems that have not yet evolved to consortium structures: computing and calibration. Volumes 2 and 3 of this IDR describe, for the single-phase and dual-phase technologies, respectively, each detector module's subsystems, the technical coordination required for its design, construction, installation, and integration, and its organizational structure.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "The DUNE IDR describes the proposed physics program and technical designs of the DUNE far detector modules in preparation for the full TDR to be published in 2019. It is intended as an intermediate milestone on the path to a full TDR, justifying the technical choices that flow down from the high-level physics goals through requirements at all levels of the Project. These design choices will enable the DUNE experiment to make the ground-breaking discoveries that will help to answer fundamental physics questions. Volume 2 describes the single-phase module's subsystems, the technical coordination required for its design, construction, installation, and integration, and its organizational structure.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "Open cluster members are coeval and share the same initial bulk chemical compositions. Consequently, differences in surface abundances between members of a cluster that are at different evolutionary stages can be used to study the effects of mixing and internal chemical processing. We carry out an abundance analysis of seven elements (Li, O, Na, Mg, Al, Si, Fe) in 66 stars belonging to the open cluster M67, based on high resolution GALAH spectra, 1D MARCS model atmospheres, and, for the first time for a large sample of M67 stars, non-local thermodynamic equilibrium (non-LTE) radiative transfer. From the non-LTE analysis, we find a typical star-to-star scatter in the abundance ratios of around 0.05 dex; this scatter is slightly but systematically larger when LTE is assumed instead. We find trends in the abundance ratios with effective temperature, indicating systematic differences in the surface abundances between turn-off and giant stars; these trends are more pronounced when LTE is assumed. However, in the non-LTE analysis, most of the element trends have been flattened. Two species are exceptions to this behaviour, namely Al and Si, which both clearly display remaining trends in the non-LTE analysis. We comment on the possible origin of these trends, by comparing them with recent stellar models that include atomic diffusion.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "GALAH is a large-scale magnitude-limited southern stellar spectroscopic survey. Its second data release (GALAH DR2) provides values of stellar parameters and abundances of 23 elements for 342,682 stars (Buder et al.). Here we add a description of the public release of radial velocities with a typical accuracy of 0.1 km/s for 336,215 of these stars, achievable due to the large wavelength coverage, high resolving power and good signal to noise ratio of the observed spectra, but also because convective motions in stellar atmosphere and gravitational redshift from the star to the observer are taken into account. In the process we derive medians of observed spectra which are nearly noiseless, as they are obtained from between 100 and 1116 observed spectra belonging to the same bin with a width of 50 K in temperature, 0.2 dex in gravity, and 0.1 dex in metallicity. Publicly released 1181 median spectra have a resolving power of 28,000 and trace the well-populated stellar types with metallicities between -0.6 and +0.3. Note that radial velocities from GALAH are an excellent match to the accuracy of velocity components along the sky plane derived by Gaia for the same stars. The level of accuracy achieved here is adequate for studies of dynamics within stellar clusters, associations and streams in the Galaxy. So it may be relevant for studies of the distribution of dark matter.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "The Galactic Archaeology with HERMES (GALAH) survey is a large-scale stellar spectroscopic survey of the Milky Way and designed to deliver chemical information complementary to a large number of stars covered by the $Gaia$ mission. We present the GALAH second public data release (GALAH DR2) containing 342,682 stars. For these stars, the GALAH collaboration provides stellar parameters and abundances for up to 23 elements to the community. Here we present the target selection, observation, data reduction and detailed explanation of how the spectra were analysed to estimate stellar parameters and element abundances. For the stellar analysis, we have used a multi-step approach. We use the physics-driven spectrum synthesis of Spectroscopy Made Easy (SME) to derive stellar labels ($T_\\mathrm{eff}$, $\\log g$, $\\mathrm{[Fe/H]}$, $\\mathrm{[X/Fe]}$, $v_\\mathrm{mic}$, $v \\sin i$, $A_{K_S}$) for a representative training set of stars. This information is then propagated to the whole survey with the data-driven method of $The~Cannon$. Special care has been exercised in the spectral synthesis to only consider spectral lines that have reliable atomic input data and are little affected by blending lines. Departures from local thermodynamic equilibrium (LTE) are considered for several key elements, including Li, O, Na, Mg, Al, Si, and Fe, using 1D MARCS stellar atmosphere models. Validation tests including repeat observations, Gaia benchmark stars, open and globular clusters, and K2 asteroseismic targets lend confidence in our methods and results. Combining the GALAH DR2 catalogue with the kinematic information from $Gaia$ will enable a wide range of Galactic Archaeology studies, with unprecedented detail, dimensionality, and scope.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "We report the serendipitous observations of 571 luminous supergiants in the Magellanic Clouds by the spectroscopic GALAH and TESS-HERMES surveys: 434 stars in the Large Magellanic Cloud and 137 in the Small Magellanic Cloud. We also find one star that appears associated with structured star formation in the Magellanic Bridge. Both of these surveys are aimed at the local volume of the Galaxy but have simple, magnitude-limited selection functions that mean they include some observations of luminous extra-Galactic stars. The surveys determine stellar parameter and abundances using The Cannon, a data-driven generative modelling approach. In this work, we explore the results from The Cannon when it is fed the spectra of these intrinsically luminous supergiants in the Magellanic Clouds, which are well outside the normal bounds of The Cannon's training set. We find that, although the parameters are astrophysically incorrect, the $v\\sin i$ and the abundances of lithium, barium, and magnesium are excellent discriminants of these stars. It shows that in the future, with an expanded training set, it should be possible to determine accurate values for these types of stars.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "Accurate and precise radius estimates of transiting exoplanets are critical for understanding their compositions and formation mechanisms. To know the planet, we must know the host star in as much detail as possible. We present first results from the K2-HERMES project, which uses the HERMES multi-object spectrograph on the Anglo-Australian Telescope to obtain R$\\sim$28,000 spectra of up to 360 stars in one exposure. This ongoing project aims to derive self-consistent spectroscopic parameters for about half of K2 target stars. We present complete stellar parameters and isochrone-derived masses and radii for 46 stars hosting 57 K2 candidate planets in Campaigns 1-3. Our revised host-star radii cast severe doubt on three candidate planets: EPIC\\,201407812.01, EPIC\\,203070421.01, and EPIC\\,202843107.01, all of which now have inferred radii well in excess of the largest known inflated Jovian planets.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "The technique of chemical tagging uses the elemental abundances of stellar atmospheres to `reconstruct' chemically homogeneous star clusters that have long since dispersed. The GALAH spectroscopic survey --which aims to observe one million stars using the Anglo-Australian Telescope -- allows us to measure up to 30 elements or dimensions in the stellar chemical abundance space, many of which are not independent. How to find clustering reliably in a noisy high-dimensional space is a difficult problem that remains largely unsolved. Here we explore t-distributed stochastic neighbour embedding (t-SNE) -- which identifies an optimal mapping of a high-dimensional space into fewer dimensions -- whilst conserving the original clustering information. Typically, the projection is made to a 2D space to aid recognition of clusters by eye. We show that this method is a reliable tool for chemical tagging because it can: (i) resolve clustering in chemical space alone, (ii) recover known open and globular clusters with high efficiency and low contamination, and (iii) relate field stars to known clusters. t-SNE also provides a useful visualization of a high-dimensional space. We demonstrate the method on a dataset of 13 abundances measured in the spectra of 187,000 stars by the GALAH survey. We recover 7 of the 9 observed clusters (6 globular and 3 open clusters) in chemical space with minimal contamination from field stars and low numbers of outliers. With chemical tagging, we also identify two Pleiades supercluster members (which we confirm kinematically), one as far as 6$^\\circ$ -- one tidal radius away from the cluster centre.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "The Transiting Exoplanet Survey Satellite (TESS) will provide high precision time-series photometry for millions of stars with at least a half-hour cadence. Of particular interest are the circular regions of 12-degree radius centered around the ecliptic poles that will be observed continuously for a full year. Spectroscopic stellar parameters are desirable to characterize and select suitable targets for TESS, whether they are focused on exploring exoplanets, stellar astrophysics, or Galactic archaeology. Here, we present spectroscopic stellar parameters ($T_{\\rm eff}$, $\\log g$, [Fe/H], $v \\sin i$, $v_{\\rm micro}$) for about 16,000 dwarf and subgiant stars in TESS' southern continuous viewing zone. For almost all the stars, we also present Bayesian estimates of stellar properties including distance, extinction, mass, radius, and age using theoretical isochrones. Stellar surface gravity and radius are made available for an additional set of roughly 8,500 red giants. All our target stars are in the range $10<V<13.1$. Among them, we identify and list 227 stars belonging to the Large Magellanic Cloud. The data were taken using the the High Efficiency and Resolution Multi-Element Spectrograph (HERMES, R $\\sim 28,000$) at the Anglo-Australian Telescope as part of the TESS-HERMES survey. Comparing our results with the TESS Input Catalog (TIC) shows that the TIC is generally efficient in separating dwarfs and giants, but it has flagged more than hundred cool dwarfs ($T_{\\rm eff}< 4800$ K) as giants, which ought to be high-priority targets for the exoplanet search. The catalog can be accessed via http://www.physics.usyd.edu.au/tess-hermes/ , or at MAST via https://archive.stsci.edu/prepds/tess-hermes/ .\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "ProtoDUNE-SP is the single-phase DUNE Far Detector prototype that is under construction and will be operated at the CERN Neutrino Platform (NP) starting in 2018. ProtoDUNE-SP, a crucial part of the DUNE effort towards the construction of the first DUNE 10-kt fiducial mass far detector module (17 kt total LAr mass), is a significant experiment in its own right. With a total liquid argon (LAr) mass of 0.77 kt, it represents the largest monolithic single-phase LArTPC detector to be built to date. It's technical design is given in this report.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "The Galactic Archaeology with HERMES (GALAH) Survey is a massive observational project to trace the Milky Way's history of star formation, chemical enrichment, stellar migration and minor mergers. Using high-resolution (R$\\simeq$28,000) spectra taken with the High Efficiency and Resolution Multi-Element Spectrograph (HERMES) instrument at the Anglo-Australian Telescope (AAT), GALAH will determine stellar parameters and abundances of up to 29 elements for up to one million stars. Selecting targets from a colour-unbiased catalogue built from 2MASS, APASS and UCAC4 data, we expect to observe dwarfs at 0.3 to 3 kpc and giants at 1 to 10 kpc. This enables a thorough local chemical inventory of the Galactic thin and thick disks, and also captures smaller samples of the bulge and halo. In this paper we present the plan, process and progress as of early 2016 for GALAH survey observations. In our first two years of survey observing we have accumulated the largest high-quality spectroscopic data set at this resolution, over 200,000 stars. We also present the first public GALAH data catalogue: stellar parameters (Teff, log(g), [Fe/H], [alpha/Fe]), radial velocity, distance modulus and reddening for 10680 observations of 9860 Tycho-2 stars that may be included in the first Gaia data release.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "We present the data reduction procedures being used by the GALAH survey, carried out with the HERMES fibre-fed, multi-object spectrograph on the 3.9~m Anglo-Australian Telescope. GALAH is a unique survey, targeting 1 million stars brighter than magnitude V=14 at a resolution of 28,000 with a goal to measure the abundances of 29 elements. Such a large number of high resolution spectra necessitates the development of a reduction pipeline optimized for speed, accuracy, and consistency. We outline the design and structure of the Iraf-based reduction pipeline that we developed, specifically for GALAH, to produce fully calibrated spectra aimed for subsequent stellar atmospheric parameter estimation. The pipeline takes advantage of existing Iraf routines and other readily available software so as to be simple to maintain, testable and reliable. A radial velocity and stellar atmospheric parameter estimator code is also presented, which is used for further data analysis and yields a useful verification of the reduction quality. We have used this estimator to quantify the data quality of GALAH for fibre cross-talk level ($\\lesssim0.5$%) and scattered light ($\\sim5$ counts in a typical 20 minutes exposure), resolution across the field, sky spectrum properties, wavelength solution reliability (better than $1$ $\\mathrm{km\\ s^{-1}}$ accuracy) and radial velocity precision.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "The ACS Nearby Galaxy Survey Treasury (ANGST) is a systematic survey to establish a legacy of uniform multi-color photometry of resolved stars for a volume-limited sample of nearby galaxies (D<4 Mpc). The survey volume encompasses 69 galaxies in diverse environments, including close pairs, small & large groups, filaments, and truly isolated regions. The galaxies include a nearly complete range of morphological types spanning a factor of ~10^4 in luminosity and star formation rate. The survey data consists of images taken with ACS on HST, supplemented with archival data and new WFPC2 imaging taken after the failure of ACS. Survey images include wide field tilings covering the full radial extent of each galaxy, and single deep pointings in uncrowded regions of the most massive galaxies in the volume. The new wide field imaging in ANGST reaches median 50% completenesses of m_F475W=28.0 mag, m_F606W=27.3 mag, and m_F814W=27.3 mag, several magnitudes below the tip of the red giant branch (TRGB). The deep fields reach magnitudes sufficient to fully resolve the structure in the red clump. The resulting photometric catalogs are publicly accessible and contain over 34 million photometric measurements of >14 million stars. In this paper we present the details of the sample selection, imaging, data reduction, and the resulting photometric catalogs, along with an analysis of the photometric uncertainties (systematic and random), for both the ACS and WFPC2 imaging. We also present uniformly derived relative distances measured from the apparent magnitude of the TRGB.\n        \u25b3 Less", "author": "Dennis Freeman"}, {"abstract": "We use extensive spectroscopy from the MOSFIRE Deep Evolution Field (MOSDEF) survey to investigate the relationships between rest-frame optical emission line equivalent widths ($W$) and a number of galaxy and ISM characteristics for a sample of $1134$ star-forming galaxies at redshifts $1.4\\lesssim z\\lesssim 3.8$. We examine how the equivalent widths of [OII]$\u03bb\u03bb3727, 3730$, H$\u03b2$, [OIII]$\u03bb\u03bb4960, 5008$, [OIII]$+$H$\u03b2$, H$\u03b1$, and H$\u03b1$+[NII]$\u03bb\u03bb6550, 6585$, depend on stellar mass, UV slope, age, star-formation rate (SFR) and specific SFR (sSFR), ionization parameter and excitation conditions (O32 and [OIII]/H$\u03b2$), gas-phase metallicity, and ionizing photon production efficiency ($\u03be_{\\rm ion}$). The trend of increasing $W$ with decreasing stellar mass is strongest for [OIII] (and [OIII]+H$\u03b2$). More generally, the equivalent widths of all the lines increase with redshift at a fixed stellar mass or fixed gas-phase metallicity, suggesting that high equivalent width galaxies are common at high redshift. This redshift evolution in equivalent widths can be explained by the increase in SFR and decrease in metallicity with redshift at a fixed stellar mass. Consequently, the dependence of $W$ on sSFR is largely invariant with redshift, particularly when examined for galaxies of a given metallicity. Our results show that high equivalent width galaxies, specifically those with high $W({\\rm [OIII]})$, have low stellar masses, blue UV slopes, young ages, high sSFRs, ISM line ratios indicative of high ionization parameters, high $\u03be_{\\rm ion}$, and low metallicities. As these characteristics are often attributed to galaxies with high ionizing escape fractions, galaxies with high $W$ are likely candidates for the population that dominates cosmic reionization.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n  In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Deep neural networks, trained with large amount of labeled data, can fail to generalize well when tested with examples from a \\emph{target domain} whose distribution differs from the training data distribution, referred as the \\emph{source domain}. It can be expensive or even infeasible to obtain required amount of labeled data in all possible domains. Unsupervised domain adaptation sets out to address this problem, aiming to learn a good predictive model for the target domain using labeled examples from the source domain but only unlabeled examples from the target domain. Domain alignment approaches this problem by matching the source and target feature distributions, and has been used as a key component in many state-of-the-art domain adaptation methods. However, matching the marginal feature distributions does not guarantee that the corresponding class conditional distributions will be aligned across the two domains. We propose co-regularized domain alignment for unsupervised domain adaptation, which constructs multiple diverse feature spaces and aligns source and target distributions in each of them individually, while encouraging that alignments agree with each other with regard to the class predictions on the unlabeled target examples. The proposed method is generic and can be used to improve any domain adaptation method which uses domain alignment. We instantiate it in the context of a recent state-of-the-art method and observe that it provides significant performance improvements on several domain adaptation benchmarks.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Using the MOSDEF rest-frame optical spectroscopic survey, we investigate the star-formation histories (SFHs) of different galaxy types, ranging from actively star forming to quiescent at $1.4\\leq~z\\leq2.6$. SFHs are constrained utilizing stellar continuum spectroscopy, specifically through a combination of Balmer absorption lines, the 4000$\u00c5$ break, and the equivalent width of the H$\u03b1$ emission line. To attain a sufficiently high signal-to-noise ratio (S/N) to conduct these measurements we stack spectra of galaxies with similar spectral types, as determined from their rest-frame $U-V$ and $V-J$ colors. We bin the MOSDEF sample into five spectral types, subdividing the quiescent and star-forming bins to better explore galaxies transitioning between the two. We constrain the average SFHs for each type, finding that quiescent and transitional galaxies in the MOSDEF sample are dominated by an SFH with an average star-formation timescale of $\u03c4\\sim0.1-0.2$~Gyr. These findings contrast with measurements from the low-redshift Universe where, on average, galaxies form their stars over a more extended time period ($\u03c4>1$~Gyr). Furthermore, our spectral index measurements correlate with mass surface density for all spectral types. Finally, we compare the average properties of the galaxies in our transitional bins to investigate possible paths to quiescence, and speculate on the viability of a dusty post-starburst phase.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and therefore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects including contact and can be seamlessly incorporated into inference, control and co-design systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of control tasks for soft robots, including problems with nearly 3,000 decision variables.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present a system that allows users to visualize complex human motion via 3D motion sculptures---a representation that conveys the 3D structure swept by a human body as it moves through space. Given an input video, our system computes the motion sculptures and provides a user interface for rendering it in different styles, including the options to insert the sculpture back into the original video, render it in a synthetic scene or physically print it.\n  To provide this end-to-end workflow, we introduce an algorithm that estimates that human's 3D geometry over time from a set of 2D images and develop a 3D-aware image-based rendering approach that embeds the sculpture back into the scene. By automating the process, our system takes motion sculpture creation out of the realm of professional artists, and makes it applicable to a wide range of existing video material.\n  By providing viewers with 3D information, motion sculptures reveal space-time motion information that is difficult to perceive with the naked eye, and allow viewers to interpret how different parts of the object interact over time. We validate the effectiveness of this approach with user studies, finding that our motion sculpture visualizations are significantly more informative about motion than existing stroboscopic and space-time visualization methods.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Objects are made of parts, each with distinct geometry, physics, functionality, and affordances. Developing such a distributed, physical, interpretable representation of objects will facilitate intelligent agents to better explore and interact with the world. In this paper, we study physical primitive decomposition---understanding an object through its components, each with physical and geometric attributes. As annotated data for object parts and physics are rare, we propose a novel formulation that learns physical primitives by explaining both an object's appearance and its behaviors in physical events. Our model performs well on block towers and tools in both synthetic and real scenarios; we also demonstrate that visual and physical observations often provide complementary signals. We further present ablation and behavioral studies to better understand our model and contrast it with human performance.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "The problem of single-view 3D shape completion or reconstruction is challenging, because among the many possible shapes that explain an observation, most are implausible and do not correspond to natural objects. Recent research in the field has tackled this problem by exploiting the expressiveness of deep convolutional networks. In fact, there is another level of ambiguity that is often overlooked: among plausible shapes, there are still multiple shapes that fit the 2D image equally well; i.e., the ground truth shape is non-deterministic given a single-view input. Existing fully supervised approaches fail to address this issue, and often produce blurry mean shapes with smooth surfaces but no fine details.\n  In this paper, we propose ShapeHD, pushing the limit of single-view shape completion and reconstruction by integrating deep generative models with adversarially learned shape priors. The learned priors serve as a regularizer, penalizing the model only if its output is unrealistic, not if it deviates from the ground truth. Our design thus overcomes both levels of ambiguity aforementioned. Experiments demonstrate that ShapeHD outperforms state of the art by a large margin in both shape completion and shape reconstruction on multiple real datasets.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Humans recognize object structure from both their appearance and motion; often, motion helps to resolve ambiguities in object structure that arise when we observe object appearance only. There are particular scenarios, however, where neither appearance nor spatial-temporal motion signals are informative: occluding twigs may look connected and have almost identical movements, though they belong to different, possibly disconnected branches. We propose to tackle this problem through spectrum analysis of motion signals, because vibrations of disconnected branches, though visually similar, often have distinctive natural frequencies. We propose a novel formulation of tree structure based on a physics-based link model, and validate its effectiveness by theoretical analysis, numerical simulation, and empirical experiments. With this formulation, we use nonparametric Bayesian inference to reconstruct tree structure from both spectral vibration signals and appearance cues. Our model performs well in recognizing hierarchical tree structure from real-world videos of trees and vessels.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We study the properties of 32 spectroscopically-identified pairs of galaxies observed during the peak epoch of star formation in the universe. These systems are drawn from the MOSFIRE Deep Evolution Field (MOSDEF) Survey at $1.4 \\leq z \\leq 3.8$, and are interpreted as early-stage galaxy mergers. Galaxy pairs in our sample are identified as two objects whose spectra were collected on the same Keck/MOSFIRE spectroscopic slit. Accordingly, all pairs in the sample have projected separations $R_{\\rm proj}\\leq 60$~kpc. The redshift separation for pairs was required to be $\u0394z \\leq 0.01$ ($\u0394v \\lesssim 1000 \\mbox{ km s}^{-1}$), but, in practice, all but two pairs have velocity separations consistent with $\u0394v \\leq 500 \\mbox{ km s}^{-1}$, which is the standard threshold for defining interacting galaxy pairs at low redshift. Stellar mass ratios in our sample range from 1.1 to 550, with 13 ratios closer than 3:1, the common definition of a \"major merger.\" Studies of merging pairs in the local universe indicate an enhancement in star-formation activity and deficit in gas-phase oxygen abundance relative to isolated galaxies of the same mass. We compare the MOSDEF pairs sample to a control sample of isolated galaxies at the same redshift, finding no measurable SFR enhancement or metallicity deficit at fixed stellar mass for the pairs sample, in contrast to low-redshift studies. These results are consistent with some theoretical works suggesting a reduced differential effect of pre-coalescence mergers on galaxy properties at high redshift -- specifically that pre-coalescence mergers do not drive strong starbursts.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous scene representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for semantics, geometry, and appearance into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object-wise representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance supports 3D-aware scene manipulation, e.g., rotating and moving objects freely while keeping the consistent shape and texture, and changing the object appearance without affecting its shape. Experiments demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present an algorithm for creating high resolution anatomically plausible images consistent with acquired clinical brain MRI scans with large inter-slice spacing. Although large data sets of clinical images contain a wealth of information, time constraints during acquisition result in sparse scans that fail to capture much of the anatomy. These characteristics often render computational analysis impractical as many image analysis algorithms tend to fail when applied to such images. Highly specialized algorithms that explicitly handle sparse slice spacing do not generalize well across problem domains. In contrast, we aim to enable application of existing algorithms that were originally developed for high resolution research scans to significantly undersampled scans. We introduce a generative model that captures fine-scale anatomical structure across subjects in clinical image collections and derive an algorithm for filling in the missing data in scans with large inter-slice spacing. Our experimental results demonstrate that the resulting method outperforms state-of-the-art upsampling super-resolution techniques, and promises to facilitate subsequent analysis not previously possible with scans of this quality. Our implementation is freely available at https://github.com/adalca/papago .\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Perceiving accurate 3D object shape is important for robots to interact with the physical world. Current research along this direction has been primarily relying on visual observations. Vision, however useful, has inherent limitations due to occlusions and the 2D-3D ambiguities, especially for perception with a monocular camera. In contrast, touch gets precise local shape information, though its efficiency for reconstructing the entire shape could be low. In this paper, we propose a novel paradigm that efficiently perceives accurate 3D object shape by incorporating visual and tactile observations, as well as prior knowledge of common object shapes learned from large-scale shape repositories. We use vision first, applying neural networks with learned shape priors to predict an object's 3D shape from a single-view color image. We then use tactile sensing to refine the shape; the robot actively touches the object regions where the visual prediction has high uncertainty. Our method efficiently builds the 3D shape of common objects from a color image and a small number of tactile explorations (around 10). Our setup is easy to apply and has potentials to help robots better perform grasping or manipulation tasks on real-world objects.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods that have tackled this problem in a deterministic or non-parametric way, we propose to model future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, and on real-world video frames. We present analyses of the learned network representations, showing it is implicitly learning a compact encoding of object appearance and motion. We also demonstrate a few of its applications, including visual analogy-making and video extrapolation.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present an analysis using the MOSFIRE Deep Evolution Field (MOSDEF) survey on the nature of \"MIR-excess\" galaxies, which have star formation rates (SFR) inferred from mid-infrared (MIR) data that is substantially elevated relative to that estimated from dust-corrected UV data. We use a sample of $\\sim$200 galaxies and AGN at $1.40<z<2.61$ with 24 $\u03bc$m detections (rest-frame 8$\u03bc$m) from MIPS/\\textit{Spitzer}. We find that the identification of MIR-excess galaxies strongly depends on the methodologies used to estimate IR luminosity ($\\rm L_{IR}$) and to correct the UV light for dust attenuation. We find that extrapolations of the SFR from the observed 24 $\u03bc$m flux, using luminosity-dependent templates based on local galaxies, substantially overestimate $\\rm L_{IR}$ in $z\\sim2$ galaxies. By including \\textit{Herschel} observations and using a stellar mass-dependent, luminosity-independent $\\rm L_{IR}$, we obtain more reliable estimates of the SFR and a lower fraction of MIR-excess galaxies. Once stellar mass selection biases are taken into account, we identify $\\sim24\\%$ of our galaxies as MIR-excess. However, $\\rm SFR_{H\u03b1}$ is not elevated in MIR-excess galaxies compared to MIR-normal galaxies, indicating that the intrinsic fraction of MIR-excess may be lower. Using X-ray, IR, and optically-selected AGN in MOSDEF, we do not find a higher prevalence for AGN in MIR-excess galaxies relative to MIR-normal galaxies. A stacking analysis of X-ray undetected galaxies does not reveal a harder spectrum in MIR-excess galaxies relative to MIR-normal galaxies. Our analysis indicates that AGN activity does not contribute substantially to the MIR excess and instead implies that it is likely due to the enhanced PAH emission.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We study 3D shape modeling from a single image and make contributions to it in three aspects. First, we present Pix3D, a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc. Building such a large-scale dataset, however, is highly challenging; existing datasets either contain only synthetic data, or lack precise alignment between 2D images and 3D shapes, or only have a small number of images. Second, we calibrate the evaluation criteria for 3D shape reconstruction through behavioral studies, and use them to objectively and systematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction and pose estimation; our multi-task learning approach achieves state-of-the-art performance on both tasks.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Video motion magnification techniques allow us to see small motions previously invisible to the naked eyes, such as those of vibrating airplane wings, or swaying buildings under the influence of the wind. Because the motion is small, the magnification results are prone to noise or excessive blurring. The state of the art relies on hand-designed filters to extract representations that may not be optimal. In this paper, we seek to learn the filters directly from examples using deep convolutional neural networks. To make training tractable, we carefully design a synthetic dataset that captures small motion well, and use two-frame input for training. We show that the learned filters achieve high-quality results on real videos, with less ringing artifacts and better noise characteristics than previous methods. While our model is not trained with temporal filters, we found that the temporal filters can be used with our extracted representations up to a moderate magnification, enabling a frequency-based motion selection. Finally, we analyze the learned filters and show that they behave similarly to the derivative filters used in previous works. Our code, trained model, and datasets will be available online.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Understanding 3D object structure from a single image is an important but challenging task in computer vision, mostly due to the lack of 3D object annotations to real images. Previous research tackled this problem by either searching for a 3D shape that best explains 2D annotations, or training purely on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Networks (3D-INN), an end-to-end trainable framework that sequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses. Our system learns from both 2D-annotated real images and synthetic 3D data. This is made possible mainly by two technical innovations. First, heatmaps of 2D keypoints serve as an intermediate representation to connect real and synthetic data. 3D-INN is trained on real images to estimate 2D keypoint heatmaps from an input image; it then predicts 3D object structure from heatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN benefits from the variation and abundance of synthetic 3D objects, without suffering from the domain difference between real and synthesized images, often due to imperfect rendering. Second, we propose a Projection Layer, mapping estimated 3D structure back to 2D. During training, it ensures 3D-INN to predict 3D structure whose projection is consistent with the 2D annotations to real images. Experiments show that the proposed system performs well on both 2D keypoint estimation and 3D structure recovery. We also demonstrate that the recovered 3D information has wide vision applications, such as image retrieval.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We study the problem of reconstructing an image from information stored at contour locations. We show that high-quality reconstructions with high fidelity to the source image can be obtained from sparse input, e.g., comprising less than $6\\%$ of image pixels. This is a significant improvement over existing contour-based reconstruction methods that require much denser input to capture subtle texture information and to ensure image quality. Our model, based on generative adversarial networks, synthesizes texture and details in regions where no input information is provided. The semantic knowledge encoded into our model and the sparsity of the input allows to use contours as an intuitive interface for semantically-aware image manipulation: local edits in contour domain translate to long-range and coherent changes in pixel space. We can perform complex structural changes such as changing facial expression by simple edits of contours. Our experiments demonstrate that humans as well as a face recognition system mostly cannot distinguish between our reconstructions and the source images.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "The sound of crashing waves, the roar of fast-moving cars -- sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds. This paper extends an earlier conference paper, Owens et al. 2016, with additional experiments and discussion.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Many video processing algorithms rely on optical flow to register different frames within a sequence. However, a precise estimation of optical flow is often neither tractable nor optimal for a particular task. In this paper, we propose task-oriented flow (TOFlow), a flow representation tailored for specific video processing tasks. We design a neural network with a motion estimation component and a video processing component. These two parts can be jointly trained in a self-supervised manner to facilitate learning of the proposed TOFlow. We demonstrate that TOFlow outperforms the traditional optical flow on three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution. We also introduce Vimeo-90K, a large-scale, high-quality video dataset for video processing to better evaluate the proposed algorithm.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Active non-line-of-sight imaging systems are of growing interest for diverse applications. The most commonly proposed approaches to date rely on exploiting time-resolved measurements, i.e., measuring the time it takes for short light pulses to transit the scene. This typically requires expensive, specialized, ultrafast lasers and detectors that must be carefully calibrated. We develop an alternative approach that exploits the valuable role that natural occluders in a scene play in enabling accurate and practical image formation in such settings without such hardware complexity. In particular, we demonstrate that the presence of occluders in the hidden scene can obviate the need for collecting time-resolved measurements, and develop an accompanying analysis for such systems and their generalizations. Ultimately, the results suggest the potential to develop increasingly sophisticated future systems that are able to identify and exploit diverse structural features of the environment to reconstruct scenes hidden from view.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "3D object reconstruction from a single image is a highly under-determined problem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenges for learning-based approaches, as 3D object annotations are scarce in real images. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from domain adaptation when tested on real data. In this work, we propose MarrNet, an end-to-end trainable model that sequentially estimates 2.5D sketches and 3D object shape. Our disentangled, two-step formulation has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data. Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data. This is because we can easily render realistic 2.5D sketches without modeling object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations. Our model achieves state-of-the-art performance on 3D shape reconstruction.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Very long baseline interferometry (VLBI) makes it possible to recover images of astronomical sources with extremely high angular resolution. Most recently, the Event Horizon Telescope (EHT) has extended VLBI to short millimeter wavelengths with a goal of achieving angular resolution sufficient for imaging the event horizons of nearby supermassive black holes. VLBI provides measurements related to the underlying source image through a sparse set spatial frequencies. An image can then be recovered from these measurements by making assumptions about the underlying image. One of the most important assumptions made by conventional imaging methods is that over the course of a night's observation the image is static. However, for quickly evolving sources, such as the galactic center's supermassive black hole (Sgr A*) targeted by the EHT, this assumption is violated and these conventional imaging approaches fail. In this work we propose a new way to model VLBI measurements that allows us to recover both the appearance and dynamics of an evolving source by reconstructing a video rather than a static image. By modeling VLBI measurements using a Gaussian Markov Model, we are able to propagate information across observations in time to reconstruct a video, while simultaneously learning about the dynamics of the source's emission region. We demonstrate our proposed Expectation-Maximization (EM) algorithm, StarWarps, on realistic synthetic observations of black holes, and show how it substantially improves results compared to conventional imaging algorithms. Additionally, we demonstrate StarWarps on real VLBI data of the M87 Jet from the VLBA.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We investigate the nature of the relation among stellar mass, star-formation rate, and gas-phase metallicity (the M$_*$-SFR-Z relation) at high redshifts using a sample of 260 star-forming galaxies at $z\\sim2.3$ from the MOSDEF survey. We present an analysis of the high-redshift M$_*$-SFR-Z relation based on several emission-line ratios for the first time. We show that a M$_*$-SFR-Z relation clearly exists at $z\\sim2.3$. The strength of this relation is similar to predictions from cosmological hydrodynamical simulations. By performing a direct comparison of stacks of $z\\sim0$ and $z\\sim2.3$ galaxies, we find that $z\\sim2.3$ galaxies have $\\sim0.1$ dex lower metallicity at fixed M$_*$ and SFR. In the context of chemical evolution models, this evolution of the M$_*$-SFR-Z relation suggests an increase with redshift of the mass-loading factor at fixed M$_*$, as well as a decrease in the metallicity of infalling gas that is likely due to a lower importance of gas recycling relative to accretion from the intergalactic medium at high redshifts. Performing this analysis simultaneously with multiple metallicity-sensitive line ratios allows us to rule out the evolution in physical conditions (e.g., N/O ratio, ionization parameter, and hardness of the ionizing spectrum) at fixed metallicity as the source of the observed trends with redshift and with SFR at fixed M$_*$ at $z\\sim2.3$. While this study highlights the promise of performing high-order tests of chemical evolution models at high redshifts, detailed quantitative comparisons ultimately await a full understanding of the evolution of metallicity calibrations with redshift.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We combine spectroscopic measurements of H$\u03b1$ and H$\u03b2$ and UV continuum photometry for a sample of 673 galaxies from the MOSFIRE Deep Evolution Field survey to constrain hydrogen ionizing photon production efficiencies ($\u03be_{\\rm ion}$, xi_ion) at z=1.4-2.6. We find average log(xi_ion/[Hz erg$^{-1}$])=25.06 (25.34), assuming the Calzetti (SMC) curve for the UV dust correction and a scatter of 0.28 dex in xi_ion distribution. After accounting for observational uncertainties and variations in dust attenuation, we conclude that the remaining scatter in xi_ion is likely dominated by galaxy-to-galaxy variations in stellar populations, including the slope and upper-mass cutoff of the initial mass function, stellar metallicity, star-formation burstiness, and stellar evolution (e.g., single/binary star evolution). Moreover, xi_ion is elevated in galaxies with high ionization states (high [OIII]/[OII]) and low oxygen abundances (low [NII]/H$\u03b1$ and high [OIII]/H$\u03b2$) in the ionized ISM. However, xi_ion does not correlate with the offset from the z~0 star-forming locus in the BPT diagram, suggesting no change in the hardness of ionizing radiation accompanying the offset from the z~0 sequence. We also find that galaxies with blue UV spectral slopes ($\\langle\u03b2\\rangle$=-2.1) have elevated xi_ion by a factor of ~2 relative to the average xi_ion of the sample ($\\langle\u03b2\\rangle$=-1.4). If these blue galaxies are similar to those at z > 6, our results suggest that a lower Lyman continuum escape fraction is required for galaxies to maintain reionization, compared to the canonical xi_ion predictions from stellar population models. Furthermore, we demonstrate that even with robustly dust-corrected H$\u03b1$, the UV dust attenuation can cause on average a ~0.3dex systematic uncertainty in xi_ion calculations.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Techniques for approximately contracting tensor networks are limited in how efficiently they can make use of parallel computing resources. In this work we demonstrate and characterize a Monte Carlo approach to the tensor network renormalization group method which can be used straightforwardly on modern computing architectures. We demonstrate the efficiency of the technique and show that Monte Carlo tensor network renormalization provides an attractive path to improving the accuracy of a wide class of challenging computations while also providing useful estimates of uncertainty and a statistical guarantee of unbiased results.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on broad flux from the nebular emission lines H$\u03b1$, [NII], [OIII], H$\u03b2$, and [SII]. The sample consists of 127 star-forming galaxies at $1.37 < z < 2.61$ and 84 galaxies at $2.95 < z < 3.80$. We decompose the emission lines using narrow ($\\text{FWHM} < 275 \\ \\text{km s}^{-1}$) and broad ($\\text{FWHM} > 300 \\ \\text{km s}^{-1}$) Gaussian components for individual galaxies and stacks. Broad emission is detected at $>3\u03c3$ in $<10$% of galaxies and the broad flux accounts for 10-70% of the total flux. We find a slight increase in broad to narrow flux ratio with mass but note that we cannot reliably detect broad emission with $\\text{FWHM} < 275 \\ \\text{km s}^{-1}$, which may be significant at low masses. Notably, there is a correlation between higher signal-to-noise (S/N) spectra and a broad component detection indicating a S/N dependence in our ability to detect broad flux. When placed on the N2-BPT diagram ([OIII]/H$\u03b2$ vs. [NII]/H$\u03b1$) the broad components of the stacks are shifted towards higher [OIII]/H$\u03b2$ and [NII]/$\u03b1$ ratios compared to the narrow component. We compare the location of the broad components to shock models and find that the broad component could be due to shocks, but we do not rule out other possibilities such as the presence of an AGN. We estimate the mass loading factor (mass outflow rate/star formation rate) assuming the broad component is a photoionized outflow and find that the mass loading factor increases as a function of mass which agrees with previous studies. We show that adding emission from shocked gas to $z\\sim0$ SDSS spectra shifts galaxies towards the location of $z\\sim2$ galaxies on several emission line diagnostic diagrams.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present the first spectroscopic measurement of multiple rest-frame optical emission lines at $z>4$. During the MOSFIRE Deep Evolution Field (MOSDEF) survey, we observed the galaxy GOODSN-17940 with the Keck I/MOSFIRE spectrograph. The K-band spectrum of GOODSN-17940 includes significant detections of the [OII]$\u03bb\u03bb3726,3729$, [NeIII]$\\lambda3869$, and H$\u03b3$ emission lines and a tentative detection of H$\u03b4$, indicating $z_{\\rm{spec}}=4.4121$. GOODSN-17940 is an actively star-forming $z>4$ galaxy based on its K-band spectrum and broadband spectral energy distribution. A significant excess relative to the surrounding continuum is present in the Spitzer/IRAC channel 1 photometry of GOODSN-17940, due primarily to strong H$\u03b1$ emission with a rest-frame equivalent width of $\\mbox{EW(H}\u03b1)=1200$ \u00c5. Based on the assumption of $0.5 Z_{\\odot}$ models and the Calzetti attenuation curve, GOODSN-17940 is characterized by $M_*=5.0^{+4.3}_{-0.2}\\times 10^9 M_{\\odot}$. The Balmer decrement inferred from H$\u03b1$/H$\u03b3$ is used to dust correct the H$\u03b1$ emission, yielding $\\mbox{SFR(H}\u03b1)=320^{+190}_{-140} M_{\\odot}\\mbox{ yr}^{-1}$. These $M_*$ and SFR values place GOODSN-17940 an order of magnitude in SFR above the $z\\sim 4$ star-forming \"main sequence.\" Finally, we use the observed ratio of [NeIII]/[OII] to estimate the nebular oxygen abundance in GOODSN-17940, finding $\\mbox{O/H}\\sim 0.2 \\mbox{ (O/H)}_{\\odot}$. Combining our new [NeIII]/[OII] measurement with those from stacked spectra at $z\\sim 0, 2, \\mbox{ and } 3$, we show that GOODSN-17940 represents an extension to $z>4$ of the evolution towards higher [NeIII]/[OII] (i.e., lower $\\mbox{O/H}$) at fixed stellar mass. It will be possible to perform the measurements presented here out to $z\\sim 10$ using the James Webb Space Telescope.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Using observations from the first two years of the MOSFIRE Deep Evolution Field (MOSDEF) survey, we study 13 AGN-driven outflows detected from a sample of 67 X-ray, IR and/or optically-selected AGN at $z \\sim 2$. The AGN have bolometric luminosities of $\\sim10^{44}-10^{46} ~\\mathrm{erg~s^{-1}}$, including both quasars and moderate-luminosity AGN. We detect blueshifted, ionized gas outflows in the H$\u03b2$ , [OIII], H$\u03b1$ ~and/or [NII] emission lines of $19\\%$ of the AGN, while only 1.8\\% of the MOSDEF galaxies have similarly-detected outflows. The outflow velocities span $\\sim$300 to 1000 km s$^{-1}$. Eight of the 13 outflows are spatially extended on similar scales as the host galaxies, with spatial extents of 2.5 to 11.0 kpc. Outflows are detected uniformly across the star-forming main sequence, showing little trend with the host galaxy SFR. Line ratio diagnostics indicate that the outflowing gas is photoionized by the AGN. We do not find evidence for positive AGN feedback, in either our small MOSDEF sample or a much larger SDSS sample, using the BPT diagram. Given that a galaxy with an AGN is ten times more likely to have a detected outflow, the outflowing gas is photoionzed by the AGN, and estimates of the mass and energy outflow rates indicate that stellar feedback is insufficient to drive at least some of these outflows, they are very likely to be AGN-driven. The outflows have mass-loading factors of the order of unity, suggesting that they help regulate star formation in their host galaxies, though they may be insufficient to fully quench it.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present a method for synthesizing a frontal, neutral-expression image of a person's face given an input face photograph. This is achieved by learning to generate facial landmarks and textures from features extracted from a facial-recognition network. Unlike previous approaches, our encoding feature vector is largely invariant to lighting, pose, and facial expression. Exploiting this invariance, we train our decoder network using only frontal, neutral-expression photographs. Since these photographs are well aligned, we can decompose them into a sparse set of landmark points and aligned texture maps. The decoder then predicts landmarks and textures independently and combines them using a differentiable image warping operation. The resulting images can be used for a number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating a 3-D avatar.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present results on the variation of 7.7 micron Polycyclic Aromatic Hydrocarbon (PAH) emission in galaxies spanning a wide range in metallicity at z ~ 2. For this analysis, we use rest-frame optical spectra of 476 galaxies at 1.37 < z < 2.61 from the MOSFIRE Deep Evolution Field (MOSDEF) survey to infer metallicities and ionization states. Spitzer/MIPS 24 micron and Herschel/PACS 100 and 160 micron observations are used to derive rest-frame 7.7 micron luminosities (L(7.7)) and total IR luminosities (L(IR)), respectively. We find significant trends between the ratio of L(7.7) to L(IR) (and to dust-corrected SFR) and both metallicity and [OIII]/[OII] (O32) emission-line ratio. The latter is an empirical proxy for the ionization parameter. These trends indicate a paucity of PAH emission in low metallicity environments with harder and more intense radiation fields. Additionally, L(7.7)/L(IR) is significantly lower in the youngest quartile of our sample (ages of 500 Myr) compared to older galaxies, which may be a result of the delayed production of PAHs by AGB stars. The relative strength of L(7.7) to L(IR) is also lower by a factor of ~ 2 for galaxies with masses $M_* < 10^{10}M_{\\odot}$, compared to the more massive ones. We demonstrate that commonly-used conversions of L(7.7) (or 24 micron flux density; f(24)) to L(IR) underestimate the IR luminosity by more than a factor of 2 at $M_*$ ~ $10^{9.6-10.0} M_{\\odot}$. We adopt a mass-dependent conversion of L(7.7) to L(IR) with L(7.7)/L(IR)= 0.09 and 0.22 for $M_* < 10^{10}$ and $> 10^{10} M_{\\odot}$, respectively. Based on the new scaling, the SFR-$M_*$ relation has a shallower slope than previously derived. Our results also suggest a higher IR luminosity density at z ~ 2 than previously measured, corresponding to a ~ 30% increase in the SFR density.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We propose a novel method for template matching in unconstrained environments. Its essence is the Best-Buddies Similarity (BBS), a useful, robust, and parameter-free similarity measure between two sets of points. BBS is based on counting the number of Best-Buddies Pairs (BBPs)--pairs of points in source and target sets, where each point is the nearest neighbor of the other. BBS has several key features that make it robust against complex geometric deformations and high levels of outliers, such as those arising from background clutter and occlusions. We study these properties, provide a statistical analysis that justifies them, and demonstrate the consistent success of BBS on a challenging real-world dataset while using different types of features.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "The sound of crashing waves, the roar of fast-moving cars -- sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on the identification, selection biases, and host galaxy properties of 55 X-ray, IR and optically-selected active galactic nuclei (AGN) at $1.4 < z < 3.8$. We obtain rest-frame optical spectra of galaxies and AGN and use the BPT diagram to identify optical AGN. We examine the uniqueness and overlap of the AGN identified at different wavelengths. There is a strong bias against identifying AGN at any wavelength in low mass galaxies, and an additional bias against identifying IR AGN in the most massive galaxies. AGN hosts span a wide range of star formation rate (SFR), similar to inactive galaxies once stellar mass selection effects are accounted for. However, we find (at $\\sim 2-3\u03c3$ significance) that IR AGN are in less dusty galaxies with relatively higher SFR and optical AGN in dusty galaxies with relatively lower SFR. X-ray AGN selection does not display a bias with host galaxy SFR. These results are consistent with those from larger studies at lower redshifts. Within star-forming galaxies, once selection biases are accounted for, we find AGN in galaxies with similar physical properties as inactive galaxies, with no evidence for AGN activity in particular types of galaxies. This is consistent with AGN being fueled stochastically in any star-forming host galaxy. We do not detect a significant correlation between SFR and AGN luminosity for individual AGN hosts, which may indicate the timescale difference between the growth of galaxies and their supermassive black holes.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Originally developed to image the shadow region of the central black hole in Sagittarius A* and in the nearby galaxy M87, the Event Horizon Telescope (EHT) provides deep, very high angular resolution data on other AGN sources too. The challenges of working with EHT data have spurred the development of new image reconstruction algorithms. This work briefly reviews the status of the EHT and its utility for observing AGN sources, with emphasis on novel imaging techniques that offer the promise of better reconstructions at 1.3 mm and other wavelengths.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach that models future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. Future frame synthesis is challenging, as it involves low- and high-level image and motion understanding. We propose a novel network structure, namely a Cross Convolutional Network to aid in synthesizing future frames; this network structure encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold videos. We also show that our model can be applied to tasks such as visual analogy-making, and present an analysis of the learned network representations.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present measurements of the electron-temperature based oxygen abundance for a highly star-forming galaxy at z=3.08, COSMOS-1908. This is the highest redshift at which [OIII]$\u03bb$4363 has been detected, and the first time that this line has been measured at z>2. We estimate an oxygen abundance of 12+log(O/H)$=8.00^{+0.13}_{-0.14}$. This galaxy is a low-mass ($10^{9.3}$ M$_{\\odot}$), highly star-forming ($\\sim50$ M$_{\\odot}$ yr$^{-1}$) system that hosts a young stellar population ($\\sim160$ Myr). We investigate the physical conditions of the ionized gas in COSMOS-1908 and find that this galaxy has a high ionization parameter, little nebular reddening ($E(B-V)_{\\rm gas}<0.14$), and a high electron density ($n_e\\sim500$ cm$^{-3}$). We compare the ratios of strong oxygen, neon, and hydrogen lines to the direct-method oxygen abundance for COSMOS-1908 and additional star-forming galaxies at z=0-1.8 with [OIII]$\u03bb$4363 measurements, and show that galaxies at z$\\sim$1-3 follow the same strong-line correlations as galaxies in the local universe. This agreement suggests that the relationship between ionization parameter and O/H is similar for z$\\sim$0 and high-redshift galaxies. These results imply that metallicity calibrations based on lines of oxygen, neon, and hydrogen do not strongly evolve with redshift and can reliably estimate abundances out to z$\\sim$3, paving the way for robust measurements of the evolution of the mass-metallicity relation to high redshift.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "[Abridged] We present a robust measurement of the rest-frame UV luminosity function (LF) and its evolution during the peak epoch of cosmic star formation at 1<z<3. We use our deep near ultraviolet imaging from WFC3/UVIS on the Hubble Space Telescope (HST) and existing ACS/WFC and WFC3/IR imaging of three lensing galaxy clusters, Abell 2744 and MACSJ0717 from the Hubble Frontier Field survey and Abell 1689. We use photometric redshifts to identify 780 ultra-faint galaxies with $M_{UV}$<-12.5 AB mag at 1<z<3. From these samples, we identified 5 new, faint, multiply imaged systems in A1689. We compute the rest-frame UV LF and find the best-fit faint-end slopes of $\u03b1=-1.56\\pm0.04$, $\u03b1=-1.72\\pm0.04$ and $\u03b1=-1.94\\pm0.06$ at 1.0<z<1.6, 1.6<z<2.2 and 2.2<z<3.0, respectively. Our results demonstrate that the UV LF becomes steeper from z\\sim1.3 to z\\sim2.6 with no sign of a turnover down to $M_{UV}=-14$ AB mag. We further derive the UV LFs using the Lyman break \"dropout\" selection and confirm the robustness of our conclusions against different selection methodologies. Because the sample sizes are so large, and extend to such faint luminosities, the statistical uncertainties are quite small, and systematic uncertainties (due to the assumed size distribution, for example), likely dominate. If we restrict our analysis to galaxies and volumes above > 50% completeness in order to minimize these systematics, we still find that the faint-end slope is steep and getting steeper with redshift, though with slightly shallower (less negative) values ($\u03b1=-1.55\\pm0.06$, $-1.69\\pm0.07$ and $-1.79\\pm0.08$ for $z\\sim1.3$, 1.9 and 2.6, respectively). Finally, we conclude that the faint star-forming galaxies with UV magnitudes of $-18.5<M_{UV}<-12.5$ covered in this study, produce the majority (55%-60%) of the unobscured UV luminosity density at 1<z<3.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Humans demonstrate remarkable abilities to predict physical events in complex scenes. Two classes of models for physical scene understanding have recently been proposed: \"Intuitive Physics Engines\", or IPEs, which posit that people make predictions by running approximate probabilistic simulations in causal mental models similar in nature to video-game physics engines, and memory-based models, which make judgments based on analogies to stored experiences of previously encountered scenes and physical outcomes. Versions of the latter have recently been instantiated in convolutional neural network (CNN) architectures. Here we report four experiments that, to our knowledge, are the first rigorous comparisons of simulation-based and CNN-based models, where both approaches are concretely instantiated in algorithms that can run on raw image inputs and produce as outputs physical judgments such as whether a stack of blocks will fall. Both approaches can achieve super-human accuracy levels and can quantitatively predict human judgments to a similar degree, but only the simulation-based models generalize to novel situations in ways that people do, and are qualitatively consistent with systematic perceptual illusions and judgment asymmetries that people show.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an end-to-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technical innovations. First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as 3D rendering and image retrieval.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present the first direct comparison between Balmer line and panchromatic SED-based SFRs for z~2 galaxies. For this comparison we used 17 star-forming galaxies selected from the MOSFIRE Deep Evolution Field (MOSDEF) survey, with $3\u03c3$ detections for H$\u03b1$ and at least two IR bands (Spitzer/MIPS 24$\u03bc$m and Herschel/PACS 100 and 160$\u03bc$m, and in some cases Herschel/SPIRE 250, 350, and 500$\u03bc$m). The galaxies have total IR (8-1000$\u03bc$m) luminosities of $\\sim10^{11.4}-10^{12.4}\\,\\textrm{L}_\\odot$ and star-formation rates (SFRs) of $\\sim30-250\\,\\textrm{M}_\\odot\\,\\mathrm{yr^{-1}}$. We fit the UV-to-far-IR SEDs with flexible stellar population synthesis (FSPS) models - which include both stellar and dust emission - and compare the inferred SFRs with the SFR(H$\u03b1$,H$\u03b2$) values corrected for dust attenuation using Balmer decrements. The two SFRs agree with a scatter of 0.17 dex. Our results imply that the Balmer decrement accurately predicts the obscuration of the nebular lines and can be used to robustly calculate SFRs for star-forming galaxies at z~2 with SFRs up to $\\sim200\\,\\textrm{M}_\\odot\\,\\mathrm{yr^{-1}}$. We also use our data to assess SFR indicators based on modeling the UV-to-mid-IR SEDs or by adding SFR(UV) and SFR(IR), for which the latter is based on the mid-IR only or on the full IR SED. All these SFRs show a poorer agreement with SFR(H$\u03b1$,H$\u03b2$) and in some cases large systematic biases are observed. Finally, we show that the SFR and dust attenuation derived from the UV-to-near-IR SED alone are unbiased when assuming a delayed exponentially declining star-formation history.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "A dynamical characterization of dark matter dominated dwarf galaxies from their observed effects on galactic disks (i.e. Galactoseismology) has remained an elusive goal. Here, we present preliminary results from spectroscopic observations of three clustered Cepheid candidates identified from $K$-band light curves towards Norma. The average heliocentric radial velocity of these stars is $\\sim$ 156 km/s, which is large and distinct from that of the Galaxy's stellar disk. These objects at $l \\sim 333 ^\\circ$ and $b \\sim -1 ^\\circ$ are therefore halo stars; using the $3.6~\\micron$ period-luminosity relation of Type I Cepheids, they are at $\\sim$ 73 kpc. Our ongoing $I$-band photometry indicates variability on the same time scale as the period determined from the $K_{s}$-band light curve. Distances determined from the $K$-band period-luminosity relation and the 3.6 $\\micron$ period-luminosity relation are comparable. The observed radial velocity of these stars agrees roughly with predictions from dynamical models. If these stars are indeed members of the predicted dwarf galaxy that perturbed the outer HI disk of the Milky Way, this would mark the first application of Galactoseismology.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Very long baseline interferometry (VLBI) is a technique for imaging celestial radio emissions by simultaneously observing a source from telescopes distributed across Earth. The challenges in reconstructing images from fine angular resolution VLBI data are immense. The data is extremely sparse and noisy, thus requiring statistical image models such as those designed in the computer vision community. In this paper we present a novel Bayesian approach for VLBI image reconstruction. While other methods often require careful tuning and parameter selection for different types of data, our method (CHIRP) produces good results under different settings such as low SNR or extended emission. The success of our method is demonstrated on realistic synthetic experiments as well as publicly available real data. We present this problem in a way that is accessible to members of the community, and provide a dataset website (vlbiimaging.csail.mit.edu) that facilitates controlled comparisons across algorithms.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present H$\u03b1$ gas kinematics for 178 star-forming galaxies at z~2 from the MOSFIRE Deep Evolution Field survey. We have developed models to interpret the kinematic measurements from fixed-angle multi-object spectroscopy, using structural parameters derived from CANDELS HST/F160W imaging. For 35 galaxies we measure resolved rotation with a median $(V/\u03c3_{V,0})_{R_E}=2.1$. We derive dynamical masses from the kinematics and sizes and compare them to baryonic masses, with gas masses estimated from dust-corrected H$\u03b1$ star formation rates (SFRs) and the Kennicutt-Schmidt relation. When assuming that galaxies with and without observed rotation have the same median $(V/\u03c3_{V,0})_{R_E}$, we find good agreement between the dynamical and baryonic masses, with a scatter of $\u03c3_{RMS}=0.34$dex and a median offset of $\u0394\\log_{10}M=0.04$dex. This comparison implies a low dark matter fraction (8% within an effective radius) for a Chabrier initial mass function (IMF), and disfavors a Salpeter IMF. Moreover, the requirement that $M_{dyn}/M_{baryon}$ should be independent of inclination yields a median value of $(V/\u03c3_{V,0})_{R_E}=2.1$ for galaxies without observed rotation. If instead we treat the galaxies without detected rotation as early-type galaxies, the masses are also in reasonable agreement ($\u0394\\log_{10}M=-0.07$dex, $\u03c3_{RMS}=0.37$dex). The inclusion of gas masses is critical in this comparison; if gas masses are excluded there is an increasing trend of $M_{dyn}/M_{*}$ with higher specific SFR (SSFR). Furthermore, we find indications that $V/\u03c3$ decreases with increasing H$\u03b1$ SSFR for our full sample, which may reflect disk settling. We also study the Tully-Fisher relation and find that at fixed stellar mass $S_{0.5}=(0.5V_{2.2}^2+\u03c3_{V,0}^2)^{1/2}$ was higher at earlier times. At fixed baryonic mass, we observe the opposite trend. [abridged]\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "Using observations from the MOSFIRE Deep Evolution Field (MOSDEF) survey, we investigate the physical conditions of star-forming regions in $z\\sim2.3$ galaxies, specifically the electron density and ionization state. From measurements of the [O II]$\u03bb\u03bb$3726,3729 and [S II]$\u03bb\u03bb$6716,6731 doublets, we find a median electron density of $\\sim250$ cm$^{-3}$ at $z\\sim2.3$, an increase of an order of magnitude compared to measurements of galaxies at $z\\sim0$. While $z\\sim2.3$ galaxies are offset towards significantly higher O$_{32}$ values relative to local galaxies at fixed stellar mass, we find that the high-redshift sample follows a similar distribution to the low-metallicity tail of the local distribution in the O$_{32}$ vs. R$_{23}$ and O3N2 diagrams. Based on these results, we propose that $z\\sim2.3$ star-forming galaxies have the same ionization parameter as local galaxies at fixed metallicity. In combination with simple photoionization models, the position of local and $z\\sim2.3$ galaxies in excitation diagrams suggests that there is no significant change in the hardness of the ionizing spectrum at fixed metallicity from $z\\sim0$ to $z\\sim2.3$. We find that $z\\sim2.3$ galaxies show no offset compared to low-metallicity local galaxies in emission line ratio diagrams involving only lines of hydrogen, oxygen, and sulfur, but show a systematic offset in diagrams involving [N II]$\u03bb$6584. We conclude that the offset of $z\\sim2.3$ galaxies from the local star-forming sequence in the [N II] BPT diagram is primarily driven by elevated N/O at fixed O/H compared to local galaxies. These results suggest that the local gas-phase and stellar metallicity sets the ionization state of star-forming regions at $z\\sim0$ and $z\\sim2$.\n        \u25b3 Less", "author": "William Freeman"}, {"abstract": "We present the survey design, data reduction, construction of images, and source catalog of the Atacama Large Millimeter/submillimeter Array (ALMA) twenty-six arcmin^2 survey of GOODS-S at one-millimeter (ASAGAO). ASAGAO is a deep (1sigma ~ 61 uJy/beam for a 250 klambda-tapered map with a synthesized beam size of 0.51\" x 0.45\") and wide area (26 arcmin^2) survey on a contiguous field at 1.2 mm. By combining with ALMA archival data in the GOODS-South field, we obtained a deeper map in the same region (1sigma ~ 30 uJy/beam for a deep region with a 250 klambda-taper, and a synthesized beam size of 0.59\" x 0.53\"), providing the largest sample of sources (25 sources at >=5.0sigma, 45 sources at >=4.5sigma) among ALMA blank-field surveys to date. The number counts shows that 52(+11 -8)% of the extragalactic background light at 1.2 mm is resolved into discrete sources at S1.2m > 135 uJy. We create infrared (IR) luminosity functions (LFs) in the redshift range of z = 1-3 from the ASAGAO sources with KS-band counterparts, and constrain the faintest luminosity of the LF at 2.0 < z < 3.0. The LFs are consistent with previous results based on other ALMA and SCUBA-2 observations, which suggest a positive luminosity evolution and negative density evolution with increasing redshift. We find that obscured star-formation of sources with IR luminosities of log(L(IR)/Lsun)} ~> 11.8 account for ~~60%-90% of the z ~ 2 cosmic star-formation rate density.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "This paper introduces an universal and structure-preserving regularization term, called quantile sparse image (QuaSI) prior. The prior is suitable for denoising images from various medical imaging modalities. We demonstrate its effectiveness on volumetric optical coherence tomography (OCT) and computed tomography (CT) data, which show different noise and image characteristics. OCT offers high-resolution scans of the human retina but is inherently impaired by speckle noise. CT on the other hand has a lower resolution and shows high-frequency noise. For the purpose of denoising, we propose a variational framework based on the QuaSI prior and a Huber data fidelity model that can handle 3-D and 3-D+t data. Efficient optimization is facilitated through the use of an alternating direction method of multipliers (ADMM) scheme and the linearization of the quantile filter. Experiments on multiple datasets emphasize the excellent performance of the proposed method.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "Optical coherence tomography (OCT) enables high-resolution and non-invasive 3D imaging of the human retina but is inherently impaired by speckle noise. This paper introduces a spatio-temporal denoising algorithm for OCT data on a B-scan level using a novel quantile sparse image (QuaSI) prior. To remove speckle noise while preserving image structures of diagnostic relevance, we implement our QuaSI prior via median filter regularization coupled with a Huber data fidelity model in a variational approach. For efficient energy minimization, we develop an alternating direction method of multipliers (ADMM) scheme using a linearization of median filtering. Our spatio-temporal method can handle both, denoising of single B-scans and temporally consecutive B-scans, to gain volumetric OCT data with enhanced signal-to-noise ratio. Our algorithm based on 4 B-scans only achieved comparable performance to averaging 13 B-scans and outperformed other current denoising methods.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "The Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) is a three-layered imaging survey aimed at addressing some of the most outstanding questions in astronomy today, including the nature of dark matter and dark energy. The survey has been awarded 300 nights of observing time at the Subaru Telescope and it started in March 2014. This paper presents the first public data release of HSC-SSP. This release includes data taken in the first 1.7 years of observations (61.5 nights) and each of the Wide, Deep, and UltraDeep layers covers about 108, 26, and 4 square degrees down to depths of i~26.4, ~26.5, and ~27.0 mag, respectively (5sigma for point sources). All the layers are observed in five broad bands (grizy), and the Deep and UltraDeep layers are observed in narrow bands as well. We achieve an impressive image quality of 0.6 arcsec in the i-band in the Wide layer. We show that we achieve 1-2 per cent PSF photometry (rms) both internally and externally (against Pan-STARRS1), and ~10 mas and 40 mas internal and external astrometric accuracy, respectively. Both the calibrated images and catalogs are made available to the community through dedicated user interfaces and database servers. In addition to the pipeline products, we also provide value-added products such as photometric redshifts and a collection of public spectroscopic redshifts. Detailed descriptions of all the data can be found online. The data release website is https://hsc-release.mtk.nao.ac.jp/.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "With ALMA making it possible to resolve giant molecular clouds (GMCs) in other galaxies, it is becoming necessary to quantify the observational bias on measured GMC properties. Using a hydrodynamical simulation of a barred spiral galaxy, we compared the physical properties of GMCs formed in position-position-position space (PPP) to the observational position-position-velocity space (PPV). We assessed the effect of disc inclination: face-on (PPV_face) and edge-on (PPV_edge), and resolution: 1.5 pc versus 24 pc, on GMC properties and the further implications of using Larson's scaling relations for mass-radius and velocity dispersion-radius. The low-resolution PPV data are generated by simulating ALMA Cycle 3 observations using the CASA package. Results show that the median properties do not differ strongly between PPP and PPV_face under both resolutions, but PPV_edge clouds deviate from these two. The differences become magnified when switching to the lower, but more realistic resolution. The discrepancy can lead to opposite results for the virial parameter's measure of gravitational binding, and therefore the dynamical state of the clouds. The power-law indices for the two Larson's scaling relations decrease going from PPP, PPV_face to PPV_edge and decrease from high to low resolutions. We conclude that the relations are not entirely driven by the underlying physical origin and therefore have to be used with caution when considering the environmental dependence, dynamical state, and the extragalactic CO-to-H2 conversion factor of GMCs.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "As both simulations and observations reach the resolution of the star-forming molecular clouds, it becomes important to clarify if these two techniques are discussing the same objects in galaxies. We compare clouds formed in a high resolution galaxy simulation identified as continuous structures within a contour, in the simulator's position-position-position (PPP) co-ordinate space and the observer's position-position-velocity space (PPV). Results indicate that the properties of the cloud populations are similar in both methods and up to 70% of clouds have a single counterpart in the opposite data structure. Comparing individual clouds in a one-to-one match reveals a scatter in properties mostly within a factor of two. However, the small variations in mass, radius and velocity dispersion produce significant differences in derived quantities such as the virial parameter. This makes it difficult to determine if a structure is truely gravitationally bound. The three cloud types originally found in the simulation in Fujimoto et al. (2014) are identified in both data sets, with around 80% of the clouds retaining their type between identification methods. We also compared our results when using a peak decomposition method to identify clouds in both PPP and PPV space. The number of clouds increased with this technique, but the overall cloud properties remained similar. However, the more crowded environment lowered the ability to match clouds between techniques to 40%. The three cloud types also became harder to separate, especially in the PPV data set. The method used for cloud identification therefore plays a critical role in determining cloud properties, but both PPP and PPV can potentially identify the same structures.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "The joint JAXA/NASA ASTRO-H mission is the sixth in a series of highly successful X-ray missions developed by the Institute of Space and Astronautical Science (ISAS), with a planned launch in 2015. The ASTRO-H mission is equipped with a suite of sensitive instruments with the highest energy resolution ever achieved at E > 3 keV and a wide energy range spanning four decades in energy from soft X-rays to gamma-rays. The simultaneous broad band pass, coupled with the high spectral resolution of Delta E < 7 eV of the micro-calorimeter, will enable a wide variety of important science themes to be pursued. ASTRO-H is expected to provide breakthrough results in scientific areas as diverse as the large-scale structure of the Universe and its evolution, the behavior of matter in the gravitational strong field regime, the physical conditions in sites of cosmic-ray acceleration, and the distribution of dark matter in galaxy clusters at different redshifts.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "In 2009-2010, the Laser Interferometer Gravitational-wave Observa- tory (LIGO) operated together with international partners Virgo and GEO600 as a network to search for gravitational waves of astrophysical origin. The sensitiv- ity of these detectors was limited by a combination of noise sources inherent to the instrumental design and its environment, often localized in time or frequency, that couple into the gravitational-wave readout. Here we review the performance of the LIGO instruments during this epoch, the work done to characterize the de- tectors and their data, and the effect that transient and continuous noise artefacts have on the sensitivity of LIGO to a variety of astrophysical sources.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "Searches for a stochastic gravitational-wave background (SGWB) using terrestrial detectors typically involve cross-correlating data from pairs of detectors. The sensitivity of such cross-correlation analyses depends, among other things, on the separation between the two detectors: the smaller the separation, the better the sensitivity. Hence, a co-located detector pair is more sensitive to a gravitational-wave background than a non-co-located detector pair. However, co-located detectors are also expected to suffer from correlated noise from instrumental and environmental effects that could contaminate the measurement of the background. Hence, methods to identify and mitigate the effects of correlated noise are necessary to achieve the potential increase in sensitivity of co-located detectors. Here we report on the first SGWB analysis using the two LIGO Hanford detectors and address the complications arising from correlated environmental noise. We apply correlated noise identification and mitigation techniques to data taken by the two LIGO Hanford detectors, H1 and H2, during LIGO's fifth science run. At low frequencies, 40 - 460 Hz, we are unable to sufficiently mitigate the correlated noise to a level where we may confidently measure or bound the stochastic gravitational-wave signal. However, at high frequencies, 460-1000 Hz, these techniques are sufficient to set a $95%$ confidence level (C.L.) upper limit on the gravitational-wave energy density of \u03a9(f)<7.7 x 10^{-4} (f/ 900 Hz)^3, which improves on the previous upper limit by a factor of $\\sim 180$. In doing so, we demonstrate techniques that will be useful for future searches using advanced detectors, where correlated noise (e.g., from global magnetic fields) may affect even widely separated detectors.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We report on an all-sky search for periodic gravitational waves in the frequency range $\\mathrm{50-1000 Hz}$ with the first derivative of frequency in the range $-8.9 \\times 10^{-10}$ Hz/s to zero in two years of data collected during LIGO's fifth science run. Our results employ a Hough transform technique, introducing a $\u03c7^2$ test and analysis of coincidences between the signal levels in years 1 and 2 of observations that offers a significant improvement in the product of strain sensitivity with compute cycles per data sample compared to previously published searches. Since our search yields no surviving candidates, we present results taking the form of frequency dependent, 95$%$ confidence upper limits on the strain amplitude $h_0$. The most stringent upper limit from year 1 is $1.0\\times 10^{-24}$ in the $\\mathrm{158.00-158.25 Hz}$ band. In year 2, the most stringent upper limit is $\\mathrm{8.9\\times10^{-25}}$ in the $\\mathrm{146.50-146.75 Hz}$ band. This improved detection pipeline, which is computationally efficient by at least two orders of magnitude better than our flagship Einstein$@$Home search, will be important for \"quick-look\" searches in the Advanced LIGO and Virgo detector era.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "Cosmic strings can give rise to a large variety of interesting astrophysical phenomena. Among them, powerful bursts of gravitational waves (GWs) produced by cusps are a promising observational signature. In this Letter we present a search for GWs from cosmic string cusps in data collected by the LIGO and Virgo gravitational wave detectors between 2005 and 2010, with over 625 days of live time. We find no evidence of GW signals from cosmic strings. From this result, we derive new constraints on cosmic string parameters, which complement and improve existing limits from previous searches for a stochastic background of GWs from cosmic microwave background measurements and pulsar timing data. In particular, if the size of loops is given by the gravitational backreaction scale, we place upper limits on the string tension $G\u03bc$ below $10^{-8}$ in some regions of the cosmic string parameter space.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "During the LIGO and Virgo joint science runs in 2009-2010, gravitational wave (GW) data from three interferometer detectors were analyzed within minutes to select GW candidate events and infer their apparent sky positions. Target coordinates were transmitted to several telescopes for follow-up observations aimed at the detection of an associated optical transient. Images were obtained for eight such GW candidates. We present the methods used to analyze the image data as well as the transient search results. No optical transient was identified with a convincing association with any of these candidates, and none of the GW triggers showed strong evidence for being astrophysical in nature. We compare the sensitivities of these observations to several model light curves from possible sources of interest, and discuss prospects for future joint GW-optical observations of this type.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We present the results of a directed search for continuous gravitational waves from unknown, isolated neutron stars in the Galactic Center region, performed on two years of data from LIGO's fifth science run from two LIGO detectors. The search uses a semi-coherent approach, analyzing coherently 630 segments, each spanning 11.5 hours, and then incoherently combining the results of the single segments. It covers gravitational wave frequencies in a range from 78 to 496 Hz and a frequency-dependent range of first order spindown values down to -7.86 x 10^-8 Hz/s at the highest frequency. No gravitational waves were detected. We place 90% confidence upper limits on the gravitational wave amplitude of sources at the Galactic Center. Placing 90% confidence upper limits on the gravitational wave amplitude of sources at the Galactic Center, we reach ~3.35x10^-25 for frequencies near 150 Hz. These upper limits are the most constraining to date for a large-parameter-space search for continuous gravitational wave signals.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "Long gamma-ray bursts (GRBs) have been linked to extreme core-collapse supernovae from massive stars. Gravitational waves (GW) offer a probe of the physics behind long GRBs. We investigate models of long-lived (~10-1000s) GW emission associated with the accretion disk of a collapsed star or with its protoneutron star remnant. Using data from LIGO's fifth science run, and GRB triggers from the swift experiment, we perform a search for unmodeled long-lived GW transients. Finding no evidence of GW emission, we place 90% confidence level upper limits on the GW fluence at Earth from long GRBs for three waveforms inspired by a model of GWs from accretion disk instabilities. These limits range from F<3.5 ergs cm^-2 to $F<1200 ergs cm^-2, depending on the GRB and on the model, allowing us to probe optimistic scenarios of GW production out to distances as far as ~33 Mpc. Advanced detectors are expected to achieve strain sensitivities 10x better than initial LIGO, potentially allowing us to probe the engines of the nearest long GRBs.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We present the results of searches for gravitational waves from a large selection of pulsars using data from the most recent science runs (S6, VSR2 and VSR4) of the initial generation of interferometric gravitational wave detectors LIGO (Laser Interferometric Gravitational-wave Observatory) and Virgo. We do not see evidence for gravitational wave emission from any of the targeted sources but produce upper limits on the emission amplitude. We highlight the results from seven young pulsars with large spin-down luminosities. We reach within a factor of five of the canonical spin-down limit for all seven of these, whilst for the Crab and Vela pulsars we further surpass their spin-down limits. We present new or updated limits for 172 other pulsars (including both young and millisecond pulsars). Now that the detectors are undergoing major upgrades, and, for completeness, we bring together all of the most up-to-date results from all pulsars searched for during the operations of the first-generation LIGO, Virgo and GEO600 detectors. This gives a total of 195 pulsars including the most recent results described in this paper.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "Compact binary systems with neutron stars or black holes are one of the most promising sources for ground-based gravitational wave detectors. Gravitational radiation encodes rich information about source physics; thus parameter estimation and model selection are crucial analysis steps for any detection candidate events. Detailed models of the anticipated waveforms enable inference on several parameters, such as component masses, spins, sky location and distance that are essential for new astrophysical studies of these sources. However, accurate measurements of these parameters and discrimination of models describing the underlying physics are complicated by artifacts in the data, uncertainties in the waveform models and in the calibration of the detectors. Here we report such measurements on a selection of simulated signals added either in hardware or software to the data collected by the two LIGO instruments and the Virgo detector during their most recent joint science run, including a \"blind injection\" where the signal was not initially revealed to the collaboration. We exemplify the ability to extract information about the source physics on signals that cover the neutron star and black hole parameter space over the individual mass range 1 Msun - 25 Msun and the full range of spin parameters. The cases reported in this study provide a snap-shot of the status of parameter estimation in preparation for the operation of advanced detectors.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We report a search for gravitational waves from the inspiral, merger and ringdown of binary black holes (BBH) with total mass between 25 and 100 solar masses, in data taken at the LIGO and Virgo observatories between July 7, 2009 and October 20, 2010. The maximum sensitive distance of the detectors over this period for a (20,20) Msun coalescence was 300 Mpc. No gravitational wave signals were found. We thus report upper limits on the astrophysical coalescence rates of BBH as a function of the component masses for non-spinning components, and also evaluate the dependence of the search sensitivity on component spins aligned with the orbital angular momentum. We find an upper limit at 90% confidence on the coalescence rate of BBH with non-spinning components of mass between 19 and 28 Msun of 3.3 \\times 10^-7 mergers /Mpc^3 /yr.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "This paper presents results of an all-sky searches for periodic gravitational waves in the frequency range [50, 1190] Hz and with frequency derivative ranges of [-2 x 10^-9, 1.1 x 10^-10] Hz/s for the fifth LIGO science run (S5). The novelty of the search lies in the use of a non-coherent technique based on the Hough-transform to combine the information from coherent searches on timescales of about one day. Because these searches are very computationally intensive, they have been deployed on the Einstein@Home distributed computing project infrastructure. The search presented here is about a factor 3 more sensitive than the previous Einstein@Home search in early S5 LIGO data. The post-processing has left us with eight surviving candidates. We show that deeper follow-up studies rule each of them out. Hence, since no statistically significant gravitational wave signals have been detected, we report upper limits on the intrinsic gravitational wave amplitude h0. For example, in the 0.5 Hz-wide band at 152.5 Hz, we can exclude the presence of signals with h0 greater than 7.6 x 10^-25 with a 90% confidence level.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We present the results of the first search for gravitational wave bursts associated with high energy neutrinos. Together, these messengers could reveal new, hidden sources that are not observed by conventional photon astronomy, particularly at high energy. Our search uses neutrinos detected by the underwater neutrino telescope ANTARES in its 5 line configuration during the period January - September 2007, which coincided with the fifth and first science runs of LIGO and Virgo, respectively. The LIGO-Virgo data were analysed for candidate gravitational-wave signals coincident in time and direction with the neutrino events. No significant coincident events were observed. We place limits on the density of joint high energy neutrino - gravitational wave emission events in the local universe, and compare them with densities of merger and core-collapse events.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We present the results of a search for gravitational waves associated with 154 gamma-ray bursts (GRBs) that were detected by satellite-based gamma-ray experiments in 2009-2010, during the sixth LIGO science run and the second and third Virgo science runs. We perform two distinct searches: a modeled search for coalescences of either two neutron stars or a neutron star and black hole; and a search for generic, unmodeled gravitational-wave bursts. We find no evidence for gravitational-wave counterparts, either with any individual GRB in this sample or with the population as a whole. For all GRBs we place lower bounds on the distance to the progenitor, under the optimistic assumption of a gravitational-wave emission energy of 10^-2 M c^2 at 150 Hz, with a median limit of 17 Mpc. For short hard GRBs we place exclusion distances on binary neutron star and neutron star-black hole progenitors, using astrophysically motivated priors on the source parameters, with median values of 16 Mpc and 28 Mpc respectively. These distance limits, while significantly larger than for a search that is not aided by GRB satellite observations, are not large enough to expect a coincidence with a GRB. However, projecting these exclusions to the sensitivities of Advanced LIGO and Virgo, which should begin operation in 2015, we find that the detection of gravitational waves associated with GRBs will become quite possible.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We present the first multi-wavelength follow-up observations of two candidate gravitational-wave (GW) transient events recorded by LIGO and Virgo in their 2009-2010 science run. The events were selected with low latency by the network of GW detectors and their candidate sky locations were observed by the Swift observatory. Image transient detection was used to analyze the collected electromagnetic data, which were found to be consistent with background. Off-line analysis of the GW data alone has also established that the selected GW events show no evidence of an astrophysical origin; one of them is consistent with background and the other one was a test, part of a \"blind injection challenge\". With this work we demonstrate the feasibility of rapid follow-ups of GW transients and establish the sensitivity improvement joint electromagnetic and GW observations could bring. This is a first step toward an electromagnetic follow-up program in the regime of routine detections with the advanced GW instruments expected within this decade. In that regime multi-wavelength observations will play a significant role in completing the astrophysical identification of GW sources. We present the methods and results from this first combined analysis and discuss its implications in terms of sensitivity for the present and future instruments.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "Between 2007 and 2010 Virgo collected data in coincidence with the LIGO and GEO gravitational-wave (GW) detectors. These data have been searched for GWs emitted by cataclysmic phenomena in the universe, by non-axisymmetric rotating neutron stars or from a stochastic background in the frequency band of the detectors. The sensitivity of GW searches is limited by noise produced by the detector or its environment. It is therefore crucial to characterize the various noise sources in a GW detector. This paper reviews the Virgo detector noise sources, noise propagation, and conversion mechanisms which were identified in the three first Virgo observing runs. In many cases, these investigations allowed us to mitigate noise sources in the detector, or to selectively flag noise events and discard them from the data. We present examples from the joint LIGO-GEO-Virgo GW searches to show how well noise transients and narrow spectral lines have been identified and excluded from the Virgo data. We also discuss how detector characterization can improve the astrophysical reach of gravitational-wave searches.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We present results from a search for gravitational-wave bursts in the data collected by the LIGO and Virgo detectors between July 7, 2009 and October 20, 2010: data are analyzed when at least two of the three LIGO-Virgo detectors are in coincident operation, with a total observation time of 207 days. The analysis searches for transients of duration < 1 s over the frequency band 64-5000 Hz, without other assumptions on the signal waveform, polarization, direction or occurrence time. All identified events are consistent with the expected accidental background. We set frequentist upper limits on the rate of gravitational-wave bursts by combining this search with the previous LIGO-Virgo search on the data collected between November 2005 and October 2007. The upper limit on the rate of strong gravitational-wave bursts at the Earth is 1.3 events per year at 90% confidence. We also present upper limits on source rate density per year and Mpc^3 for sample populations of standard-candle sources. As in the previous joint run, typical sensitivities of the search in terms of the root-sum-squared strain amplitude for these waveforms lie in the range 5 10^-22 Hz^-1/2 to 1 10^-20 Hz^-1/2. The combination of the two joint runs entails the most sensitive all-sky search for generic gravitational-wave bursts and synthesizes the results achieved by the initial generation of interferometric detectors.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "We present the results of a weakly modeled burst search for gravitational waves from mergers of non-spinning intermediate mass black holes (IMBH) in the total mass range 100--450 solar masses and with the component mass ratios between 1:1 and 4:1. The search was conducted on data collected by the LIGO and Virgo detectors between November of 2005 and October of 2007. No plausible signals were observed by the search which constrains the astrophysical rates of the IMBH mergers as a function of the component masses. In the most efficiently detected bin centered on 88+88 solar masses, for non-spinning sources, the rate density upper limit is 0.13 per Mpc^3 per Myr at the 90% confidence level.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "A stochastic background of gravitational waves is expected to arise from a superposition of many incoherent sources of gravitational waves, of either cosmological or astrophysical origin. This background is a target for the current generation of ground-based detectors. In this article we present the first joint search for a stochastic background using data from the LIGO and Virgo interferometers. In a frequency band of 600-1000 Hz, we obtained a 95% upper limit on the amplitude of $\u03a9_{\\rm GW}(f) = \u03a9_3 (f/900 \\mathrm{Hz})^3$, of $\u03a9_3 < 0.33$, assuming a value of the Hubble parameter of $h_{100}=0.72$. These new limits are a factor of seven better than the previous best in this frequency band.\n        \u25b3 Less", "author": "James Fujimoto"}, {"abstract": "Schalkwijk and Kailath (1966) developed a class of block codes for Gaussian channels with ideal feedback for which the probability of decoding error decreases as a second-order exponent in block length for rates below capacity. This well-known but surprising result is explained and simply derived here in terms of a result by Elias (1956) concerning the minimum mean-square distortion achievable in transmitting a single Gaussian random variable over multiple uses of the same Gaussian channel. A simple modification of the Schalkwijk-Kailath scheme is then shown to have an error probability that decreases with an exponential order which is linearly increasing with block length. In the infinite bandwidth limit, this scheme produces zero error probability using bounded expected energy at all rates below capacity. A lower bound on error probability for the finite bandwidth case is then derived in which the error probability decreases with an exponential order which is linearly increasing in block length at the same rate as the upper bound.\n        \u25b3 Less", "author": "Robert Gallager"}, {"abstract": "Local explanation frameworks aim to rationalize particular decisions made by a black-box prediction model. Existing techniques are often restricted to a specific type of predictor or based on input saliency, which may be undesirably sensitive to factors unrelated to the model's decision making process. We instead propose sufficient input subsets that identify minimal subsets of features whose observed values alone suffice for the same decision to be reached, even if all other input feature values are missing. General principles that globally govern a model's decision-making can also be revealed by searching for clusters of such input patterns across many data points. Our approach is conceptually straightforward, entirely model-agnostic, simply implemented using instance-wise backward selection, and able to produce more concise rationales than existing techniques. We demonstrate the utility of our interpretation method on various neural network models trained on text, image, and genomic data.\n        \u25b3 Less", "author": "David Gifford"}, {"abstract": "We present a nonparametric framework to model a short sequence of probability distributions that vary both due to underlying effects of sequential progression and confounding noise. To distinguish between these two types of variation and estimate the sequential-progression effects, our approach leverages an assumption that these effects follow a persistent trend. This work is motivated by the recent rise of single-cell RNA-sequencing experiments over a brief time course, which aim to identify genes relevant to the progression of a particular biological process across diverse cell populations. While classical statistical tools focus on scalar-response regression or order-agnostic differences between distributions, it is desirable in this setting to consider both the full distributions as well as the structure imposed by their ordering. We introduce a new regression model for ordinal covariates where responses are univariate distributions and the underlying relationship reflects consistent changes in the distributions over increasing levels of the covariate. This concept is formalized as a \"trend\" in distributions, which we define as an evolution that is linear under the Wasserstein metric. Implemented via a fast alternating projections algorithm, our method exhibits numerous strengths in simulations and analyses of single-cell gene expression data.\n        \u25b3 Less", "author": "David Gifford"}, {"abstract": "In this paper we study the fine-grained complexity of finding exact and approximate solutions to problems in P. Our main contribution is showing reductions from exact to approximate solution for a host of such problems.\n  As one (notable) example, we show that the Closest-LCS-Pair problem (Given two sets of strings $A$ and $B$, compute exactly the maximum $\\textsf{LCS}(a, b)$ with $(a, b) \\in A \\times B$) is equivalent to its approximation version (under near-linear time reductions, and with a constant approximation factor). More generally, we identify a class of problems, which we call BP-Pair-Class, comprising both exact and approximate solutions, and show that they are all equivalent under near-linear time reductions.\n  Exploring this class and its properties, we also show:\n  $\\bullet$ Under the NC-SETH assumption (a significantly more relaxed assumption than SETH), solving any of the problems in this class requires essentially quadratic time.\n  $\\bullet$ Modest improvements on the running time of known algorithms (shaving log factors) would imply that NEXP is not in non-uniform $\\textsf{NC}^1$.\n  $\\bullet$ Finally, we leverage our techniques to show new barriers for deterministic approximation algorithms for LCS.\n  At the heart of these new results is a deep connection between interactive proof systems for bounded-space computations and the fine-grained complexity of exact and approximate solutions to problems in P. In particular, our results build on the proof techniques from the classical IP = PSPACE result.\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "We introduce a new coordination problem in distributed computing that we call the population stability problem. A system of agents each with limited memory and communication, as well as the ability to replicate and self-destruct, is subjected to attacks by a worst-case adversary that can at a bounded rate (1) delete agents chosen arbitrarily and (2) insert additional agents with arbitrary initial state into the system. The goal is perpetually to maintain a population whose size is within a constant factor of the target size $N$. The problem is inspired by the ability of complex biological systems composed of a multitude of memory-limited individual cells to maintain a stable population size in an adverse environment. Such biological mechanisms allow organisms to heal after trauma or to recover from excessive cell proliferation caused by inflammation, disease, or normal development.\n  We present a population stability protocol in a communication model that is a synchronous variant of the population model of Angluin et al. In each round, pairs of agents selected at random meet and exchange messages, where at least a constant fraction of agents is matched in each round. Our protocol uses three-bit messages and $\u03c9(\\log^2 N)$ states per agent. We emphasize that our protocol can handle an adversary that can both insert and delete agents, a setting in which existing approximate counting techniques do not seem to apply. The protocol relies on a novel coloring strategy in which the population size is encoded in the variance of the distribution of colors. Individual agents can locally obtain a weak estimate of the population size by sampling from the distribution, and make individual decisions that robustly maintain a stable global population size.\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "We introduce pseudo-deterministic interactive proofs (psdAM): interactive proof systems for search problems where the verifier is guaranteed with high probability to output the same output on different executions. As in the case with classical interactive proofs, the verifier is a probabilistic polynomial time algorithm interacting with an untrusted powerful prover.\n  We view pseudo-deterministic interactive proofs as an extension of the study of pseudo-deterministic randomized polynomial time algorithms: the goal of the latter is to find canonical solutions to search problems whereas the goal of the former is to prove that a solution to a search problem is canonical to a probabilistic polynomial time verifier. Alternatively, one may think of the powerful prover as aiding the probabilistic polynomial time verifier to find canonical solutions to search problems, with high probability over the randomness of the verifier. The challenge is that pseudo-determinism should hold not only with respect to the randomness, but also with respect to the prover: a malicious prover should not be able to cause the verifier to output a solution other than the unique canonical one.\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "The availability of vast amounts of data is changing how we can make medical discoveries, predict global market trends, save energy, and develop educational strategies. In some settings such as Genome Wide Association Studies or deep learning, sheer size of data seems critical. When data is held distributedly by many parties, they must share it to reap its full benefits.\n  One obstacle to this revolution is the lack of willingness of different parties to share data, due to reasons such as loss of privacy or competitive edge. Cryptographic works address privacy aspects, but shed no light on individual parties' losses/gains when access to data carries tangible rewards. Even if it is clear that better overall conclusions can be drawn from collaboration, are individual collaborators better off by collaborating? Addressing this question is the topic of this paper.\n  * We formalize a model of n-party collaboration for computing functions over private inputs in which participants receive their outputs in sequence, and the order depends on their private inputs. Each output \"improves\" on preceding outputs according to a score function.\n  * We say a mechanism for collaboration achieves collaborative equilibrium if it ensures higher reward for all participants when collaborating (rather than working alone). We show that in general, computing a collaborative equilibrium is NP-complete, yet we design efficient algorithms to compute it in a range of natural model settings.\n  Our collaboration mechanisms are in the standard model, and thus require a central trusted party; however, we show this assumption is unnecessary under standard cryptographic assumptions. We show how to implement the mechanisms in a decentralized way with new extensions of secure multiparty computation that impose order/timing constraints on output delivery to different players, as well as privacy and correctness.\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "The full-information model was introduced by Ben-Or and Linial in 1985 to study collective coin-flipping: the problem of generating a common bounded-bias bit in a network of $n$ players with $t=t(n)$ faults. They showed that the majority protocol can tolerate $t=O(\\sqrt n)$ adaptive corruptions, and conjectured that this is optimal in the adaptive setting. Lichtenstein, Linial, and Saks proved that the conjecture holds for protocols in which each player sends a single bit. Their result has been the main progress on the conjecture in the last 30 years.\n  In this work we revisit this question and ask: what about protocols involving longer messages? Can increased communication allow for a larger fraction of faulty players?\n  We introduce a model of strong adaptive corruptions, where in each round, the adversary sees all messages sent by honest parties and, based on the message content, decides whether to corrupt a party (and intercept his message) or not. We prove that any one-round coin-flipping protocol, regardless of message length, is secure against at most $\\tilde{O}(\\sqrt n)$ strong adaptive corruptions. Thus, increased message length does not help in this setting.\n  We then shed light on the connection between adaptive and strongly adaptive adversaries, by proving that for any symmetric one-round coin-flipping protocol secure against $t$ adaptive corruptions, there is a symmetric one-round coin-flipping protocol secure against $t$ strongly adaptive corruptions. Returning to the standard adaptive model, we can now prove that any symmetric one-round protocol with arbitrarily long messages can tolerate at most $\\tilde{O}(\\sqrt n)$ adaptive corruptions.\n  At the heart of our results lies a novel use of the Minimax Theorem and a new technique for converting any one-round secure protocol into a protocol with messages of $polylog(n)$ bits. This technique may be of independent interest.\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "In this paper we show that the existence of general indistinguishability obfuscators conjectured in a few recent works implies, somewhat counterintuitively, strong impossibility results for virtual black box obfuscation. In particular, we show that indistinguishability obfuscation for all circuits implies:\n  * The impossibility of average-case virtual black box obfuscation with auxiliary input for any circuit family with super-polynomial pseudo-entropy. Such circuit families include all pseudo-random function families, and all families of encryption algorithms and randomized digital signatures that generate their required coin flips pseudo-randomly. Impossibility holds even when the auxiliary input depends only on the public circuit family, and not the specific circuit in the family being obfuscated.\n  * The impossibility of average-case virtual black box obfuscation with a universal simulator (with or without any auxiliary input) for any circuit family with super-polynomial pseudo-entropy.\n  These bounds significantly strengthen the impossibility results of Goldwasser and Kalai (STOC 2005).\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "Madhu Sudan's work spans many areas of computer science theory including computational complexity theory, the design of efficient algorithms, algorithmic coding theory, and the theory of program checking and correcting.\n  Two results of Sudan stand out in the impact they have had on the mathematics of computation. The first work shows a probabilistic characterization of the class NP -- those sets for which short and easily checkable proofs of membership exist, and demonstrates consequences of this characterization to classifying the complexity of approximation problems. The second work shows a polynomial time algorithm for list decoding the Reed Solomon error correcting codes.\n  This short note will be devoted to describing Sudan's work on probabilistically checkable proofs -- the so called {\\it PCP theorem} and its implications.\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "Theoretical computer science has found fertile ground in many areas of mathematics. The approach has been to consider classical problems through the prism of computational complexity, where the number of basic computational steps taken to solve a problem is the crucial qualitative parameter. This new approach has led to a sequence of advances, in setting and solving new mathematical challenges as well as in harnessing discrete mathematics to the task of solving real-world problems.\n  In this talk, I will survey the development of modern cryptography -- the mathematics behind secret communications and protocols -- in this light. I will describe the complexity theoretic foundations underlying the cryptographic tasks of encryption, pseudo-randomness number generators and functions, zero knowledge interactive proofs, and multi-party secure protocols. I will attempt to highlight the paradigms and proof techniques which unify these foundations, and which have made their way into the mainstream of complexity theory.\n        \u25b3 Less", "author": "Shafi Goldwasser"}, {"abstract": "We propose a new iterative segmentation model which can be accurately learned from a small dataset. A common approach is to train a model to directly segment an image, requiring a large collection of manually annotated images to capture the anatomical variability in a cohort. In contrast, we develop a segmentation model that recursively evolves a segmentation in several steps, and implement it as a recurrent neural network. We learn model parameters by optimizing the interme- diate steps of the evolution in addition to the final segmentation. To this end, we train our segmentation propagation model by presenting incom- plete and/or inaccurate input segmentations paired with a recommended next step. Our work aims to alleviate challenges in segmenting heart structures from cardiac MRI for patients with congenital heart disease (CHD), which encompasses a range of morphological deformations and topological changes. We demonstrate the advantages of this approach on a dataset of 20 images from CHD patients, learning a model that accurately segments individual heart chambers and great vessels. Com- pared to direct segmentation, the iterative method yields more accurate segmentation for patients with the most severe CHD malformations.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "We present an algorithm for creating high resolution anatomically plausible images consistent with acquired clinical brain MRI scans with large inter-slice spacing. Although large data sets of clinical images contain a wealth of information, time constraints during acquisition result in sparse scans that fail to capture much of the anatomy. These characteristics often render computational analysis impractical as many image analysis algorithms tend to fail when applied to such images. Highly specialized algorithms that explicitly handle sparse slice spacing do not generalize well across problem domains. In contrast, we aim to enable application of existing algorithms that were originally developed for high resolution research scans to significantly undersampled scans. We introduce a generative model that captures fine-scale anatomical structure across subjects in clinical image collections and derive an algorithm for filling in the missing data in scans with large inter-slice spacing. Our experimental results demonstrate that the resulting method outperforms state-of-the-art upsampling super-resolution techniques, and promises to facilitate subsequent analysis not previously possible with scans of this quality. Our implementation is freely available at https://github.com/adalca/papago .\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "We introduce an approach for image segmentation based on sparse correspondences between keypoints in testing and training images. Keypoints represent automatically identified distinctive image locations, where each keypoint correspondence suggests a transformation between images. We use these correspondences to transfer label maps of entire organs from the training images to the test image. The keypoint transfer algorithm includes three steps: (i) keypoint matching, (ii) voting-based keypoint labeling, and (iii) keypoint-based probabilistic transfer of organ segmentations. We report segmentation results for abdominal organs in whole-body CT and MRI, as well as in contrast-enhanced CT and MRI. Our method offers a speed-up of about three orders of magnitude in comparison to common multi-atlas segmentation, while achieving an accuracy that compares favorably. Moreover, keypoint transfer does not require the registration to an atlas or a training phase. Finally, the method allows for the segmentation of scans with highly variable field-of-view.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "A reliable Ultrasound (US)-to-US registration method to compensate for brain shift would substantially improve Image-Guided Neurological Surgery. Developing such a registration method is very challenging, due to factors such as missing correspondence in images, the complexity of brain pathology and the demand for fast computation. We propose a novel feature-driven active framework. Here, landmarks and their displacement are first estimated from a pair of US images using corresponding local image features. Subsequently, a Gaussian Process (GP) model is used to interpolate a dense deformation field from the sparse landmarks. Kernels of the GP are estimated by using variograms and a discrete grid search method. If necessary, the user can actively add new landmarks based on the image context and visualization of the uncertainty measure provided by the GP to further improve the result. We retrospectively demonstrate our registration framework as a robust and accurate brain shift compensation solution on clinical data acquired during neurosurgery.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "Estimating the uncertainty in image registration is an area of current research that is aimed at providing information that will enable surgeons to assess the operative risk based on registered image data and the estimated registration uncertainty. If they receive inaccurately calculated registration uncertainty and misplace confidence in the alignment solutions, severe consequences may result. For probabilistic image registration (PIR), most research quantifies the registration uncertainty using summary statistics of the transformation distributions. In this paper, we study a rarely examined topic: whether those summary statistics of the transformation distribution truly represent the registration uncertainty. Using concrete examples, we show that there are two types of uncertainties: the transformation uncertainty, Ut, and label uncertainty Ul. Ut indicates the doubt concerning transformation parameters and can be estimated by conventional uncertainty measures, while Ul is strongly linked to the goal of registration. Further, we show that using Ut to quantify Ul is inappropriate and can be misleading. In addition, we present some potentially critical findings regarding PIR.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "We present a robust method to correct for motion and deformations for in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI requires robust alignment across time in the presence of substantial and unpredictable motion. We make a Markov assumption on the nature of deformations to take advantage of the temporal structure in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We demonstrate the utility of the temporal model by showing that its use improves the accuracy of the segmentation propagation through temporal registration. Our results suggest that the proposed model captures accurately the temporal dynamics of deformations in in-utero MRI time series.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "Despite the popularity and empirical success of patch-based nearest-neighbor and weighted majority voting approaches to medical image segmentation, there has been no theoretical development on when, why, and how well these nonparametric methods work. We bridge this gap by providing a theoretical performance guarantee for nearest-neighbor and weighted majority voting segmentation under a new probabilistic model for patch-based image segmentation. Our analysis relies on a new local property for how similar nearby patches are, and fuses existing lines of work on modeling natural imagery patches and theory for nonparametric classification. We use the model to derive a new patch-based segmentation algorithm that iterates between inferring local label patches and merging these local segmentations to produce a globally consistent image segmentation. Many existing patch-based algorithms arise as special cases of the new algorithm.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystr\u00f6m method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "We propose a novel diverse feature selection method based on determinantal point processes (DPPs). Our model enables one to flexibly define diversity based on the covariance of features (similar to orthogonal matching pursuit) or alternatively based on side information. We introduce our approach in the context of Bayesian sparse regression, employing a DPP as a variational approximation to the true spike and slab posterior distribution. We subsequently show how this variational DPP approximation generalizes and extends mean-field approximation, and can be learned efficiently by exploiting the fast sampling properties of DPPs. Our motivating application comes from bioinformatics, where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network. We also explore an application in spatial statistics. In both cases, we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods, without compromising accuracy.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "Manifold learning has been successfully applied to a variety of medical imaging problems. Its use in real-time applications requires fast projection onto the low-dimensional space. To this end, out-of-sample extensions are applied by constructing an interpolation function that maps from the input space to the low-dimensional manifold. Commonly used approaches such as the Nystr\u00f6m extension and kernel ridge regression require using all training points. We propose an interpolation function that only depends on a small subset of the input training data. Consequently, in the testing phase each new point only needs to be compared against a small number of input training data in order to project the point onto the low-dimensional space. We interpret our method as an out-of-sample extension that approximates kernel ridge regression. Our method involves solving a simple convex optimization problem and has the attractive property of guaranteeing an upper bound on the approximation error, which is crucial for medical applications. Tuning this error bound controls the sparsity of the resulting interpolation function. We illustrate our method in two clinical applications that require fast mapping of input images onto a low-dimensional space.\n        \u25b3 Less", "author": "Polina Golland"}, {"abstract": "We present VoxelMorph, a fast, unsupervised, learning-based algorithm for deformable pairwise medical image registration. Traditional registration methods optimize an objective function independently for each pair of images, which is time-consuming for large datasets. We define registration as a parametric function, implemented as a convolutional neural network (CNN). We optimize its global parameters given a set of images from a collection of interest. Given a new pair of scans, VoxelMorph rapidly computes a deformation field by directly evaluating the function. Our model is flexible, enabling the use of any differentiable objective function to optimize these parameters. In this work, we propose and extensively evaluate a standard image matching objective function as well as an objective function that can use auxiliary data such as anatomical segmentations available only at training time. We demonstrate that the unsupervised model's accuracy is comparable to state-of-the-art methods, while operating orders of magnitude faster. We also show that VoxelMorph trained with auxiliary data significantly improves registration accuracy at test time. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is freely available at voxelmorph.csail.mit.edu.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "Thanks to the rapid proliferation of connected devices, sensor-generated time series constitute a large and growing portion of the world's data. Often, this data is collected from distributed, resource-constrained devices and centralized at one or more servers. A key challenge in this setup is reducing the size of the transmitted data without sacrificing its quality. Lower quality reduces the data's utility, but smaller size enables both reduced network and storage costs at the servers and reduced power consumption in sensing devices. A natural solution is to compress the data at the sensing devices. Unfortunately, existing compression algorithms either violate the memory and latency constraints common for these devices or, as we show experimentally, perform poorly on sensor-generated time series.\n  We introduce a time series compression algorithm that achieves state-of-the-art compression ratios while requiring less than 1KB of memory and adding virtually no latency. This method is suitable not only for low-power devices collecting data, but also for servers storing and querying data; in the latter context, it can decompress at over 3GB/s in a single thread, even faster than many algorithms with much lower compression ratios. A key component of our method is a high-speed forecasting algorithm that can be trained online and significantly outperforms alternatives such as delta coding.\n  Extensive experiments on datasets from many domains show that these results hold not only for sensor data but also across a wide array of other time series.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "Machine learning approaches have been effective in predicting adverse outcomes in different clinical settings. These models are often developed and evaluated on datasets with heterogeneous patient populations. However, good predictive performance on the aggregate population does not imply good performance for specific groups.\n  In this work, we present a two-step framework to 1) learn relevant patient subgroups, and 2) predict an outcome for separate patient populations in a multi-task framework, where each population is a separate task. We demonstrate how to discover relevant groups in an unsupervised way with a sequence-to-sequence autoencoder. We show that using these groups in a multi-task framework leads to better predictive performance of in-hospital mortality both across groups and overall. We also highlight the need for more granular evaluation of performance when dealing with heterogeneous populations.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "Electronic Health Records (EHRs) contain a large volume of heterogeneous patient data, which are useful at the point of care and for retrospective research. These data are typically stored in relational databases. Gaining an integrated view of these data for a single patient typically requires complex SQL queries joining multiple tables. In this work, we present a visualization tool that integrates heterogeneous health care data (e.g., clinical notes, laboratory test values, vital signs) into a single timeline. We train risk models offline and dynamically generate and present their predictions alongside patient data. Our visualization is designed to enable users to understand the heterogeneous temporal data quickly and comprehensively, and to place the output of analytic models in the context of the underlying data.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "Traditional deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates. In this paper, we present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task, and provide an empirical analysis of the algorithm. Our approach results in state of the art accuracy and very fast runtimes, while providing diffeomorphic guarantees and uncertainty estimates. Our implementation is available online at http://voxelmorph.csail.mit.edu .\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "We address the computational problem of novel human pose synthesis. Given an image of a person and a desired pose, we produce a depiction of that person in that pose, retaining the appearance of both the person and background. We present a modular generative neural network that synthesizes unseen poses using training pairs of images and poses taken from human action videos. Our network separates a scene into different body part and background layers, moves body parts to new locations and refines their appearances, and composites the new foreground with a hole-filled background. These subtasks, implemented with separate modules, are trained jointly using only a single target image as a supervised label. We use an adversarial discriminator to force our network to synthesize realistic details conditioned on pose. We demonstrate image synthesis results on three action classes: golf, yoga/workouts and tennis, and show that our method produces accurate results within action classes as well as across action classes. Given a sequence of desired poses, we also produce coherent videos of actions.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "We present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a convolutional neural network (CNN), and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph .\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "When an infection spreads in a community, an individual's probability of becoming infected depends on both her susceptibility and exposure to the contagion through contact with others. While one often has knowledge regarding an individual's susceptibility, in many cases, whether or not an individual's contacts are contagious is unknown. We study the problem of predicting if an individual will adopt a contagion in the presence of multiple modes of infection (exposure/susceptibility) and latent neighbor influence. We present a generative probabilistic model and a variational inference method to learn the parameters of our model. Through a series of experiments on synthetic data, we measure the ability of the proposed model to identify latent spreaders, and predict the risk of infection. Applied to a real dataset of 20,000 hospital patients, we demonstrate the utility of our model in predicting the onset of a healthcare associated infection using patient room-sharing and nurse-sharing networks. Our model outperforms existing benchmarks and provides actionable insights for the design and implementation of targeted interventions to curb the spread of infection.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "Vectors of data are at the heart of machine learning and data mining. Recently, vector quantization methods have shown great promise in reducing both the time and space costs of operating on vectors. We introduce a vector quantization algorithm that can compress vectors over 12x faster than existing techniques while also accelerating approximate vector operations such as distance and dot product computations by up to 10x. Because it can encode over 2GB of vectors per second, it makes vector quantization cheap enough to employ in many more circumstances. For example, using our technique to compute approximate dot products in a nested loop can multiply matrices faster than a state-of-the-art BLAS implementation, even when our algorithm must first compress the matrices.\n  In addition to showing the above speedups, we demonstrate that our approach can accelerate nearest neighbor search and maximum inner product search by over 100x compared to floating point operations and up to 10x compared to other vector quantization methods. Our approximate Euclidean distance and dot product computations are not only faster than those of related algorithms with slower encodings, but also faster than Hamming distance computations, which have direct hardware support on the tested platforms. We also assess the errors of our algorithm's approximate distances and dot products, and find that it is competitive with existing, slower vector quantization algorithms.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "For many movement disorders, such as Parkinson's disease and ataxia, disease progression is visually assessed by a clinician using a numerical disease rating scale. These tests are subjective, time-consuming, and must be administered by a professional. This can be problematic where specialists are not available, or when a patient is not consistently evaluated by the same clinician. We present an automated method for quantifying the severity of motion impairment in patients with ataxia, using only video recordings. We consider videos of the finger-to-nose test, a common movement task used as part of the assessment of ataxia progression during the course of routine clinical checkups.\n  Our method uses neural network-based pose estimation and optical flow techniques to track the motion of the patient's hand in a video recording. We extract features that describe qualities of the motion such as speed and variation in performance. Using labels provided by an expert clinician, we train a supervised learning model that predicts severity according to the Brief Ataxia Rating Scale (BARS). The performance of our system is comparable to that of a group of ataxia specialists in terms of mean error and correlation, and our system's predictions were consistently within the range of inter-rater variability. This work demonstrates the feasibility of using computer vision and machine learning to produce consistent and clinically useful measures of motor impairment.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "Thanks to the rise of wearable and connected devices, sensor-generated time series comprise a large and growing fraction of the world's data. Unfortunately, extracting value from this data can be challenging, since sensors report low-level signals (e.g., acceleration), not the high-level events that are typically of interest (e.g., gestures). We introduce a technique to bridge this gap by automatically extracting examples of real-world events in low-level data, given only a rough estimate of when these events have taken place.\n  By identifying sets of features that repeat in the same temporal arrangement, we isolate examples of such diverse events as human actions, power consumption patterns, and spoken words with up to 96% precision and recall. Our method is fast enough to run in real time and assumes only minimal knowledge of which variables are relevant or the lengths of events. Our evaluation uses numerous publicly available datasets and over 1 million samples of manually labeled sensor data.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "Voice disorders affect an estimated 14 million working-aged Americans, and many more worldwide. We present the first large scale study of vocal misuse based on long-term ambulatory data collected by an accelerometer placed on the neck. We investigate an unsupervised data mining approach to uncovering latent information about voice misuse.\n  We segment signals from over 253 days of data from 22 subjects into over a hundred million single glottal pulses (closures of the vocal folds), cluster segments into symbols, and use symbolic mismatch to uncover differences between patients and matched controls, and between patients pre- and post-treatment. Our results show significant behavioral differences between patients and controls, as well as between some pre- and post-treatment patients. Our proposed approach provides an objective basis for helping diagnose behavioral voice disorders, and is a first step towards a more data-driven understanding of the impact of voice therapy.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "In many domains such as medicine, training data is in short supply. In such cases, external knowledge is often helpful in building predictive models. We propose a novel method to incorporate publicly available domain expertise to build accurate models. Specifically, we use word2vec models trained on a domain-specific corpus to estimate the relevance of each feature's text description to the prediction problem. We use these relevance estimates to rescale the features, causing more important features to experience weaker regularization.\n  We apply our method to predict the onset of five chronic diseases in the next five years in two genders and two age groups. Our rescaling approach improves the accuracy of the model, particularly when there are few positive examples. Furthermore, our method selects 60% fewer features, easing interpretation by physicians. Our method is applicable to other domains where feature and outcome descriptions are available.\n        \u25b3 Less", "author": "John Guttag"}, {"abstract": "We recently put forth a new fundamental lattice Hamiltonian based on an underlying picture of electrons and deuterons as elementary Dirac particles. Within this model there appears a term in which lattice vibrations are coupled to internal nuclear transitions. This is interesting as it has the potential to provide a connection between experiment and models that describe coherent energy transfer between two-level systems and an oscillator. In this work we describe a calculation of the coupling matrix element in the case of the deuteron based on the old empirical Hamada-Johnston model for the nucleon-nucleon interaction. The triplet S and D states of the the deuteron in the rest frame couples to a singlet P state through this new interaction. The singlet P state in this calculation is a virtual state with an energy of 125 MeV, and a coupling matrix element for $z$-directed motion given by $2.98 \\times 10^{-3} ~M_J c \\hat{P}_z$.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "Motivated by many observations of anomalies in condensed matter systems, we consider a new fundamental Hamiltonian in which condensed matter and nuclear systems are described initially on the same footing. Since it may be possible that the lattice will respond to the mass change associated with a excited nuclear state, we adopt a relativistic description throughout based on a many-particle Dirac formalism. This approach has not been used in the past, perhaps due to the difficulty in separating the center of mass and relative degrees of freedom of the nuclear system, or perhaps due to an absence of applications for such a model. In response to some recent ideas about how to think about the center of mass and relative separation, we obtained from the Dirac model a new fundamental Hamiltonian in which the lattice couples to different states within the composite nuclei within the lattice. In this description the different nuclear states have different mass energies and kinetic energies, as we had expected. In addition there appear new terms which provide for nuclear excitation as a result of coupling to the composite momentum. This new effect comes about because of changes in the composite nuclear state as a result of the dynamical Lorentz boost in the lattice.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "We are interested in the energy-momentum relation for a moving composite in relativistic quantum mechanics in many-particle Dirac models. For a manifestly covariant model one can apply the Lorentz transform to go from the rest frame to a moving frame to establish an energy-momentum relation of the form $\\sqrt{(M^*c^2)^2+c^2|{\\bf P}|^2}$ where $M^*$ is the kinematic mass. However, the many-particle Dirac model is not manifestly covariant, and some other approach is required. We have found a simple approach that allows for a separation of relative and center of mass contributions to the energy. We are able to define the associated kinematic energy and determine the energy-momentum relation. Our result can be expressed as a modified deBroglie relation of the form\n  $$ \\hbar \u03c9({\\bf P}) = <\u03a6' | \\sum_j {m_j \\over M} \u03b2_j | \u03a6' >~ \\sqrt{[M^*({\\bf P}) c^2]^2 + c^2 |{\\bf P}|^2} $$\n  where the kinematic mass $M^*$ will depend on the total momentum ${\\bf P}$ for a general noncovariant potential. The prefactor that occurs we associate with a time dilation effect, the existence of which has been discussed previously in the literature.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "We consider a generalization of the spin-boson model in which two different two-level systems are coupled to an oscillator, under conditions where the oscillator energy is much less than the two-level system energies, and where the oscillator is highly excited. We find that the two-level system transition energy is shifted, producing a Bloch-Siegert shift in each two-level system similar to what would be obtained if the other were absent. At resonances associated with energy exchange between a two-level system and the oscillator, the level splitting is about the same as would be obtained in the spin-boson model at a Bloch-Siegert resonance. However, there occur resonances associated with the transfer of excitation between one two-level system and the other, an effect not present in the spin-boson model. We use a unitary transformation leading to a rotated system in which terms responsible for the shift and splittings can be identified. The level splittings at the anticrossings associated with both energy exchange and excitation transfer resonances are accounted for with simple two-state models and degenerate perturbation theory using operators that appear in the rotated Hamiltonian.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "In previous work we studied the spin-boson model in the multiphoton regime, using a rotation that provides a separation between terms that contribute most of the level energies away from resonance, and terms responsible for the level splittings at the anticrossing. Here, we consider a generalization of the spin-boson model consisting of a three-level system coupled to an oscillator. We construct a similar rotation and apply it to the more complicated model. We find that the rotation provides a useful approximation to the energy levels in the multiphoton region of the new problem. We find that good results can be obtained for the level splittings at the anticrossings for resonances involving the lower two levels in regions away from accidental or low-order resonances of the upper two levels.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "The electron mass is known to be sensitive to local fluctuations in the electromagnetic field, and undergoes a small shift in a thermal field. It was claimed recently that a very large electron mass shift should be expected near the surface of a metal hydride [{\\it Eur. Phys. J. C}, {\\bf 46} 107 (2006)]. We examine the shift using a formulation based on the Coulomb gauge, which leads to a much smaller shift. The maximization of the electron mass shift under nonequilibrium conditions seems nonetheless to be an interesting problem. We consider a scheme in which a current in a hollow wire produces a large vector potential in the wire center. Fluctuations in an LC circuit with nearly matched loss and gain can produce large current fluctuations; and these can increase the electron mass shift by orders of magnitude over its room temperature value.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "We consider a spin-boson model in which a spin 1 system is coupled to an oscillator. A unitary transformation is applied which allows a separation of terms responsible for the Bloch-Siegert shift, and terms responsible for the level splittings at anticrossings associated with Bloch-Siegert resonances. When the oscillator is highly excited, the system can maintain resonance for sequential multiphoton transitions. At lower levels of excitation, resonance cannot be maintained because energy exchange with the oscillator changes the level shift. An estimate for the critical excitation level of the oscillator is developed.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "We present a unitary equivalent spin-boson Hamiltonian in which terms can be identified which contribute to the Bloch-Siegert shift, and to the level splittings at the anticrossings associated with the Bloch-Siegert resonances. First-order degenerate perturbation theory is used to develop approximate results in the case of moderate coupling for the level splitting.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "Recently there has been theoretical and experimental interest in Bloch-Siegert shifts in an intense photon field. A perturbative treatment becomes difficult in this multiphoton regime. We present a unitary transform and rotated model, which allows us to get accurate results away from the level anticrossings. A simple variational energy estimate leads to a new expression for the dressed two-level system energy which is accurate, and useful over a wide range of the dimensionless coupling constant.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "We consider models in which two sets of matched two-level systems are coupled to a common oscillator in the case where the oscillator energy is small relative to the two-level transition energies. Since the two sets of two-level systems are coupled indirectly through the oscillator, excitation transfer from one set of two-level systems to the other is possible. In addition, the excitation energy from the two-level systems may be exchanged with the oscillator coherently, even though the oscillator energy may be orders of magnitude smaller than the two-level system transition energy. In the lossless case, we demonstrate these effects numerically, and also use an approximate diagonalization to show that these effects are expected from the model Hamiltonian.\n  We augment the model to include loss effects, and show that loss enhances the excitation transfer effect by breaking the severe cancelation between different paths that occurs in the lossless case. We describe a simple approximate model wavefunction appropriate when the loss increases rapidly with energy. Within this model approximation, we present numerical and analytical results for excitation transfer and energy transfer rates, showing that they are greatly increased.\n  Our study of these models is motivated in part by claims of excess heat production in electrochemical experiments in heavy water. We examine the question of whether the rates associated with this kind of model are sufficiently large to be relevant to the experimental claims. We find that consistency is possible given recent experimental results showing strong screening effects in low energy deuteron-deuteron fusion experiments in metals.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "Phonon exchange with nuclei in the course of fusion reactions that occur in a solid has not been analyzed previously. This problem has become of interest in connection with claims of observations of anomalies in metal deuterides. If the strong force interaction were dependent only on position (and not spin or isospin), then the coupling with phonons can be developed directly. Since a nuclear interaction can change the lattice constituents, the initial and final state lattices can be different, and we must include this in the formulation. For more realistic strong force models with spin and isospin dependence, we can use correlated nuclear wavefunctions which are made up of products of space, spin and isospin components. In this case, the spin and isospin algebra can be done analytically, producing channel-dependent potentials that are only space dependent. The formulation that results can be used for quantitative estimates of phonon exchange.\n        \u25b3 Less", "author": "Peter Hagelstein"}, {"abstract": "We report a study of the decay $D^0 \\rightarrow \\bar{K}^0\u03c0^-e^+\u03bd_{e}$ based on a sample of $2.93~\\mathrm{fb}^{-1}$ $e^+e^-$ annihilation data collected at the center-of-mass energy of 3.773~GeV with the BESIII detector at the BEPCII collider. The total branching fraction is determined to be $\\mathcal{B}(D^0\\rightarrow \\bar{K}^0\u03c0^-e^+\u03bd_{e})=(1.434\\pm0.029({\\rm stat.})\\pm0.032({\\rm syst.}))\\%$, which is the most precise to date. According to a detailed analysis of the involved dynamics, we find this decay is dominated with the $K^{*}(892)^-$ contribution and present an improved measurement of its branching fraction to be $\\mathcal{B}(D^0\\rightarrow K^{*}(892)^-e^+\u03bd_e)=(2.033\\pm0.046({\\rm stat.})\\pm0.047({\\rm syst.}))\\%$. We further access their hadronic form-factor ratios for the first time as $r_{V}=V(0)/A_1(0)=1.46\\pm0.07({\\rm stat.})\\pm0.02({\\rm syst.})$ and $r_{2}=A_2(0)/A_1(0)=0.67\\pm0.06({\\rm stat.})\\pm0.01({\\rm syst.})$. In addition, we observe a significant $\\bar{K}^0\u03c0^-$ $S$-wave component accounting for $(5.51\\pm0.97({\\rm stat.})\\pm0.62({\\rm syst.}))\\%$ of the total decay rate.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using a $3.19~\\mathrm{fb}^{-1}$ data sample collected at an $e^+e^-$ center-of-mass energy of $E_{\\rm cm}=4.178$\\,GeV with the BESIII detector, we measure the branching fraction of the leptonic decay $D_s^+\\to\u03bc^+\u03bd_\u03bc$ to be $\\mathcal{B}_{D_s^+\\to\u03bc^+\u03bd_\u03bc}=(5.49\\pm0.16_{\\rm stat.}\\pm0.15_{\\rm syst.})\\times10^{-3}$. Combining our branching fraction with the masses of the $D_s^+$ and $\u03bc^+$ and the lifetime of the $D_s^+$, we determine $f_{D_s^+}|V_{cs}|=246.2\\pm3.6_{\\rm stat.}\\pm3.5_{\\rm syst.}~\\mathrm{MeV}$. Using the $c\\to s$ quark mixing matrix element $|V_{cs}|$ determined from a global standard model fit, we evaluate the $D_s^+$ decay constant $f_{D_s^+}=252.9\\pm3.7_{\\rm stat.}\\pm3.6_{\\rm syst.}$\\,MeV. Alternatively, using the value of $f_{D_s^+}$ calculated by lattice quantum chromodynamics, we find $|V_{cs}| = 0.985\\pm0.014_{\\rm stat.}\\pm0.014_{\\rm syst.}$. These values of $\\mathcal{B}_{D_s^+\\to\u03bc^+\u03bd_\u03bc}$, $f_{D_s^+}|V_{cs}|$, $f_{D_s^+}$ and $|V_{cs}|$ are each the most precise results to date.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support flexible bitwidth (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, power, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in an uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, power and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "The cross section of the process $e^{+} e^{-} \\rightarrow K^{+} K^{-}$ is measured at a number of center-of-mass energies $\\sqrt{s}$ from 2.00 to 3.08 GeV with the BESIII detector at the Beijing Electron Positron Collider (BEPCII). The results provide the best precision achieved so far. A resonant structure around 2.2 GeV is observed in the cross section line shape. A Breit-Wigner fit yields a mass of $M=2239.2 \\pm 7.1 \\pm 11.3$~and a width of $\u0393=139.8\\pm12.3\\pm20.6$ MeV, where the first uncertainties are statistical and the second ones are systematic. In addition, the time-like electromagnetic form factor of the kaon is determined at the individual center-of-mass energy points.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "The explosive growth in online video streaming gives rise to challenges on efficiently extracting the spatial-temporal information to perform video understanding. Conventional 2D CNNs are computationally cheap but cannot capture long-term temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D complexity. The central idea of TSM is to shift part of the channels along the temporal dimension, which facilitates information exchange among neighboring frames. TSM can be inserted into 2D CNNs to achieve temporal modeling at the cost of zero FLOPs and zero parameters. On the Something-Something-V1 dataset which focuses on temporal modeling, we achieved better results than I3D family and ECO family using 6X and 2.7X fewer FLOPs respectively. Measured on P100 GPU, our single model achieved 1.8% higher accuracy at 8X lower latency and 12X higher throughput compared to I3D. Remarkably, our framework ranks the first on both Something-Something V1 and V2 leaderboards upon this paper's submission.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We study the hadronic decays of $\u039b_{c}^{+}$ to the final states $\u03a3^{+}\u03b7$ and $\u03a3^+\u03b7^\\prime$, using an $e^{+}e^{-}$ annihilation data sample of 567 pb$^{-1}$ taken at a center-of-mass energy of 4.6 GeV with the BESIII detector at the BEPCII collider. We find evidence for the decays $\u039b_{c}^{+}\\rightarrow\u03a3^{+}\u03b7$ and $\u03a3^+\u03b7^\\prime$ with statistical significance of $2.5\u03c3$ and $3.2\u03c3$, respectively. Normalizing to the reference decays $\u039b_c^+\\to\u03a3^+\u03c0^0$ and $\u03a3^+\u03c9$, we obtain the ratios of the branching fractions $\\frac{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03b7)}{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03c0^0)}$ and $\\frac{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03b7^\\prime)}{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03c9)}$ to be $0.35 \\pm 0.16 \\pm 0.03$ and $0.86 \\pm 0.34 \\pm 0.07$, respectively. The upper limits at the 90\\% confidence level are set to be $\\frac{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03b7)}{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03c0^0)}<0.58$ and $\\frac{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03b7^\\prime)}{{\\mathcal B}(\u039b_c^+\\to\u03a3^+\u03c9)}<1.2$. Using BESIII measurements of the branching fractions of the reference decays, we determine $\\mathcal B({\u039b_{c}^{+}\\rightarrow\u03a3^{+}\u03b7})=(0.41\\pm0.19\\pm0.05)\\%$ ($<0.68\\%$) and $\\mathcal B({\u039b_{c}^{+}\\rightarrow\u03a3^{+}\u03b7'})=(1.34\\pm0.53\\pm0.21)\\%$ ($<1.9\\%$). Here, the first uncertainties are statistical and the second systematic. The obtained branching fraction of $\u039b_c^+\\to\u03a3^+\u03b7$ is consistent with the previous measurement, and the branching fraction of $\u039b_{c}^{+}\\rightarrow\u03a3^{+}\u03b7^{\\prime}$ is measured for the first time.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We consider the problem of clustering graph nodes over large-scale dynamic graphs, such as citation networks, images and web networks, when graph updates such as node/edge insertions/deletions are observed distributively. We propose communication-efficient algorithms for two well-established communication models namely the message passing and the blackboard models. Given a graph with $n$ nodes that is observed at $s$ remote sites over time $[1,t]$, the two proposed algorithms have communication costs $\\tilde{O}(ns)$ and $\\tilde{O}(n+s)$ ($\\tilde{O}$ hides a polylogarithmic factor), almost matching their lower bounds, $\u03a9(ns)$ and $\u03a9(n+s)$, respectively, in the message passing and the blackboard models. More importantly, we prove that at each time point in $[1,t]$ our algorithms generate clustering quality nearly as good as that of centralizing all updates up to that time and then applying a standard centralized clustering algorithm. We conducted extensive experiments on both synthetic and real-life datasets which confirmed the communication efficiency of our approach over baseline algorithms while achieving comparable clustering results.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We study the faint stellar halo of isolated central galaxies, by stacking galaxy images in the HSC survey and accounting for the residual sky background sampled with random points. The surface brightness profiles in HSC $r$-band are measured up to 120 kpc from the galaxy center for a wide range of galaxy stellar mass ($9.2<\\log_{10}M_\\ast/M_\\odot<11.4$), and down to a surface brightness of about 32.8 $\\mathrm{mag}/\\mathrm{arcsec}^2$, with an indication of signals to even larger scales and fainter magnitudes. Failing to account for the outer stellar halo below the noise level of individual images will lead to underestimates of the total luminosity by $\\leq 20\\%$. Splitting galaxies according to the concentration parameter of their light distributions, we find that the surface brightness profiles of low concentration galaxies drop faster between 20 kpc and 100 kpc and are more extended beyond 100 kpc than those of high concentration galaxies. The profiles of low concentration galaxies persist out to the average halo virial radius. Albeit the large galaxy-to-galaxy scatter, we find a strong self-similarity of the stellar halo profiles. They show unified forms once the projected distance is scaled by the halo virial radius. The colour of the stellar halo is redder in the center and bluer outside, with high concentration galaxies having redder and flatter colour profiles. Such a colour gradient persists to about 80 kpc for galaxies more massive than $10^{10.2}M_\\odot$, whereas for galaxies with $9.2<\\log_{10}M_\\ast/M_\\odot<10.2$, the gradient is consistent with being flat between 10 kpc and 30 kpc.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using a low-background sample of $2.6\\times 10^5$ $J/\u03c8\\rightarrow\u03c9\u03b7(\u03c9\\rightarrow\u03c0^{+}\u03c0^{-}\u03c0^{0},\u03b7\\rightarrow\u03b3\u03b3)$ events, about 5 times larger statistics than previous experiments, we present a Dalitz plot analysis of the decay $\u03c9\\rightarrow\u03c0^{+}\u03c0^{-}\u03c0^{0}$. It is found that the Dalitz plot distribution differs from the pure $P$-wave phase space with a statistical significance of $18.9\u03c3$. The parameters from the fit to data are in reasonable agreement with those without the cross-channel effect within the dispersive framework, which indicates that the cross-channel effect in $\u03c9\\rightarrow\u03c0^+\u03c0^-\u03c0^0$ is not significant.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We report on new measurements of Cabibbo-suppressed semileptonic $D_s^+$ decays using $3.19~\\mathrm{fb}^{-1}$ of $e^+e^-$ annihilation data sample collected at a center-of-mass energy of 4.178~GeV with the BESIII detector at the BEPCII collider. Our results include branching fractions $\\mathcal B({D^+_s\\rightarrow K^0 e^+\u03bd_{e}})=(3.25\\pm0.38({\\rm stat.})\\pm0.16({\\rm syst.}))\\times10^{-3}$ and $\\mathcal B({D^+_s\\rightarrow K^{*0} e^+\u03bd_{e}})=(2.37\\pm0.26({\\rm stat.})\\pm0.20({\\rm syst.}))\\times10^{-3}$ which are much improved relative to previous measurements, and the first measurements of the hadronic form-factor parameters for these decays. For $D^+_s\\rightarrow K^0 e^+\u03bd_{e}$, we obtain $f_+(0)=0.720\\pm0.084({\\rm stat.})\\pm0.013({\\rm syst.})$, and for $D^+_s\\rightarrow K^{*0} e^+\u03bd_{e}$, we find form-factor ratios $r_V=V(0)/A_1(0)=1.67\\pm0.34({\\rm stat.})\\pm0.16({\\rm syst.})$ and $r_2=A_2(0)/A_1(0)=0.77\\pm0.28({\\rm stat.})\\pm0.07({\\rm syst.})$.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e. 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that undergone gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "The baryonic decay $D^+_s\\rightarrow p\\bar{n}$ is observed for the first time with a statistical significance of much larger than 10 standard deviations, and the corresponding branching fraction is measured to be $(1.21\\pm0.10\\pm0.06)\\times10^{-3}$, where the first uncertainty is statistical and second systematic. The data sample used in this analysis was collected with the BESIII detector operating at the BEPCII $e^+e^-$ double-ring collider with a center-of-mass energy of 4.178~GeV and an integrated luminosity of 3.19~fb$^{-1}$. The result confirms the previous measurement by the CLEO Collaboration and is of greatly improved precision, which may deepen our understanding of the dynamical enhancement of the W-annihilation topology in the charmed meson decays.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Many advances of deep learning techniques originate from the efforts of addressing the image classification task on large-scale datasets. However, the construction of such clean datasets is costly and time-consuming since the Internet is overwhelmed by noisy images with inadequate and inaccurate tags. In this paper, we propose a Ubiquitous Reweighting Network (URNet) that learns an image classification model from large-scale noisy data. By observing the web data, we find that there are five key challenges, \\ie, imbalanced class sizes, high intra-classes diversity and inter-class similarity, imprecise instances, insufficient representative instances, and ambiguous class labels. To alleviate these challenges, we assume that every training instance has the potential to contribute positively by alleviating the data bias and noise via reweighting the influence of each instance according to different class sizes, large instance clusters, its confidence, small instance bags and the labels. In this manner, the influence of bias and noise in the web data can be gradually alleviated, leading to the steadily improving performance of URNet. Experimental results in the WebVision 2018 challenge with 16 million noisy training images from 5000 classes show that our approach outperforms state-of-the-art models and ranks the first place in the image classification task.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We report on the observation of the $W$-annihilation decay $D^{+}_{s} \\rightarrow \u03c9\u03c0^{+}$ and the evidence for $D_{s}^{+} \\rightarrow \u03c9K^{+}$ with a data sample corresponding to an integrated luminosity of 3.19 fb$^{-1}$ collected with the BESIII detector at the center-of-mass energy $\\sqrt{s} = 4.178$ GeV. We obtain the branching fractions $\\mathcal{B}(D^{+}_{s} \\rightarrow \u03c9\u03c0^{+}) = (1.77\\pm0.32_{\\rm stat.}\\pm0.11_{\\rm sys.}) \\times 10^{-3}$ and $\\mathcal{B}(D^{+}_{s} \\rightarrow \u03c9K^{+}) = (0.87\\pm0.24_{\\rm stat.}\\pm0.07_{\\rm sys.}) \\times 10^{-3}$, respectively.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Based on $(4.48 \\pm 0.03) \\times 10^{8}$ $\u03c8(3686)$ events collected with the BESIII detector, five $h_c$ hadronic decays are searched for via process $\u03c8(3686) \\to \u03c0^0 h_c$. Three of them, $h_c \\to p \\bar{p} \u03c0^+ \u03c0^-$, $\u03c0^+ \u03c0^- \u03c0^0$, and $2(\u03c0^+ \u03c0^-) \u03c0^0$ are observed for the first time, with statistical significances of 7.4$\u03c3$, $4.9\u03c3$, and 9.1$\u03c3$, and branching fractions of $(2.89\\pm0.32\\pm0.55)\\times10^{-3}$, $(1.60\\pm0.40\\pm0.32)\\times10^{-3}$, and $(7.44\\pm0.94\\pm1.56)\\times10^{-3}$, respectively, where the first uncertainties are statistical and the second systematic. No significant signal is observed for the other two decay modes, and the corresponding upper limits of the branching fractions are determined to be $B(h_c \\to 3(\u03c0^+ \u03c0^-) \u03c0^0)<8.7\\times10^{-3}$ and $B(h_c \\to K^+ K^- \u03c0^+ \u03c0^-)<5.8\\times10^{-4}$ at 90% confidence level.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Decays $\u03c7_{cJ}~(J=0,1,2)\\to\u03c9\u03c6$ are studied using $(448.1\\pm2.9)\\times 10^{6} ~\u03c8(3686)$ events collected with the BESIII detector in 2009 and 2012. In addition to the previously established $\u03c7_{c0}\\to\u03c9\u03c6$, first observation of $\u03c7_{c1} \\to \u03c9\u03c6$ is reported in this paper. The measured product branching fractions are ${\\cal{B}}(\u03c8(3686)\\to\u03b3\u03c7_{c0})\\times{\\cal{B}}(\u03c7_{c0}\\to\u03c9\u03c6)=(13.83\\pm 0.70\\pm 1.01)\\times10^{-6}$ and ${\\cal{B}}(\u03c8(3686)\\to\u03b3\u03c7_{c1})\\times{\\cal{B}}(\u03c7_{c1}\\to\u03c9\u03c6)=(2.67\\pm 0.31\\pm 0.27)\\times10^{-6}$, and the absolute branching fractions are ${\\cal{B}}(\u03c7_{c0}\\to\u03c9\u03c6)=(13.84\\pm 0.70\\pm 1.08)\\times10^{-5}$ and ${\\cal{B}}(\u03c7_{c1}\\to\u03c9\u03c6)=(2.80\\pm 0.32\\pm 0.30)\\times10^{-5}$. We also find a strong evidence for $\u03c7_{c2}\\to\u03c9\u03c6$ with a statistical significance of 4.8$\u03c3$, and the corresponding product and absolute branching fractions are measured to be ${\\cal{B}}(\u03c8(3686)\\to\u03b3\u03c7_{c2})\\times{\\cal{B}}(\u03c7_{c2}\\to\u03c9\u03c6)=(0.91\\pm0.23\\pm0.12)\\times10^{-6} $ and ${\\cal{B}}(\u03c7_{c2}\\to\u03c9\u03c6)=(1.00\\pm0.25\\pm0.14)\\times10^{-5}$. Here, the first errors are statistical and the second ones systematic.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using $5.2 \\ \\mathrm{fb}^{-1}$ $e^+ e^-$ annihilation data samples collected with the BESIII detector, we measure the cross sections of $e^+e^- \\to K_S^0 K^\\pm \u03c0^\\mp \u03c0^0$ and $K_{S}^{0}K^{\\pm}\u03c0^{\\mp}\u03b7$ at center-of-mass energies from $3.90$ to $4.60$ GeV. In addition, we search for the charmonium-like resonance $Y(4260)$ decays into $K_{S}^{0}K^{\\pm}\u03c0^{\\mp}\u03c0^0$ and $K_{S}^{0}K^{\\pm}\u03c0^{\\mp}\u03b7$, and $Z_c^{0,\\pm}(3900)$ decays into $K_{S}^{0}K^{\\pm}\u03c0^{\\mp,0}$ and $K_{S}^{0}K^{\\pm}\u03b7$. Corresponding upper limits are provided since no clear signal is observed.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "High quality semipolar and nonpolar GaN is crucial in achieving high-performance GaN-based optoelectronic devices, yet it has been very challenging to achieve large-area wafers that are free of basal-plane stacking faults (BSFs). In this work, we report an approach to prepare large-area, stacking-fault-free (SF-free) semipolar GaN on (4-inch) sapphire substrates. A root cause of the formation of BSFs is the emergence of N-polar (000-1) facets during semipolar and non-polar heteroepitaxy. Invoking the concept of kinetic Wulff plot, we succeeded in suppressing the occurrence of N-polar GaN (000-1) facets, and consequently in eliminating the stacking faults generated in (000-1) basal-planes. The result was confirmed by transmission electron microscopy, cathodoluminescence, and low-temperature photoluminescence characterizations. Furthermore, InGaN light emitting diodes with promising characteristics have been produced on the SF-free semipolar (20-21) GaN on sapphire substrates. Our work opens up a new insight about the heteroepitaxial growth of nonpolar/semipolar GaN and provides an approach of producing SF-free nonpolar/semipolar GaN material over large-area wafers which will create new opportunities in GaN optoelectronic and microelectronic research.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Schr\u00f6dinger cat states are crucial for exploration of fundamental issues of quantum mechanics and have important applications in quantum information processing. Here, we propose and experimentally demonstrate a method for manipulating cat states in a cavity with the Aharonov-Anandan phase acquired by a superconducting qubit, which is dispersively coupled to the cavity. Based on this dispersive coupling, the qubit can be forced to trace out a circuit in the projective Hilbert space conditional on one coherent state. By preparing the cavity in a superposition of two coherent states, the geometric phase associated with this transport is encoded to the relative probability amplitude of these two coherent states. We demonstrate the photon-number parity of a cat state in a cavity can be controlled by adjusting this geometric phase, which offers the possibility for protecting its quantum coherence from single-photon loss. Based on this geometric effect, we realize phase gates for one and two photonic qubits whose logical basis states are encoded in two quasi-orthogonal coherent states. We further demonstrate two-cavity gates with symmetric and asymmetric Fock state encoding schemes. Our method can be directly extended to implementation of controlled-phase gates between error-correctable logical qubits.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using $e^+e^-$ annihilation data of $2.93~\\mathrm{fb}^{-1}$ collected at center-of-mass energy $\\sqrt{s}=3.773$ GeV with the BESIII detector, we measure the absolute branching fraction of $D^{0}\\to K^{-}\u03bc^{+}\u03bd_\u03bc$ with significantly improved precision: ${\\mathcal B}_{D^{0}\\to K^{-}\u03bc^{+}\u03bd_\u03bc}=(3.413\\pm0.019_{\\rm stat.}\\pm0.035_{\\rm syst.})\\%$. Combining with our previous measurement of ${\\mathcal B}_{D^0\\to K^-e^+\u03bd_e}$, the ratio of the two branching fractions is determined to be ${\\mathcal B}_{D^0\\to K^-\u03bc^+\u03bd_\u03bc}/{\\mathcal B}_{D^0\\to K^-e^+\u03bd_e}=0.974\\pm0.007_{\\rm stat.}\\pm0.012_{\\rm syst.}$, which agrees with the theoretical expectation of lepton flavor universality within the uncertainty. A study of the ratio of the two branching fractions in different four-momentum transfer regions is also performed, and no evidence for lepton flavor universality violation is found with current statistics. Taking inputs from CKMFitter and LQCD separately, we determine $f_{+}^{K}(0)=0.7327\\pm0.0039_{\\rm stat.}\\pm0.0030_{\\rm syst.}$ and $|V_{cs}| = 0.955\\pm0.005_{\\rm stat.}\\pm0.004_{\\rm syst.}\\pm0.024_{\\rm LQCD}$.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We study the electromagnetic Dalitz decay $J/\u03c8\\to e^+e^- \u03b7$ and search for di-electron decays of a dark gauge boson ($\u03b3'$) in $J/\u03c8\\to \u03b3' \u03b7$ with the two $\u03b7$ decay modes $\u03b7\\rightarrow \u03b3\u03b3$ and $\u03b7\\rightarrow \u03c0^+\u03c0^-\u03c0^0$ using $(1310.6\\pm 7.0)\\times10^6$ $J/\u03c8$ events collected with the BESIII detector. The branching fraction of $J/\u03c8\\to e^+e^- \u03b7$ is measured to be $(1.43 \\pm 0.04 ({\\rm stat}) \\pm 0.06 ({\\rm syst}))\\times 10^{-5}$, with a precision that is improved by a factor of $1.5$ over the previous BESIII measurement. The corresponding di-electron invariant mass dependent modulus square of the transition form factor is explored for the first time, and the pole mass is determined to be $\u039b= 2.84 \\pm 0.11({\\rm stat}) \\pm 0.08({\\rm syst})$ GeV/$c^2$. We find no evidence of $\u03b3'$ production and set $90\\%$ confidence level upper limits on the product branching fraction $\\mathcal{B}(J/\u03c8\\to \u03b3' \u03b7)\\times \\mathcal{B}(\u03b3' \\to e^+e^-)$ as well as the kinetic mixing strength between the Standard Model photon and $\u03b3'$ in the mass range of $0.01 \\le m_{\u03b3'} \\le 2.4$ GeV/$c^2$.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Regularized online learning is widely used in machine learning applications. In this paper we analyze a class of regularized online algorithms without linearizing the loss function or the regularizer, which we call \\emph{fully implicit online learning} (FIOL). We show that the FIOL algorithm admits a better regret than the linearization approximate algorithm if each iteration in FIOL can be solved exactly. Then we show that by exploring the structure of a large class of loss functions and regularizers, the computational complexity of FIOL in each iteration is comparable to its linearized part, even if no closed-form solution exists. Experiments validate the proposed approaches.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Although the automotive industry has been among the sectors that best-understands the importance of drivers' affect, the focus of design and research in the automotive field has long emphasized the visceral aspects of exterior and interior design. With the adoption of Advanced Driver Assistance Systems (ADAS), endowing 'semi-autonomy' to the vehicles, however, the scope of affective design should be expanded to include the behavioural aspects of the vehicle. In such a 'shared-control' system wherein the vehicle can intervene in the human driver's operations, a certain degree of 'intrusive feelings' are unavoidable. For example, when the Lane Keeping Assistance System (LKAS), one of the most popular examples of ADAS, operates the steering wheel in a dangerous situation, the driver may feel interrupted or surprised because of the abrupt torque generated by LKAS. This kind of unpleasant experience can lead to prolonged negative feelings such as irritation, anxiety, and distrust of the system. Therefore, there are increasing needs of investigating the driver's affective responses towards the vehicle's dynamic behaviour. In this study, four types of intrusive feelings caused by LKAS were identified to be proposed as a quantitative performance indicator in designing the affectively satisfactory behaviour of LKAS. A metric as well as a statistical data analysis method to quantitatively measure the intrusive feelings through the vehicle sensor log data.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using a data sample corresponding to an integrated luminosity of 2.93~fb$^{-1}$ recorded by the BESIII detector at a center-of-mass energy of $3.773$ GeV, we present an analysis of the decays $\\bar{D}^0\\to\u03c0^+\u03c0^0 e^-\\bar\u03bd_e$ and $D^+\\to\u03c0^-\u03c0^+ e^+\u03bd_e$. Using a partial wave analysis, the $\u03c0^+\u03c0^-$ $S$-wave contribution to $D^+\\to\u03c0^-\u03c0^+ e^+\u03bd_e$ is observed for the first time besides the dominant $P$-wave contribution; the statistical significance is greater than 10$\u03c3$ with its measured fraction $(25.7\\pm1.6\\pm1.1)\\%$. We measure the branching fractions $\\mathcal{B}(D^{0} \\to \u03c1^- e^+ \u03bd_e) = (1.445\\pm 0.058 \\pm 0.039) \\times10^{-3}$, $\\mathcal{B}(D^{+} \\to \u03c1^0 e^+ \u03bd_e) = (1.860\\pm 0.070 \\pm 0.061) \\times10^{-3}$ and $\\mathcal{B}(D^{+} \\to f_0(500) e^+ \u03bd_e, f_0(500)\\to\u03c0^+\u03c0^-) = (6.30\\pm 0.43 \\pm 0.32) \\times10^{-4}$. An upper limit of $\\mathcal{B}(D^{+} \\to f_0(980) e^+ \u03bd_e, f_0(980)\\to\u03c0^+\u03c0^-) < 2.8 \\times10^{-5}$ is set at the 90% confidence level. We also obtain the hadronic form factor ratios of $D\\to \u03c1e^+\u03bd_e$ at $q^{2}=0$ assuming the single-pole dominance parameterization: $r_{V}=\\frac{V(0)}{A_{1}(0)}=1.695\\pm0.083\\pm0.051$, $r_{2}=\\frac{A_{2}(0)}{A_{1}(0)}=0.845\\pm0.056\\pm0.039$.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "By analyzing $2.93\\ \\rm fb^{-1}$ of data taken at a center-of-mass energy of 3.773~GeV with the BESIII detector, we measure the branching fractions of the Cabibbo-favored hadronic decays $D^0\\to K^-\u03c0^+\u03b7^\\prime$, $D^0\\to K^0_S\u03c0^0\u03b7^\\prime$ and $D^+\\to K^0_S\u03c0^+\u03b7^\\prime$, which are determined to be $(6.43 \\pm 0.15_{\\rm stat.} \\pm 0.31_{\\rm syst.})\\times 10^{-3}$, $(2.52 \\pm 0.22_{\\rm stat.} \\pm 0.15_{\\rm syst.})\\times 10^{-3}$ and $(1.90 \\pm 0.17_{\\rm stat.} \\pm 0.13_{\\rm syst.})\\times 10^{-3}$, respectively. The precision of the branching fraction of $D^0\\to K^-\u03c0^+\u03b7^\\prime$ is significantly improved, and the processes $D^0\\to K^0_S\u03c0^0\u03b7^\\prime$ and $D^+\\to K^0_S\u03c0^+\u03b7^\\prime$ are observed for the first time.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Sensing the medical scenario can ensure the safety during the surgical operations. So, in this regard, a monitor platform which can obtain the accurate location information of the surgery room is desperately needed. Compared to 2D camera image, 3D data contains more information of distance and direction. Therefore, 3D sensors are more suitable to be used in surgical scene monitoring. However, each 3D sensor has its own limitations. For example, Lidar (Light Detection and Ranging) can detect large-scale environment with high precision, but the point clouds or depth maps are very sparse. As for commodity RGBD sensors, such as Kinect, can accurately capture denser data, but limited to a small range from 0.5 to 4.5m. So, a proper method which can address these problems for fusing different modalities data is important. In this paper, we proposed a method which can fuse different modalities 3D data to get a large-scale and dense point cloud. The key contributions of our work are as follows. First, we proposed a 3D data collecting system to reconstruct the medical scenes. By fusing the Lidar and Kinect data, a large-scale medical scene with more details can be reconstructed. Second, we proposed a location-based fast point clouds registration algorithm to deal with different modality datasets.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Real-time traffic signal control with environmental objectives has been a difficult challenge due to (1) the highly dynamic and uncertain nature of road traffic and their emission profile; (2) the need for generating timely and robust decisions for large-scale networks; (3) the incorporation of multi-source and heterogeneous data; and (4) the balance between traffic and environmental objectives. To address these challenges, this paper proposes a real-time traffic signal control framework based on a nonlinear decision rule (NDR), encapsulating an artificial neural network (ANN),which is integrated with realistic traffic and emission modeling via agent-based microsimulation. The implementation of the NDR method is divided into an off-line module and an on-line module. The off-line module trains the ANN and the NDR using historical traffic data, which amounts to a stochastic optimization with traffic and environmental objectives. The on-line module efficiently implements the trained NDR, whose performance is guaranteed by the off-line training. Simulation study of the proposed signal control framework in a real network in West Glasgow demonstrates the advantage of the proposed method over the current signal control parameters in terms of alleviating congestion and reducing traffic emissions.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using a data sample of $(1310.6\\pm7.0)\\times10^{6}$ $J/\u03c8$ events collected with the BESIII detector at BEPCII, we study the electromagnetic Dalitz decay $J/\u03c8\\to \u03b7' e^+e^-$ with two dominant $\u03b7'$ decay modes, $\u03b7' \\to \u03b3\u03c0^+ \u03c0^-$ and $\u03b7' \\to \u03c0^+\u03c0^-\u03b7$. The branching fraction is determined to be $\\mathcal{B}(J/\u03c8\\to \u03b7' e^+e^-) = (6.79 \\pm 0.08\\pm 0.35)\\times10^{-5}$, which supersedes the previous BESIII measurement. For the first time, a search for the dark photon ($\u03b3^{\\prime}$) is performed via charmonium decays $J/\u03c8\\to\u03b7' \u03b3^{\\prime}, \u03b3^{\\prime} \\to e^{+}e^{-}$. Excluding the $\u03c9$ and $\u03c6$ mass regions, no significant signal is observed in the mass range from 0.1 to 2.1 GeV/$c^{2}$. We set upper limits at the 90\\% confidence level on $\\mathcal{B}(J/\u03c8\\to \u03b7' \u03b3^{\\prime})\\times\\mathcal{B}(\u03b3^{\\prime} \\to e^+e^-)$, $\\mathcal{B}(J/\u03c8\\to\u03b7' \u03b3^{\\prime}$) and the mixing strength as a function of dark photon mass.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Studying the properties of the Higgs boson can be an important window to explore the physics beyond the Standard Model (SM). In this work, we present studies on the implications of the Higgs precision measurements at future Higgs Factories. We perform a global fit to various Higgs search channels to obtain the 95 % C.L. constraints on the model parameter spaces of Two Higgs Double Model (2HDM) and Minimal Supersymmetric Standard Model (MSSM). In the 2HDM, we analyze tree level effects as well as one-loop contributions from the heavy Higgs bosons. The strong constraints on $\\cos(\u03b2-\u03b1)$, heavy Higgs masses and their mass splitting are complementary to direct search of the LHC as well as possible future Z pole precision measurements. For the MSSM, we study both the Higgs couplings and mass precisions. The constraints on the CP-odd Higgs mass $m_A$ and stop mass scale $m_{SUSY}$ can be complementary to the direct search of HL-LHC. We also compare the sensitivity of various future Higgs factories, namely Circular Electron Positron Collider (CEPC), Future Circular Collider (FCC)-ee and International Linear Collider (ILC).\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using a sample of $1.31\\times 10^9$ $J/\u03c8$ events collected with the BESIII detector, we report the first observation of spin polarization of $\u039b$ and $\\bar\u039b$ hyperons from the coherent production in the $J/\u03c8\\to\u039b\\bar\u039b$ decay. We measure the phase between the hadronic form factors to be $\u0394\u03a6=(42.4\\pm0.6\\pm0.5)^\\circ$. The decay parameters for $\u039b\\to p\u03c0^-$ ($\u03b1_-$), $\\bar\u039b\\to\\bar p\u03c0^+$ ($\u03b1_+$) and $\\bar\u039b\\to\\bar n\u03c0^0$ ($\\bar\u03b1_0$) are measured to be $\u03b1_-=0.750\\pm0.009\\pm0.004$, $\u03b1_+=-0.758\\pm0.010\\pm0.007$ and $\\bar\u03b1_0=-0.692\\pm0.016\\pm0.006$, respectively. The obtained value of $\u03b1_-$ is higher by $(17\\pm 3)\\%$ than the current world average. In addition, the $CP$ asymmetry of $-0.006\\pm0.012\\pm0.007$ is extracted with substantially improved precision. The ratio $\\bar\u03b1_0/\u03b1_{+} = 0.913\\pm 0.028 \\pm 0.012$ is also measured.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using data samples collected by the BESIII detector operating at BEPCII with center-of-mass energies between 3.8 and 4.6\\,GeV and with a total luminosity of about 5.0~fb$^{-1}$, we measure the Born cross sections of $e^+e^- \\to K_{S}^{0}K^{\\pm}\u03c0^{\\mp}$, $\u03c3_{B}(e^+e^- \\to K_{S}^{0}K^{\\pm}\u03c0^{\\mp})$. The cross sections are generally smaller than those of BABAR but with significantly improved precision. The line shape of the Born cross sections from this work cannot be well described by a simple $1/s^n$ dependence, which may suggest contributions from excited charmonium or charmonium-like states.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "An amplitude analysis of the $K_{S}K_{S}$ system produced in radiative $J/\u03c8$ decays is performed using the $(1310.6\\pm7.0)\\times10^{6}$ $J/\u03c8$ decays collected by the BESIII detector. Two approaches are presented. A mass-dependent analysis is performed by parameterizing the $K_{S}K_{S}$ invariant mass spectrum as a sum of Breit-Wigner line shapes. Additionally, a mass-independent analysis is performed to extract a piecewise function that describes the dynamics of the $K_{S}K_{S}$ system while making minimal assumptions about the properties and number of poles in the amplitude. The dominant amplitudes in the mass-dependent analysis include the $f_{0}(1710)$, $f_{0}(2200)$, and $f_{2}^\\prime(1525)$. The mass-independent results, which are made available as input for further studies, are consistent with those of the mass-dependent analysis and are useful for a systematic study of hadronic interactions. The branching fraction of radiative $J/\u03c8$ decays to $K_{S}K_{S}$ is measured to be $(8.1 \\pm 0.4) \\times 10^{-4}$, where the uncertainty is systematic and the statistical uncertainty is negligible.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We present extensive spectroscopic observations for one of the closest type Ia supernovae (SNe Ia), SN 2014J discovered in M82, ranging from 10.4 days before to 473.2 days after B-band maximum light. The diffuse interstellar band (DIB) features detected in a high-resolution spectrum allow an estimate of line-of-sight extinction as Av=1.9+/-0.6 mag. Spectroscopically, SN 2014J can be put into the high-velocity (HV) subgroup in Wang's classification with a velocity of Si~II 6355 at maximum light as about 12200 km/s, but has a low velocity gradient (LVG, following Benetti's classification) as 41+/-2 km/s/day, which is inconsistent with the trend that HV SNe Ia generally have larger velocity gradients. We find that the HV SNe Ia with LVGs tend to have relatively stronger Si III (at ~4400 Angstrom) absorptions in early spectra, larger ratios of S II 5468 to S II 5640, and weaker Si II 5972 absorptions compared to their counterparts with similar velocities but high velocity gradients. This shows that the HV+LVG subgroup of SNe Ia may have intrinsically higher photospheric temperature, which indicates that their progenitors may experience more complete burning in the explosions relative to the typical HV SNe Ia.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We present the X-ray timing results of the new black hole candidate (BHC) MAXI J1535-571 during its 2017 outburst from Hard X-ray Modulation Telescope (\\emph{Insight}-HXMT) observations taken from 2017 September 6 to 23. Following the definitions given by \\citet{Belloni2010}, we find that the source exhibits state transitions from Low/Hard state (LHS) to Hard Intermediate state (HIMS) and eventually to Soft Intermediate state (SIMS). Quasi-periodic oscillations (QPOs) are found in the intermediate states, which suggest different types of QPOs. With the large effective area of \\emph{Insight}-HXMT at high energies, we are able to present the energy dependence of the QPO amplitude and centroid frequency up to 100 keV which is rarely explored by previous satellites. We also find that the phase lag at the type-C QPOs centroid frequency is negative (soft lags) and strongly correlated with the centroid frequency. By assuming a geometrical origin of type-C QPOs, the source is consistent with being a high inclination system.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "The cross section of the process $e^+e^-\\to \u03c0^+D^0D^{*-}$ for center-of-mass energies from 4.05 to 4.60~GeV is measured precisely using data samples collected with the BESIII detector operating at the BEPCII storage ring. Two enhancements are clearly visible in the cross section around 4.23 and 4.40~GeV. Using several models to describe the dressed cross section yields stable parameters for the first enhancement, which has a mass of $4228.6 \\pm 4.1 \\pm 5.9 {\\rm MeV}/c^2$ and a width of $77.1 \\pm 6.8 \\pm 6.9 {\\rm MeV}$, where the first uncertainties are statistical and the second ones are systematic. Our resonant mass is consistent with previous observations of the $Y(4220)$ state and the theoretical prediction of a $D\\bar{D}_1(2420)$ molecule. This result is the first observation of $Y(4220)$ associated with an open-charm final state. Fits with three resonance functions with additional $Y(4260)$, $Y(4320)$, $Y(4360)$, $\u03c8(4415)$, or a new resonance, do not show significant contributions from either of these resonances. The second enhancement is not from a single known resonance. It could contain contributions from $\u03c8(4415)$ and other resonances, and a detailed amplitude analysis is required to better understand this enhancement.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "This paper presents a study of the production of a single $W$ boson in association with one or more jets in proton-antiproton collisions at $\\sqrt{s}=1.96$ TeV, using the entire data set collected in 2001-2011 by the Collider Detector at Fermilab at the Tevatron, which corresponds to an integrated luminosity of $9.0$ fb$^{-1}$. The $W$ boson is identified through its leptonic decays into electron and muon. The production cross sections are measured for each leptonic decay mode and combined after testing that the ratio of the $W(\\rightarrow \u03bc\u03bd)+$jets cross section to the $W(\\rightarrow e\u03bd)+$jets cross section agrees with the hypothesis of $e$-$\u03bc$ lepton universality. The combination of measured cross sections, differential in the inclusive jet multiplicity ($W+\\geqslant N$ jets with $N=1,\\,2,\\,3, \\textrm{or }4$) and in the transverse energy of the leading jet, are compared with theoretical predictions.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using 16 points of $e^{+}e^{-}$ annihilation data collected in the vicinity of the $J/\u03c8$ resonance with the BESIII detector and with a total integrated luminosity of around 100 pb$^{-1}$, we study the relative phase between the strong and electromagnetic amplitudes of $J/\u03c8$ decays. The relative phase between the $J/\u03c8$ electromagnetic decay and the continuum process ($e^{+}e^{-}$ annihilation without the $J/\u03c8$ resonance) is confirmed to be zero by studying the Born cross section lineshape of $\u03bc^{+}\u03bc^{-}$ production. The relative phase between the $J/\u03c8$ strong and electromagnetic decays is then measured to be $\\pm(84.2\\pm3.3)^\\circ$ for the $2(\u03c0^{+}\u03c0^{-})\u03c0^{0}$ final state by investigating the interference pattern between the $J/\u03c8$ decay and the continuum processes. This is the first measurement of the relative phase between $J/\u03c8$ strong and electromagnetic decays into a multi-hadron final state using the lineshape of the production cross section. We also study the production lineshape of the multi-hadron final state $\u03b7\u03c0^{+}\u03c0^{-}$ with $\u03b7\\to\u03c0^{+}\u03c0^{-}\u03c0^{0}$, which provides additional information about the phase between the $J/\u03c8$ electromagnetic decay amplitude and the continuum process. Additionally, the branching fractions of $J/\u03c8\\to2(\u03c0^{+}\u03c0^{-})\u03c0^{0}$ and $J/\u03c8\\to\u03b7\u03c0^{+}\u03c0^{-}$ are measured to be $(4.37\\pm0.40)\\%$ and $(3.77\\pm0.68)\\times10^{-4}$, respectively, which are consistent with the world average values. The quoted uncertainties include both statistical and systematic uncertainties.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "The process $e^+e^-\\rightarrow pK^{0}_{S}\\bar{n}K^{-} + c.c.$ and its intermediate processes are studied for the first time, using data samples collected with the BESIII detector at BEPCII at center-of-mass energies of 3.773, 4.008, 4.226, 4.258, 4.358, 4.416, and 4.600 GeV, with a total integrated luminosity of 7.4 fb$^{-1}$. The Born cross section of $e^+e^- \\to p K^{0}_S\\bar{n}K^- + c.c.$ is measured at each center-of-mass energy, but no significant resonant structure in the measured cross-section line shape between 3.773 and 4.600 GeV is observed. No evident structure is detected in the $pK^-$, $nK^{0}_S$, $pK^0_{S}$, $nK^+$, $p\\bar{n}$, or $K^{0}_S K^-$ invariant mass distributions except for $\u039b(1520)$. The Born cross sections of $e^+e^-\\rightarrow\u039b(1520)\\bar{n}K^{0}_{S} + c.c.$ and $e^+e^-\\rightarrow \u039b(1520)\\bar{p}K^{+} + c.c.$ are measured, and the 90\\% confidence level upper limits on the Born cross sections of $e^+e^-\\rightarrow\u039b(1520)\\bar\u039b(1520)$ are determined at the seven center-of-mass energies.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "The Collider Detector at Fermilab collected a unique sample of jets originating from bottom-quark fragmentation ($b$-jets) by selecting online proton-antiproton ($p\\bar{p}$) collisions with a vertex displaced from the $p\\bar{p}$ interaction point, consistent with the decay of a bottom-quark hadron. This data set, collected at a center-of-mass energy of $\\sqrt{s}=$1.96 TeV, and corresponding to an integrated luminosity of $5.4~\\rm{fb}^{-1}$, is used to measure the $Z$-boson production cross section times branching ratio into $b\\bar{b}$. The number of $Z\\rightarrow b\\bar{b}$ events is determined by fitting the dijet-mass distribution while constraining the dominant $b$-jet background, originating from QCD multijet events, with data. The result, $\u03c3(p\\bar{p} \\rightarrow Z) \\times \\mathcal{B}(Z \\rightarrow b\\bar{b})= 1.11\\pm 0.08(\\text{stat}) \\pm 0.14(\\text{syst})~\\text{nb}$, is the most precise measurement of this process, and is consistent with the standard-model prediction. The data set is also used to search for Higgs-boson production. No significant signal is expected in our data and the first upper limit on the cross section for the inclusive $p\\bar p \\rightarrow H\\rightarrow b\\bar b$ process at $\\sqrt{s}=$1.96 TeV is set, corresponding to 33 times the expected standard-model cross section, or $\u03c3= 40.6$ pb, at the 95\\% confidence level.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "This paper investigates sufficient conditions for a Feynman-Kac functional up to an exit time to be the generalized viscosity solution of a Dirichlet problem. The key ingredient is to find out the continuity of exit operator under Skorokhod topology, which reveals the intrinsic connection of overfitting Dirichlet boundary with fine topology. As an application, we establish the sub and supersolutions for a class of non-stationary HJB (Hamilton-Jacobi-Bellman) equations with fractional Laplacian operator via Feynman-Kac functionals associated to $\u03b1$-stable processes, which enables us to verify its solvability together with comparison principle and Perron's method.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Back-n is a white neutron experimental facility at China Spallation Neutron Source (CSNS). The time structure of the primary proton beam make it fully applicable to use TOF (time-of-flight) method for neutron energy measuring. We implement the electronics of TOF measurement on the general-purpose readout electronics designed for all of the seven detectors in Back-n. The electronics is based on PXIe (Peripheral Component Interconnect Express eXtensions for Instrumentation) platform, which is composed of FDM (Field Digitizer Modules), TCM (Trigger and Clock Module), and SCM (Signal Conditioning Module). T0 signal synchronous to the CSNS accelerator represents the neutron emission from the target. It is the start of time stamp. The trigger and clock module (TCM) receives, synchronizes and distributes the T0 signal to each FDM based on the PXIe backplane bus. Meantime, detector signals after being conditioned are fed into FDMs for waveform digitizing. First sample point of the signal is the stop of time stamp. According to the start, stop time stamp and the time of signal over threshold, the total TOF can be obtained. FPGA-based (Field Programmable Gate Array) TDC is implemented on TCM to accurately acquire the time interval between the asynchronous T0 signal and the global synchronous clock phase. There is also an FPGA-based TDC on FDM to accurately acquire the time interval between T0 arriving at FDM and the first sample point of the detector signal, the over threshold time of signal is obtained offline. This method for TOF measurement is efficient and not needed for additional modules. Test result shows the accuracy of TOF is sub-nanosecond and can meet the requirement for Back-n at CSNS.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "the main physics goal for Back-n white neutron facility at China Spallation Neutron Source (CSNS) is to measure nuclear data. The energy of neutrons is one of the most important parameters for measuring nuclear data. Method of time of flight (TOF) is used to obtain the energy of neutrons. The time when proton bunches hit the thick tungsten target is considered as the start point of TOF. T0 signal, generated from the CSNS accelerator, represents this start time. Besides, the T0 signal is also used as the gate control signal that triggers the readout electronics. Obviously, the timing precision of T0 directly affects the measurement precision of TOF and controls the running or readout electronics. In this paper, the T0 fan-out for Back-n white neutron facility at CSNS is proposed. The T0 signal travelling from the CSNS accelerator is fanned out to the two underground experiment stations respectively over long cables. To guarantee the timing precision, T0 signal is conditioned with good signal edge. Furthermore, techniques of signal pre-emphasizing and equalizing are used to improve signal quality after T0 being transmitted over long cables with about 100 m length. Experiments show that the T0 fan-out works well, the T0 signal transmitted over 100 m remains a good time resolution with a standard deviation of 25 ps. It absolutely meets the required accuracy of the measurement of TOF.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "As the development of electronic science and technology, electronic data acquisition (DAQ) system is more and more widely applied to nuclear physics experiments. Workstations are often utilized for data storage, data display, data processing and data analysis by researchers. Nevertheless, the workstations are ordinarily separated from detectors in nuclear physics experiments by several kilometers or even tens of kilometers. Thus a DAQ system that can transmit data for long distance is in demand. In this paper, we designed a DAQ system suitable for high-speed and high-precision sampling for remote data transfer. An 8-channel, 24-bit simultaneous sampling analog-to-digital converter(ADC) named AD7779 was utilized for high-speed and high-precision sampling, the maximum operating speed of which runs up to 16 kilo samples per second(KSPS). ADC is responsible for collecting signals from detectors, which is sent to Field Programmable Gate Array(FPGA) for processing and long-distance transmission to the workstation through optical fiber. As the central processing unit of DAQ system, FPGA provides powerful computing capability and has enough flexibility. The most prominent feature of the system is real-time mass data transfer based on streaming transmission mode, highly reliable data transmission based on error detection and correction and high-speed high-precision data acquisition. The results of our tests show that the system is able to transmit data stably at the bandwidth of 1Gbps.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Faithfully transferring the quantum state is essential for quantum information processing. Here we demonstrate a fast (in 84 ns) and high-fidelity (99.2%) transfer of arbitrary quantum states in a chain of four superconducting qubits with nearest-neighbor coupling. This transfer relies on full control of the effective couplings between neighboring qubits, which is realized only by our parametrically modulating the qubits without increasing circuit complexity. Once the couplings between qubits fulfill a specific ratio, perfect quantum state transfer can be achieved in a single step, and is therefore robust to noise and accumulation of experimental errors. This quantum state transfer can be extended to a larger qubit chain and thus adds a desirable tool for future quantum information processing. The demonstrated flexibility of the coupling tunability is suitable for quantum simulation of many-body physics, which requires different configurations of qubit couplings.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "We introduce a new function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional tree-structured reinforcement learning meta-controller to explore a simple yet highly expressive tree-structured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "This work demonstrates dramatically modified spin dynamics of magnetic insulator (MI) by the spin-momentum locked Dirac surface states of the adjacent topological insulator (TI) which can be harnessed for spintronic applications. As the Bi-concentration x is systematically tuned in 5 nm thick (BixSb1-x)2Te3 TI film, the weight of the surface relative to bulk states peaks at x = 0.32 when the chemical potential approaches the Dirac point. At this concentration, the Gilbert damping constant of the precessing magnetization in 10 nm thick Y3Fe5O12 MI film in the MI/TI heterostructures is enhanced by an order of magnitude, the largest among all concentrations. In addition, the MI acquires additional strong magnetic anisotropy that favors the in-plane orientation with similar Bi-concentration dependence. These extraordinary effects of the Dirac surface states distinguish TI from other materials such as heavy metals in modulating spin dynamics of the neighboring magnetic layer.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Due to the extremely volatile nature of financial markets, it is commonly accepted that stock price prediction is a task full of challenge. However in order to make profits or understand the essence of equity market, numerous market participants or researchers try to forecast stock price using various statistical, econometric or even neural network models. In this work, we survey and compare the predictive power of five neural network models, namely, back propagation (BP) neural network, radial basis function (RBF) neural network, general regression neural network (GRNN), support vector machine regression (SVMR), least squares support vector machine regresssion (LS-SVMR). We apply the five models to make price prediction of three individual stocks, namely, Bank of China, Vanke A and Kweichou Moutai. Adopting mean square error and average absolute percentage error as criteria, we find BP neural network consistently and robustly outperforms the other four models.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using a data sample corresponding to an integrated luminosity of 567 pb$^{-1}$ collected at a center-of-mass energy of $\\sqrt{s}=4.6$ GeV with the BESIII detector, we measure the absolute branching fraction of the inclusive semileptonic $\u039b_c^+$ decay with a double-tag method. We obtain $\\mathcal{B}(\u039b_c^+ \\rightarrow X e^+ \u03bd_e) = (3.95\\pm0.34\\pm0.09)\\%$, where the first uncertainty is statistical and the second systematic. Using the known $\u039b_c^+$ lifetime and the charge-averaged semileptonic decay width of nonstrange charmed measons ($D^0$ and $D^+$), we obtain the ratio of the inclusive semileptonic decay widths $\u0393(\u039b_c^+ \\rightarrow X e^+ \u03bd_e)/\\bar\u0393(D\\rightarrow X e^+ \u03bd_e)= 1.26\\pm0.12$.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Using a data sample of $(1310.6 \\pm 7.0)\\times 10^6$ $J/\u03c8$ events collected with the BESIII detector operating at the BEPCII collider, we perform the first experimental search for invisible decays of a light vector meson ($V=\u03c9,\u03c6$) via $J/\u03c8\\to V \u03b7$ decays. The decay of $\u03b7\\to \u03c0^+\u03c0^-\u03c0^0$ is utilized to tag the $V$ meson decaying into the invisible final state. No evidence for a significant invisible signal is observed, and the upper limits on the ratio of branching fractions at the 90\\% confidence level are determined to be $\\frac{\\mathcal{B}(\u03c9\\rightarrow \\rm{invisible})}{\\mathcal{B}(\u03c9\\to \u03c0^+\u03c0^-\u03c0^0)} < 8.1 \\times 10^{-5}$ and $\\frac{\\mathcal{B}(\u03c6\\to \\rm{invisible})}{\\mathcal{B}(\u03c6\\to K^+K^-)} < 3.4\\times 10^{-4}$. By using the world average values of $\\mathcal{B}(\u03c9\\to \u03c0^+\u03c0^-\u03c0^0)$ and $\\mathcal{B}(\u03c6\\to K^+K^-)$, the upper limits on the decay branching fractions at the $90\\%$ confidence level are set as\n  $\\mathcal{B}(\u03c9\\to \\rm{invisible})< 7.3 \\times 10^{-5}$ and $\\mathcal{B}(\u03c6\\to \\rm{invisible})< 1.7\\times 10^{-4}$, respectively.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.\n        \u25b3 Less", "author": "Song Han"}, {"abstract": "Series of short contributions that are part of Nobel Symposium 162 - Microfluidics arXiv:1712.08369.\n        \u25b3 Less", "author": "Jongyoon Han"}, {"abstract": "We develop the first theoretical model for the analytical description of ion concentration polarization (ICP)-based electrokinetic molecular concentration, which had not been possible due to the extraordinary complexity of the system. We define the two separate limits for the enrichment factor achievable in a given system and derive the scaling laws for critical parameters, which are validated by numerical simulations and experiments. This work provides clear theoretical explanations on the diverse experimental behaviors previously observed yet unexplainable, while setting solid foundation for the engineering of ICP-based concentrators and other fluid-coupled electrokinetic systems.\n        \u25b3 Less", "author": "Jongyoon Han"}, {"abstract": "This paper studies mechanism of preconcentration of charged particles in a straight micro-channel embedded with permselective membranes, by numerically solving coupled transport equations of ions, charged particles and solvent fluid without any simplifying assumptions. It is demonstrated that trapping and preconcentration of charged particles are determined by the interplay between drag force from the electroosmotic fluid flow and the electrophoretic force applied trough the electric field. Several insightful characteristics are revealed, including the diverse dynamics of co-ions and counter ions, replacement of co-ions by focused particles, lowered ion concentrations in particle enriched zone, and enhanced electroosmotic pumping effect etc. Conditions for particles that may be concentrated are identified in terms of charges, sizes and electrophoretic mobilities of particles and co-ions. Dependences of enrichment factor on cross-membrane voltage, initial particle concentration and buffer ion concentrations are analyzed and the underlying reasons are elaborated. Finally, post priori a condition for validity of decoupled simulation model is given based on charges carried by focused charge particles and that by buffer co-ions. These results provide important guidance in the design and optimization of nanofluidic preconcentration and other related devices.\n        \u25b3 Less", "author": "Jongyoon Han"}, {"abstract": "The nitrogen vacancy (NV) center in diamond has emerged as a leading solid-state quantum sensor for applications including magnetometry, electrometry, thermometry, and chemical sensing. However, an outstanding challenge for practical applications is that existing NV-based sensing techniques require bulky and discrete instruments for spin control and detection. Here, we address this challenge by integrating NV based quantum sensing with complementary metal-oxide-semiconductor (CMOS) technology. Through tailored CMOS-integrated microwave generation and photodetection, this work dramatically reduces the instrumentation footprint for quantum magnetometry and thermometry. This hybrid diamond-CMOS integration enables an ultra-compact and scalable platform for quantum sensing and quantum information processing.\n        \u25b3 Less", "author": "Ruonan Han"}, {"abstract": "A binary beat-by-beat classification algorithm for cerebral blood flow velocity (CBFV) recordings based on amplitude, spectral and morphological features is presented. The classification difference between 15 manually and algorithmically annotated CBFV records is around 5%.\n        \u25b3 Less", "author": "Thomas Heldt"}, {"abstract": "For identification of change information in image sequences, most studies focus on change detection in one image sequence, while few studies have considered the change level comparison between two different image sequences. Moreover, most studies require the detection of image information in details, for example, object detection. Based on Uncertainty Coefficient(UC), this paper proposes an innovative method CCUC for change comparison between two image sequences. The proposed method is computationally efficient and simple to implement. The change comparison stems from video monitoring system. The limited number of provided screens and a large number of monitoring cameras require the videos or image sequences ordered by change level. We demonstrate this new method by applying it on two publicly available image sequences. The results are able to show the method can distinguish the different change level for sequences.\n        \u25b3 Less", "author": "Berthold Horn"}, {"abstract": "Convolutions have long been regarded as fundamental to applied mathematics, physics and engineering. Their mathematical elegance allows for common tasks such as numerical differentiation to be computed efficiently on large data sets. Efficient computation of convolutions is critical to artificial intelligence in real-time applications, like machine vision, where convolutions must be continuously and efficiently computed on tens to hundreds of kilobytes per second. In this paper, we explore how convolutions are used in fundamental machine vision applications. We present an accelerated n-dimensional convolution package in the high performance computing language, Julia, and demonstrate its efficacy in solving the time to contact problem for machine vision. Results are measured against synthetically generated videos and quantitatively assessed according to their mean squared error from the ground truth. We achieve over an order of magnitude decrease in compute time and allocated memory for comparable machine vision applications. All code is packaged and integrated into the official Julia Package Manager to be used in various other scenarios.\n        \u25b3 Less", "author": "Berthold Horn"}, {"abstract": "We provide a detailed study of the interface Trap Assisted Tunneling (TAT) mechanism in tunnel field effect transistors to show how it contributes a major leakage current path before the Band To Band Tunneling (BTBT) is initiated. With a modified Shockley-Read-Hall formalism, we show that at room temperature, the phonon assisted TAT current always dominates and obscures the steep turn ON of the BTBT current for common densities of traps. Our results are applicable to top gate, double gate and gate all around structures where the traps are positioned between the source-channel tunneling region. Since the TAT has strong dependence on electric field, any effort to increase the BTBT current by enhancing local electric field also increases the leakage current. Unless the BTBT current can be increased separately, calculations show that the trap density Dit has to be decreased by 40-100 times compared with the state of the art in order for the steep turn ON (for III-V materials) to be clearly observable at room temperature. We find that the combination of the intrinsic sharpness of the band edges (Urbach tail) and the surface trap density determines the subthreshold swing.\n        \u25b3 Less", "author": "Judy Hoyt"}, {"abstract": "We have developed a physically-intuitive method to calculate the local lattice constant as a function of position in a high-resolution transmission electron microscopy image by performing a two-dimensional fast Fourier transform. We apply a Gaussian filter with appropriate spatial full-width-half-max (FWHM) bandwidth to the image centered at the desired location to calculate the local lattice constant (as opposed to the average lattice constant). Fourier analysis of the filtered image yields the vertical and horizontal lattice constants at this location. The process is repeated by stepping the Gaussian filter across the image to produce a set of local lattice constants in the vertical and horizontal direction as a function of position in the image. The method has been implemented in a freely available tool on nanoHUB.\n        \u25b3 Less", "author": "Judy Hoyt"}, {"abstract": "The properties of graphene depend sensitively on doping with respect to the charge-neutrality point (CNP). Tuning the CNP usually requires electrical gating or chemical doping. Here, we describe a technique to reversibly control the CNP in graphene with nanoscale precision, utilizing LaAlO$_3$/SrTiO$_3$ (LAO/STO) heterostructures and conductive atomic force microscope (c-AFM) lithography. The local electron density and resulting conductivity of the LAO/STO interface can be patterned with a conductive AFM tip, and placed within two nanometers of an active graphene device. The proximal LAO/STO nanostructures shift the position of graphene CNP by ~ $10^{12}$ cm$^{-2}$, and are also gateable. Here we use this effect to create reconfigurable edge states in graphene, which are probed using the quantum Hall effect. Quantized resistance plateaus at $h/e^2$ and $h/3e^2$ are observed in a split Hall device, demonstrating edge transport along the c-AFM written edge that depends on the polarity of both the magnetic field and direction of currents. This technique can be readily extended to other device geometries.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "The problem of anomaly detection among multiple processes is considered within the framework of sequential design of experiments. The objective is an active inference strategy consisting of a selection rule governing which process to probe at each time, a stopping rule on when to terminate the detection, and a decision rule on the final detection outcome. The performance measure is the Bayes risk that takes into account of not only sample complexity and detection errors, but also costs associated with switching across processes. While the problem is a partially observable Markov decision process to which optimal solutions are generally intractable, a low-complexity deterministic policy is shown to be asymptotically optimal and offer significant performance improvement over existing methods in the finite regime.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "The metallicity and its relationship with other galactic properties is a fundamental probe of the evolution of galaxies. In this work, we select about 750,000 star-forming spatial pixels from 1122 blue galaxies in the MaNGA survey to investigate the global stellar mass - local stellar mass surface density - gas-phase metallicity ($M_*$ - $\u03a3_*$ - $Z$ ) relation. At a fixed $M_*$, the metallicity increases steeply with increasing $\u03a3_*$. Similarly, at a fixed $\u03a3_*$, the metallicity increases strongly with increasing $M_*$ at low mass end, while this trend becomes less obvious at high mass end. We find the metallicity to be more strongly correlated to $\u03a3_*$ than to $M_*$. Furthermore, we construct a tight (0.07 dex scatter) $M_*$ - $\u03a3_*$ - $Z$ relation, which reduces the scatter in the $\u03a3_*$ - $Z$ relation by about 30$\\%$ for galaxies with $7.8 < {\\rm log}(M_*/M_\\odot) < 11.0$, while the reduction of scatter is much weaker for high-mass galaxies. This result suggests that, especially for low-mass galaxies, the $M_*$ - $\u03a3_*$ - $Z$ relation is largely more fundamental than the $M_*$ - $Z$ and $\u03a3_*$ - $Z$ relations, meaning that both $M_*$ and $\u03a3_*$ play important roles in shaping the local metallicity. We also find that the local metallicity is probably independent on the local star formation rate surface density at a fixed $M_*$ and $\u03a3_*$. Our results are consistent with the scenario that the local metallicities in galaxies are shaped by the combination of the local stars formed in the history and the metal loss caused by galactic winds.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Topological superconductors are in the focus of research because of their high potential for future applications of quantum computation. With the recent discovery of the quantum anomalous Hall insulator (QAHI), which exhibits the conductive quantum Hall edge states without external magnetic field, it becomes possible to create a novel topological superconductor by introducing superconductivity into these edge states. In this case, two distinct topological superconducting phases with one or two chiral Majorana edge modes formed, characterized by Chern numbers (N ) of 1 and 2, respectively. Recent experiments on a QAHI / superconductor (SC) heterostructure revealed the presence of integer and half-integer quantized plateaus in the conductance over a deposited SC strip and presented the quantization evidence of these states. However, these results also provoked a few controversies and thus additional direct evidence of a superconducting origin is urgently needed. We provided spectroscopic evidence for a superconducting QAHI state using nano-point contacts at the edge of a QAHI / SC heterostructure and obtained unique signatures of these two different topological superconducting phases. The phase with N = 1 with a 2e^2/h conduction signature occurs in a narrow field regime during the QAHI magnetization reversal just before the QAHI enters the trivial insulating state. These results are consistent with theoretical analysis and further reaffirm the previous result of 1/2 quantization due to Majorana fermion.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "For many applications Optical Frequency Combs (OFCs) require a high degree of temporal coherence (narrow linewidth). Commonly OFCs are generated in nonlinear media from a monochromatic narrow linewidth laser sources or from a mode-locked laser pulses but in the all-important mid-infrared (MIR) and terahertz (THz) regions of spectrum OFCs can be generated intrinsically by the free-running quantum cascade lasers (QCLs) with high efficiency. These combs do not look like conventional OFCs as the phases of each mode are different and in temporal domain the OFC is a seemingly random combination of amplitude- and phase-modulated signals rather than a short pulse. Despite this pseudo-randomness, the experimental evidence suggests that the linewidth of the QCL OFC is just as narrow as that of a QCL operating in the single mode. While universally acknowledged, this seemingly observation is not fully understood. In this work we rigorously prove this fact by deriving the expression for the Schawlow-Townes linewidth of QCL OFC and offer a transparent physical interpretation based on orthogonality of laser modes, indicating that despite their very different temporal profiles MIR and THz QCL OFCs are just as good for most applications as any other OFC.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Entity resolution is the process of identifying groups of records within or across data sets where each group represents a real-world entity. Novel techniques that consider temporal features to improve the quality of entity resolution have recently attracted significant attention. However, there are currently no large data sets available that contain both temporal information as well as ground truth information to evaluate the quality of temporal entity resolution approaches. In this paper, we describe the preparation of a temporal data set based on author profiles extracted from the Digital Bibliography and Library Project (DBLP). We completed missing links between publications and author profiles in the DBLP data set using the DBLP public API. We then used the Microsoft Academic Graph (MAG) to link temporal affiliation information for DBLP authors. We selected around 80K (1%) of author profiles that cover 2 million (50%) publications using information in DBLP such as alternative author names and personal web profile to improve the reliability of the resulting ground truth, while at the same time keeping the data set challenging for temporal entity resolution research.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Recent breakthroughs in recurrent deep neural networks with long short-term memory (LSTM) units has led to major advances in artificial intelligence. State-of-the-art LSTM models with significantly increased complexity and a large number of parameters, however, have a bottleneck in computing power resulting from limited memory capacity and data communication bandwidth. Here we demonstrate experimentally that LSTM can be implemented with a memristor crossbar, which has a small circuit footprint to store a large number of parameters and in-memory computing capability that circumvents the 'von Neumann bottleneck'. We illustrate the capability of our system by solving real-world problems in regression and classification, which shows that memristor LSTM is a promising low-power and low-latency hardware platform for edge inference.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Solution-processed organic-inorganic lead halide perovskites have recently emerged as promising gain media for tunable semiconductor lasers, and have come to rival inorganic III-V group semiconductors as the material candidate for chip-scale lasers. Although electrically pumped lasing at room temperature is the ultimate goal, optically pumped continuous-wave lasing at room temperature,a prerequisite for a laser diode,has not been achieved so far. Here, we report lasing action in a surface emitting distributed feedback methylammonium lead iodide (MAPbI3) perovskite laser on silicon substrate, at room temperature under continuous-wave optical pumping, in ambient air environment. This outstanding performance is achieved by the ultra-low lasing threshold of 13 W/cm2, which is enabled by the thermal nanoimprint lithography that directly patterns perovskite into a high Q cavity with large mode confinement, while at the same time improves perovskite emission characteristics. Our results represent a major step toward the realization of perovskite laser diodes, which is essential in the future insertion of perovskite lasers into photonic integrated circuits, for applications in optical computing, sensing and on-chip quantum information.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "With pervasive applications of medical imaging in health-care, biomedical image segmentation plays a central role in quantitative analysis, clinical diagno- sis, and medical intervention. Since manual anno- tation su ers limited reproducibility, arduous e orts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), par- ticularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmenta- tion, attaining much improved performance. At the same time, quantization of DNNs has become an ac- tive research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing litera- ture on quantization which primarily targets memory and computation complexity reduction, we apply quan- tization as a method to reduce over tting in FCNs for better accuracy. Speci cally, we focus on a state-of- the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annota- tion samples from the original training dataset, obtain- ing an e ective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantiza- tion for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method has a reduction of up to 6.4x on memory usage.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Topological materials bear gapped excitations in bulk yet protected gapless excitations at boundaries. Magnetoplasmons (MPs), as high-frequency density excitations of two-dimensional electron gas (2DEG) in a perpendicular magnetic field, embody a prototype of band topology for bosons. The time-reversal-breaking magnetic field opens a topological gap for bulk MPs up to the cyclotron frequency; topologically-protected edge magnetoplasmons (EMPs) bridge the bulk gap and propagate unidirectionally along system's boundaries. However, all the EMPs known to date adhere to physical edges where the electron density terminates abruptly. This restriction has made device application extremely difficult. Here we demonstrate a new class of topological edge plasmons -- domain-boundary magnetoplasmons (DBMPs), within a uniform edgeless 2DEG. Such DBMPs arise at the domain boundaries of an engineered sign-changing magnetic field and are protected by the difference of gap Chern numbers (+/-1) across the magnetic domains. They propagate unidirectionally along the domain boundaries and are immune to domain defects. Moreover, they exhibit wide tunability in the microwave frequency range under an applied magnetic field or gate voltage. Our study opens a new direction to realize high-speed reconfigurable topological devices.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "In e-commerce platforms such as Amazon and TaoBao, ranking items in a search session is a typical multi-step decision-making problem. Learning to rank (LTR) methods have been widely applied to ranking problems. However, such methods often consider different ranking steps in a session to be independent, which conversely may be highly correlated to each other. For better utilizing the correlation between different ranking steps, in this paper, we propose to use reinforcement learning (RL) to learn an optimal ranking policy which maximizes the expected accumulative rewards in a search session. Firstly, we formally define the concept of search session Markov decision process (SSMDP) to formulate the multi-step ranking problem. Secondly, we analyze the property of SSMDP and theoretically prove the necessity of maximizing accumulative rewards. Lastly, we propose a novel policy gradient algorithm for learning an optimal ranking policy, which is able to deal with the problem of high reward variance and unbalanced reward distribution of an SSMDP. Experiments are conducted in simulation and TaoBao search engine. The results demonstrate that our algorithm performs much better than online LTR methods, with more than 40% and 30% growth of total transaction amount in the simulation and the real application, respectively.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Modifying phonon thermal conductivity in nanomaterials is important not only for fundamental research but also for practical applications. However, the experiments on tailoring the thermal conductivity in nanoscale, especially in two-dimensional materials, are rare due to technical challenges. In this work, we demonstrate in-situ thermal conduction measurement of MoS2 and find that its thermal conductivity can be continuously tuned to a required value from crystalline to amorphous limits. The reduction of thermal conductivity is understood from phonon-defects scatterings that decrease the phonon transmission coefficient. Beyond a threshold, a sharp drop in thermal conductivity is observed, which is believed to be a crystalline-amorphous transition. Our method and results provide guidance for potential applications in thermoelectrics, photoelectronics, and energy harvesting where thermal management is critical with further integration and miniaturization.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We study the co-evolution of supermassive black holes (SMBHs) with galaxies by means of semi-analytic model (SAM) of galaxy formation based on sub-halo merger trees built from Millennium and Millennium-II simulation. We utilize the simulation results from Guo 2013 and Henriques 2015 to study two aspects of the co-evolution, \\emph{i.e.} the stochastic gravitational wave (GW) background generated by SMBH merger and the SMBH/galaxy clustering. The characteristic strain amplitude of GW background predicted by Guo 2013 and Henriques 2015 models are $A_{yr^{-1}}=5.00\\times10^{-16}$ and $A_{yr^{-1}}=9.42\\times10^{-17}$, respectively. We find the GW amplitude is very sensitive to the galaxy merger rate. The difference in the galaxy merger rate between Guo 2013 and Henriques 2015, results in a factor $5$ deviation in the GW strain amplitude. For clusterings, we calculate the spatially isotropic two point auto- and cross-correlation functions (2PCFs) for both SMBHs and galaxies by using the mock catalogs generated from Guo 2013 model. We find that all 2PCFs have positive dependence on both SMBH and galaxy mass. And there exist a significant time evolution in 2PCFs, namely, the clustering effect is enhanced at lower redshifts. Interestingly, this result is not reported in the active galactic nuclei samples in SDSS. Our analysis also shows that, roughly, SMBHs and galaxies, with galaxy mass $10^2\\sim10^3$ larger than SMBH mass, have similar pattern of clustering, which is a reflection of the co-evolution of SMBH and galaxy. Finally, we calculate the first ten multiples of the angular power spectrum of the energy density of GW background. We find the amplitude of angular power spectrum of the first ten multiples is about $10\\%$ to $60\\%$ of the monopole component in the whole frequency range.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "The magnetic transition-metal (TM) @ oxide nanoparticles have been of great interest due to their wide range of applications, from medical sensors in magnetic resonance imaging to photo-catalysis. Although several studies on small clusters of TM@oxide have been reported, the understanding of the physical electronic properties of TMn@(ZnO)42 is far from sufficient. In this work, the electronic, magnetic and optical properties of TMn@(ZnO)42 (TM = Fe, Co and Ni) hetero-nanostructure are investigated using the density functional theory (DFT). It has been found that the core-shell nanostructure Fe13@(ZnO)42, Co15@(ZnO)42 and Ni15@(ZnO)42 are the most stable structures. Moreover, it is also predicted that the variation of the magnetic moment and magnetism of Fe, Co and Ni in TMn@ZnO42 hetero-nanostructure mainly stems from effective hybridization between core TM-3d orbitals and shell O-2p orbitals, and a magnetic moment inversion for Fe15@(ZnO)42 is investigated. Finally, optical properties studied by calculations show a red shift phenomenon in the absorption spectrum compared with the case of (ZnO)48.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Environment, such as the accretion disk, could modify the signal of the gravitational wave from the astrophysical black hole binaries. In this article, we model the matter field around the intermediate-mass binary black holes by means of an axion-like scalar field and investigate their joint evolution. In details, we consider the equal mass binary black holes surrounded by a shell of axion-like scalar field both in spherical symmetric and non-spherical symmetric cases, and with different strength of the scalar field. Our result shows that the environmental scalar field could essentially modify the dynamics. Firstly, in the spherical symmetric case, with increasing of the scalar field strength, the number of circular orbit of the binary black hole is reduced. It means that the scalar field could significantly accelerate the merger process. Secondly, once the scalar field strength exceeds certain critical value, the scalar field could collapse into a third black hole with its mass being larger than the binary. Consequently, the new black hole collapsed from the environmental scalar field could accrete the binary promptly and the binary collides head-on between each other. In this process, there is almost no any quadrupole signal produced, namely the gravitational wave is greatly suppressed. Thirdly, when the scalar field strength is relatively smaller than the critical value, the black hole orbit could develop eccentricity through the accretion of the scalar field. Fourthly, during the initial stage of the inspire, the gravitational attractive force from the axion-like scalar field could induce a sudden turn in the binary orbits, hence result in a transient wiggle in the gravitational waveform. Finally, in the non-spherical case, the scalar field could gravitationally attract the binary moving toward the mass center of the scalar field and slow down the merger process.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We report the first result on Ge-76 neutrinoless double beta decay from CDEX-1 experiment at China Jinping Underground Laboratory. A mass of 994 g p-type point-contact high purity germanium detector has been installed to search the neutrinoless double beta decay events, as well as to directly detect dark matter particles. An exposure of 304 kg*day has been analyzed. The wideband spectrum from 500 keV to 3 MeV was obtained and the average event rate at the 2.039 MeV energy range is about 0.012 count per keV per kg per day. The half-life of Ge-76 neutrinoless double beta decay has been derived based on this result as: T 1/2 > 6.4*10^22 yr (90% C.L.). An upper limit on the effective Majorana-neutrino mass of 5.0 eV has been achieved. The possible methods to further decrease the background level have been discussed and will be pursued in the next stage of CDEX experiment.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Using 106~million $\u03c8(3686)$ events collected with the BESIII detector, we measure multipole amplitudes for the decay $\u03c8(3686)\\rightarrow\u03b3\u03c7_{c1,2}\\to\u03b3\u03b3J/\u03c8$ beyond the dominant electric-dipole amplitudes. The normalized magnetic-quadrupole amplitude for $\u03c8(3686)\\rightarrow\u03b3\u03c7_{c1,2}\\rightarrow\u03b3\u03b3J/\u03c8$ and the normalized electric-octupole amplitudes for $\u03c8(3686)rightarrow\u03b3\u03c7_{c2}$,~$\u03c7_{c2}\\rightarrow\u03b3J/\u03c8$ are determined. The M2 amplitudes for $\u03c8(3686)\\rightarrow\u03b3\u03c7_{c1}$ and $\u03c7_{c1,2}\\rightarrow\u03b3J/\u03c8$ are found to differ significantly from zero and are consistent with theoretical predictions. We also obtain the ratios of M2 contributions of $\u03c8(3686)$ and $J/\u03c8$ decays to $\u03c7_{c1,2}$, $b_{2}^{1}/b_{2}^{2} = 1.35\\pm0.72$ and $a_{2}^{1}/a_{2}^{2} = 0.617\\pm0.083$, which agree well with theoretical expectations. By considering the multipole contributions of $\u03c7_{c1,2}$, we measure the product branching fractions for the cascade decays $\u03c8(3686)\\rightarrow\u03b3\u03c7_{c0,1,2}\\to\u03b3\u03b3J/\u03c8$ and search for the process $\u03b7_{c}(2S)\\to\u03b3J/\u03c8$ through $\u03c8(3686)\\rightarrow\u03b3\u03b7_{c}(2S)$. The product branching fraction for $\u03c8(3686)\\rightarrow\u03b3\u03c7_{c0}\\to\u03b3\u03b3J/\u03c8$ is 3$\u03c3$ larger than published measurements, while those of $\u03c8(3686)rightarrow\u03b3\u03c7_{c1,2}\\to\u03b3\u03b3J/\u03c8$ are consistent. No significant signal for the decay $\u03c8(3686)\\to\u03b3\u03b7_c(2S)\\to\u03b3\u03b3J/\u03c8$ is observed, and the upper limit of the product branching fraction at the 90\\% confidence level is determined.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Currently, a growing number of health consumers are asking health-related questions online, at any time and from anywhere, which effectively lowers the cost of health care. The most common approach is using online health expert question-answering (HQA) services, as health consumers are more willing to trust answers from professional physicians. However, these answers can be of varying quality depending on circumstance. In addition, as the available HQA services grow, how to predict the answer quality of HQA services via machine learning becomes increasingly important and challenging. In an HQA service, answers are normally short texts, which are severely affected by the data sparsity problem. Furthermore, HQA services lack community features such as best answer and user votes. Therefore, the wisdom of the crowd is not available to rate answer quality. To address these problems, in this paper, the prediction of HQA answer quality is defined as a classification task. First, based on the characteristics of HQA services and feedback from medical experts, a standard for HQA service answer quality evaluation is defined. Next, based on the characteristics of HQA services, several novel non-textual features are proposed, including surface linguistic features and social features. Finally, a deep belief network (DBN)-based HQA answer quality prediction framework is proposed to predict the quality of answers by learning the high-level hidden semantic representation from the physicians' answers. Our results prove that the proposed framework overcomes the problem of overly sparse textual features in short text answers and effectively identifies high-quality answers.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We study the ground-state behavior of a Bose-Einstein Condensate (BEC) in a Raman-laser-assisted one-dimensional (1D) optical lattice potential forming a multilayer system. We find that, such system can be described by an effective model with spin-orbit coupling (SOC) of pseudospin $(N-1)/2$, where $N$ is the number of layers. Due to the intricate interplay between atomic interactions, SOC and laser-assisted tunnelings, the ground-state phase diagrams generally consist of three phases -- a stripe, a plane wave and a normal phase with zero-momentum, touching at a quantum tricritical point. More important, even though the single-particle states only minimize at zero-momentum for odd $N$, the many-body ground states may still develop finite momenta. The underlying mechanisms are elucidated. Our results provide an alternative way to realize an effective spin-orbit coupling of Bose gas with the Raman-laser-assisted optical lattice, and would also be beneficial to the studies on SOC effects in spinor Bose systems with large spin.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Solitons play a fundamental role in dynamics of nonlinear excitations. Here we explore the motion of solitons in one-dimensional uniform Bose-Einstein condensates subjected to a spin-orbit coupling (SOC). We demonstrate that the spin dynamics of solitons is governed by a nonlinear Bloch equation. The spin dynamics influences the orbital motion of the solitons leading to the spin-orbit effects in the dynamics of the macroscopic quantum objects (mean-field solitons). The latter perform oscillations with a frequency determined by the SOC, Raman coupling, and intrinsic nonlinearity. These findings reveal unique features of solitons affected by the SOC, which is confirmed by analytical considerations and numerical simulations of the underlying Gross-Pitaevskii equations.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "With the agreement of my coauthors, I Zhangyang Wang would like to withdraw the manuscript \"Stacked Approximated Regression Machine: A Simple Deep Learning Approach\". Some experimental procedures were not included in the manuscript, which makes a part of important claims not meaningful. In the relevant research, I was solely responsible for carrying out the experiments; the other coauthors joined in the discussions leading to the main algorithm.\n  Please see the updated text for more details.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We report the WIMP dark matter search results using the first physics-run data of the PandaX-II 500 kg liquid xenon dual-phase time-projection chamber, operating at the China JinPing Underground Laboratory. No dark matter candidate is identified above background. In combination with the data set during the commissioning run, with a total exposure of 3.3$\\times10^4$ kg-day,the most stringent limit to the spin-independent interaction between the ordinary and WIMP dark matter is set for a range of dark matter mass between 3.5 and 1000 GeV/c$^2$. The best upper limit on the scattering cross section is found $2.5\\times 10^{-46}$ cm$^2$ for the WIMP mass 40 GeV/c$^2$ at 90% confidence level.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Infrared spectroscopy, especially for molecular vibrations in the fingerprint region between 600 and 1500 cm-1, is a powerful characterization method for bulk materials. However, molecular fingerprinting at the nanoscale level still remains a significant challenge, due to weak light-matter interaction between micron-wavelengthed infrared light and nano-sized molecules. Here, we demonstrate molecular fingerprinting at the nanoscale level using our specially designed graphene plasmonic structure on CaF2 nanofilm. This structure not only avoids the plasmon-phonon hybridization, but also provides in situ electrically-tunable graphene plasmon covering the entire infrared fingerprint region, which was previously unattainable. In addition, undisturbed and highly-confined graphene plasmon offers simultaneous detection of in-plane and out-of-plane vibrational modes with ultrahigh detection sensitivity down to the sub-monolayer level, significantly pushing the current detection limit of far-field mid-infrared spectroscopy. Our results provide a platform, fulfilling the long-awaited expectation of high sensitivity and selectivity far-field fingerprint detection of nano-scale molecules for numerous applications.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "With the help of information and communication technologies, studies on the overall social networks have been extensively reported recently. However, investigations on the directed Ego Communication Networks (ECNs) remain insufficient, where an ECN stands for a sub network composed of a centralized individual and his/her direct contacts. In this paper, the directed ECNs are built on the Call Detail Records (CDRs), which cover more than 7 million people of a provincial capital city in China for half a year. Results show that there is a critical size for ECN at about 150, above which the average emotional closeness between ego and alters drops, the balanced relationship between ego and network collapses, and the proportion of strong ties decreases. This paper not only demonstrate the significance of ECN size in affecting its properties, but also shows accordance with the \"Dunbar's Number\". These results can be viewed as a cross-culture supportive evidence to the well-known Social Brain Hypothesis (SBH).\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Dual comb spectroscopy allows for high-resolution spectra to be measured over broad bandwidths, but an essential requirement for coherent integration is the availability of a phase reference. Usually, this means that the combs' phase and timing errors must be measured and either minimized by stabilization or removed by correction, limiting the technique's applicability. In this work, we demonstrate that it is possible to extract the phase and timing signals of a multiheterodyne spectrum completely computationally, without any extra measurements or optical elements. These techniques are viable even when the relative linewidth exceeds the repetition rate difference, and can tremendously simplify any dual comb system. By reconceptualizing frequency combs in terms of the temporal structure of their phase noise, not their frequency stability, we are able to greatly expand the scope of multiheterodyne techniques.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "By identifying the Schr\u00f6dinger equation with the hydrodynamic equations in superfluid ${^3}$He, the effective potential is introduced in the Schr\u00f6dinger equation to solve the quantum pressure in steady state. The pure gauge velocity solutions of hydrodynamic equations provide an analogous Wu-Yang monopole potential. The two cases of velocity in superfluid are equivalent to the two regions of Wu-Yang monopole potential. Due to the compressibility of superfluid, the physical models are limited, such as hard core and harmonic oscillator. It is important that the constraint condition of $\\nabla\u03c1\\cdot \\textbf{v}=0$ plays a key role, which determines that the integral quantum numbers are selected in monopole harmonics and shows the special analogous Wu-Yang monopole. The results provide a new possibility for the simulation of Wu-Yang monopole.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Frequency combs based on terahertz quantum cascade lasers feature broadband coverage and high output powers in a compact package, making them an attractive option for broadband spectroscopy. Here, we demonstrate the first multi-heterodyne spectroscopy using two terahertz quantum cascade laser combs. With just 100 $\u03bc$s of integration time, we achieve peak signal-to-noise ratios exceeding 60 dB and a spectral coverage greater than 250 GHz centered at 2.8 THz. Even with room-temperature detectors we are able to achieve peak signal-to-noise ratios of 50 dB, and as a proof-of-principle we use these combs to measure the broadband transmission spectrum of etalon samples. Finally, we show that with proper signal processing, it is possible to extend the multi-heterodyne spectroscopy to quantum cascade laser combs operating in pulsed mode, greatly expanding the range of quantum cascade lasers that could be suitable for these techniques.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Two-dimensional molecular aggregate (2DMA), a thin sheet of strongly interacting dipole molecules self-assembled at close distance on an ordered lattice, is a fascinating fluorescent material. It is distinctively different from the single or colloidal dye molecules or quantum dots in most previous research. In this paper, we verify for the first time that when a 2DMA is placed at a nanometric distance from a metallic substrate, the strong and coherent interaction between the dipoles inside the 2DMA dominates its fluorescent decay at picosecond timescale. Our streak-camera lifetime measurement and interacting lattice-dipole calculation reveal that the metal-mediated dipole-dipole interaction shortens the fluorescent lifetime to about one half and increases the energy dissipation rate by ten times than expected from the noninteracting single-dipole picture. Our finding can enrich our understanding of nanoscale energy transfer in molecular excitonic systems and may designate a new direction for developing fast and efficient optoelectronic devices.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We present the results of a search for WIMPs from the commissioning run of the PandaX-II experiment located at the China Jinping underground Laboratory. A WIMP search data set with an exposure of 306$\\times$19.1 kg-day was taken, while its dominant $^{85}$Kr background was used as the electron recoil calibration. No WIMP candidates are identified, and a 90\\% upper limit is set on the spin-independent elastic WIMP-nucleon cross section with a lowest excluded cross section of 2.97$\\times$10$^{-45}$~cm$^2$ at a WIMP mass of 44.7~GeV/c$^2$.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "In the last decade many works has been done on the Internet topology at router or autonomous system (AS) level. As routers is the essential composition of ASes while ASes dominate the behavior of their routers. It is no doubt that identifying the affiliation between routers and ASes can let us gain a deeper understanding on the topology. However, the existing methods that assign a router to an AS just based on the origin ASes of its IP addresses, which does not make full use of information in our hand. In this paper, we propose a methodology to assign routers to their owner ASes based on community discovery tech. First, we use the origin ASes information along with router-pairs similarities to construct a weighted router level topology, secondly, for enormous topology data (more than 2M nodes and 19M edges) from CAIDA ITDK project, we propose a fast hierarchy clustering which time and space complex are both linear to do ASes community discovery, last we do router-to-AS mapping based on these ASes communities. Experiments show that combining with ASes communities our methodology discovers, the best accuracy rate of router-to-AS mapping can reach to 82.62%, which is drastically high comparing to prior works that stagnate on 65.44%.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "From 2011 to 2014, the BESIII experiment collected about 5 fb$^{-1}$ data at center-of-mass energies around 4 GeV for the studies of the charmonium-like and higher excited charmonium states. By analyzing the di-muon process $e^{+}e^{-}\\rightarrow\u03b3_{\\rm ISR/FSR}\u03bc^{+}\u03bc^{-}$, the center-of-mass energies of the data samples are measured with a precision of 0.8 MeV. The center-of-mass energy is found to be stable for most of time during the data taking.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "In this paper, the self-propelled motion of Leidenfrost droplets on ratchet surfaces is numerically investigated with a thermal multiphase lattice Boltzmann model with liquid-vapor phase change. The capability of the model for simulating evaporation is validated via the D2 law. Using the model, we first study the performances of Leidenfrost droplets on horizontal ratchet surfaces. It is numerically shown that the motion of self-propelled Leidenfrost droplets on ratchet surfaces is owing to the asymmetry of the ratchets and the vapor flows beneath the droplets. It is found that the Leidenfrost droplets move in the direction toward the slowly inclined side from the ratchet peaks, which agrees with the direction of droplet motion in experiments [Linke et al., Phys. Rev. Lett., 2006, 96, 154502]. Moreover, the influences of the ratchet aspect ratio are investigated. For the considered ratchet surfaces, a critical value of the ratchet aspect ratio is approximately found, which corresponds to the maximum droplet moving velocity. Furthermore, the processes that the Leidenfrost droplets climb uphill on inclined ratchet surfaces are also studied. Numerical results show that the maximum inclination angle at which a Leidenfrost droplet can still climb uphill successfully is affected by the initial radius of the droplet.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We present new measurements of the scintillation and ionization yields in liquid xenon for low energy electronic (about 3--7 keV$_{ee}$) and nuclear recoils (about 8--20 keV$_{nr}$) at different drift fields from 236 V/cm to 3.93 kV/cm, using a three-dimensional sensitive liquid xenon time projection chamber with high energy and position resolutions. Our measurement of signal responses to nuclear recoils agrees with predictions from the NEST model. However, our measured ionization (scintillation) yields for electronic recoils are consistently higher (lower) than those from the NEST model by about 5 e$^-$/keV$_{ee}$ (ph/keV$_{ee}$) at all scanned drift fields. New recombination parameters based on the Thomas-Imel box model are derived from our data. Given the lack of precise measurement of scintillation and ionization yields for low energy electronic recoils in liquid xenon previously, our new measurement provides so far the best available data covering low energy region at different drift fields for liquid xenon detectors relevant to dark matter searches.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We demonstrate an unexpectedly strong surface-plasmonic absorption at the interface of silver and high-index dielectrics based on electron and photon spectroscopy. The measured bandwidth and intensity of absorption deviate significantly from the classical theory. Our density-functional calculation well predicts the occurrence of this phenomenon. It reveals that due to the low metal-to-dielectric work function at such interfaces, conduction electrons can display a drastic quantum spillover, causing the interfacial electron-hole pair production to dominate the decay of surface plasmons. This finding can be of fundamental importance in understanding and designing quantum nano-plasmonic devices that utilize noble metals and high-index dielectrics.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Graphene plasmonics is of great interest for compact optical devices working in broad frequency domains with ultrahigh speed and very low energy consumption. However, graphene plasmons damp out quickly on most substrates mainly due to scattering loss from substrate surface phonons and impurities. Here we discover a new hybridized plasmon-phonon polariton mode in graphene/h-BN van der Waals heterostructures, which enables ultralong hybrid plasmon lifetime up to 1.6 picosecond, the longest plasmon lifetime ever demonstrated. Such remarkably long lifetime arises from the coupling of long-lifetime h-BN transverse optical phonon with graphene plasmons, which uniquely exists in monolayer heterostructures. Our findings and understanding of this unexploited hybrid mode offer a novel approach to tune the plasmon behaviours in the frequency, time and space domains. This can potentially introduce a new paradigm to generate highly-confined plasmons with ultra-long lifetime for various applications, such as deep-subwavelength metamaterials, ultra-low-loss waveguides, and ultrafast optical switches.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Nowadays, online clothes-selling business has become popular and extremely attractive because of its convenience and cheap-and-fine price. Good examples of these successful Web sites include Yintai.com, Vancl.com and Shop.vipshop.com which provide thousands of clothes for online shoppers. The challenge for online shoppers lies on how to find a good product from lots of options. In this article, we propose a collaborative clothes recommender for easy shopping. One of the unique features of this system is the ability to recommend clothes in terms of both user ratings and clothing attributes. Experiments in our simulation environment show that the proposed recommender can better satisfy the needs of users.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "The Tsinghua University-National Astronomical Observatories of China (NAOC) Transient Survey (TNTS) is an automatic survey for a systematic exploration of optical transients (OTs), conducted with a 60/90 cm Schmidt telescope at Xinglong station of NAOC. This survey repeatedly covers ~ 1000 square degrees of the north sky with a cadence of 3-4 days. With an exposure of 60 s, the survey reaches a limited unfiltered magnitude of about 19.5 mag. This enables us to discover supernovae at their relatively young stages. In this paper, we describe the overall performance of our survey during the first year and present some preliminary results.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Stainless steel is the material used for the storage vessels and piping systems of LAB-based liquid scintillator in JUNO experiment. Aging is recognized as one of the main degradation mechanisms affecting the properties of liquid scintillator. LAB-based liquid scintillator aging experiments were carried out in different material of containers (type 316 and 304 stainless steel and glass) at two different temperature (40 and 25 degrees Celsius). For the continuous liquid scintillator properties tests, the light yield and the absorption spectrum are nearly the same as that of the unaged one. The attenuation length of the aged samples is 6%~12% shorter than that of the unaged one. But the concentration of element Fe in the LAB-based liquid scintillator does not show a clear change. So the self aging has small effect on liquid scintillator, as well as the stainless steel impurity quenching. Type 316 and 304 stainless steel can be used as LAB-based liquid scintillator vessel, transportation pipeline material.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We report on the first dark-matter (DM) search results from PandaX-I, a low threshold dual-phase xenon experiment operating at the China Jinping Underground Laboratory. In the 37-kg liquid xenon target with 17.4 live-days of exposure, no DM particle candidate event was found. This result sets a stringent limit for low-mass DM particles and disfavors the interpretation of previously-reported positive experimental results. The minimum upper limit, $3.7\\times10^{-44}$\\,cm$^2$, for the spin-independent isoscalar DM-particle-nucleon scattering cross section is obtained at a DM-particle mass of 49\\,GeV/c$^2$ at 90\\% confidence level.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "This paper considers the problem of partially observed optimal control for forward stochastic systems which are driven by Brownian motions and an independent Poisson random measure with a feature that the cost functional is of mean-field type. When all the system coefficients and the objective performance functionals are allowed to be random, possibly non-Markovian, Malliavin calculus is employed to derive a maximum principle for the optimal control of such a system where the adjointprocess is explicitly expressed. We also investigate the mean-field type optimal control problems for systems driven by mean-field type stochastic differential equations (SDEs in short) with jump processes, in which the coefficients contain not only the state process but also its marginal distribution under partially observed information. The maximum principle is established using convex variational technique with an illustrating example about linear-quadratic optimal control.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "On-chip nanophotonics serves as the foundation for the new generation of information technology, but it is challenged by the diffraction limit of light. With the capabilities of confining light into (deep) subwavelength volumes, plasmonics makes it possible to dramatically miniaturize optical devices so as to integrate them into silicon chips. Here we demonstrate that by cascading nano-corrugation gratings with different periodicities on silver nanowires atop silicon, different colors can be spatially separated and chronologically released at different grating junctions. The released light frequency depends on the grating arrangement and corrugation periodicities. Hence the nanowire acts as a spectral splitter for sorting/demultiplexing photons at different nano-scale positions with a ten-femtosecond-level interval. Such nanowires can be constructed further into compact 2D networks or circuits. We believe that this study provides a new and promising approach for realizing spatiotemporal-sensitive spectral splitting and optical signal processing on nanoscales, and for general integration of nanophotonics with microelectronics.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Dual phase Xenon Time Projection Chambers (XeTPCs) are being used by several experiments as a promising technique for direct detection of dark matter. We report on the design and performance of a small 3-D sensitive dual phase XeTPC. The position resolution is 2 mm in the center of detector, limited by the hole size of the mesh at the proportional scintillation region. An energy resolution of 1.6%(\u03c3 /E) for 662 keV gamma rays is achieved by combining the ionization and scintillation signals at a drift field of 0.5 kV/cm. This represents the best energy resolution achieved among liquid xenon detectors. The energy resolution is only slightly dependent on drift field. Better than 2% energy resolution (\u03c3 /E) for 662 keV gamma rays can be achieved for drift fields between 100 V/cm and 2 kV/cm. With high position and energy resolutions, a dual phase XeTPC has also potential applications in surveys for neutrinoless double-beta decay and in gamma ray imaging.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We present a synchrotron grazing incidence x-ray diffraction analysis of the domain structure and polar symmetry of highly strained BiFeO3 thin films grown on LaAlO3 substrate. We revealed the existence of periodic elastic nanodomains in the pure tetragonal-like BFO ultrathin films down to a thickness of 6 nm. A unique shear strain accommodation mechanism is disclosed. We further demonstrated that the periodicity of the nanodomains increases with film thickness but deviates from the classical Kittel's square root law in ultrathin thickness regime (6 - 30 nm). Temperature-dependent experiments also reveal the disappearance of periodic modulation above 90C due to a MC-MA structural phase transition.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "The CDEX Collaboration has been established for direct detection of light dark matter particles, using ultra-low energy threshold p-type point-contact germanium detectors, in China JinPing underground Laboratory (CJPL). The first 1 kg point-contact germanium detector with a sub-keV energy threshold has been tested in a passive shielding system located in CJPL. The outputs from both the point-contact p+ electrode and the outside n+ electrode make it possible to scan the lower energy range of less than 1 keV and at the same time to detect the higher energy range up to 3 MeV. The outputs from both p+ and n+ electrode may also provide a more powerful method for signal discrimination for dark matter experiment. Some key parameters, including energy resolution, dead time, decay times of internal X-rays, and system stability, have been tested and measured. The results show that the 1 kg point-contact germanium detector, together with its shielding system and electronics, can run smoothly with good performances. This detector system will be deployed for dark matter search experiments.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Weakly Interacting Massive Particles (WIMPs) are the candidates of dark matter in our universe. Up to now any direct interaction of WIMP with nuclei has not been observed yet. The exclusion limits of the spin-independent cross section of WIMP-nucleon which have been experimentally obtained is about 10^{-7}pb at high mass region and only 10^{-5}pb} at low mass region. China Jin-Ping underground laboratory CJPL is the deepest underground lab in the world and provides a very promising environment for direct observation of dark matter. The China Dark Matter Experiment (CDEX) experiment is going to directly detect the WIMP flux with high sensitivity in the low mass region. Both CJPL and CDEX have achieved a remarkable progress in recent two years. The CDEX employs a point-contact germanium semi-conductor detector PCGe whose detection threshold is less than 300 eV. We report the measurement results of Muon flux, monitoring of radioactivity and Radon concentration carried out in CJPL, as well describe the structure and performance of the 1 kg PCGe detector CDEX-1 and 10kg detector array CDEX-10 including the detectors, electronics, shielding and cooling systems. Finally we discuss the physics goals of the CDEX-1, CDEX-10 and the future CDEX-1T detectors.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We show that synthesis-induced Metal -Insulator transition (MIT) for electronic transport along the orthorombic c axis of FeSb$_{2}$ single crystals has greatly enhanced electrical conductivity while keeping the thermopower at a relatively high level. By this means, the thermoelectric power factor is enhanced to a new record high S$^{2}$$\u03c3$ $\\sim$ 8000 $\u03bc$WK$^{-2}$cm$^{-1}$ at 28 K. We find that the large thermopower in FeSb$_{2}$ can be rationalized within the correlated electron model with two bands having large quasiparaticle disparity, whereas MIT is induced by subtle structural differences. The results in this work testify that correlated electrons can produce extreme power factor values.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We report on a heterodyne receiver designed to observe the astrophysically important neutral atomic oxygen [OI] line at 4.7448 THz. The local oscillator is a third-order distributed feedback Quantum Cascade Laser operating in continuous wave mode at 4.741 THz. A quasi-optical, superconducting NbN hot electron bolometer is used as the mixer. We recorded a double sideband receiver noise temperature (T^DSB_rec) of 815 K, which is ~7 times the quantum noise limit (h\u03bd/2k_B) and an Allan variance time of 15 s at an effective noise fluctuation bandwidth of 18 MHz. Heterodyne performance was confirmed by measuring a methanol line spectrum.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Density-functional calculations are carried out to investigate incommensurate magnetic structures and ferroelectric polarization in newly discovered multiferroic material MnI$_2$. The exchange interactions among local moments on Mn are parameterized by mapping the mean-field Heisenberg model on to total energy difference of several magnetic ordering states. The experimentally observed noncollinear magnetic states are well reproduced by using this model and exchange interaction parameters. The direction of polarization experimentally measured is also consistent with the result derived from the symmetry analysis of the magnetically ordered state. In particular, we find that the inter-plane magnetic exchange coupling is pivotal to the emergence of the spiral magnetic structure. This noncollinear magnetic structure, combined with spin-orbit coupling mainly from I ions, is responsible for the appearance of ferroelectric polarization.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "Revised entirely and a new figure added.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We study a \"strongly-coupled\" (SC) polariton system formed between the atom-like intersubband transitions in a semiconductor nanostructure and the THz optical modes that are localised at the edges of a gold aperture. The polaritons can be excited optically, by incoherent excitation with bandgap radiation, and we find that they also coherently scatter the same input laser, to give strikingly sharp \"sideband\" (SB) spectral peaks in the backscattered spectrum. The SB intensity is a sensitive track of the polariton density and they can be detected down to a quantum noise floor that is more than 2500 times lower than the excitation thresholds of comparable quantum cascade laser diodes. Compared with other coherent scattering mechanisms, higher order SB scattering events are readily observable, and we speculate that the effect may find utility as a passive all-optical wavelength shifting mechanism in telecommunications systems.\n        \u25b3 Less", "author": "Qing Hu"}, {"abstract": "We study a spectral generalization of classical combinatorial graph spanners to the spectral setting. Given a set of vectors $V\\subseteq \\Re^d$, we say a set $U\\subseteq V$ is an $\u03b1$-spectral spanner if for all $v\\in V$ there is a probability distribution $\u03bc_v$ supported on $U$ such that $$vv^\\intercal \\preceq \u03b1\\cdot\\mathbb{E}_{u\\sim\u03bc_v} uu^\\intercal.$$ We show that any set $V$ has an $\\tilde{O}(d)$-spectral spanner of size $\\tilde{O}(d)$ and this bound is almost optimal in the worst case.\n  We use spectral spanners to study composable core-sets for spectral problems. We show that for many objective functions one can use a spectral spanner, independent of the underlying functions, as a core-set and obtain almost optimal composable core-sets. For example, for the determinant maximization problem we obtain an $\\tilde{O}(k)^k$-composable core-set and we show that this is almost optimal in the worst case.\n  Our algorithm is a spectral analogue of the classical greedy algorithm for finding (combinatorial) spanners in graphs. We expect that our spanners find many other applications in distributed or parallel models of computation. Our proof is spectral. As a side result of our techniques, we show that the rank of diagonally dominant lower-triangular matrices are robust under `small perturbations' which could be of independent interests.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We consider the $(1+\u03b5)$-approximate nearest neighbor search problem: given a set $X$ of $n$ points in a $d$-dimensional space, build a data structure that, given any query point $y$, finds a point $x \\in X$ whose distance to $y$ is at most $(1+\u03b5) \\min_{x \\in X} \\|x-y\\|$ for an accuracy parameter $\u03b5\\in (0,1)$. Our main result is a data structure that occupies only $O(\u03b5^{-2} n \\log(n) \\log(1/\u03b5))$ bits of space, assuming all point coordinates are integers in the range $\\{-n^{O(1)} \\ldots n^{O(1)}\\}$, i.e., the coordinates have $O(\\log n)$ bits of precision. This improves over the best previously known space bound of $O(\u03b5^{-2} n \\log(n)^2)$, obtained via the randomized dimensionality reduction method of Johnson and Lindenstrauss (1984). We also consider the more general problem of estimating all distances from a collection of query points to all data points $X$, and provide almost tight upper and lower bounds for the space complexity of this problem.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "The nearest neighbor problem is defined as follows: Given a set $P$ of $n$ points in some metric space $(X,D)$, build a data structure that, given any point $q$, returns a point in $P$ that is closest to $q$ (its \"nearest neighbor\" in $P$). The data structure stores additional information about the set $P$, which is then used to find the nearest neighbor without computing all distances between $q$ and $P$. The problem has a wide range of applications in machine learning, computer vision, databases and other fields.\n  To reduce the time needed to find nearest neighbors and the amount of memory used by the data structure, one can formulate the {\\em approximate} nearest neighbor problem, where the the goal is to return any point $p' \\in P$ such that the distance from $q$ to $p'$ is at most $c \\cdot \\min_{p \\in P} D(q,p)$, for some $c \\geq 1$. Over the last two decades, many efficient solutions to this problem were developed. In this article we survey these developments, as well as their connections to questions in geometric functional analysis and combinatorial geometry.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We introduce a new distance-preserving compact representation of multi-dimensional point-sets. Given $n$ points in a $d$-dimensional space where each coordinate is represented using $B$ bits (i.e., $dB$ bits per point), it produces a representation of size $O( d \\log(d B/\u03b5) + \\log n)$ bits per point from which one can approximate the distances up to a factor of $1 \\pm \u03b5$. Our algorithm almost matches the recent bound of~\\cite{indyk2017near} while being much simpler. We compare our algorithm to Product Quantization (PQ)~\\cite{jegou2011product}, a state of the art heuristic metric compression method. We evaluate both algorithms on several data sets: SIFT (used in \\cite{jegou2011product}), MNIST~\\cite{lecun1998mnist}, New York City taxi time series~\\cite{guha2016robust} and a synthetic one-dimensional data set embedded in a high-dimensional space. With appropriately tuned parameters, our algorithm produces representations that are comparable to or better than those produced by PQ, while having provable guarantees on its performance.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "There is much interest in integrating millimeter wave radios (mmWave) into wireless LANs and 5G cellular networks to benefit from their multiple GHz of available spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios require highly directional antennas. Since the antennas have pencil-beams, the transmitter and receiver need to align their antenna beams before they can communicate. Existing solutions scan the entire space to find the best alignment. Such a process has been shown to introduce up to seconds of delay, and is unsuitable for wireless networks where an access point has to quickly switch between users and accommodate mobile clients.\n  This paper presents Rapid-Link, a new protocol that can find the best mmWave beam alignment without scanning the space. Given all possible directions for setting the antenna beam, Rapid-Link provably finds the optimal direction in logarithmic number of measurements. Further, Rapid-Link works within the existing 802.11ad standard for mmWave LAN, and can support both clients and access points. We have implemented Rapid-Link in a mmWave radio and evaluated it empirically. Our results show that it reduces beam alignment delay by orders of magnitude. In particular, for highly directional mmWave devices operating under 802.11ad, the delay drops from over a second to 2.5 ms.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "Empirical risk minimization (ERM) is ubiquitous in machine learning and underlies most supervised learning methods. While there has been a large body of work on algorithms for various ERM problems, the exact computational complexity of ERM is still not understood. We address this issue for multiple popular ERM problems including kernel SVMs, kernel ridge regression, and training the final layer of a neural network. In particular, we give conditional hardness results for these problems based on complexity-theoretic assumptions such as the Strong Exponential Time Hypothesis. Under these assumptions, we show that there are no algorithms that solve the aforementioned ERM problems to high accuracy in sub-quadratic time. We also give similar hardness results for computing the gradient of the empirical loss, which is the main computational burden in many non-convex learning tasks.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "In the Sparse Linear Regression (SLR) problem, given a $d \\times n$ matrix $M$ and a $d$-dimensional query $q$, the goal is to compute a $k$-sparse $n$-dimensional vector $\u03c4$ such that the error $||M \u03c4-q||$ is minimized. This problem is equivalent to the following geometric problem: given a set $P$ of $n$ points and a query point $q$ in $d$ dimensions, find the closest $k$-dimensional subspace to $q$, that is spanned by a subset of $k$ points in $P$. In this paper, we present data-structures/algorithms and conditional lower bounds for several variants of this problem (such as finding the closest induced $k$ dimensional flat/simplex instead of a subspace).\n  In particular, we present approximation algorithms for the online variants of the above problems with query time $\\tilde O(n^{k-1})$, which are of interest in the \"low sparsity regime\" where $k$ is small, e.g., $2$ or $3$. For $k=d$, this matches, up to polylogarithmic factors, the lower bound that relies on the affinely degenerate conjecture (i.e., deciding if $n$ points in $\\mathbb{R}^d$ contains $d+1$ points contained in a hyperplane takes $\u03a9(n^d)$ time). Moreover, our algorithms involve formulating and solving several geometric subproblems, which we believe to be of independent interest.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "The metric sketching problem is defined as follows. Given a metric on $n$ points, and $\u03b5>0$, we wish to produce a small size data structure (sketch) that, given any pair of point indices, recovers the distance between the points up to a $1+\u03b5$ distortion. In this paper we consider metrics induced by $\\ell_2$ and $\\ell_1$ norms whose spread (the ratio of the diameter to the closest pair distance) is bounded by $\u03a6>0$. A well-known dimensionality reduction theorem due to Johnson and Lindenstrauss yields a sketch of size $O(\u03b5^{-2} \\log (\u03a6n) n\\log n)$, i.e., $O(\u03b5^{-2} \\log (\u03a6n) \\log n)$ bits per point. We show that this bound is not optimal, and can be substantially improved to $O(\u03b5^{-2}\\log(1/\u03b5) \\cdot \\log n + \\log\\log \u03a6)$ bits per point. Furthermore, we show that our bound is tight up to a factor of $\\log(1/\u03b5)$.\n  We also consider sketching of general metrics and provide a sketch of size $O(n\\log(1/\u03b5)+ \\log\\log \u03a6)$ bits per point, which we show is optimal.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "Motivated by applications in computer vision and databases, we introduce and study the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of data points, the goal of SNN is to design a data structure that, given a collection of queries, finds a collection of close points that are compatible with each other. Formally, we are given $k$ query points $Q=q_1,\\cdots,q_k$, and a compatibility graph $G$ with vertices in $Q$, and the goal is to return data points $p_1,\\cdots,p_k$ that minimize (i) the weighted sum of the distances from $q_i$ to $p_i$ and (ii) the weighted sum, over all edges $(i,j)$ in the compatibility graph $G$, of the distances between $p_i$ and $p_j$. The problem has several applications, where one wants to return a set of consistent answers to multiple related queries. This generalizes well-studied computational problems, including NN, Aggregate NN and the 0-extension problem.\n  In this paper we propose and analyze the following general two-step method for designing efficient data structures for SNN. In the first step, for each query point $q_i$ we find its (approximate) nearest neighbor point $\\hat{p}_i$; this can be done efficiently using existing approximate nearest neighbor structures. In the second step, we solve an off-line optimization problem over sets $q_1,\\cdots,q_k$ and $\\hat{p}_1,\\cdots,\\hat{p}_k$; this can be done efficiently given that $k$ is much smaller than $n$. Even though $\\hat{p}_1,\\cdots,\\hat{p}_k$ might not constitute the optimal answers to queries $q_1,\\cdots,q_k$, we show that, for the unweighted case, the resulting algorithm is $O(\\log k/\\log \\log k)$-approximation. Also, we show that the approximation factor can be in fact reduced to a constant for compatibility graphs frequently occurring in practice.\n  Finally, we show that the \"empirical approximation factor\" provided by the above approach is very close to 1.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "Regular expressions constitute a fundamental notion in formal language theory and are frequently used in computer science to define search patterns. A classic algorithm for these problems constructs and simulates a non-deterministic finite automaton corresponding to the expression, resulting in an $O(mn)$ running time (where $m$ is the length of the pattern and $n$ is the length of the text). This running time can be improved slightly (by a polylogarithmic factor), but no significantly faster solutions are known. At the same time, much faster algorithms exist for various special cases of regular expressions, including dictionary matching, wildcard matching, subset matching, word break problem etc.\n  In this paper, we show that the complexity of regular expression matching can be characterized based on its {\\em depth} (when interpreted as a formula). Our results hold for expressions involving concatenation, OR, Kleene star and Kleene plus. For regular expressions of depth two (involving any combination of the above operators), we show the following dichotomy: matching and membership testing can be solved in near-linear time, except for \"concatenations of stars\", which cannot be solved in strongly sub-quadratic time assuming the Strong Exponential Time Hypothesis (SETH). For regular expressions of depth three the picture is more complex. Nevertheless, we show that all problems can either be solved in strongly sub-quadratic time, or cannot be solved in strongly sub-quadratic time assuming SETH.\n  An intriguing special case of membership testing involves regular expressions of the form \"a star of an OR of concatenations\", e.g., $[a|ab|bc]^*$. This corresponds to the so-called {\\em word break} problem, for which a dynamic programming algorithm with a runtime of (roughly) $O(n\\sqrt{m})$ is known. We show that the latter bound is not tight and improve the runtime to $O(nm^{0.44\\ldots})$.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn 2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.\n  We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We consider the classic Set Cover problem in the data stream model. For $n$ elements and $m$ sets ($m\\geq n$) we give a $O(1/\u03b4)$-pass algorithm with a strongly sub-linear $\\tilde{O}(mn^\u03b4)$ space and logarithmic approximation factor. This yields a significant improvement over the earlier algorithm of Demaine et al. [DIMV14] that uses exponentially larger number of passes. We complement this result by showing that the tradeoff between the number of passes and space exhibited by our algorithm is tight, at least when the approximation factor is equal to $1$. Specifically, we show that any algorithm that computes set cover exactly using $({1 \\over 2\u03b4}-1)$ passes must use $\\tilde\u03a9(mn^\u03b4)$ space in the regime of $m=O(n)$. Furthermore, we consider the problem in the geometric setting where the elements are points in $\\mathbb{R}^2$ and sets are either discs, axis-parallel rectangles, or fat triangles in the plane, and show that our algorithm (with a slight modification) uses the optimal $\\tilde{O}(n)$ space to find a logarithmic approximation in $O(1/\u03b4)$ passes.\n  Finally, we show that any randomized one-pass algorithm that distinguishes between covers of size 2 and 3 must use a linear (i.e., $\u03a9(mn)$) amount of space. This is the first result showing that a randomized, approximate algorithm cannot achieve a space bound that is sublinear in the input size.\n  This indicates that using multiple passes might be necessary in order to achieve sub-linear space bounds for this problem while guaranteeing small approximation factors.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "For every fixed constant $\u03b1> 0$, we design an algorithm for computing the $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \\in \\mathbb{R}^N$ in time $k^{1+\u03b1} (\\log N)^{O(1)}$. Specifically, the algorithm is given query access to $x$ and computes a $k$-sparse $\\tilde{x} \\in \\mathbb{R}^N$ satisfying $\\|\\tilde{x} - \\hat{x}\\|_1 \\leq c \\|\\hat{x} - H_k(\\hat{x})\\|_1$, for an absolute constant $c > 0$, where $\\hat{x}$ is the transform of $x$ and $H_k(\\hat{x})$ is its best $k$-sparse approximation. Our algorithm is fully deterministic and only uses non-adaptive queries to $x$ (i.e., all queries are determined and performed in parallel when the algorithm starts).\n  An important technical tool that we use is a construction of nearly optimal and linear lossless condensers which is a careful instantiation of the GUV condenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design a deterministic and non-adaptive $\\ell_1/\\ell_1$ compressed sensing scheme based on general lossless condensers that is equipped with a fast reconstruction algorithm running in time $k^{1+\u03b1} (\\log N)^{O(1)}$ (for the GUV-based condenser) and is of independent interest. Our scheme significantly simplifies and improves an earlier expander-based construction due to Berinde, Gilbert, Indyk, Karloff, Strauss (Allerton 2008).\n  Our methods use linear lossless condensers in a black box fashion; therefore, any future improvement on explicit constructions of such condensers would immediately translate to improved parameters in our framework (potentially leading to $k (\\log N)^{O(1)}$ reconstruction time with a reduced exponent in the poly-logarithmic factor, and eliminating the extra parameter $\u03b1$).\n  Finally, by allowing the algorithm to use randomness, while still using non-adaptive queries, the running time of the algorithm can be improved to $\\tilde{O}(k \\log^3 N)$.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We initiate the study of trade-offs between sparsity and the number of measurements in sparse recovery schemes for generic norms. Specifically, for a norm $\\|\\cdot\\|$, sparsity parameter $k$, approximation factor $K>0$, and probability of failure $P>0$, we ask: what is the minimal value of $m$ so that there is a distribution over $m \\times n$ matrices $A$ with the property that for any $x$, given $Ax$, we can recover a $k$-sparse approximation to $x$ in the given norm with probability at least $1-P$? We give a partial answer to this problem, by showing that for norms that admit efficient linear sketches, the optimal number of measurements $m$ is closely related to the doubling dimension of the metric induced by the norm $\\|\\cdot\\|$ on the set of all $k$-sparse vectors. By applying our result to specific norms, we cast known measurement bounds in our general framework (for the $\\ell_p$ norms, $p \\in [1,2]$) as well as provide new, measurement-efficient schemes (for the Earth-Mover Distance norm). The latter result directly implies more succinct linear sketches for the well-studied planar $k$-median clustering problem. Finally, our lower bound for the doubling dimension of the EMD norm enables us to address the open question of [Frahling-Sohler, STOC'05] about the space complexity of clustering problems in the dynamic streaming model.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "Visualizations are frequently used as a means to understand trends and gather insights from datasets, but often take a long time to generate. In this paper, we focus on the problem of rapidly generating approximate visualizations while preserving crucial visual proper- ties of interest to analysts. Our primary focus will be on sampling algorithms that preserve the visual property of ordering; our techniques will also apply to some other visual properties. For instance, our algorithms can be used to generate an approximate visualization of a bar chart very rapidly, where the comparisons between any two bars are correct. We formally show that our sampling algorithms are generally applicable and provably optimal in theory, in that they do not take more samples than necessary to generate the visualizations with ordering guarantees. They also work well in practice, correctly ordering output groups while taking orders of magnitude fewer samples and much less time than conventional sampling schemes.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "The edit distance (a.k.a. the Levenshtein distance) between two strings is defined as the minimum number of insertions, deletions or substitutions of symbols needed to transform one string into another. The problem of computing the edit distance between two strings is a classical computational task, with a well-known algorithm based on dynamic programming. Unfortunately, all known algorithms for this problem run in nearly quadratic time.\n  In this paper we provide evidence that the near-quadratic running time bounds known for the problem of computing edit distance might be tight. Specifically, we show that, if the edit distance can be computed in time $O(n^{2-\u03b4})$ for some constant $\u03b4>0$, then the satisfiability of conjunctive normal form formulas with $N$ variables and $M$ clauses can be solved in time $M^{O(1)} 2^{(1-\u03b5)N}$ for a constant $\u03b5>0$. The latter result would violate the Strong Exponential Time Hypothesis, which postulates that such algorithms do not exist.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "Compressive Sensing (CS) stipulates that a sparse signal can be recovered from a small number of linear measurements, and that this recovery can be performed efficiently in polynomial time. The framework of model-based compressive sensing (model-CS) leverages additional structure in the signal and prescribes new recovery schemes that can reduce the number of measurements even further. However, model-CS requires an algorithm that solves the model-projection problem: given a query signal, produce the signal in the model that is also closest to the query signal. Often, this optimization can be computationally very expensive. Moreover, an approximation algorithm is not sufficient for this optimization task. As a result, the model-projection problem poses a fundamental obstacle for extending model-CS to many interesting models.\n  In this paper, we introduce a new framework that we call approximation-tolerant model-based compressive sensing. This framework includes a range of algorithms for sparse recovery that require only approximate solutions for the model-projection problem. In essence, our work removes the aforementioned obstacle to model-based compressive sensing, thereby extending the applicability of model-CS to a much wider class of models. We instantiate this new framework for the Constrained Earth Mover Distance (CEMD) model, which is particularly useful for signal ensembles where the positions of the nonzero coefficients do not change significantly as a function of spatial (or temporal) location. We develop novel approximation algorithms for both the maximization and the minimization versions of the model-projection problem via graph optimization techniques. Leveraging these algorithms into our framework results in a nearly sample-optimal sparse recovery scheme for the CEMD model.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We give an algorithm for $\\ell_2/\\ell_2$ sparse recovery from Fourier measurements using $O(k\\log N)$ samples, matching the lower bound of \\cite{DIPW} for non-adaptive algorithms up to constant factors for any $k\\leq N^{1-\u03b4}$. The algorithm runs in $\\tilde O(N)$ time. Our algorithm extends to higher dimensions, leading to sample complexity of $O_d(k\\log N)$, which is optimal up to constant factors for any $d=O(1)$. These are the first sample optimal algorithms for these problems.\n  A preliminary experimental evaluation indicates that our algorithm has empirical sampling complexity comparable to that of other recovery methods known in the literature, while providing strong provable guarantees on the recovery quality.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We present a new data structure for the c-approximate near neighbor problem (ANN) in the Euclidean space. For n points in R^d, our algorithm achieves O(n^\u03c1 + d log n) query time and O(n^{1 + \u03c1} + d log n) space, where \u03c1<= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over the result by Andoni and Indyk (FOCS 2006) and the first data structure that bypasses a locality-sensitive hashing lower bound proved by O'Donnell, Wu and Zhou (ICS 2011). By a standard reduction we obtain a data structure for the Hamming space and \\ell_1 norm with \u03c1<= 7/(8c) + O(1/c^{3/2}) + o(1), which is the first improvement over the result of Indyk and Motwani (STOC 1998).\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "The Restricted Isometry Property (RIP) is a fundamental property of a matrix enabling sparse recovery. Informally, an m x n matrix satisfies RIP of order k in the l_p norm if ||Ax||_p \\approx ||x||_p for any vector x that is k-sparse, i.e., that has at most k non-zeros. The minimal number of rows m necessary for the property to hold has been extensively investigated, and tight bounds are known. Motivated by signal processing models, a recent work of Baraniuk et al has generalized this notion to the case where the support of x must belong to a given model, i.e., a given family of supports. This more general notion is much less understood, especially for norms other than l_2. In this paper we present tight bounds for the model-based RIP property in the l_1 norm. Our bounds hold for the two most frequently investigated models: tree-sparsity and block-sparsity. We also show implications of our results to sparse recovery problems.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We present the first sample-optimal sublinear time algorithms for the sparse Discrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our algorithms are analyzed for /average case/ signals. For signals whose spectrum is exactly sparse, our algorithms use O(k) samples and run in O(k log k) time, where k is the expected sparsity of the signal. For signals whose spectrum is approximately sparse, our algorithm uses O(k log n) samples and runs in O(k log^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of samples used by our algorithms matches the known lower bounds for the respective signal models.\n  By a known reduction, our algorithms give similar results for the one-dimensional sparse Discrete Fourier Transform when n is a power of a small composite number (e.g., n = 6^t).\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We propose a framework for compressive sensing of images with local distinguishable objects, such as stars, and apply it to solve a problem in celestial navigation. Specifically, let x be an N-pixel real-valued image, consisting of a small number of local distinguishable objects plus noise. Our goal is to design an m-by-N measurement matrix A with m << N, such that we can recover an approximation to x from the measurements Ax.\n  We construct a matrix A and recovery algorithm with the following properties: (i) if there are k objects, the number of measurements m is O((k log N)/(log k)), undercutting the best known bound of O(k log(N/k)) (ii) the matrix A is very sparse, which is important for hardware implementations of compressive sensing algorithms, and (iii) the recovery algorithm is empirically fast and runs in time polynomial in k and log(N).\n  We also present a comprehensive study of the application of our algorithm to attitude determination, or finding one's orientation in space. Spacecraft typically use cameras to acquire an image of the sky, and then identify stars in the image to compute their orientation. Taking pictures is very expensive for small spacecraft, since camera sensors use a lot of power. Our algorithm optically compresses the image before it reaches the camera's array of pixels, reducing the number of sensors that are required.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We consider the problem of computing the k-sparse approximation to the discrete Fourier transform of an n-dimensional signal. We show:\n  * An O(k log n)-time randomized algorithm for the case where the input signal has at most k non-zero Fourier coefficients, and\n  * An O(k log n log(n/k))-time randomized algorithm for general input signals.\n  Both algorithms achieve o(n log n) time, and thus improve over the Fast Fourier Transform, for any k = o(n). They are the first known algorithms that satisfy this property. Also, if one assumes that the Fast Fourier Transform is optimal, the algorithm for the exactly k-sparse case is optimal for any k = n^{\u03a9(1)}.\n  We complement our algorithmic results by showing that any algorithm for computing the sparse Fourier transform of a general signal must use at least \u03a9(k log(n/k)/ log log n) signal samples, even if it is allowed to perform adaptive sampling.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "The goal of (stable) sparse recovery is to recover a $k$-sparse approximation $x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is to recover $x*$ such that\n||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q\nfor some constant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or $p=q=2$, this task can be accomplished using $m=O(k \\log (n/k))$ non-adaptive measurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].\n  In this paper we show that if one is allowed to perform measurements that are adaptive, then the number of measurements can be considerably reduced. Specifically, for $C=1+eps$ and $p=q=2$ we show\n- A scheme with $m=O((1/eps)k log log (n eps/k))$ measurements that uses $O(log* k \\log \\log (n eps/k))$ rounds. This is a significant improvement over the best possible non-adaptive bound.\n- A scheme with $m=O((1/eps) k log (k/eps) + k \\log (n/k))$ measurements that uses /two/ rounds. This improves over the best possible non-adaptive bound.\nTo the best of our knowledge, these are the first results of this type.\nAs an independent application, we show how to solve the problem of finding a duplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using $O(log n)$ bits of space and $O(log log n)$ passes, improving over the best possible space complexity achievable using a single pass.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We consider the following k-sparse recovery problem: design an m x n matrix A, such that for any signal x, given Ax we can efficiently recover x' satisfying\n  ||x-x'||_1 <= C min_{k-sparse} x\"} ||x-x\"||_1.\n  It is known that there exist matrices A with this property that have only O(k log (n/k)) rows.\n  In this paper we show that this bound is tight. Our bound holds even for the more general /randomized/ version of the problem, where A is a random variable and the recovery algorithm is required to work for any fixed x with constant probability (over A).\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "We initiate the study of sparse recovery problems under the Earth-Mover Distance (EMD). Specifically, we design a distribution over m x n matrices A such that for any x, given Ax, we can recover a k-sparse approximation to x under the EMD distance. One construction yields m = O(k log(n/k)) and a 1 + epsilon approximation factor, which matches the best achievable bound for other error measures, such as the L_1 norm. Our algorithms are obtained by exploiting novel connections to other problems and areas, such as streaming algorithms for k-median clustering and model-based compressive sensing. We also provide novel algorithms and results for the latter problems.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "It has been known since 1970's that the N-dimensional $\\ell_1$-space contains nearly Euclidean subspaces whose dimension is $\u03a9(N)$. However, proofs of existence of such subspaces were probabilistic, hence non-constructive, which made the results not-quite-suitable for subsequently discovered applications to high-dimensional nearest neighbor search, error-correcting codes over the reals, compressive sensing and other computational problems. In this paper we present a \"low-tech\" scheme which, for any $a > 0$, allows to exhibit nearly Euclidean $\u03a9(N)$-dimensional subspaces of $\\ell_1^N$ while using only $N^a$ random bits. Our results extend and complement (particularly) recent work by Guruswami-Lee-Wigderson. Characteristic features of our approach include (1) simplicity (we use only tensor products) and (2) yielding \"almost Euclidean\" subspaces with arbitrarily small distortions.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "In the first part of the paper, we present an (1+\u03bc)-approximation algorithm to the minimum-spanning tree of points in a planar arrangement of lines, where the metric is the number of crossings between the spanning tree and the lines. The expected running time is O((n/\u03bc^5) alpha^3(n) log^5 n), where \u03bc> 0 is a prescribed constant.\n  In the second part of our paper, we show how to embed such a crossing metric, into high-dimensions, so that the distances are preserved. As a result, we can deploy a large collection of subquadratic approximations algorithms \\cite im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric as a distance function. Applications include matching, clustering, nearest-neighbor, and furthest-neighbor.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "In this paper we present algorithms for a number of problems in geometric pattern matching where the input consist of a collections of segments in the plane. Our work consists of two main parts. In the first, we address problems and measures that relate to collections of orthogonal line segments in the plane. Such collections arise naturally from problems in mapping buildings and robot exploration.\n  We propose a new measure of segment similarity called a \\emph{coverage measure}, and present efficient algorithms for maximising this measure between sets of axis-parallel segments under translations. Our algorithms run in time $O(n^3\\polylog n)$ in the general case, and run in time $O(n^2\\polylog n)$ for the case when all segments are horizontal. In addition, we show that when restricted to translations that are only vertical, the Hausdorff distance between two sets of horizontal segments can be computed in time roughly $O(n^{3/2}{\\sl polylog}n)$. These algorithms form significant improvements over the general algorithm of Chew et al. that takes time $O(n^4 \\log^2 n)$. In the second part of this paper we address the problem of matching polygonal chains. We study the well known \\Frd, and present the first algorithm for computing the \\Frd under general translations. Our methods also yield algorithms for computing a generalization of the \\Fr distance, and we also present a simple approximation algorithm for the \\Frd that runs in time $O(n^2\\polylog n)$.\n        \u25b3 Less", "author": "Piotr Indyk"}, {"abstract": "A model for THz generation by optical rectification using tilted-pulse-fronts is developed. It simultaneously accounts for (i) the spatio-temporal distortions of the optical pump pulse, (ii) the nonlinear coupled interaction of THz and optical radiation in two spatial dimensions (2-D), (iii) self-phase modulation and (iv) stimulated Raman scattering. The model is validated by quantitative agreement with experiments and analytic calculations. We show that the optical pump beam is significantly broadened in the transverse-momentum (kx) domain as a consequence of the spectral broadening caused by THz generation. In the presence of this large frequency and transverse-momentum (or angular) spread, group velocity dispersion causes a spatio-temporal break-up of the optical pump pulse which inhibits further THz generation. The implications of these effects on energy scaling and optimization of optical-to-THz conversion efficiency are discussed. This suggests the use of optical pump pulses with elliptical beam profiles for large optical pump energies. It is seen that optimization of the setup is highly dependent on optical pump conditions. Trade-offs of optimizing the optical-to-THz conversion efficiency on the spatial and spectral properties of THz radiation is discussed to guide the development of such sources.\n        \u25b3 Less", "author": "Erich Ippen"}, {"abstract": "We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "Sketch-based modeling strives to bring the ease and immediacy of drawing to the 3D world. However, while drawings are easy for humans to create, they are very challenging for computers to interpret due to their sparsity and ambiguity. We propose a data-driven approach that tackles this challenge by learning to reconstruct 3D shapes from one or more drawings. At the core of our approach is a deep convolutional neural network (CNN) that predicts occupancy of a voxel grid from a line drawing. This CNN provides us with an initial 3D reconstruction as soon as the user completes a single drawing of the desired shape. We complement this single-view network with an updater CNN that refines an existing prediction given a new drawing of the shape created from a novel viewpoint. A key advantage of our approach is that we can apply the updater iteratively to fuse information from an arbitrary number of viewpoints, without requiring explicit stroke correspondences between the drawings. We train both CNNs by rendering synthetic contour drawings from hand-modeled shape collections as well as from procedurally-generated abstract shapes. Finally, we integrate our CNNs in a minimal modeling interface that allows users to seamlessly draw an object, rotate it to see its 3D reconstruction, and refine it by re-drawing from another vantage point using the 3D reconstruction as guidance. The main strengths of our approach are its robustness to freehand bitmap drawings, its ability to adapt to different object categories, and the continuum it offers between single-view and multi-view sketch-based modeling.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user \"hints\" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user \"hints\" to the desired colorization, showing an application to color histogram transfer. Our code and models are available at https://richzhang.github.io/ideepcolor.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "Manipulation of deformable objects, such as ropes and cloth, is an important but challenging problem in robotics. We present a learning-based system where a robot takes as input a sequence of images of a human manipulating a rope from an initial to goal configuration, and outputs a sequence of actions that can reproduce the human demonstration, using only monocular images as input. To perform this task, the robot learns a pixel-level inverse dynamics model of rope manipulation directly from images in a self-supervised manner, using about 60K interactions with the rope collected autonomously by the robot. The human demonstration provides a high-level plan of what to do and the low-level inverse model is used to execute the plan. We show that by combining the high and low-level plans, the robot can successfully manipulate a rope into a variety of target shapes using only a sequence of human-provided images for direction.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a \"colorization Turing test,\" asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "In this paper, we study the problem of reproducing the world lighting from a single image of an object covered with random specular microfacets on the surface. We show that such reflectors can be interpreted as a randomized mapping from the lighting to the image. Such specular objects have very different optical properties from both diffuse surfaces and smooth specular objects like metals, so we design special imaging system to robustly and effectively photograph them. We present simple yet reliable algorithms to calibrate the proposed system and do the inference. We conduct experiments to verify the correctness of our model assumptions and prove the effectiveness of our pipeline.\n        \u25b3 Less", "author": "Phillip Isola"}, {"abstract": "Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Hierarchical Bayesian methods can unify many related tasks (e.g. k-shot classification, conditional and unconditional generation) as inference within a single generative model. However, when this generative model is expressed as a powerful neural network such as a PixelCNN, we show that existing learning techniques typically fail to effectively use latent variables. To address this, we develop a modification of the Variational Autoencoder in which encoded observations are decoded to new elements from the same class. This technique, which we call a Variational Homoencoder (VHE), produces a hierarchical latent variable model which better utilises latent variables. We use the VHE framework to learn a hierarchical PixelCNN on the Omniglot dataset, which outperforms all existing models on test set likelihood and achieves strong performance on one-shot generation and classification tasks. We additionally validate the VHE on natural images from the YouTube Faces database. Finally, we develop extensions of the model that apply to richer dataset structures such as factorial and hierarchical categories.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Interpretability has arisen as a key desideratum of machine learning models alongside performance. Approaches so far have been primarily concerned with fixed dimensional inputs emphasizing feature relevance or selection. In contrast, we focus on temporal modeling and the problem of tailoring the predictor, functionally, towards an interpretable family. To this end, we propose a co-operative game between the predictor and an explainer without any a priori restrictions on the functional class of the predictor. The goal of the explainer is to highlight, locally, how well the predictor conforms to the chosen interpretable family of temporal models. Our co-operative game is setup asymmetrically in terms of information sets for efficiency reasons. We develop and illustrate the framework in the context of temporal sequence models with examples.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Many problems in machine learning involve calculating correspondences between sets of objects, such as point clouds or images. Discrete optimal transport (OT) provides a natural and successful approach to such tasks whenever the two sets of objects can be represented in the same space or when we can evaluate distances between the objects. Unfortunately neither requirement is likely to hold when object representations are learned from data. Indeed, automatically derived representations such as word embeddings are typically fixed only up to some global transformations, for example, reflection or rotation. As a result, pairwise distances across the two types of objects are ill-defined without specifying their relative transformation. In this work, we propose a general framework for optimal transport in the presence of latent global transformations. We discuss algorithms for the specific case of orthonormal transformations, and show promising results in unsupervised word alignment.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Most recent work on interpretability of complex machine learning models has focused on estimating $\\textit{a posteriori}$ explanations for previously trained models around specific predictions. $\\textit{Self-explaining}$ models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Reparameterization of variational auto-encoders with continuous latent spaces is an effective method for reducing the variance of their gradient estimates. However, using the same approach when latent variables are discrete is problematic, due to the resulting non-differentiable objective. In this work, we present a direct optimization method that propagates gradients through a non-differentiable $\\arg \\max$ prediction operation. We apply this method to discrete variational auto-encoders, by modeling a discrete random variable by the $\\arg \\max$ function of the Gumbel-Max perturbation model.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Optimal Transport has recently gained interest in machine learning for applications ranging from domain adaptation, sentence similarities to deep learning. Yet, its ability to capture frequently occurring structure beyond the \"ground metric\" is limited. In this work, we develop a nonlinear generalization of (discrete) optimal transport that is able to reflect much additional structure. We demonstrate how to leverage the geometry of this new model for fast algorithms, and explore connections and properties. Illustrative experiments highlight the benefit of the induced structured couplings for tasks in domain adaptation and natural language processing.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized representation to effectively utilize entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an \"explanation\" consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Our goal is to identify beneficial interventions from observational data. We consider interventions that are narrowly focused (impacting few covariates) and may be tailored to each individual or globally enacted over a population. For applications where harmful intervention is drastically worse than proposing no change, we propose a conservative definition of the optimal intervention. Assuming the underlying relationship remains invariant under intervention, we develop efficient algorithms to identify the optimal intervention policy from limited data and provide theoretical guarantees for our approach in a Gaussian Process setting. Although our methods assume covariates can be precisely adjusted, they remain capable of improving outcomes in misspecified settings where interventions incur unintentional downstream effects. Empirically, our approach identifies good interventions in two practical applications: gene perturbation and writing improvement.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "This paper presents a new approach, called perturb-max, for high-dimensional statistical inference that is based on applying random perturbations followed by optimization. This framework injects randomness to maximum a-posteriori (MAP) predictors by randomly perturbing the potential function for the input. A classic result from extreme value statistics asserts that perturb-max operations generate unbiased samples from the Gibbs distribution using high-dimensional perturbations. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. However, when the perturbations are of low dimension, sampling the perturb-max prediction is as efficient as MAP optimization. This paper shows that the expected value of perturb-max inference with low dimensional perturbations can be used sequentially to generate unbiased samples from the Gibbs distribution. Furthermore the expected value of the maximal perturbations is a natural bound on the entropy of such perturb-max models. A measure concentration result for perturb-max values shows that the deviation of their sampled average from its expectation decays exponentially in the number of samples, allowing effective approximation of the expectation.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs).\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We present a nonparametric framework to model a short sequence of probability distributions that vary both due to underlying effects of sequential progression and confounding noise. To distinguish between these two types of variation and estimate the sequential-progression effects, our approach leverages an assumption that these effects follow a persistent trend. This work is motivated by the recent rise of single-cell RNA-sequencing experiments over a brief time course, which aim to identify genes relevant to the progression of a particular biological process across diverse cell populations. While classical statistical tools focus on scalar-response regression or order-agnostic differences between distributions, it is desirable in this setting to consider both the full distributions as well as the structure imposed by their ordering. We introduce a new regression model for ordinal covariates where responses are univariate distributions and the underlying relationship reflects consistent changes in the distributions over increasing levels of the covariate. This concept is formalized as a \"trend\" in distributions, which we define as an evolution that is linear under the Wasserstein metric. Implemented via a fast alternating projections algorithm, our method exhibits numerous strengths in simulations and analyses of single-cell gene expression data.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Large unweighted directed graphs are commonly used to capture relations between entities. A fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices. Despite the significance of this problem, statistical characterization of the proposed metrics has been limited. We introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus. Using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the Laplace transformed hitting time (LTHT). The metric serves as a natural, provably well-behaved alternative to the expected hitting time. We establish a general correspondence between hitting times of the Brownian motion and analogous hitting times on the graph. We show that the LTHT is consistent with respect to the underlying metric of a geometric graph, preserves clustering tendency, and remains robust against random addition of non-geometric edges. Tests on simulated and real-world data show that the LTHT matches theoretical predictions and outperforms alternatives.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Continuous vector representations of words and objects appear to carry surprisingly rich semantic content. In this paper, we advance both the conceptual and theoretical understanding of word embeddings in three ways. First, we ground embeddings in semantic spaces studied in cognitive-psychometric literature and introduce new evaluation tasks. Second, in contrast to prior work, we take metric recovery as the key object of study, unify existing algorithms as consistent metric recovery methods based on co-occurrence counts from simple Markov random walks, and propose a new recovery algorithm. Third, we generalize metric recovery to graphs and manifolds, relating co-occurence counts on random walks in graphs and random processes on manifolds to the underlying metric to be recovered, thereby reconciling manifold estimation and embedding algorithms. We compare embedding algorithms across a range of tasks, from nonlinear dimensionality reduction to three semantic language tasks, including analogies, sequence completion, and classification.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Contemporary deep neural networks exhibit impressive results on practical problems. These networks generalize well although their inherent capacity may extend significantly beyond the number of training examples. We analyze this behavior in the context of deep, infinite neural networks. We show that deep infinite layers are naturally aligned with Gaussian processes and kernel methods, and devise stochastic kernels that encode the information of these networks. We show that stability results apply despite the size, offering an explanation for their empirical success.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the fine-grained sentiment classification task.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \\cite{Altun03,Collins04b,Taskar03}. In natural language processing, recent work \\cite{Zhang14,Zhang15} has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \\cite{McAllester07}. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \\cite{Zhang14,Zhang15}, our theoretical results show that more general techniques are possible.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We present a framework for clustering with cluster-specific feature selection. The framework, CRAFT, is derived from asymptotic log posterior formulations of nonparametric MAP-based clustering models. CRAFT handles assorted data, i.e., both numeric and categorical data, and the underlying objective functions are intuitively appealing. The resulting algorithm is simple to implement and scales nicely, requires minimal parameter tuning, obviates the need to specify the number of clusters a priori, and compares favorably with other methods on real datasets.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We analyze directed, unweighted graphs obtained from $x_i\\in \\mathbb{R}^d$ by connecting vertex $i$ to $j$ iff $|x_i - x_j| < \u03b5(x_i)$. Examples of such graphs include $k$-nearest neighbor graphs, where $\u03b5(x_i)$ varies from point to point, and, arguably, many real world graphs such as co-purchasing graphs. We ask whether we can recover the underlying Euclidean metric $\u03b5(x_i)$ and the associated density $p(x_i)$ given only the directed graph and $d$.\n  We show that consistent recovery is possible up to isometric scaling when the vertex degree is at least $\u03c9(n^{2/(2+d)}\\log(n)^{d/(d+2)})$. Our estimator is based on a careful characterization of a random walk over the directed graph and the associated continuum limit. As an algorithm, it resembles the PageRank centrality metric. We demonstrate empirically that the estimator performs well on simulated examples as well as on real-world co-purchasing graphs even with a small number of points and degree scaling as low as $\\log(n)$.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "The maximum a-posteriori (MAP) perturbation framework has emerged as a useful approach for inference and learning in high dimensional complex models. By maximizing a randomly perturbed potential function, MAP perturbations generate unbiased samples from the Gibbs distribution. Unfortunately, the computational cost of generating so many high-dimensional random variables can be prohibitive. More efficient algorithms use sequential sampling strategies based on the expected value of low dimensional MAP perturbations. This paper develops new measure concentration inequalities that bound the number of samples needed to estimate such expected values. Applying the general result to MAP perturbations can yield a more efficient algorithm to approximate sampling from the Gibbs distribution. The measure concentration result is of general interest and may be applicable to other areas involving expected estimations.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "In this paper we describe how MAP inference can be used to sample efficiently from Gibbs distributions. Specifically, we provide means for drawing either approximate or unbiased samples from Gibbs' distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical \"high signal - high coupling\" regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We propose maximum likelihood estimation for learning Gaussian graphical models with a Gaussian (ell_2^2) prior on the parameters. This is in contrast to the commonly used Laplace (ell_1) prior for encouraging sparseness. We show that our optimization problem leads to a Riccati matrix equation, which has a closed form solution. We propose an efficient algorithm that performs a singular value decomposition of the training data. Our algorithm is O(NT^2)-time and O(NT)-space for N variables and T samples. Our method is tailored to high-dimensional problems (N gg T), in which sparseness promoting methods become intractable. Furthermore, instead of obtaining a single solution for a specific regularization parameter, our algorithm finds the whole solution path. We show that the method has logarithmic sample complexity under the spiked covariance model. We also propose sparsification of the dense solution with provable performance guarantees. We provide techniques for using our learnt models, such as removing unimportant variables, computing likelihoods and conditional distributions. Finally, we show promising results in several gene expressions datasets.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We present deterministic techniques for computing upper and lower bounds on marginal probabilities in sigmoid and noisy-OR networks.  These techniques become useful when the size of the network (or clique size) precludes exact computations.  We illustrate the tightness of the bounds by numerical experiments.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time.  This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al. 1995) constrain the tree parameter priors to be a compactly parameterized product of Dirichlet distributions. Beside allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Incorporating feature selection into a classification or regression method often carries a number of advantages. In this paper we formalize feature selection specifically from a discriminative perspective of improving classification/regression accuracy. The feature selection method is developed as an extension to the recently proposed maximum entropy discrimination (MED) framework. We describe MED as a flexible (Bayesian) regularization approach that subsumes, e.g., support vector classification, regression and exponential family models. For brevity, we restrict ourselves primarily to feature selection in the context of linear classification/regression methods and demonstrate that the proposed approach indeed carries substantial improvements in practice. Moreover, we discuss and develop various extensions of feature selection, including the problem of dealing with example specific but unobserved degrees of freedom -- alignments or invariants.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Bounds on the log partition function are important in a variety of contexts, including approximate inference, model fitting, decision theory, and large deviations analysis. We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model.  In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe free energy, but distinguished by the following desirable properties: i. they are cnvex, and have a unique global minimum; and ii. the global minimum gives an upper bound on the log partition function. The global minimum is defined by stationary conditions very similar to those defining fixed points of belief propagation or tree-based reparameterization Wainwright et al., 2001.  As with BP fixed points, the elements of the minimizing argument can be used as approximations to the marginals of the original model.  The analysis described here can be extended to structures of higher treewidth e.g., hypertrees, thereby making connections with more advanced approximations e.g., Kikuchi and variants Yedidia et al., 2001; Minka, 2001.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Active learning is a powerful approach to analyzing data effectively. We show that the feasibility of active learning depends crucially on the choice of measure with respect to which the query is being optimized.  The standard information gain, for example, does not permit an accurate evaluation with a small committee, a representative subset of the model space. We propose a surrogate measure requiring only a small committee and discuss the properties of this new measure. We devise, in addition, a bootstrap approach for committee selection. The advantages of this approach are illustrated in the context of recovering (regulatory) network models. \n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "A number of modern learning tasks involve estimation from heterogeneous information sources. This includes classification with labeled and unlabeled data as well as other problems with analogous structure such as competitive (game theoretic) problems. The associated estimation problems can be typically reduced to solving a set of fixed point equations (consistency conditions). We introduce a general method for combining a preferred information source with another in this setting by evolving continuous paths of fixed points at intermediate allocations. We explicitly identify critical points along the unique paths to either increase the stability of estimation or to ensure a significant departure from the initial source.  The homotopy continuation approach is guaranteed to terminate at the second source, and involves no combinatorial effort. We illustrate the power of these ideas both in classification tasks with labeled and unlabeled data, as well as in the context of a competitive (min-max) formulation of DNA sequence motif discovery.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We formulate a principle for classification with the knowledge of the marginal distribution over the data points (unlabeled data). The principle is cast in terms of Tikhonov style regularization where the regularization penalty articulates the way in which the marginal density should constrain otherwise unrestricted conditional distributions. Specifically, the regularization penalty penalizes any information introduced between the examples and labels beyond what is provided by the available labeled examples. The work extends Szummer and Jaakkola's information regularization (NIPS 2002) to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation. We show in addition how the information regularizer can be used as a measure of complexity of the classification task with unlabeled data and prove a relevant sample-complexity bound. We illustrate the regularization principle in practice by restricting the class of conditional distributions to be logistic regression models and constructing the regularization penalty from a finite set of unlabeled examples.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "This is the Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29 2005.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "In this paper, we present $\\ell_{1,p}$ multi-task structure learning for Gaussian graphical models. We analyze the sufficient number of samples for the correct recovery of the support union and edge signs. We also analyze the necessary number of samples for any conceivable method by providing information-theoretic lower bounds. We compare the statistical efficiency of multi-task learning versus that of single-task learning. For experiments, we use a block coordinate descent method that is provably convergent and generates a sequence of positive definite solutions. We provide experimental validation on synthetic data as well as on two publicly available real-world data sets, including functional magnetic resonance imaging and gene expression data.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "In this paper we relate the partition function to the max-statistics of random variables. In particular, we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models. As a result, we can use efficient MAP solvers such as graph-cuts to evaluate the corresponding partition function. We show that our method excels in the typical \"high signal - high coupling\" regime that results in ragged energy landscapes difficult for alternative approaches.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Inference problems in graphical models are often approximated by casting them as constrained optimization problems. Message passing algorithms, such as belief propagation, have previously been suggested as methods for solving these optimization problems. However, there are few convergence guarantees for such algorithms, and the algorithms are therefore not guaranteed to solve the corresponding optimization problem. Here we present an oriented tree decomposition algorithm that is guaranteed to converge to the global optimum of the Tree-Reweighted (TRW) variational problem. Our algorithm performs local updates in the convex dual of the TRW problem - an unconstrained generalized geometric program. Primal updates, also local, correspond to oriented reparametrization operations that leave the distribution intact.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Linear Programming (LP) relaxations have become powerful tools for finding the most probable (MAP) configuration in graphical models. These relaxations can be solved efficiently using message-passing algorithms such as belief propagation and, when the relaxation is tight, provably find the MAP configuration. The standard LP relaxation is not tight enough in many real-world problems, however, and this has lead to the use of higher order cluster-based LP relaxations. The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use. We propose to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution. Our dual message-passing algorithm finds the MAP configuration in protein sidechain placement, protein design, and stereo problems, in cases where the standard LP relaxation fails.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "We develop and analyze methods for computing provably optimal {\\em maximum a posteriori} (MAP) configurations for a subclass of Markov random fields defined on graphs with cycles. By decomposing the original distribution into a convex combination of tree-structured distributions, we obtain an upper bound on the optimal value of the original problem (i.e., the log probability of the MAP assignment) in terms of the combined optimal values of the tree problems. We prove that this upper bound is tight if and only if all the tree distributions share an optimal configuration in common. An important implication is that any such shared configuration must also be a MAP configuration for the original distribution. Next we develop two approaches to attempting to obtain tight upper bounds: (a) a {\\em tree-relaxed linear program} (LP), which is derived from the Lagrangian dual of the upper bounds; and (b) a {\\em tree-reweighted max-product message-passing algorithm} that is related to but distinct from the max-product algorithm. In this way, we establish a connection between a certain LP relaxation of the mode-finding problem, and a reweighted form of the max-product (min-sum) message-passing algorithm.\n        \u25b3 Less", "author": "Tommi Jaakkola"}, {"abstract": "Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e. 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that undergone gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Purpose: A combined diffusion-relaxometry MR acquisition and analysis pipeline for in-vivo human placenta, which allows for exploration of coupling between T2* and apparent diffusion coefficient (ADC) measurements in a sub 10 minute scan time.\n  Methods: We present a novel acquisition combining a diffusion prepared spin-echo with subsequent gradient echoes. The placentas of 17 pregnant women were scanned in-vivo, including both healthy controls and participants with various pregnancy complications. We estimate the joint T2*-ADC spectra using an inverse Laplace transform.\n  Results: T2*-ADC spectra demonstrate clear quantitative separation between normal and dysfunctional placentas.\n  Conclusions: Combined T2*-diffusivity MRI is promising for assessing fetal and maternal health during pregnancy. The T2*-ADC spectrum potentially provides additional information on tissue microstructure, compared to measuring these two contrasts separately. The presented method is immediately applicable to the study of other organs.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The Einstein-Yang-Mills equations are the source of many interesting solutions within general relativity, including families of particle-like and black hole solutions, and critical phenomena of more than one type. These solutions, discovered in the last thirty years, all assume a restricted form for the Yang-Mills gauge potential known as the \"magnetic\" ansatz. In this thesis we relax that assumption and investigate the most general solutions of the Einstein-Yang-Mills system assuming spherically symmetry, a Yang-Mills gauge group of SU(2), and zero cosmological constant. We proceed primarily by numerically integrating the equations and find new static solutions, for both regular and black hole boundary conditions, which are not asymptotically flat, and attempt to classify the possible static behaviours. We develop a code to solve the dynamic equations that uses a novel adaptive mesh refinement algorithm making full use of double-null coordinates. We find that the \"type II\" critical behaviour in the general case exhibits non-universal critical solutions, in contrast to the magnetic case and to all previously observed type II critical behaviour.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Operando calorimetry has previously been utilized to study degradation, side reactions, and other electrochemical effects in electrochemical cells such as batteries at or near room temperature. Calorimetric data can provide important information on the lifetime and thermal properties of electrochemical cells and can be used in practical engineering applications such as thermal management. High temperature electrochemical cells such as solid oxide fuel cells or electrolyzers can also benefit from operando calorimetry, although to our knowledge no such unit has been developed commercially. Herein, we report an operando calorimeter capable of simultaneous calorimetry and electrochemistry at temperatures up to 1,000 \u00b0C and in both oxidizing and reducing atmospheres. The calorimeter is constructed by modifying a commercial apparatus originally designed to study high temperature electrochemical cells in various gas environments. We utilize a grey-box, nonlinear system identification model to analyze calorimetric data and achieve an electrochemical cell power sensitivity of 16.1 +/- 11.7 mW. This operando calorimeter provides the tools needed to study both the thermal and kinetic behavior of electrochemical cells at elevated temperatures.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Metal hydrides are critical materials in numerous technologies including hydrogen storage, gas separation, and electrocatalysis. Here, using Pd-H as a model metal hydride, we perform electrochemical insertion studies of hydrogen via liquid and solid state electrolytes at 1 atm ambient pressure, and achieve H:Pd ratios near unity, the theoretical solubility limit. We show that the compositions achieved result from a dynamic balance between the rate of hydrogen insertion and evolution from the Pd lattice, the combined kinetics of which are sufficiently rapid that operando experiments are necessary to characterize instantaneous PdHx composition. We use simultaneous electrochemical insertion and X-ray diffraction measurements, combined with a new calibration of lattice parameter versus hydrogen concentration, to enable accurate quantification of the composition of electrochemically synthesized PdHx. Furthermore, we show that the achievable hydrogen concentration is severely limited by electrochemomechanical damage to the palladium and/or substrate. The understanding embodied in these results helps to establish new design rules for achieving high hydrogen concentrations in metal hydrides.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Characterizing electrochemical energy conversion devices during operation is an important strategy for correlating device performance with the properties of cell materials under real operating conditions. While operando characterization has been used extensively for low temperature electrochemical cells, these techniques remain challenging for solid oxide electrochemical cells due to the high temperatures and reactive gas atmospheres these cells require. Operando X-ray diffraction measurements of solid oxide electrochemical cells could detect changes in the crystal structure of the cell materials, which can be useful for understanding degradation process that limit device lifetimes, but the experimental capability to perform operando X-ray diffraction on the fuel electrodes of these cells has not been demonstrated. Here we present the first experimental apparatus capable of performing X-ray diffraction measurements on the fuel electrodes of high temperature solid oxide electrochemical cells during operation under reducing gas atmospheres. We present data from an example experiment with a model solid oxide cell to demonstrate that this apparatus can collect X-ray diffraction spectra during electrochemical cell operation at high temperatures in humidified H2 gas. Measurements performed using this apparatus can reveal new insights about solid oxide fuel cell and solid oxide electrolyzer cell degradation mechanisms to enable the design of durable, high performance devices.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The X-ray Integral Field Unit (X-IFU) is the high resolution X-ray spectrometer of the ESA Athena X-ray observatory. Over a field of view of 5' equivalent diameter, it will deliver X-ray spectra from 0.2 to 12 keV with a spectral resolution of 2.5 eV up to 7 keV on ~5 arcsecond pixels. The X-IFU is based on a large format array of super-conducting molybdenum-gold Transition Edge Sensors cooled at about 90 mK, each coupled with an absorber made of gold and bismuth with a pitch of 249 microns. A cryogenic anti-coincidence detector located underneath the prime TES array enables the non X-ray background to be reduced. A bath temperature of about 50 mK is obtained by a series of mechanical coolers combining 15K Pulse Tubes, 4K and 2K Joule-Thomson coolers which pre-cool a sub Kelvin cooler made of a 3He sorption cooler coupled with an Adiabatic Demagnetization Refrigerator. Frequency domain multiplexing enables to read out 40 pixels in one single channel. A photon interacting with an absorber leads to a current pulse, amplified by the readout electronics and whose shape is reconstructed on board to recover its energy with high accuracy. The defocusing capability offered by the Athena movable mirror assembly enables the X-IFU to observe the brightest X-ray sources of the sky (up to Crab-like intensities) by spreading the telescope point spread function over hundreds of pixels. Thus the X-IFU delivers low pile-up, high throughput (>50%), and typically 10 eV spectral resolution at 1 Crab intensities, i.e. a factor of 10 or more better than Silicon based X-ray detectors. In this paper, the current X-IFU baseline is presented, together with an assessment of its anticipated performance in terms of spectral resolution, background, and count rate capability. The X-IFU baseline configuration will be subject to a preliminary requirement review that is scheduled at the end of 2018.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The recently discovered (Rb,Cs)EuFe4As4 compounds exhibit an unusual combination of superconductivity (Tc = 35 K) and ferromagnetism (Tm = 15 K). We have performed a series of x-ray diffraction, ac magnetic susceptibility, dc magnetization, and electrical resistivity measurements on both RbEuFe4As4 and CsEuFe4As4 to pressures as high as 30 GPa. We find that the superconductivity onset is suppressed monotonically by pressure while the magnetic transition is enhanced at initial rates of dTm/dP = 1.7 K/GPa and 1.5 K/GPa for RbEuFe4As4 and CsEuFe4As4, respectively. Near 7 GPa, Tc onset and Tm become comparable. At higher pressures, signatures of bulk superconductivity gradually disappear. Room temperature x-ray diffraction measurements suggest the onset of a transition from tetragonal (T) to a half collapsed-tetragonal (hcT) phase at 10 GPa (RbEuFe4As4) and 12 GPa (CsEuFe4As4). The ability to tune Tc and Tm into coincidence with relatively modest pressures highlights (Rb,Cs)EuFe4As4 compounds as ideal systems to study the interplay of superconductivity and ferromagnetism.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Crater counting on the Moon and other bodies is crucial to constrain the dynamical history of the Solar System. This has traditionally been done by visual inspection of images, thus limiting the scope, efficiency, and/or accuracy of retrieval. In this paper we demonstrate the viability of using convolutional neural networks (CNNs) to determine the positions and sizes of craters from Lunar digital elevation maps (DEMs). We recover 92% of craters from the human-generated test set and almost double the total number of crater detections. Of these new craters, 15% are smaller in diameter than the minimum crater size in the ground-truth dataset. Our median fractional longitude, latitude and radius errors are 11% or less, representing good agreement with the human-generated datasets. From a manual inspection of 361 new craters we estimate the false positive rate of new craters to be 11%. Moreover, our Moon-trained CNN performs well when tested on DEM images of Mercury, detecting a large fraction of craters in each map. Our results suggest that deep learning will be a useful tool for rapidly and automatically extracting craters on various Solar System bodies. We make our code and data publicly available at https://github.com/silburt/DeepMoon.git and https://doi.org/10.5281/zenodo.1133969 .\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "In single star systems like our own Solar system, comets dominate the mass budget of bodies that are ejected into interstellar space, since they form further away and are less tightly bound. However 1I/`Oumuamua, the first interstellar object detected, appears asteroidal in its spectra and in its lack of detectable activity. We argue that the galactic budget of interstellar objects like 1I/`Oumuamua should be dominated by planetesimal material ejected during planet formation in circumbinary systems, rather than in single star systems or widely separated binaries. We further show that in circumbinary systems, rocky bodies should be ejected in comparable numbers to icy ones. This suggests that a substantial fraction of additional interstellar objects discovered in the future should display an active coma. We find that the rocky population, of which 1I/`Oumuamua seems to be a member, should be predominantly sourced from A-type and late B-star binaries.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Using an \\textit{ab initio} approach based on the GW approximation which includes strong local \\textbf{k}-space correlations, the Metal-Insulator Transition of M$_2$ vanadium dioxide is broken down into its component parts and investigated. Similarly to the M$_{1}$ structure, the Peierls pairing of the M$_{2}$ structure results in bonding-antibonding splitting which stabilizes states in which the majority of the charge density resides on the Peierls chain. This is insufficient to drop all of the bonding states into the lower Hubbard band however. An antiferroelectric distortion on the neighboring vanadium chain is required to reduce the repulsion felt by the Peierls bonding states by increasing the distances between the vanadium and apical oxygen atoms, lowering the potential overlap thus reducing the charge density accumulation and thereby the electronic repulsion. The antibonding states are simultaneously pushed into the upper Hubbard band. The data indicate that sufficiently modified GW calculations are able to describe the interplay of the atomic and electronic structures occurring in Mott metal-insulator transitions.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Using a membrane-driven diamond anvil cell and both ac magnetic susceptibility and electrical resistivity measurements, we have characterized the superconducting phase diagram of elemental barium to pressures as high as 65 GPa. We have determined the superconducting properties of the recently discovered Ba-VI crystal structure, which can only be accessed via the application of pressure at low temperature. We find that Ba-VI exhibits a maximum Tc near 8 K, which is substantially higher than the maximum Tc found when pressure is applied at room temperature.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We describe ColliderBit, a new code for the calculation of high energy collider observables in theories of physics beyond the Standard Model (BSM). ColliderBit features a generic interface to BSM models, a unique parallelised Monte Carlo event generation scheme suitable for large-scale supercomputer applications, and a number of LHC analyses, covering a reasonable range of the BSM signatures currently sought by ATLAS and CMS. ColliderBit also calculates likelihoods for Higgs sector observables, and LEP searches for BSM particles. These features are provided by a combination of new code unique to ColliderBit, and interfaces to existing state-of-the-art public codes. ColliderBit is both an important part of the GAMBIT framework for BSM inference, and a standalone tool for efficiently applying collider constraints to theories of new physics.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We report the detection of an ultra-bright fast radio burst (FRB) from a modest, 3.4-day pilot survey with the Australian Square Kilometre Array Pathfinder. The survey was conducted in a wide-field fly's-eye configuration using the phased-array-feed technology deployed on the array to instantaneously observe an effective area of $160$ deg$^2$, and achieve an exposure totaling $13200$ deg$^2$ hr. We constrain the position of FRB 170107 to a region $8'\\times8'$ in size (90% containment) and its fluence to be $58\\pm6$ Jy ms. The spectrum of the burst shows a sharp cutoff above $1400$ MHz, which could be either due to scintillation or an intrinsic feature of the burst. This confirms the existence of an ultra-bright ($>20$ Jy ms) population of FRBs.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We describe the Sloan Digital Sky Survey IV (SDSS-IV), a project encompassing three major spectroscopic programs. The Apache Point Observatory Galactic Evolution Experiment 2 (APOGEE-2) is observing hundreds of thousands of Milky Way stars at high resolution and high signal-to-noise ratio in the near-infrared. The Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey is obtaining spatially-resolved spectroscopy for thousands of nearby galaxies (median redshift of z = 0.03). The extended Baryon Oscillation Spectroscopic Survey (eBOSS) is mapping the galaxy, quasar, and neutral gas distributions between redshifts z = 0.6 and 3.5 to constrain cosmology using baryon acoustic oscillations, redshift space distortions, and the shape of the power spectrum. Within eBOSS, we are conducting two major subprograms: the SPectroscopic IDentification of eROSITA Sources (SPIDERS), investigating X-ray AGN and galaxies in X-ray clusters, and the Time Domain Spectroscopic Survey (TDSS), obtaining spectra of variable sources. All programs use the 2.5-meter Sloan Foundation Telescope at Apache Point Observatory; observations there began in Summer 2014. APOGEE-2 also operates a second near-infrared spectrograph at the 2.5-meter du Pont Telescope at Las Campanas Observatory, with observations beginning in early 2017. Observations at both facilities are scheduled to continue through 2020. In keeping with previous SDSS policy, SDSS-IV provides regularly scheduled public data releases; the first one, Data Release 13, was made available in July 2016.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The s manifold energy levels for phosphorus donors in silicon are important input parameters for the design and modelling of electronic devices on the nanoscale. In this paper we calculate these energy levels from first principles using density functional theory. The wavefunction of the donor electron's ground state is found to have a form that is similar to an atomic s orbital, with an effective Bohr radius of 1.8 nm. The corresponding binding energy of this state is found to be 41 meV, which is in good agreement with the currently accepted value of 45.59 meV. We also calculate the energies of the excited 1s(T) and 1s(E) states, finding them to be 32 and 31 meV respectively. These results constitute the first ab initio confirmation of the s manifold energy levels for phosphorus donors in silicon.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The rankable and compressible sets have been studied for more than a quarter of a century, ever since Allender [1] and Goldberg and Sipser [6] introduced the formal study of polynomial-time ranking. Yet even after all that time, whether the rankable and compressible sets are closed under the most important boolean and other operations remains essentially unexplored. The present paper studies these questions for both polynomial-time and recursion-theoretic compression and ranking, and for almost every case arrives at a Closed, a Not-Closed, or a Closed-Iff-Well-Known-Complexity-Classes-Collapse result for the given operation. Even though compression and ranking classes are capturing something quite natural about the structure of sets, it turns out that they are quite fragile with respect to closure properties, and many fail to possess even the most basic of closure properties. For example, we show that with respect to the join (aka disjoint union) operation: the P-rankable sets are not closed, whether the semistrongly P-rankable sets are closed is closely linked to whether P = UP $\\cap$ coUP, and the strongly P-rankable sets are closed.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The X-ray Integral Field Unit (X-IFU) on board the Advanced Telescope for High-ENergy Astrophysics (Athena) will provide spatially resolved high-resolution X-ray spectroscopy from 0.2 to 12 keV, with 5 arc second pixels over a field of view of 5 arc minute equivalent diameter and a spectral resolution of 2.5 eV up to 7 keV. In this paper, we first review the core scientific objectives of Athena, driving the main performance parameters of the X-IFU, namely the spectral resolution, the field of view, the effective area, the count rate capabilities, the instrumental background. We also illustrate the breakthrough potential of the X-IFU for some observatory science goals. Then we briefly describe the X-IFU design as defined at the time of the mission consolidation review concluded in May 2016, and report on its predicted performance. Finally, we discuss some options to improve the instrument performance while not increasing its complexity and resource demands (e.g. count rate capability, spectral resolution).\n  The X-IFU will be provided by an international consortium led by France, The Netherlands and Italy, with further ESA member state contributions from Belgium, Finland, Germany, Poland, Spain, Switzerland and two international partners from the United States and Japan.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We describe the performance of the Boolardy Engineering Test Array (BETA), the prototype for the Australian Square Kilometre Array Pathfinder telescope ASKAP. BETA is the first aperture synthesis radio telescope to use phased array feed technology, giving it the ability to electronically form up to nine dual-polarization beams. We report the methods developed for forming and measuring the beams, and the adaptations that have been made to the traditional calibration and imaging procedures in order to allow BETA to function as a multi-beam aperture synthesis telescope. We describe the commissioning of the instrument and present details of BETA's performance: sensitivity, beam characteristics, polarimetric properties and image quality. We summarise the astronomical science that it has produced and draw lessons from operating BETA that will be relevant to the commissioning and operation of the final ASKAP telescope.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We analyse a 154 MHz image made from a 12 h observation with the Murchison Widefield Array (MWA) to determine the noise contribution and behaviour of the source counts down to 30 mJy. The MWA image has a bandwidth of 30.72 MHz, a field-of-view within the half-power contour of the primary beam of 570 deg^2, a resolution of 2.3 arcmin and contains 13,458 sources above 5 sigma. The rms noise in the centre of the image is 4-5 mJy/beam. The MWA counts are in excellent agreement with counts from other instruments and are the most precise ever derived in the flux density range 30-200 mJy due to the sky area covered. Using the deepest available source count data, we find that the MWA image is affected by sidelobe confusion noise at the ~3.5 mJy/beam level, due to incompletely-peeled and out-of-image sources, and classical confusion becomes apparent at ~1.7 mJy/beam. This work highlights that (i) further improvements in ionospheric calibration and deconvolution imaging techniques would be required to probe to the classical confusion limit and (ii) the shape of low-frequency source counts, including any flattening towards lower flux densities, must be determined from deeper ~150 MHz surveys as it cannot be directly inferred from higher frequency data.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "New data are reported from a second run of the 2-liter PICO-2L C$_3$F$_8$ bubble chamber with a total exposure of 129$\\,$kg-days at a thermodynamic threshold energy of 3.3$\\,$keV. These data show that measures taken to control particulate contamination in the superheated fluid resulted in the absence of the anomalous background events observed in the first run of this bubble chamber. One single nuclear-recoil event was observed in the data, consistent both with the predicted background rate from neutrons and with the observed rate of unambiguous multiple-bubble neutron scattering events. The chamber exhibits the same excellent electron-recoil and alpha decay rejection as was previously reported. These data provide the most stringent direct detection constraints on weakly interacting massive particle (WIMP)-proton spin-dependent scattering to date for WIMP masses $<$ 50$\\,$GeV/c$^2$.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "In Type Ia Supernovae (\\sneia), the relative abundances of chemical elements are affected by the neutron excess in the composition of the progenitor white dwarf. Since these products leave signatures in the spectra near maximum light, spectral features may be used to constrain the composition of the progenitor. We calculate the nucleosynthetic yields for three \\snia simulations, assuming single degenerate, Chandrasekhar mass progenitors, for a wide range of progenitor metallicities, and calculate synthetic light curves and spectra to explore correlations between progenitor metallicity and the strength of spectral features. We use two 2D simulations of the deflagration-detonation-transition scenario with different $^{56}$Ni yields and the W7 simulation to control for differences between explosion models and total yields. While the overall yields of intermediate mass elements (16 $<$ A $\\leq$ 40) differ between the three cases, trends in the yields are similar. With increasing metallicity, $^{28}$Si yields remain nearly constant, $^{40}$Ca yields decline, and Ti and $^{54}$Fe yields increase. In the synthetic spectra, we identify two features at 30 days post explosion that appear to deepen with progenitor metallicity: a Ti feature around 4200\\,\u00c5 and a Fe feature around 5200\\,\u00c5\\@. In all three simulations, their pseudo equivalent widths show a systematic trend with progenitor metallicity. This suggests that these two features may allow differentiation among progenitor metallicities of observed \\sneia and potentially help reduce the intrinsic Hubble scatter.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Despite the importance of Si:P delta-doped wires for modern nanoelectronics, there are currently no computational models of electron transport in these devices. In this paper we present a nonequilibrium Green's function model for electronic transport in a delta-doped wire, which is described by a tight-binding Hamiltonian matrix within a single-band effective-mass approximation. We use this transport model to calculate the current-voltage characteristics of a number of delta-doped wires, achieving good agreement with experiment. To motivate our transport model we have performed density-functional calculations for a variety of delta-doped wires, each with different donor configurations. These calculations also allow us to accurately define the electronic extent of a delta-doped wire, which we find to be at least 4.6 nm.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "A common method of adjusting the metal-insulator transition temperature of M$_{1}$ VO$_{2}$ is via disruption of the Peierls pairing by doping, or inputting stress or strain. However, since adding even small amounts of dopants will change the band structure, it is unclear how doped VO$_{2}$ retains its insulating character observed in experiments. While strong correlations may be responsible for maintaining a gap, theoretical evidence for this has been very difficult to obtain due to the complexity of the many-body problem involved. In this work we use GW calculations modified to include strong local $\\textbf{k}$-space interactions to investigate the changes in band structure from tungsten doping. We find that the combination of carrier doping and the experimentally observed structural defects introduced by inclusion of tungsten are consistent with a change from band-like to Mott-insulating behavior.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "GLEAM, the GaLactic and Extragalactic All-sky MWA survey, is a survey of the entire radio sky south of declination +25 deg at frequencies between 72 and 231 MHz, made with the Murchison Widefield Array (MWA) using a drift scan method that makes efficient use of the MWA's very large field-of-view. We present the observation details, imaging strategies and theoretical sensitivity for GLEAM. The survey ran for two years, the first year using 40 kHz frequency resolution and 0.5 s time resolution; the second year using 10 kHz frequency resolution and 2 s time resolution. The resulting image resolution and sensitivity depends on observing frequency, sky pointing and image weighting scheme. At 154 MHz the image resolution is approximately 2.5 x 2.2/cos(DEC+26.7) arcmin with sensitivity to structures up to ~10 deg in angular size. We provide tables to calculate the expected thermal noise for GLEAM mosaics depending on pointing and frequency and discuss limitations to achieving theoretical noise in Stokes I images. We discuss challenges, and their solutions, that arise for GLEAM including ionospheric effects on source positions and linearly polarised emission, and the instrumental polarisation effects inherent to the MWA's primary beam.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "It is demonstrated that the signatures of the Hubbard Model in the strongly interacting regime can be simulated by modifying the screening in the limit of zero wavevector in Projector-Augmented Wave GW calculations for systems without significant nesting. This modification, when applied to the Mott insulator CuO, results in the opening of the Mott gap by the splitting of states at the Fermi level into upper and lower Hubbard bands, and exhibits a giant transfer of spectral weight upon electron doping. The method is also employed to clearly illustrate that the M$_{1}$ and M$_{2}$ forms of vanadium dioxide are fundamentally different types of insulator. Standard GW calculations are sufficient to open a gap in M$_{1}$ VO$_{2}$, which arise from the Peierls pairings filling the valence band, creating homopolar bonds. The valence band wavefunctions are stabilized with respect to the conduction band, reducing polarizability and pushing the conduction band eigenvalues to higher energy. The M$_{2}$ structure however opens a gap from strong on-site interactions; it is a Mott insulator.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "New data are reported from the operation of a 2-liter C$_3$F$_8$ bubble chamber in the 2100 meter deep SNOLAB underground laboratory, with a total exposure of 211.5 kg-days at four different recoil energy thresholds ranging from 3.2 keV to 8.1 keV. These data show that C3F8 provides excellent electron recoil and alpha rejection capabilities at very low thresholds, including the first observation of a dependence of acoustic signal on alpha energy. Twelve single nuclear recoil event candidates were observed during the run. The candidate events exhibit timing characteristics that are not consistent with the hypothesis of a uniform time distribution, and no evidence for a dark matter signal is claimed. These data provide the most sensitive direct detection constraints on WIMP-proton spin-dependent scattering to date, with significant sensitivity at low WIMP masses for spin-independent WIMP-nucleon scattering.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Whilst young massive clusters (YMCs; $M$ $\\gtrsim$ 10$^{4}$ M$_{\\odot}$, age $\\lesssim$ 100 Myr) have been identified in significant numbers, their progenitor gas clouds have eluded detection. Recently, four extreme molecular clouds residing within 200 pc of the Galactic centre have been identified as having the properties thought necessary to form YMCs. Here we utilise far-IR continuum data from the Herschel Infrared Galactic Plane Survey (HiGAL) and millimetre spectral line data from the Millimetre Astronomy Legacy Team 90 GHz Survey (MALT90) to determine their global physical and kinematic structure. We derive their masses, dust temperatures and radii and use virial analysis to conclude that they are all likely gravitationally bound -- confirming that they are likely YMC progenitors. We then compare the density profiles of these clouds to those of the gas and stellar components of the Sagittarius B2 Main and North proto-clusters and the stellar distribution of the Arches YMC. We find that even in these clouds -- the most massive and dense quiescent clouds in the Galaxy -- the gas is not compact enough to form an Arches-like ($M$ = 2x10$^{4}$ M$_{\\odot}$, R$_{eff}$ = 0.4 pc) stellar distribution. Further dynamical processes would be required to condense the resultant population, indicating that the mass becomes more centrally concentrated as the (proto)-cluster evolves. These results suggest that YMC formation may proceed hierarchically rather than through monolithic collapse.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The effects of the spin and lattice degrees of freedom on the electronic structure of M1 vanadium dioxide are explored using a quasiparticle description. Contraction of the inter-vanadium spacing of the Peierls pairings stabilizes bonding electrons, reducing polarizability and thus widening the band gap. Increases of this inter-vanadium spacing of as little as 1 \\% reduce this stabilization, resulting in a crossover to ferromagnetic behaviour accomplished by half of the valence electrons inhabiting the leading edge of the conduction band in localized atomic-like orbitals, as the antiferromagnetic order becomes unstable with respect to rearrangement according to Hund's first rule. The data indicates that the magnetic structure of M1 vanadium dioxide may be finely balanced; the antiferromagnetic order is a consequence of the overlapping nuclear potential of the Peierls pairs, and input which disrupts this will have a significant effect on magnetic properties.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The Open Cluster Chemical Analysis and Mapping (OCCAM) Survey aims to produce a comprehensive, uniform, infrared-based dataset for hundreds of open clusters, and constrain key Galactic dynamical and chemical parameters from this sample. This first contribution from the OCCAM survey presents analysis of 141 members stars in 28 open clusters with high-resolution metallicities derived from a large uniform sample collected as part of the SDSS-III/Apache Point Observatory Galactic Evolution Experiment (APOGEE). This sample includes the first high-resolution metallicity measurements for 22 open clusters. With this largest ever uniformly observed sample of open cluster stars we investigate the Galactic disk gradients of both [M/H] and [alpha/M]. We find basically no gradient across this range in [alpha/M], but [M/H] does show a gradient for R_{GC} < 10 kpc and a significant flattening beyond R_{GC} = 10 kpc. In particular, whereas fitting a single linear trend yields an [M/H] gradient of -0.09 +/- 0.03$ dex/kpc --- similar to previously measure gradients inside 13 kpc --- by independently fitting inside and outside 10 kpc separately we find a significantly steeper gradient near the Sun (7.9 <= R_{GC} <= 10) than previously found (-0.20 +/- 0.08 dex/kpc) and a nearly flat trend beyond 10 kpc (-0.02 +/- 0.09 dex/kpc).\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The Sloan Digital Sky Survey (SDSS) has been in operation since 2000 April. This paper presents the tenth public data release (DR10) from its current incarnation, SDSS-III. This data release includes the first spectroscopic data from the Apache Point Observatory Galaxy Evolution Experiment (APOGEE), along with spectroscopic data from the Baryon Oscillation Spectroscopic Survey (BOSS) taken through 2012 July. The APOGEE instrument is a near-infrared R~22,500 300-fiber spectrograph covering 1.514--1.696 microns. The APOGEE survey is studying the chemical abundances and radial velocities of roughly 100,000 red giant star candidates in the bulge, bar, disk, and halo of the Milky Way. DR10 includes 178,397 spectra of 57,454 stars, each typically observed three or more times, from APOGEE. Derived quantities from these spectra (radial velocities, effective temperatures, surface gravities, and metallicities) are also included.DR10 also roughly doubles the number of BOSS spectra over those included in the ninth data release. DR10 includes a total of 1,507,954 BOSS spectra, comprising 927,844 galaxy spectra; 182,009 quasar spectra; and 159,327 stellar spectra, selected over 6373.2 square degrees.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "In 1950 Maharam asked whether every disintegration of a $\u03c3$-finite measure into $\u03c3$-finite measures is necessarily uniformly $\u03c3$-finite. Over the years under special conditions on the disintegration, the answer was shown to be yes. However, we show here that the answer may depend on the axioms of set theory in the following sense. If CH, the continuum hypothesis holds, then the answer is no. One proof of this leads to some interesting problems in infinitary combinatorics. If G\u00f6del's axiom of constructibility $\\mathbf{V}=\\mathbf{L}$ holds, then not only is the answer no, but, of equal interest is the construction of $\\mathbf\u03a0^1_1$ sets with very special properties.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We describe an algorithm for distinguishing hyperbolic components in the parameter space of quadratic rational maps with a periodic critical point. We then illustrate computer images of the hyperbolic components of the parameter spaces V1 - V4, which were produced using our algorithm. We also resolve the singularities of the projective closure of V5 by blowups, giving an alternative proof that as an algebraic curve, the geometric genus of V5 is 1. This explains why we are unable to produce an image for V5.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "A large sub-mm survey with Herschel will enable many exciting science opportunities, especially in an era of wide-field optical and radio surveys and high resolution cosmic microwave background experiments. The Herschel-SPIRE Legacy Survey (HSLS), will lead to imaging data over 4000 sq. degrees at 250, 350, and 500 micron. Major Goals of HSLS are: (a) produce a catalog of 2.5 to 3 million galaxies down to 26, 27 and 33 mJy (50% completeness; 5 sigma confusion noise) at 250, 350 and 500 micron, respectively, in the southern hemisphere (3000 sq. degrees) and in an equatorial strip (1000 sq. degrees), areas which have extensive multi-wavelength coverage and are easily accessible from ALMA. Two thirds of the of the sources are expected to be at z > 1, one third at z > 2 and about a 1000 at z > 5. (b) Remove point source confusion in secondary anisotropy studies with Planck and ground-based CMB data. (c) Find at least 1200 strongly lensed bright sub-mm sources leading to a 2% test of general relativity. (d) Identify 200 proto-cluster regions at z of 2 and perform an unbiased study of the environmental dependence of star formation. (e) Perform an unbiased survey for star formation and dust at high Galactic latitude and make a census of debris disks and dust around AGB stars and white dwarfs.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "SWAS and Odin provided stringent upper limits on the gas phase water abundance of dark clouds (x(H2O) < 7x10^-9). We investigate the chemistry of water vapor in starless cores beyond the previous upper limits using the highly improved angular resolution and sensitivity of Herschel and measure the abundance of water vapor during evolutionary stages just preceding star formation. High spectral resolution observations of the fundamental ortho water (o-H2O) transition (557 GHz) were carried out with Herschel HIFI toward two starless cores: B68, a Bok globule, and L1544, a prestellar core embedded in the Taurus molecular cloud complex. The rms in the brightness temperature measured for the B68 and L1544 spectra is 2.0 and 2.2 mK, respectively, in a velocity bin of 0.59 km s^-1. The continuum level is 3.5+/-0.2 mK in B68 and 11.4+/-0.4 mK in L1544. No significant feature is detected in B68 and the 3 sigma upper limit is consistent with a column density of o-H2O N(o-H2O) < 2.5x10^13 cm^-2, or a fractional abundance x(o-H2O) < 1.3x10^-9, more than an order of magnitude lower than the SWAS upper limit on this source. The L1544 spectrum shows an absorption feature at a 5 sigma level from which we obtain the first value of the o-H2O column density ever measured in dark clouds: N(o-H2O) = (8+/-4)x10^12 cm^-2. The corresponding fractional abundance is x(o-H2O) ~ 5x10^-9 at radii > 7000 AU and ~2x10^-10 toward the center. The radiative transfer analysis shows that this is consistent with a x(o-H2O) profile peaking at ~10^-8, 0.1 pc away from the core center, where both freeze-out and photodissociation are negligible. Herschel has provided the first measurement of water vapor in dark regions. Prestellar cores such as L1544 (with their high central densities, strong continuum, and large envelopes) are very promising tools to finally shed light on the solid/vapor balance of water in molecular clouds.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Deviations from the Bunch-Davies vacuum during an inflationary period can leave a testable imprint on the higher-order correlations of the CMB and large scale structures in the Universe. The effect is particularly pronounced if the statistical non-Gaussianity is inherently large, such as in models of inflation with a small speed of sound, e.g. DBI. First reviewing the motivations for a modified vacuum, we calculate the non-Gaussianity for a general action with a small speed of sound. The shape of its bispectrum is found to most resemble the 'orthogonal' or 'local' templates depending on the phase of the Bogolyubov parameter. In particular, for DBI models of inflation the bispectrum can have a profound 'local' template feature, in contrast to previous results. Determining the projection into the observational templates allows us to derive constraints on the absolute value of the Bogolyubov parameter. In the small sound speed limit, the derived constraints are generally stronger than the existing constraint derived from the power spectrum. The bound on the absolute value of the Bogolyubov parameter ranges from the 10^-6 to the 10^-3 level for H/\u039b_c = 10^-3, depending on the specific details of the model, the sound speed and the phase of the Bogolyubov parameter.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Context: Recent calculations of pulsation modes in rapidly rotating polytropic models and models based on the Self-Consistent Field method (MacGregor et al. 2007) have shown that the frequency spectrum of low degree pulsation modes can be described by an empirical formula similar to Tassoul's asymptotic formula (Tassoul 1980), provided that the underlying rotation profile is not too differential (Lignieres & Georgeot 2008, Reese et al. 2009).\n  Aims: Given the simplicity of this asymptotic formula, we investigate whether it can provide a means by which to identify pulsation modes in rapidly rotating stars.\n  Methods: We develop a new mode identification scheme which consists in scanning a multidimensional parameter space for the formula coefficients which yield the best-fitting asymptotic spectra. This mode identification scheme is then tested on artificial spectra based on the asymptotic formula, on random frequencies and on spectra based on full numerical eigenmode calculations for which the mode identification is known beforehand. We also investigate the effects of adding random frequencies to mimic the effects of chaotic modes which are also expected to show up in such stars (Lignieres & Georgeot 2008).\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Context: New observational means such as the space missions CoRoT and Kepler and ground-based networks are and will be collecting stellar pulsation data with unprecedented accuracy. A significant fraction of the stars in which pulsations are observed are rotating rapidly.\n  Aims: Our aim is to characterise pulsation modes in rapidly rotating stellar models so as to be able to interpret asteroseismic data from such stars.\n  Methods: The pulsation code developed in Ligni\u00e8res et al. (2006) and Reese et al. (2006) is applied to stellar models based on the self-consistent field (SCF) method (Jackson et al. 2004, 2005, MacGregor et al. 2007).\n  Results: Pulsation modes in SCF models follow a similar behaviour to those in uniformly rotating polytropic models, provided that the rotation profile is not too differential. Pulsation modes fall into different categories, the three main ones being island, chaotic, and whispering gallery modes, which are rotating counterparts to modes with low, medium, and high l-|m| values, respectively. The frequencies of the island modes follow an asymptotic pattern quite similar to what was found for polytropic models. Extending this asymptotic formula to higher azimuthal orders reveals more subtle behaviour as a function of m and provides a first estimate of the average advection of pulsation modes by rotation. Further calculations based on a variational principle confirm this estimate and provide rotation kernels that could be used in inversion methods. When the rotation profile becomes highly differential, it becomes more and more difficult to find island and whispering gallery modes at low azimuthal orders. At high azimuthal orders, whispering gallery modes, and in some cases island modes, reappear.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "How did the universe evolve? The fine angular scale (l>1000) temperature and polarization anisotropies in the CMB are a Rosetta stone for understanding the evolution of the universe. Through detailed measurements one may address everything from the physics of the birth of the universe to the history of star formation and the process by which galaxies formed. One may in addition track the evolution of the dark energy and discover the net neutrino mass.\n  We are at the dawn of a new era in which hundreds of square degrees of sky can be mapped with arcminute resolution and sensitivities measured in microKelvin. Acquiring these data requires the use of special purpose telescopes such as the Atacama Cosmology Telescope (ACT), located in Chile, and the South Pole Telescope (SPT). These new telescopes are outfitted with a new generation of custom mm-wave kilo-pixel arrays. Additional instruments are in the planning stages.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "A new and powerful probe of the origin and evolution of structures in the Universe has emerged and been actively developed over the last decade. In the coming decade, non-Gaussianity, i.e., the study of non-Gaussian contributions to the correlations of cosmological fluctuations, will become an important probe of both the early and the late Universe. Specifically, it will play a leading role in furthering our understanding of two fundamental aspects of cosmology and astrophysics: (i) the physics of the very early universe that created the primordial seeds for large-scale structures, and (ii) the subsequent growth of structures via gravitational instability and gas physics at later times. To date, observations of fluctuations in the Cosmic Microwave Background (CMB) and the Large-Scale Structure of the Universe (LSS) have focused largely on the Gaussian contribution as measured by the two-point correlations (or the power spectrum) of density fluctuations. However, an even greater amount of information is contained in non-Gaussianity and a large discovery space therefore still remains to be explored. Many observational probes can be used to measure non-Gaussianity, including CMB, LSS, gravitational lensing, Lyman-alpha forest, 21-cm fluctuations, and the abundance of rare objects such as clusters of galaxies and high-redshift galaxies. Not only does the study of non-Gaussianity maximize the science return from a plethora of present and future cosmological experiments and observations, but it also carries great potential for important discoveries in the coming decade.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We summarize the utility of precise cosmic microwave background (CMB) polarization measurements as probes of the physics of inflation. We focus on the prospects for using CMB measurements to differentiate various inflationary mechanisms. In particular, a detection of primordial B-mode polarization would demonstrate that inflation occurred at a very high energy scale, and that the inflaton traversed a super-Planckian distance in field space. We explain how such a detection or constraint would illuminate aspects of physics at the Planck scale. Moreover, CMB measurements can constrain the scale-dependence and non-Gaussianity of the primordial fluctuations and limit the possibility of a significant isocurvature contribution. Each such limit provides crucial information on the underlying inflationary dynamics. Finally, we quantify these considerations by presenting forecasts for the sensitivities of a future satellite experiment to the inflationary parameters.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Quantum mechanical metric fluctuations during an early inflationary phase of the universe leave a characteristic imprint in the polarization of the cosmic microwave background (CMB). The amplitude of this signal depends on the energy scale at which inflation occurred. Detailed observations by a dedicated satellite mission (CMBPol) therefore provide information about energy scales as high as $10^{15}$ GeV, twelve orders of magnitude greater than the highest energies accessible to particle accelerators, and probe the earliest moments in the history of the universe. This summary provides an overview of a set of studies exploring the scientific payoff of CMBPol in diverse areas of modern cosmology, such as the physics of inflation, gravitational lensing and cosmic reionization, as well as foreground science and removal .\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "Knowledge flow analysis offers a simple and flexible way to find flaws in security protocols. A protocol is described by a collection of rules constraining the propagation of knowledge amongst principals. Because this characterization corresponds closely to informal descriptions of protocols, it allows a succinct and natural formalization; because it abstracts away message ordering, and handles communications between principals and applications of cryptographic primitives uniformly, it is readily represented in a standard logic. A generic framework in the Alloy modelling language is presented, and instantiated for two standard protocols, and a new key management scheme.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We classify invariant curves for birational surface maps that are expanding on cohomology. When the expansion is exponential, the arithmetic genus of an invariant curve is at most one. This implies severe constraints on both the type and number of irreducible components of the curve. In the case of an invariant curve with genus equal to one, we show that there is an associated invariant meromorphic two form.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "This paper is the first of a series describing the results of the Australia Telescope Hubble Deep Field South (ATHDFS) radio survey. The survey was conducted at four wavelengths - 20, 11, 6, and 3 cm, over a 4-year period, and achieves an rms sensitivity of about 10 microJy at each wavelength. We describe the observations and data reduction processes, and present data on radio sources close to the centre of the HDF-S. We discuss in detail the properties of a subset of these sources. The sources include both starburst galaxies and galaxies powered by an active galactic nucleus, and range in redshift from 0.1 to 2.2. Some of them are characterised by unusually high radio-to-optical luminosities, presumably caused by dust extinction.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We study string dynamics in the early universe. Our motivation is the proposal of Brandenberger and Vafa, that string winding modes may play a key role in decompactifying three spatial dimensions. We model the universe as a homogeneous but anisotropic 9-torus filled with a gas of excited strings. We adopt initial conditions which fix the dilaton and the volume of the torus, but otherwise assume all states are equally likely. We study the evolution of the system both analytically and numerically to determine the late-time behavior. We find that, although dynamical evolution can indeed lead to three large spatial dimensions, such an outcome is not statistically favored.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We present the result of a decomposition of the 2dFGRS galaxy overdensity field into an orthonormal basis of spherical harmonics and spherical Bessel functions. Galaxies are expected to directly follow the bulk motion of the density field on large scales, so the absolute amplitude of the observed large-scale redshift-space distortions caused by this motion is expected to be independent of galaxy properties. By splitting the overdensity field into radial and angular components, we linearly model the observed distortion and obtain the cosmological constraint Omega_m^{0.6} sigma_8=0.46+/-0.06. The amplitude of the linear redshift-space distortions relative to the galaxy overdensity field is dependent on galaxy properties and, for L_* galaxies at redshift z=0, we measure beta(L_*,0)=0.58+/-0.08, and the amplitude of the overdensity fluctuations b(L_*,0) sigma_8=0.79+/-0.03, marginalising over the power spectrum shape parameters. Assuming a fixed power spectrum shape consistent with the full Fourier analysis produces very similar parameter constraints.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We consider the thermodynamic and cosmological properties of brane gases in the early universe. Working in the low energy limit of M-theory we assume the universe is a homogeneous but anisotropic 10-torus containing wrapped 2-branes and a supergravity gas. We describe the thermodynamics of this system and estimate a Hagedorn temperature associated with excitations on the branes. We investigate the cross-section for production of branes from the thermal bath and derive Boltzmann equations governing the number of wrapped branes. A brane gas may lead to decompactification of three spatial dimensions. To investigate this possibility we adopt initial conditions in which we fix the volume of the torus but otherwise assume all states are equally likely. We solve the Einstein-Boltzmann equations numerically, to determine the number of dimensions with no wrapped branes at late times; these unwrapped dimensions are expected to decompactify. Finally we consider holographic bounds on the initial volume, and find that for allowed initial volumes all branes typically annihilate before freeze-out can occur.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We investigate the late-time behavior of a universe containing a supergravity gas and wrapped 2-branes in the context of M-theory compactified on T^10. The supergravity gas tends to drive uniform expansion, while the branes impede the expansion of the directions about which they are wrapped. Assuming spatial homogeneity, we study the dynamics both numerically and analytically. At late times the radii obey power laws which are determined by the brane wrapping numbers, leading to interesting hierarchies of scale between the wrapped and unwrapped dimensions. The biggest hierarchy that could evolve from an initial thermal fluctuation produces three large unwrapped dimensions. We also study configurations corresponding to string winding, in which the M2-branes are all wrapped around the (small) 11th dimension, and show that this recovers the scenario discussed by Brandenberger and Vafa.\n        \u25b3 Less", "author": "Daniel Jackson"}, {"abstract": "We consider the following problem: Does there exist a probability distribution over subsets of a finite partially ordered set (poset), such that a set of constraints involving marginal probabilities of the poset's elements and maximal chains is satisfied? In this article, we present a combinatorial algorithm to positively resolve this question. We show that this result plays a crucial role in the equilibrium analysis of a generic security game on a capacitated flow network. The game involves a routing entity that sends its flow through the network while facing path transportation costs, and an interdictor who simultaneously interdicts one or more edges while facing edge interdiction costs. The first (resp. second) player seeks to maximize the value of effective (resp. interdicted) flow net the total transportation (resp. interdiction) cost. Using our existence result on posets and strict complementary slackness in linear programming, we show that the equilibrium properties of this game can be described using primal and dual solutions of a minimum cost circulation problem. Our analysis provides a new characterization of the critical network components.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "In this brief paper we find computable exponential convergence rates for a large class of stochastically ordered Markov processes. We extend the result of Lund, Meyn, and Tweedie (1996), who found exponential convergence rates for stochastically ordered Markov processes starting from a fixed initial state, by allowing for a random initial condition that is also stochastically ordered. Our bounds are formulated in terms of moment-generating functions of hitting times. To illustrate our result, we find an explicit exponential convergence rate for an M/M/1 queue beginning in equilibrium and then experiencing a change in its arrival or departure rates, a setting which has not been studied to our knowledge.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "For online resource allocation problems, we propose a new demand arrival model where the sequence of arrivals contains both an adversarial component and a stochastic one. Our model requires no demand forecasting; however, due to the presence of the stochastic component, we can partially predict future demand as the sequence of arrivals unfolds. Under the proposed model, we study the problem of the online allocation of a single resource to two types of customers, and design online algorithms that outperform existing ones. Our algorithms are adjustable to the relative size of the stochastic component, and our analysis reveals that as the portion of the stochastic component grows, the loss due to making online decisions decreases. This highlights the value of (even partial) predictability in online resource allocation. We impose no conditions on how the resource capacity scales with the maximum number of customers. However, we show that using an adaptive algorithm---which makes online decisions based on observed data---is particularly beneficial when capacity scales linearly with the number of customers. Our work serves as a first step in bridging the long-standing gap between the two well-studied approaches to the design and analysis of online algorithms based on (1) adversarial models and (2) stochastic ones. Using novel algorithm design, we demonstrate that even if the arrival sequence contains an adversarial component, we can take advantage of the limited information that the data reveals to improve allocation decisions. We also study the classical secretary problem under our proposed arrival model, and we show that randomizing over multiple stopping rules may increase the probability of success.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We study the problem of matching agents who arrive at a marketplace over time and leave after d time periods. Agents can only be matched while they are present in the marketplace. Each pair of agents can yield a different match value, and the planner's goal is to maximize the total value over a finite time horizon. First we study the case in which vertices arrive in an adversarial order. We provide a randomized 0.25-competitive algorithm building on a result by Feldman et al. (2009) and Lehman et al. (2006). We extend the model to the case in which departure times are drawn independently from a distribution with non-decreasing hazard rate, for which we establish a 1/8-competitive algorithm.\n  When the arrival order is chosen uniformly at random, we show that a batching algorithm, which computes a maximum-weighted matching every (d+1) periods, is 0.279-competitive.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "Vehicle-to-Infrastructure (V2I) communications are increasingly supporting highway operations such as electronic toll collection, carpooling, and vehicle platooning. In this paper we study the incentives of strategic misbehavior by individual vehicles who can exploit the security vulnerabilities in V2I communications and negatively impact the highway operations. We consider a V2I-enabled highway segment facing two classes of vehicles (agent populations), each with an authorized access to one server (subset of lanes). Vehicles are strategic in that they can misreport their class (type) to the system operator and get an unauthorized access to the server dedicated to the other class. This misbehavior causes additional congestion externality on the compliant vehicles, and thus, needs to be deterred. We focus on an environment where the operator is able to inspect the vehicles for misbehavior. The inspection is costly and successful detection incurs a fine on the misbehaving vehicle. We formulate a signaling game to study the strategic interaction between the vehicle classes and the operator. Our equilibrium analysis provides conditions on the cost parameters that govern the vehicles' incentive to misbehave or not. We also determine the operator's equilibrium inspection strategy.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We study the problem of matching agents who arrive at a marketplace over time and leave after d time periods. Agents can only be matched while they are present in the marketplace. Each pair of agents can yield a different match value, and the planner's goal is to maximize the total value over a finite time horizon. We study matching algorithms that perform well over any sequence of arrivals when there is no a priori information about the match values or arrival times.\n  Our main contribution is a 1/4-competitive algorithm. The algorithm randomly selects a subset of agents who will wait until right before their departure to get matched, and maintains a maximum-weight matching with respect to the other agents. The primal-dual analysis of the algorithm hinges on a careful comparison between the initial dual value associated with an agent when it first arrives, and the final value after d time steps.\n  It is also shown that no algorithm is 1/2-competitive. We extend the model to the case in which departure times are drawn i.i.d from a distribution with non-decreasing hazard rate, and establish a 1/8-competitive algorithm in this setting. Finally we show on real-world data that a modified version of our algorithm performs well in practice.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "This paper presents a novel variational inference framework for deriving a family of Bayesian sparse Gaussian process regression (SGPR) models whose approximations are variationally optimal with respect to the full-rank GPR model enriched with various corresponding correlation structures of the observation noises.\n  Our variational Bayesian SGPR (VBSGPR) models jointly treat both the distributions of the inducing variables and hyperparameters as variational parameters, which enables the decomposability of the variational lower bound that in turn can be exploited for stochastic optimization.\n  Such a stochastic optimization involves iteratively following the stochastic gradient of the variational lower bound to improve its estimates of the optimal variational distributions of the inducing variables and hyperparameters (and hence the predictive distribution) of our VBSGPR models and is guaranteed to achieve asymptotic convergence to them.\n  We show that the stochastic gradient is an unbiased estimator of the exact gradient and can be computed in constant time per iteration, hence achieving scalability to big data.\n  We empirically evaluate the performance of our proposed framework on two real-world, massive datasets.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers, while respecting a given order of retrieval. However, the assumption of knowing the full retrieval order of containers is particularly unrealistic in real operations. This paper studies the stochastic CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model, called the batch model, is introduced, motivated, and compared with an existing model (the online model). The two main contributions are an optimal algorithm called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm called PBFS-Approximate with a bounded average error. Both algorithms, applicable in the batch and online models, are based on a new family of lower bounds for which we show some theoretical properties. Moreover, we introduce two new heuristics outperforming the best existing heuristics. Algorithms, bounds and heuristics are tested in an extensive computational section. Finally, based on strong computational evidence, we conjecture the optimality of the \"Leveling\" heuristic in a special \"no information\" case, where at any retrieval stage, any of the remaining containers is equally likely to be retrieved next.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We study a convex resource allocation problem in which lower and upper bounds are imposed on partial sums of allocations. This model is linked to a large range of applications, including production planning, speed optimization, stratified sampling, support vector machines, portfolio management, and telecommunications. We propose an efficient gradient-free divide-and-conquer algorithm, which uses monotonicity arguments to generate valid bounds from the recursive calls, and eliminate linking constraints based on the information from sub-problems. This algorithm does not need strict convexity or differentiability. It produces an $\u03b5$-approximate solution for the continuous problem in $\\mathcal{O}(n \\log m \\log \\frac{n B}\u03b5)$ time and an integer solution in $\\mathcal{O}(n \\log m \\log B)$ time, where $n$ is the number of decision variables, $m$ is the number of constraints, and $B$ is the resource bound. A complexity of $\\mathcal{O}(n \\log m)$ is also achieved for the linear and quadratic cases. These are the best complexities known to date for this important problem class. Our experimental analyses confirm the good performance of the method, which produces optimal solutions for problems with up to 1,000,000 variables in a few seconds. Promising applications to the support vector ordinal regression problem are also investigated.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We propose a general approach for supervised learning with structured output spaces, such as combinatorial and polyhedral sets, that is based on minimizing estimated conditional risk functions. Given a loss function defined over pairs of output labels, we first estimate the conditional risk function by solving a (possibly infinite) collection of regularized least squares problems. A prediction is made by solving an inference problem that minimizes the estimated conditional risk function over the output space. We show that this approach enables, in some cases, efficient training and inference without explicitly introducing a convex surrogate for the original loss function, even when it is discontinuous. Empirical evaluations on real-world and synthetic data sets demonstrate the effectiveness of our method in adapting to a variety of loss functions.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We study dynamic matching in an infinite-horizon stochastic market. While all agents are potentially compatible with each other, some are hard-to-match and others are easy-to-match. Agents prefer to be matched as soon as possible and matches are formed either bilaterally or indirectly through chains. We adopt an asymptotic approach and compute tight bounds on the limit of waiting time of agents under myopic policies that differ in matching technology and prioritization.\n  We find that the market composition is a key factor in the desired matching technology and prioritization level. When hard-to-match agents arrive less frequently than easy-to-match ones (i) bilateral matching is almost as efficient as chains (waiting times scale similarly under both, though chains always outperform bilateral matching by a constant factor), and (ii) assigning priorities to hard-to-match agents improves their waiting times. When hard-to-match agents arrive more frequently, chains are much more efficient than bilateral matching and prioritization has no impact.\n  We further conduct comparative statics on arrival rates. Somewhat surprisingly, we find that in a heterogeneous market and under bilateral matching, increasing arrival rate has a non-monotone effect on waiting times, due to the fact that, under some market compositions, there is an adverse effect of competition. Our comparative statics shed light on the impact of merging markets and attracting altruistic agents (that initiate chains) or easy-to-match agents.\n  This work uncovers fundamental differences between heterogeneous and homogeneous dynamic markets, and potentially helps policy makers to generate insights on the operations of matching markets such as kidney exchange programs.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "In order to find Nash-equilibria for two-player zero-sum games where each player plays combinatorial objects like spanning trees, matchings etc, we consider two online learning algorithms: the online mirror descent (OMD) algorithm and the multiplicative weights update (MWU) algorithm. The OMD algorithm requires the computation of a certain Bregman projection, that has closed form solutions for simple convex sets like the Euclidean ball or the simplex. However, for general polyhedra one often needs to exploit the general machinery of convex optimization. We give a novel primal-style algorithm for computing Bregman projections on the base polytopes of polymatroids. Next, in the case of the MWU algorithm, although it scales logarithmically in the number of pure strategies or experts $N$ in terms of regret, the algorithm takes time polynomial in $N$; this especially becomes a problem when learning combinatorial objects. We give a general recipe to simulate the multiplicative weights update algorithm in time polynomial in their natural dimension. This is useful whenever there exists a polynomial time generalized counting oracle (even if approximate) over these objects. Finally, using the combinatorial structure of symmetric Nash-equilibria (SNE) when both players play bases of matroids, we show that these can be found with a single projection or convex minimization (without using online learning).\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of epsilon-GPP with performance guarantee. We empirically demonstrate the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "Optimal regret bounds for Multi-Armed Bandit problems are now well documented. They can be classified into two categories based on the growth rate with respect to the time horizon $T$: (i) small, distribution-dependent, bounds of order of magnitude $\\ln(T)$ and (ii) robust, distribution-free, bounds of order of magnitude $\\sqrt{T}$. The Bandits with Knapsacks model, an extension to the framework allowing to model resource consumption, lacks this clear-cut distinction. While several algorithms have been shown to achieve asymptotically optimal distribution-free bounds on regret, there has been little progress toward the development of small distribution-dependent regret bounds. We partially bridge the gap by designing a general-purpose algorithm with distribution-dependent regret bounds that are logarithmic in the initial endowments of resources in several important cases that cover many practical applications, including dynamic pricing with limited supply, bid optimization in online advertisement auctions, and dynamic procurement.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers respecting a given order of retrieval. While the problem is known to be NP-hard, certain algorithms such as the A* search and heuristics perform reasonably well on many instances of the problem. In this paper, we first focus on the A* search algorithm, and analyze lower and upper bounds that are easy to compute and can be used to prune nodes. Our analysis sheds light on which bounds result in fast computation within a given approximation gap. We present extensive simulation results that improve upon our theoretical analysis, and further show that our method finds the optimum solution on most instances of medium-size bays. On \"hard\" instances, our method finds an approximate solution with a small gap and within a time frame that is fast for practical applications. We also study the average-case asymptotic behavior of the CRP where the number of columns grows. We calculate the expected number of relocations in the limit, and show that the optimum number of relocations converges to a simple and intuitive lower-bound. We further study the CRP with incomplete information by relaxing the assumption that the order of retrieval of all containers are initially known. This assumption is particularly unrealistic in ports without an appointment system. We assume that the retrieval order of a subset of containers is known initially and the retrieval order of the remaining containers is observed later at a given specific time. Before this time, we assume a probabilistic distribution on the retrieval order of unknown containers. We combine the A* algorithm with sampling technique to solve this two-stage stochastic optimization problem. We show that our algorithm is fast and the error due to sampling and pruning is reasonably small.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We introduce a new model and mathematical formulation for planning crane moves in the storage yard of container terminals. Our objective is to develop a tool that captures customer centric elements, especially service time, and helps operators to manage costly relocation moves. Our model incorporates several practical details and provides port operators with expanded capabilities including planning repositioning moves in off-peak hours, controlling wait times of each customer as well as total service time, optimizing the number of relocations and wait time jointly, and optimizing simultaneously the container stacking and retrieval process. We also study a class of flexible service policies which allow for out-of-order retrieval. We show that under such flexible policies, we can decrease the number of relocations and retrieval delays without creating inequities.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We consider and formulate a class of distributed multi-depot routing problems, where servers are to visit a set of requests, with the aim of minimizing the total distance travelled by all servers. These problems fall into two categories: distributed offline routing problems where all the requests that need to be visited are known from the start; distributed online routing problems where the requests come to be known incrementally. A critical and novel feature of our formulations is that communications are not allowed among the servers, hence posing an interesting and challenging question: what performance can be achieved in comparison to the best possible solution obtained from an omniscience planner with perfect communication capabilities? The worst-case (over all possible request-set instances) performance metrics are given by the approximation ratio (offline case) and the competitive ratio (online case).\n  Our first result indicates that the online and offline problems are effectively equivalent: for the same request-set instance, the approximation ratio and the competitive ratio differ by at most an additive factor of 2, irrespective of the release dates in the online case. Therefore, we can restrict our attention to the offline problem. For the offline problem, we show that the approximation ratio given by the Voronoi partition is m (the number of servers). For two classes of depot configurations, when the depots form a line and when the ratios between the distances of pairs of depots are upper bounded by a sublinear function f(m) (i.e., f(m) = o(m)), we give partition schemes with sublinear approximation ratios O(log m) and \u0398(f(m)) respectively. We also discuss several interesting open problems in our formulations: in particular, how our initial results (on the two deliberately chosen classes of depots) shape our conjecture on the open problems.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "In the convex optimization approach to online regret minimization, many methods have been developed to guarantee a $O(\\sqrt{T})$ bound on regret for subdifferentiable convex loss functions with bounded subgradients, by using a reduction to linear loss functions. This suggests that linear loss functions tend to be the hardest ones to learn against, regardless of the underlying decision spaces. We investigate this question in a systematic fashion looking at the interplay between the set of possible moves for both the decision maker and the adversarial environment. This allows us to highlight sharp distinctive behaviors about the learnability of piecewise linear loss functions. On the one hand, when the decision set of the decision maker is a polyhedron, we establish $\u03a9(\\sqrt{T})$ lower bounds on regret for a large class of piecewise linear loss functions with important applications in online linear optimization, repeated zero-sum Stackelberg games, online prediction with side information, and online two-stage optimization. On the other hand, we exhibit $o(\\sqrt{T})$ learning rates, achieved by the Follow-The-Leader algorithm, in online linear optimization when the boundary of the decision maker's decision set is curved and when $0$ does not lie in the convex hull of the environment's decision set. Hence, the curvature of the decision maker's decision set is a determining factor for the optimal learning rate. These results hold in a completely adversarial setting.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "The expressive power of a Gaussian process (GP) model comes at a cost of poor scalability in the data size. To improve its scalability, this paper presents a low-rank-cum-Markov approximation (LMA) of the GP model that is novel in leveraging the dual computational advantages stemming from complementing a low-rank approximate representation of the full-rank GP based on a support set of inputs with a Markov approximation of the resulting residual process; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint and is considerably more refined than that of existing sparse GP models utilizing low-rank representations due to its more relaxed conditional independence assumption (especially with larger data). As a result, our LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than such sparse GP models while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with PIC approximation and full-rank GP at the two extremes. An advantage of our LMA method is that it is amenable to parallelization on multiple machines/cores, thereby gaining greater scalability. Empirical evaluation on three real-world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel LMA methods are significantly more time-efficient and scalable than state-of-the-art sparse and full-rank GP regression methods while achieving comparable predictive performances.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We consider the problem of finding an optimal history-dependent routing strategy on a directed graph weighted by stochastic arc costs when the objective is to minimize the risk of spending more than a prescribed budget. To help mitigate the impact of the lack of information on the arc cost probability distributions, we introduce a robust counterpart where the distributions are only known through confidence intervals on some statistics such as the mean, the mean absolute deviation, and any quantile. Leveraging recent results in distributionally robust optimization, we develop a general-purpose algorithm to compute an approximate optimal strategy. To illustrate the benefits of the robust approach, we run numerical experiments with field data from the Singapore road network.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "The problem of modeling and predicting spatiotemporal traffic phenomena over an urban road network is important to many traffic applications such as detecting and forecasting congestion hotspots. This paper presents a decentralized data fusion and active sensing (D2FAS) algorithm for mobile sensors to actively explore the road network to gather and assimilate the most informative data for predicting the traffic phenomenon. We analyze the time and communication complexity of D2FAS and demonstrate that it can scale well with a large number of observations and sensors. We provide a theoretical guarantee on its predictive performance to be equivalent to that of a sophisticated centralized sparse approximation for the Gaussian process (GP) model: The computation of such a sparse approximate GP model can thus be parallelized and distributed among the mobile sensors (in a Google-like MapReduce paradigm), thereby achieving efficient and scalable prediction. We also theoretically guarantee its active sensing performance that improves under various practical environmental conditions. Empirical evaluation on real-world urban road network data shows that our D2FAS algorithm is significantly more time-efficient and scalable than state-oftheart centralized algorithms while achieving comparable predictive performance.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We propose an exact polynomial algorithm for a resource allocation problem with convex costs and constraints on partial sums of resource consumptions, in the presence of either continuous or integer variables. No assumption of strict convexity or differentiability is needed. The method solves a hierarchy of resource allocation subproblems, whose solutions are used to convert constraints on sums of resources into bounds for separate variables at higher levels. The resulting time complexity for the integer problem is $O(n \\log m \\log (B/n))$, and the complexity of obtaining an $\u03b5$-approximate solution for the continuous case is $O(n \\log m \\log (B/\u03b5))$, $n$ being the number of variables, $m$ the number of ascending constraints (such that $m < n$), $\u03b5$ a desired precision, and $B$ the total resource. This algorithm attains the best-known complexity when $m = n$, and improves it when $\\log m = o(\\log n)$. Extensive experimental analyses are conducted with four recent algorithms on various continuous problems issued from theory and practice. The proposed method achieves a higher performance than previous algorithms, addressing all problems with up to one million variables in less than one minute on a modern computer.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "The minmax regret problem for combinatorial optimization under uncertainty can be viewed as a zero-sum game played between an optimizing player and an adversary, where the optimizing player selects a solution and the adversary selects costs with the intention of maximizing the regret of the player. The existing minmax regret model considers only deterministic solutions/strategies, and minmax regret versions of most polynomial solvable problems are NP-hard. In this paper, we consider a randomized model where the optimizing player selects a probability distribution (corresponding to a mixed strategy) over solutions and the adversary selects costs with knowledge of the player's distribution, but not its realization. We show that under this randomized model, the minmax regret version of any polynomial solvable combinatorial problem becomes polynomial solvable. This holds true for both the interval and discrete scenario representations of uncertainty. Using the randomized model, we show new proofs of existing approximation algorithms for the deterministic model based on primal-dual approaches. Finally, we prove that minmax regret problems are NP-hard under general convex uncertainty.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "Many modern and growing cities are facing declines in public transport usage, with few efficient methods to explain why. In this article, we show that urban mobility patterns and transport mode choices can be derived from cellphone call detail records coupled with public transport data recorded from smart cards. Specifically, we present new data mining approaches to determine the spatial and temporal variability of public and private transportation usage and transport mode preferences across Singapore. Our results, which were validated by Singapore's quadriennial Household Interview Travel Survey (HITS), revealed that there are 3.5 (HITS: 3.5 million) million and 4.3 (HITS: 4.4 million) million inter-district passengers by public and private transport, respectively. Along with classifying which transportation connections are weak or underserved, the analysis shows that the mode share of public transport use increases from 38 percent in the morning to 44 percent around mid-day and 52 percent in the evening.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We study the average performance of online greedy matching algorithms on $G(n,n,p)$, the random bipartite graph with $n$ vertices on each side and edges occurring independently with probability $p=p(n)$. In the online model, vertices on one side of the graph are given up front while vertices on the other side arrive sequentially; when a vertex arrives its edges are revealed and it must be immediately matched or dropped. We begin by analyzing the \\textsc{oblivious} algorithm, which tries to match each arriving vertex to a random neighbor, even if the neighbor has already been matched. The algorithm is shown to have a performance ratio of at least $1-1/e$ for all monotonic functions $p(n)$, where the performance ratio is defined asymptotically as the ratio of the expected matching size given by the algorithm to the expected maximum matching size. Next we show that the conventional \\textsc{greedy} algorithm, which assigns each vertex to a random unmatched neighbor, has a performance ratio of at least 0.837 for all monotonic functions $p(n)$. Under the $G(n,n,p)$ model, the performance of \\textsc{greedy} is equivalent to the performance of the well known \\textsc{ranking} algorithm, so our results show that \\textsc{ranking} has a performance ratio of at least 0.837. We finally consider vertex-weighted bipartite matching. Our proofs are based on simple differential equations that describe the evolution of the matching process.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "Gaussian processes (GP) are Bayesian non-parametric models that are widely used for probabilistic regression. Unfortunately, it cannot scale well with large data nor perform real-time predictions due to its cubic time cost in the data size. This paper presents two parallel GP regression methods that exploit low-rank covariance matrix approximations for distributing the computational load among parallel machines to achieve time efficiency and scalability. We theoretically guarantee the predictive performances of our proposed parallel GPs to be equivalent to that of some centralized approximate GP regression methods: The computation of their centralized counterparts can be distributed among parallel machines, hence achieving greater time efficiency and scalability. We analytically compare the properties of our parallel GPs such as time, space, and communication complexity. Empirical evaluation on two real-world datasets in a cluster of 20 computing nodes shows that our parallel GPs are significantly more time-efficient and scalable than their centralized counterparts and exact/full GP while achieving predictive performances comparable to full GP.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "We present bounds of quadratic form for the logarithm of the Gaussian Q-function. We also show an analytical method for deriving log-quadratic approximations of the Q-function and give an approximation with absolute error less than $10^{-3}$.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "Rollout algorithms have demonstrated excellent performance on a variety of dynamic and discrete optimization problems. Interpreted as an approximate dynamic programming algorithm, a rollout algorithm estimates the value-to-go at each decision stage by simulating future events while following a greedy policy, referred to as the base policy. While in many cases rollout algorithms are guaranteed to perform as well as their base policies, there have been few theoretical results showing additional improvement in performance. In this paper we perform a probabilistic analysis of the subset sum problem and knapsack problem, giving theoretical evidence that rollout algorithms perform strictly better than their base policies. Using a stochastic model from the existing literature, we analyze two rollout methods that we refer to as the consecutive rollout and exhaustive rollout, both of which employ a simple greedy base policy. For the subset sum problem, we prove that after only a single iteration of the rollout algorithm, both methods yield at least a 30% reduction in the expected gap between the solution value and capacity, relative to the base policy. Analogous results are shown for the knapsack problem.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "Current kidney exchange pools are of moderate size and thin, as they consist of many highly sensitized patients. Creating a thicker pool can be done by waiting for many pairs to arrive. We analyze a simple class of matching algorithms that search periodically for allocations. We find that if only 2-way cycles are conducted, in order to gain a significant amount of matches over the online scenario (matching each time a new incompatible pair joins the pool) the waiting period should be \"very long\". If 3-way cycles are also allowed we find regimes in which waiting for a short period also increases the number of matches considerably. Finally, a significant increase of matches can be obtained by using even one non-simultaneous chain while still matching in an online fashion. Our theoretical findings and data-driven computational experiments lead to policy recommendations.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "In this paper, we study a general online linear programming problem whose formulation encompasses many practical dynamic resource allocation problems, including internet advertising display applications, revenue management, various routing, packing, and auction problems. We propose a model, which under mild assumptions, allows us to design near-optimal learning-based online algorithms that do not require the a priori knowledge about the total number of online requests to come, a first of its kind. We then consider two variants of the problem that relax the initial assumptions imposed on the proposed model.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "The most well-known conjecture in the context of matroid secretary problems claims the existence of a constant-factor approximation applicable to any matroid. Whereas this conjecture remains open, modified forms of it were shown to be true, when assuming that the assignment of weights to the secretaries is not adversarial but uniformly random (Soto [SODA 2011], Oveis Gharan and Vondr\u00e1k [ESA 2011]). However, so far, there was no variant of the matroid secretary problem with adversarial weight assignment for which a constant-factor approximation was found. We address this point by presenting a 9-approximation for the \\emph{free order model}, a model suggested shortly after the introduction of the matroid secretary problem, and for which no constant-factor approximation was known so far. The free order model is a relaxed version of the original matroid secretary problem, with the only difference that one can choose the order in which secretaries are interviewed.\n  Furthermore, we consider the classical matroid secretary problem for the special case of laminar matroids. Only recently, a constant-factor approximation has been found for this case, using a clever but rather involved method and analysis (Im and Wang, [SODA 2011]) that leads to a 16000/3-approximation. This is arguably the most involved special case of the matroid secretary problem for which a constant-factor approximation is known. We present a considerably simpler and stronger $3\\sqrt{3}e\\approx 14.12$-approximation, based on reducing the problem to a matroid secretary problem on a partition matroid.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "The problem of modeling and predicting spatiotemporal traffic phenomena over an urban road network is important to many traffic applications such as detecting and forecasting congestion hotspots. This paper presents a decentralized data fusion and active sensing (D2FAS) algorithm for mobile sensors to actively explore the road network to gather and assimilate the most informative data for predicting the traffic phenomenon. We analyze the time and communication complexity of D2FAS and demonstrate that it can scale well with a large number of observations and sensors. We provide a theoretical guarantee on its predictive performance to be equivalent to that of a sophisticated centralized sparse approximation for the Gaussian process (GP) model: The computation of such a sparse approximate GP model can thus be parallelized and distributed among the mobile sensors (in a Google-like MapReduce paradigm), thereby achieving efficient and scalable prediction. We also theoretically guarantee its active sensing performance that improves under various practical environmental conditions. Empirical evaluation on real-world urban road network data shows that our D2FAS algorithm is significantly more time-efficient and scalable than state-of-the-art centralized algorithms while achieving comparable predictive performance.\n        \u25b3 Less", "author": "Patrick Jaillet"}, {"abstract": "In this paper, we consider the problem of Gaussian process (GP) optimization with an added robustness requirement: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation. This problem is motivated by settings in which the underlying functions during optimization and implementation stages are different, or when one is interested in finding an entire region of good inputs rather than only a single point. We show that standard GP optimization algorithms do not exhibit the desired robustness properties, and provide a novel confidence-bound based algorithm StableOpt for this purpose. We rigorously establish the required number of samples for StableOpt to find a near-optimal point, and we complement this guarantee with an algorithm-independent lower bound. We experimentally demonstrate several potential applications of interest using real-world data sets, and we show that StableOpt consistently succeeds in finding a stable maximizer where several baseline methods fail.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We consider the problem of inference in discrete probabilistic models, that is, distributions over subsets of a finite ground set. These encompass a range of well-known models in machine learning, such as determinantal point processes and Ising models. Locally-moving Markov chain Monte Carlo algorithms, such as the Gibbs sampler, are commonly used for inference in such models, but their convergence is, at times, prohibitively slow. This is often caused by state-space bottlenecks that greatly hinder the movement of such samplers. We propose a novel sampling strategy that uses a specific mixture of product distributions to propose global moves and, thus, accelerate convergence. Furthermore, we show how to construct such a mixture using semigradient information. We illustrate the effectiveness of combining our sampler with existing ones, both theoretically on an example model, as well as practically on three models learned from real-world data sets.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We demonstrate that a very deep ResNet with stacked modules with one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in $d$ dimensions, i.e. $\\ell_1(\\mathbb{R}^d)$. Because of the identity mapping inherent to ResNets, our network has alternating layers of dimension one and $d$. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension $d$ [Lu et al, 2017; Hanin and Sellke, 2017]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Many problems in machine learning involve calculating correspondences between sets of objects, such as point clouds or images. Discrete optimal transport (OT) provides a natural and successful approach to such tasks whenever the two sets of objects can be represented in the same space or when we can evaluate distances between the objects. Unfortunately neither requirement is likely to hold when object representations are learned from data. Indeed, automatically derived representations such as word embeddings are typically fixed only up to some global transformations, for example, reflection or rotation. As a result, pairwise distances across the two types of objects are ill-defined without specifying their relative transformation. In this work, we propose a general framework for optimal transport in the presence of latent global transformations. We discuss algorithms for the specific case of orthonormal transformations, and show promising results in unsupervised word alignment.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of \"neighboring\" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Robustness of deep learning models is a property that has recently gained increasing attention. We formally define a notion of robustness for generative adversarial models, and show that, perhaps surprisingly, the GAN in its original form is not robust. Indeed, the discriminator in GANs may be viewed as merely offering \"teaching feedback\". Our notion of robustness relies on a dishonest discriminator, or noisy, adversarial interference with its feedback. We explore, theoretically and empirically, the effect of model and training properties on this robustness. In particular, we show theoretical conditions for robustness that are supported by empirical evidence. We also test the effect of regularization. Our results suggest variations of GANs that are indeed more robust to noisy attacks, and have overall more stable training behavior.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Submodular functions have applications throughout machine learning, but in many settings, we do not have direct access to the underlying function $f$. We focus on stochastic functions that are given as an expectation of functions over a distribution $P$. In practice, we often have only a limited set of samples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by maximizing the sum of $f_i$. However, this ignores generalization to the true (unknown) distribution. In this paper, we achieve better performance on the actual underlying function $f$ by directly optimizing a combination of bias and variance. Algorithmically, we accomplish this by showing how to carry out distributionally robust optimization (DRO) for submodular functions, providing efficient algorithms backed by theoretical guarantees which leverage several novel contributions to the general theory of DRO. We also show compelling empirical evidence that DRO improves generalization to the unknown stochastic submodular function.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Optimal Transport has recently gained interest in machine learning for applications ranging from domain adaptation, sentence similarities to deep learning. Yet, its ability to capture frequently occurring structure beyond the \"ground metric\" is limited. In this work, we develop a nonlinear generalization of (discrete) optimal transport that is able to reflect much additional structure. We demonstrate how to leverage the geometry of this new model for fast algorithms, and explore connections and properties. Illustrative experiments highlight the benefit of the induced structured couplings for tasks in domain adaptation and natural language processing.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We introduce Graph-Sparse Logistic Regression, a new algorithm for classification for the case in which the support should be sparse but connected on a graph. We val- idate this algorithm against synthetic data and benchmark it against L1-regularized Logistic Regression. We then explore our technique in the bioinformatics context of proteomics data on the interactome graph. We make all our experimental code public and provide GSLR as an open source package.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We propose a framework for adversarial training that relies on a sample rather than a single sample point as the fundamental unit of discrimination. Inspired by discrepancy measures and two-sample tests between probability distributions, we propose two such distributional adversaries that operate and predict on samples, and show how they can be easily implemented on top of existing models. Various experimental results show that generators trained with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with pointwise prediction discriminators. The application of our framework to domain adaptation also results in considerable improvement over recent state-of-the-art.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "The need for real time analysis of rapidly producing data streams (e.g., video and image streams) motivated the design of streaming algorithms that can efficiently extract and summarize useful information from massive data \"on the fly\". Such problems can often be reduced to maximizing a submodular set function subject to various constraints. While efficient streaming methods have been recently developed for monotone submodular maximization, in a wide range of applications, such as video summarization, the underlying utility function is non-monotone, and there are often various constraints imposed on the optimization problem to consider privacy or personalization. We develop the first efficient single pass streaming algorithm, Streaming Local Search, that for any streaming monotone submodular maximization algorithm with approximation guarantee $\u03b1$ under a collection of independence systems ${\\cal I}$, provides a constant $1/\\big(1+2/\\sqrt\u03b1+1/\u03b1+2d(1+\\sqrt\u03b1)\\big)$ approximation guarantee for maximizing a non-monotone submodular function under the intersection of ${\\cal I}$ and $d$ knapsack constraints. Our experiments show that for video summarization, our method runs more than 1700 times faster than previous work, while maintaining practically the same performance.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Bayesian optimization (BO) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current BO techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization (EBO) to address three current challenges in BO simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of EBO is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up BO to tens of thousands of observations within minutes of computation.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We consider the problem of far-field sensing by means of a sensor array. Traditional array geometry design techniques are agnostic to prior information about the far-field scene. However, in many applications such priors are available and may be utilized to design more efficient array topologies. We formulate the problem of array geometry design with scene prior as one of finding a sampling configuration that enables efficient inference, which turns out to be a combinatorial optimization problem. While generic combinatorial optimization problems are NP-hard and resist efficient solvers, we show how for array design problems the theory of submodular optimization may be utilized to obtain efficient algorithms that are guaranteed to achieve solutions within a constant approximation factor from the optimum. We leverage the connection between array design problems and submodular optimization and port several results of interest. We demonstrate efficient methods for designing arrays with constraints on the sensing aperture, as well as arrays respecting combinatorial placement constraints. This novel connection between array design and submodularity suggests the possibility for utilizing other insights and techniques from the growing body of literature on submodular optimization in the field of array design.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We study dual volume sampling, a method for selecting k columns from an n x m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume spanned by the rows of the induced submatrix. This method was proposed by Avron and Boutsidis (2013), who showed it to be a promising method for column subset selection and its multiple applications. However, its wider adoption has been hampered by the lack of polynomial time sampling algorithms. We remove this hindrance by developing an exact (randomized) polynomial time sampling algorithm as well as its derandomization. Thereafter, we study dual volume sampling via the theory of real stable polynomials and prove that its distribution satisfies the \"Strong Rayleigh\" property. This result has numerous consequences, including a provably fast-mixing Markov chain sampler that makes dual volume sampling much more attractive to practitioners. This sampler is closely related to classical algorithms for popular experimental design methods that are to date lacking theoretical analysis but are known to empirically work well.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings. In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method. Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes. Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the $\\arg\\max$ of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past years, in particular in machine learning and data mining. But in applications, the parameters of the problem are rarely known exactly, and using wrong parameters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allocation or Bipartite Influence Maximization problem introduced by Alon et al. (2012) from a robust optimization perspective, where an adversary may choose the least favorable parameters within a confidence set. The resulting problem is a nonconvex-concave saddle point problem (or game). We show that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization problem. Although constrained submodular minimization is hard in general, here, we establish conditions under which such a problem can be solved to arbitrary precision $\u03b5$.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Learning the representation and the similarity metric in an end-to-end fashion with deep networks have demonstrated outstanding results for clustering and retrieval. However, these recent approaches still suffer from the performance degradation stemming from the local metric training procedure which is unaware of the global structure of the embedding space.\n  We propose a global metric learning scheme for optimizing the deep metric embedding with the learnable clustering function and the clustering metric (NMI) in a novel structured prediction framework.\n  Our experiments on CUB200-2011, Cars196, and Stanford online products datasets show state of the art performance both on the clustering and retrieval tasks measured in the NMI and Recall@K evaluation metrics.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints. We consider the task of rapidly sampling from such constrained measures, and develop fast Markov chain samplers for them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures, for which we present sharp polynomial bounds on the mixing time. As a corollary, this result yields a fast mixing sampler for Determinantal Point Processes (DPPs), yielding (to our knowledge) the first provably fast MCMC sampler for DPPs since their inception over four decades ago. Beyond SR measures, we develop MCMC samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We introduce a framework for model learning and planning in stochastic domains with continuous state and action spaces and non-Gaussian transition models. It is efficient because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows the validity and asymptotic optimality of the proposed approach. Empirically, we demonstrate the effectiveness of our algorithm on a simulated multi-modal pushing problem.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "In this note we consider sampling from (non-homogeneous) strongly Rayleigh probability measures. As an important corollary, we obtain a fast mixing Markov Chain sampler for Determinantal Point Processes.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "The Nystr\u00f6m method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystr\u00f6m using Determinantal Point Processes (DPPs), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via DPPs guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of DPPsampling, we show that (under certain conditions) Markov chain DPP sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of DPP-based landmark selection compared with existing approaches.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We present a framework for accelerating a spectrum of machine learning algorithms that require computation of bilinear inverse forms $u^\\top A^{-1}u$, where $A$ is a positive definite matrix and $u$ a given vector. Our framework is built on Gauss-type quadrature and easily scales to large, sparse matrices. Further, it allows retrospective computation of lower and upper bounds on $u^\\top A^{-1}u$, which in turn accelerates several algorithms. We prove that these bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge, ours is the first work to demonstrate these key properties of Gauss-type quadrature, which is a classical and deeply studied topic. We illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization, and observe tremendous speedups in several instances.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classification with mislabeled training samples - an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Determinantal Point Processes (DPPs) are elegant probabilistic models of repulsion and diversity over discrete sets of items. But their applicability to large sets is hindered by expensive cubic-complexity matrix operations for basic tasks such as sampling. In light of this, we propose a new method for approximate sampling from discrete $k$-DPPs. Our method takes advantage of the diversity property of subsets sampled from a DPP, and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter, it efficiently samples subsets based on the constructed coresets. As opposed to previous approaches, our algorithm aims to minimize the total variation distance to the original distribution. Experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets, and yields more accurate samples than previous approaches.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Energy minimization has been an intensely studied core problem in computer vision. With growing image sizes (2D and 3D), it is now highly desirable to run energy minimization algorithms in parallel. But many existing algorithms, in particular, some efficient combinatorial algorithms, are difficult to par-allelize. By exploiting results from convex and submodular theory, we reformulate the quadratic energy minimization problem as a total variation denoising problem, which, when viewed geometrically, enables the use of projection and reflection based convex methods. The resulting min-cut algorithm (and code) is conceptually very simple, and solves a sequence of TV denoising problems. We perform an extensive empirical evaluation comparing state-of-the-art combinatorial algorithms and convex optimization techniques. On small problems the iterative convex methods match the combinatorial max-flow algorithms, while on larger problems they offer other flexibility and important gains: (a) their memory footprint is small; (b) their straightforward parallelizability fits multi-core platforms; (c) they can easily be warm-started; and (d) they quickly reach approximately good solutions, thereby enabling faster \"inexact\" solutions. A key consequence of our approach based on submodularity and convexity is that it is allows to combine any arbitrary combinatorial or convex methods as subroutines, which allows one to obtain hybrid combinatorial and convex optimization algorithms that benefit from the strengths of both.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We introduce and study methods for inferring and learning from correspondences among neurons. The approach enables alignment of data from distinct multiunit studies of nervous systems. We show that the methods for inferring correspondences combine data effectively from cross-animal studies to make joint inferences about behavioral decision making that are not possible with the data from a single animal. We focus on data collection, machine learning, and prediction in the representative and long-studied invertebrate nervous system of the European medicinal leech. Acknowledging the computational intractability of the general problem of identifying correspondences among neurons, we introduce efficient computational procedures for matching neurons across animals. The methods include techniques that adjust for missing cells or additional cells in the different data sets that may reflect biological or experimental variation. The methods highlight the value harnessing inference and learning in new kinds of computational microscopes for multiunit neurobiological studies.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "The increasing prominence of weakly labeled data nurtures a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of \"simple\" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We study an extension of the classical graph cut problem, wherein we replace the modular (sum of edge weights) cost function by a submodular set function defined over graph edges. Special cases of this problem have appeared in different applications in signal processing, machine learning, and computer vision. In this paper, we connect these applications via the generic formulation of \"cooperative graph cuts\", for which we study complexity, algorithms, and connections to polymatroidal network flows. Finally, we compare the proposed algorithms empirically.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efficient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reflections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the benefits of our method on two image segmentation tasks.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [53]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the 'curvature' of the submodular function, and provide lower and upper bounds that refine and improve previous results [3, 16, 18, 52]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to influence approximations for submodular maximization [7, 55], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization. Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular min- imization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning. We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "Research on distributed machine learning algorithms has focused primarily on one of two extremes - algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this \"optimistic concurrency control\" paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "In the past few years powerful generalizations to the Euclidean k-means problem have been made, such as Bregman clustering [7], co-clustering (i.e., simultaneous clustering of rows and columns of an input matrix) [9,18], and tensor clustering [8,34]. Like k-means, these more general problems also suffer from the NP-hardness of the associated optimization. Researchers have developed approximation algorithms of varying degrees of sophistication for k-means, k-medians, and more recently also for Bregman clustering [2]. However, there seem to be no approximation algorithms for Bregman co- and tensor clustering. In this paper we derive the first (to our knowledge) guaranteed methods for these increasingly important clustering settings. Going beyond Bregman divergences, we also prove an approximation factor for tensor clustering with arbitrary separable metrics. Through extensive experiments we evaluate the characteristics of our method, and show that it also has practical impact.\n        \u25b3 Less", "author": "Stefanie Jegelka"}, {"abstract": "This paper explains a flaw in the published proof of the Scalable Commutativity Rule (SCR), presents a revised and formally verified proof of the SCR in the Coq proof assistant, and discusses the insights and open questions raised from our experience proving the SCR.\n        \u25b3 Less", "author": "M. Frans Kaashoek"}, {"abstract": "Bayesian optimization usually assumes that a Bayesian prior is given. However, the strong theoretical guarantees in Bayesian optimization are often regrettably compromised in practice because of unknown parameters in the prior. In this paper, we adopt a variant of empirical Bayes and show that, by estimating the Gaussian process prior from offline data sampled from the same prior and constructing unbiased estimators of the posterior, variants of both GP-UCB and probability of improvement achieve a near-zero regret bound, which decreases to a constant proportional to the observational noise as the number of offline data and the number of online evaluations increase. Empirically, we have verified our approach on challenging simulated robotic problems featuring task and motion planning.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In this paper, we analyze the effects of depth and width on the quality of local minima, without strong over-parameterization and simplification assumptions in the literature. Without any simplification assumption, for deep nonlinear neural networks with the squared loss, we theoretically show that the quality of local minima tends to improve towards the global minimum value as depth and width increase. Furthermore, with a locally-induced structure on deep nonlinear neural networks, the values of local minima of neural networks are theoretically proven to be no worse than the globally optimal values of corresponding classical machine learning models. We empirically support our theoretical observation with a synthetic dataset as well as MNIST, CIFAR-10 and SVHN datasets. When compared to previous studies with strong over-parameterization assumptions, the results in this paper do not require over-parameterization, and instead show the gradual effects of over-parameterization as consequences of general results.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We present a representation for describing transition models in complex uncertain domains using relational rules. For any action, a rule selects a set of relevant objects and computes a distribution over properties of just those objects in the resulting state given their properties in the previous state. An iterative greedy algorithm is used to construct a set of deictic references that determine which objects are relevant in any given state. Feed-forward neural networks are used to learn the transition distribution on the relevant objects' properties. This strategy is demonstrated to be both more versatile and more sample efficient than learning a monolithic transition model in a simulated domain in which a robot pushes stacks of objects on a cluttered table.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Multi-object manipulation problems in continuous state and action spaces can be solved by planners that search over sampled values for the continuous parameters of operators. The efficiency of these planners depends critically on the effectiveness of the samplers used, but effective sampling in turn depends on details of the robot, environment, and task. Our strategy is to learn functions called specializers that generate values for continuous operator parameters, given a state description and values for the discrete parameters. Rather than trying to learn a single specializer for each operator from large amounts of data on a single task, we take a modular meta-learning approach. We train on multiple tasks and learn a variety of specializers that, on a new task, can be quickly adapted using relatively little data -- thus, our system \"learns quickly to plan quickly\" using these specializers. We validate our approach experimentally in simulated 3D pick-and-place tasks with continuous state and action spaces.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "An efficient, generalizable physical simulator with universal uncertainty estimates has wide applications in robot state estimation, planning, and control. In this paper, we build such a simulator for two scenarios, planar pushing and ball bouncing, by augmenting an analytical rigid-body simulator with a neural network that learns to model uncertainty as residuals. Combining symbolic, deterministic simulators with learnable, stochastic neural nets provides us with expressiveness, efficiency, and generalizability simultaneously. Our model outperforms both purely analytical and purely learned simulators consistently on real, standard benchmarks. Compared with methods that model uncertainty using Gaussian processes, our model runs much faster, generalizes better to new object shapes, and is able to characterize the complex distribution of object trajectories.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In this paper, we propose a learning algorithm that speeds up the search in task and motion planning problems. Our algorithm proposes solutions to three different challenges that arise in learning to improve planning efficiency: what to predict, how to represent a planning problem instance, and how to transfer knowledge from one problem instance to another. We propose a method that predicts constraints on the search space based on a generic representation of a planning problem instance, called score-space, where we represent a problem instance in terms of the performance of a set of solutions attempted so far. Using this representation, we transfer knowledge, in the form of constraints, from previous problems based on the similarity in score space. We design a sequential algorithm that efficiently predicts these constraints, and evaluate it in three different challenging task and motion planning problems. Results indicate that our approach performs orders of magnitudes faster than an unguided planner\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Many prediction problems, such as those that arise in the context of robotics, have a simplifying underlying structure that could accelerate learning. In this paper, we present a strategy for learning a set of neural network modules that can be combined in different ways. We train different modular structures on a set of related tasks and generalize to new tasks by composing the learned modules in new ways. We show this improves performance in two robotics-related problems.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In many robotic applications, an autonomous agent must act within and explore a partially observed environment that is unobserved by its human teammate. We consider such a setting in which the agent can, while acting, transmit declarative information to the human that helps them understand aspects of this unseen environment. In this work, we address the algorithmic question of how the agent should plan out what actions to take and what information to transmit. Naturally, one would expect the human to have preferences, which we model information-theoretically by scoring transmitted information based on the change it induces in weighted entropy of the human's belief state. We formulate this setting as a belief MDP and give a tractable algorithm for solving it approximately. Then, we give an algorithm that allows the agent to learn the human's preferences online, through exploration. We validate our approach experimentally in simulated discrete and continuous partially observed search-and-recover domains. Visit http://tinyurl.com/chitnis-corl-18 for a supplementary video.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In many applications that involve processing high-dimensional data, it is important to identify a small set of entities that account for a significant fraction of detections. Rather than formalize this as a clustering problem, in which all detections must be grouped into hard or soft categories, we formalize it as an instance of the frequent items or heavy hitters problem, which finds groups of tightly clustered objects that have a high density in the feature space. We show that the heavy hitters formulation generates solutions that are more accurate and effective than the clustering formulation. In addition, we present a novel online algorithm for heavy hitters, called HAC, which addresses problems in continuous space, and demonstrate its effectiveness on real video and household domains.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "The objective of this work is to augment the basic abilities of a robot by learning to use new sensorimotor primitives to enable the solution of complex long-horizon problems. Solving long-horizon problems in complex domains requires flexible generative planning that can combine primitive abilities in novel combinations to solve problems as they arise in the world. In order to plan to combine primitive actions, we must have models of the preconditions and effects of those actions: under what circumstances will executing this primitive achieve some particular effect in the world?\n  We use, and develop novel improvements on, state-of-the-art methods for active learning and sampling. We use Gaussian process methods for learning the conditions of operator effectiveness from small numbers of expensive training examples collected by experimentation on a robot. We develop adaptive sampling methods for generating diverse elements of continuous sets (such as robot configurations and object poses) during planning for solving a new task, so that planning is as efficient as possible. We demonstrate these methods in an integrated system, combining newly learned models with an efficient continuous-space robot task and motion planner to learn to solve long horizon problems more efficiently than was previously possible.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In partially observed environments, it can be useful for a human to provide the robot with declarative information that represents probabilistic relational constraints on properties of objects in the world, augmenting the robot's sensory observations. For instance, a robot tasked with a search-and-rescue mission may be informed by the human that two victims are probably in the same room. An important question arises: how should we represent the robot's internal knowledge so that this information is correctly processed and combined with raw sensory information? In this paper, we provide an efficient belief state representation that dynamically selects an appropriate factoring, combining aspects of the belief when they are correlated through information and separating them when they are not. This strategy works in open domains, in which the set of possible objects is not known in advance, and provides significant improvements in inference time over a static factoring, leading to more efficient planning for complex partially observed tasks. We validate our approach experimentally in two open-domain planning problems: a 2D discrete gridworld task and a 3D continuous cooking task. A supplementary video can be found at http://tinyurl.com/chitnis-iros-18.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Many planning applications involve complex relationships defined on high-dimensional, continuous variables. For example, robotic manipulation requires planning with kinematic, collision, and motion constraints involving robot configurations, object transforms, and robot trajectories. These constraints typically require specialized procedures to sample satisfying values. We extend the STRIPS planning language to support a generic, declarative specification for these procedures while treating their implementation as blackboxes. We provide several domain-independent algorithms that reduce STRIPStream problems to a sequence of finite-domain STRIPS planning problems. Additionally, we describe cost-sensitive planning within this framework. Finally, we evaluate our algorithms on three robotic task and motion planning domains.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "This paper introduces a novel measure-theoretic theory for machine learning that does not require statistical assumptions. Based on this theory, a new regularization method in deep learning is derived and shown to outperform previous methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed theory provides a theoretical basis for a family of practically successful regularization methods in deep learning. We discuss several consequences of our results on one-shot learning, representation learning, deep learning, and curriculum learning. Unlike statistical learning theory, the proposed learning theory analyzes each problem instance individually via measure theory, rather than a set of problem instances via statistics. As a result, it provides different types of results and insights when compared to statistical learning theory.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "This paper presents a general-purpose formulation of a large class of discrete-time planning problems, with hybrid state and control-spaces, as factored transition systems. Factoring allows state transitions to be described as the intersection of several constraints each affecting a subset of the state and control variables. Robotic manipulation problems with many movable objects involve constraints that only affect several variables at a time and therefore exhibit large amounts of factoring. We develop a theoretical framework for solving factored transition systems with sampling-based algorithms. The framework characterizes conditions on the submanifold in which solutions lie, leading to a characterization of robust feasibility that incorporates dimensionality-reducing constraints. It then connects those conditions to corresponding conditional samplers that can be composed to produce values on this submanifold. We present two domain-independent, probabilistically complete planning algorithms that take, as input, a set of conditional samplers. We demonstrate the empirical efficiency of these algorithms on a set of challenging task and motion planning problems involving picking, placing, and pushing.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, mapping the inputs to their corresponding outputs exactly. Due to its precise and combinatorial nature, program synthesis is commonly formulated as a constraint satisfaction problem, where input-output examples are encoded as constraints and solved with a constraint solver. A key challenge of this formulation is scalability: while constraint solvers work well with a few well-chosen examples, a large set of examples can incur significant overhead in both time and memory. We describe a method to discover a subset of examples that is both small and representative: the subset is constructed iteratively, using a neural network to predict the probability of unchosen examples conditioned on the chosen examples in the subset, and greedily adding the least probable example. We empirically evaluate the representativeness of the subsets constructed by our method, and demonstrate such subsets can significantly improve synthesis time and stability.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In robotics, it is essential to be able to plan efficiently in high-dimensional continuous state-action spaces for long horizons. For such complex planning problems, unguided uniform sampling of actions until a path to a goal is found is hopelessly inefficient, and gradient-based approaches often fall short when the optimization manifold of a given problem is not smooth. In this paper we present an approach that guides the search of a state-space planner, such as A*, by learning an action-sampling distribution that can generalize across different instances of a planning problem. The motivation is that, unlike typical learning approaches for planning for continuous action space that estimate a policy, an estimated action sampler is more robust to error since it has a planner to fall back on. We use a Generative Adversarial Network (GAN), and address an important issue: search experience consists of a relatively large number of actions that are not on a solution path and a relatively small number of actions that actually are on a solution path. We introduce a new technique, based on an importance-ratio estimation method, for using samples from a non-target distribution to make GAN learning more data-efficient. We provide theoretical guarantees and empirical evaluation in three challenging continuous robot planning problems to illustrate the effectiveness of our algorithm.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "As drones and autonomous cars become more widespread it is becoming increasingly important that robots can operate safely under realistic conditions. The noisy information fed into real systems means that robots must use estimates of the environment to plan navigation. Efficiently guaranteeing that the resulting motion plans are safe under these circumstances has proved difficult. We examine how to guarantee that a trajectory or policy is safe with only imperfect observations of the environment. We examine the implications of various mathematical formalisms of safety and arrive at a mathematical notion of safety of a long-term execution, even when conditioned on observational information. We present efficient algorithms that can prove that trajectories or policies are safe with much tighter bounds than in previous work. Notably, the complexity of the environment does not affect our methods ability to evaluate if a trajectory or policy is safe. We then use these safety checking methods to design a safe variant of the RRT planning algorithm.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We consider the problem of diagnosis where a set of simple observations are used to infer a potentially complex hidden hypothesis. Finding the optimal subset of observations is intractable in general, thus we focus on the problem of active diagnosis, where the agent selects the next most-informative observation based on the results of previous observations. We show that under the assumption of uniform observation entropy, one can build an implication model which directly predicts the outcome of the potential next observation conditioned on the results of past observations, and selects the observation with the maximum entropy. This approach enjoys reduced computation complexity by bypassing the complicated hypothesis space, and can be trained on observation data alone, learning how to query without knowledge of the hidden hypothesis.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Many robotic planning applications involve continuous actions with highly non-linear constraints, which cannot be modeled using modern planners that construct a propositional representation. We introduce STRIPStream: an extension of the STRIPS language which can model these domains by supporting the specification of blackbox generators to handle complex constraints. The outputs of these generators interact with actions through possibly infinite streams of objects and static predicates. We provide two algorithms which both reduce STRIPStream problems to a sequence of finite-domain planning problems. The representation and algorithms are entirely domain independent. We demonstrate our framework on simple illustrative domains, and then on a high-dimensional, continuous robotic task and motion planning domain.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Mobile manipulation problems involving many objects are challenging to solve due to the high dimensionality and multi-modality of their hybrid configuration spaces. Planners that perform a purely geometric search are prohibitively slow for solving these problems because they are unable to factor the configuration space. Symbolic task planners can efficiently construct plans involving many variables but cannot represent the geometric and kinematic constraints required in manipulation. We present the FFRob algorithm for solving task and motion planning problems. First, we introduce Extended Action Specification (EAS) as a general purpose planning representation that supports arbitrary predicates as conditions. We adapt existing heuristic search ideas for solving \\proc{strips} planning problems, particularly delete-relaxations, to solve EAS problem instances. We then apply the EAS representation and planners to manipulation problems resulting in FFRob. FFRob iteratively discretizes task and motion planning problems using batch sampling of manipulation primitives and a multi-query roadmap structure that can be conditionalized to evaluate reachability under different placements of movable objects. This structure enables the EAS planner to efficiently compute heuristics that incorporate geometric and kinematic planning constraints to give a tight estimate of the distance to the goal. Additionally, we show FFRob is probabilistically complete and has finite expected runtime. Finally, we empirically demonstrate FFRob's effectiveness on complex and diverse task and motion planning tasks including rearrangement planning and navigation among movable objects.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We investigate learning heuristics for domain-specific planning. Prior work framed learning a heuristic as an ordinary regression problem. However, in a greedy best-first search, the ordering of states induced by a heuristic is more indicative of the resulting planner's performance than mean squared error. Thus, we instead frame learning a heuristic as a learning to rank problem which we solve using a RankSVM formulation. Additionally, we introduce new methods for computing features that capture temporal interactions in an approximate plan. Our experiments on recent International Planning Competition problems show that the RankSVM learned heuristics outperform both the original heuristics and heuristics learned through ordinary regression.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We introduce a framework for model learning and planning in stochastic domains with continuous state and action spaces and non-Gaussian transition models. It is efficient because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows the validity and asymptotic optimality of the proposed approach. Empirically, we demonstrate the effectiveness of our algorithm on a simulated multi-modal pushing problem.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In this paper we address planning problems in high-dimensional hybrid configuration spaces, with a particular focus on manipulation planning problems involving many objects. We present the hybrid backward-forward (HBF) planning algorithm that uses a backward identification of constraints to direct the sampling of the infinite action space in a forward search from the initial state towards a goal configuration. The resulting planner is probabilistically complete and can effectively construct long manipulation plans requiring both prehensile and nonprehensile actions in cluttered environments.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "To accomplish tasks in human-centric indoor environments, robots need to represent and understand the world in terms of objects and their attributes. We refer to this attribute-based representation as a world model, and consider how to acquire it via noisy perception and maintain it over time, as objects are added, changed, and removed in the world. Previous work has framed this as multiple-target tracking problem, where objects are potentially in motion at all times. Although this approach is general, it is computationally expensive. We argue that such generality is not needed in typical world modeling tasks, where objects only change state occasionally. More efficient approaches are enabled by restricting ourselves to such semi-static environments.\n  We consider a previously-proposed clustering-based world modeling approach that assumed static environments, and extend it to semi-static domains by applying a dependent Dirichlet-process (DDP) mixture model. We derive a novel MAP inference algorithm under this model, subject to data association constraints. We demonstrate our approach improves computational performance in semi-static environments.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Cooperative games are those in which both agents share the same payoff structure. Value-based reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We describe a probabilistic framework for synthesizing control policies for general multi-robot systems, given environment and sensor models and a cost function. Decentralized, partially observable Markov decision processes (Dec-POMDPs) are a general model of decision processes where a team of agents must cooperate to optimize some objective (specified by a shared reward or cost function) in the presence of uncertainty, but where communication limitations mean that the agents cannot share their state, so execution must proceed in a decentralized fashion. While Dec-POMDPs are typically intractable to solve for real-world problems, recent research on the use of macro-actions in Dec-POMDPs has significantly increased the size of problem that can be practically solved as a Dec-POMDP. We describe this general model, and show how, in contrast to most existing methods that are specialized to a particular problem class, it can synthesize control policies that use whatever opportunities for coordination are present in the problem, while balancing off uncertainty in outcomes, sensor information, and information about other agents. We use three variations on a warehouse task to show that a single planner of this type can generate cooperative behavior using task allocation, direct communication, and signaling, as appropriate.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We describe a method for time-critical decision making involving sequential tasks and stochastic processes.  The method employs several iterative refinement routines for solving different aspects of the decision making problem.  This paper concentrates on the meta-level control problem of deliberation scheduling, allocating computational resources to these routines.  We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints.  We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states.  We describe algorithms for precursor and recurrent models and provide the results of our empirical investigations to date.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning.  In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms.  We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly.  To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macroactions as local policies that act in certain regions of state space, and by restricting states in the abstract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational overhead of macro-action generation.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Many applications require that we learn the parameters of a model from data. EM is a method used to learn the parameters of probabilistic models for which the data for some of the variables in the models is either missing or hidden.  There are instances in which this method is slow to converge. Therefore, several accelerations have been proposed to improve the method. None of the proposed acceleration methods are theoretically dominant and experimental comparisons are lacking.  In this paper, we present the different proposed accelerations and try to compare them experimentally.  From the results of the experiments, we argue that some acceleration of EM is always possible, but that which acceleration is superior depends on properties of the problem.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Reactive (memoryless) policies are sufficient in completely observable Markov decision processes (MDPs), but some kind of memory is usually necessary for optimal control of a partially observable MDP. Policies with finite memory can be represented as finite-state automata. In this paper, we extend Baird and Moore's VAPS algorithm to the problem of learning general finite-state automata. Because it performs stochastic gradient descent, this algorithm can be shown to converge to a locally optimal finite-state controller. We provide the details of the algorithm and then consider the question of under what conditions stochastic gradient descent will outperform exact gradient descent. We conclude with empirical results comparing the performance of stochastic and exact gradient descent, and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Solving partially observable Markov decision processes (POMDPs) is highly intractable in general, at least in part because the optimal policy may be infinitely large. In this paper, we explore the problem of finding the optimal policy from a restricted set of policies, represented as finite state automata of a given size. This problem is also intractable, but we show that the complexity can be greatly reduced when the POMDP and/or policy are further constrained. We demonstrate good empirical results with a branch-and-bound method for finding globally optimal deterministic policies, and a gradient-ascent method for finding locally optimal stochastic policies.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Sampling is an important tool for estimating large, complex sums and integrals over high dimensional spaces. For instance, important sampling has been used as an alternative to exact methods for inference in belief networks. Ideally, we want to have a sampling distribution that provides optimal-variance estimators. In this paper, we present methods that improve the sampling distribution by systematically adapting it as we obtain information from the samples. We present a stochastic-gradient-descent method for sequentially updating the sampling distribution based on the direct minization of the variance. We also present other stochastic-gradient-descent methods based on the minimizationof typical notions of distance between the current sampling distribution and approximations of the target, optimal distribution. We finally validate and compare the different methods empirically by applying them to the problem of action evaluation in influence diagrams.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Most reinforcement learning methods operate on propositional representations of the world state.  Such representations are often intractably large and generalize poorly.  Using a deictic representation is believed to be a viable alternative: they promise generalization while allowing the use of existing reinforcement-learning methods.  Yet, there are few experiments on learning with deictic representations reported in the literature.  In this paper we explore the effectiveness of two forms of deictic representation and a na\u00efve propositional representation in a simple blocks-world domain.  We find, empirically, that the deictic representations actually worsen learning performance. We conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects. \n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "We apply decision theoretic techniques to construct non-player characters that are able to assist a human player in collaborative games. The method is based on solving Markov decision processes, which can be difficult when the game state is described by many variables. To scale to more complex games, the method allows decomposition of a game task into subtasks, each of which can be modelled by a Markov decision process. Intention recognition is used to infer the subtask that the human is currently performing, allowing the helper to assist the human in performing the correct task. Experiments show that the method can be effective, giving near-human level performance in helping a human in a collaborative game.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "The ways in which an agent's actions affect the world can often be modeled compactly using a set of relational probabilistic planning rules. This paper addresses the problem of learning such rule sets for multiple related tasks. We take a hierarchical Bayesian approach, in which the system learns a prior distribution over rule sets. We present a class of prior distributions parameterized by a rule set prototype that is stochastically modified to produce a task-specific rule set. We also describe a coordinate ascent algorithm that iteratively optimizes the task-specific rule sets and the prior distribution. Experiments using this algorithm show that transferring information from related tasks significantly reduces the amount of training data required to predict action effects in blocks-world domains.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "Cooperative games are those in which both agents share the same payoff structure. Value-based reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policy-search method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "In order for an agent to perform well in partially observable domains, it is usually necessary for actions to depend on the history of observations. In this paper, we explore a {\\it stigmergic} approach, in which the agent's actions include the ability to set and clear bits in an external memory, and the external memory is included as part of the input to the agent. In this case, we need to learn a reactive policy in a highly non-Markovian domain. We explore two algorithms: SARSA(\u03bb), which has had empirical success in partially observable domains, and VAPS, a new algorithm due to Baird and Moore, with convergence guarantees in partially observable domains. We compare the performance of these two algorithms on benchmark problems.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.\n        \u25b3 Less", "author": "Leslie Kaelbling"}, {"abstract": "This paper formulates a novel problem on graphs: find the minimal subset of edges in a fully connected graph, such that the resulting graph contains all spanning trees for a set of specifed sub-graphs. This formulation is motivated by an un-supervised grammar induction problem from computational linguistics. We present a reduction to some known problems and algorithms from graph theory, provide computational complexity results, and describe an approximation algorithm.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "It is shown that the requirements for high quality electron bunch generation and trapping from an underdense photocathode in plasma wakefield accelerators can be substantially relaxed through localizing it on a plasma density downramp. This depresses the phase velocity of the accelerating electric field until the generated electrons are in phase, allowing for trapping in shallow trapping potentials. As a consequence the underdense photocathode technique is applicable by a much larger number of accelerator facilities. Furthermore, dark current generation is effectively suppressed.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "Constructing a good conference schedule for a large multi-track conference needs to take into account the preferences and constraints of organizers, authors, and attendees. Creating a schedule which has fewer conflicts for authors and attendees, and thematically coherent sessions is a challenging task.\n  Cobi introduced an alternative approach to conference scheduling by engaging the community to play an active role in the planning process. The current Cobi pipeline consists of committee-sourcing and author-sourcing to plan a conference schedule. We further explore the design space of community-sourcing by introducing attendee-sourcing -- a process that collects input from conference attendees and encodes them as preferences and constraints for creating sessions and schedule. For CHI 2014, a large multi-track conference in human-computer interaction with more than 3,000 attendees and 1,000 authors, we collected attendees' preferences by making available all the accepted papers at the conference on a paper recommendation tool we built called Confer, for a period of 45 days before announcing the conference program (sessions and schedule). We compare the preferences marked on Confer with the preferences collected from Cobi's author-sourcing approach. We show that attendee-sourcing can provide insights beyond what can be discovered by author-sourcing. For CHI 2014, the results show value in the method and attendees' participation. It produces data that provides more alternatives in scheduling and complements data collected from other methods for creating coherent sessions and reducing conflicts.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "Synchronized, independently tunable and focused $\u03bc$J-class laser pulses are used to release multiple electron populations via photo-ionization inside an electron-beam driven plasma wave. By varying the laser foci in the laboratory frame and the position of the underdense photocathodes in the co-moving frame, the delays between the produced bunches and their energies are adjusted. The resulting multibunches have ultra-high quality and brightness, allowing for hitherto impossible bunch configurations such as spatially overlapping bunch populations with strictly separated energies, which opens up a new regime for light sources such as free-electron-lasers.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "Realtime crowdsourcing research has demonstrated that it is possible to recruit paid crowds within seconds by managing a small, fast-reacting worker pool. Realtime crowds enable crowd-powered systems that respond at interactive speeds: for example, cameras, robots and instant opinion polls. So far, these techniques have mainly been proof-of-concept prototypes: research has not yet attempted to understand how they might work at large scale or optimize their cost/performance trade-offs. In this paper, we use queueing theory to analyze the retainer model for realtime crowdsourcing, in particular its expected wait time and cost to requesters. We provide an algorithm that allows requesters to minimize their cost subject to performance requirements. We then propose and analyze three techniques to improve performance: push notifications, shared retainer pools, and precruitment, which involves recalling retainer workers before a task actually arrives. An experimental validation finds that precruited workers begin a task 500 milliseconds after it is posted, delivering results below the one-second cognitive threshold for an end-user to stay in flow.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "Crowdsourcing systems, in which numerous tasks are electronically distributed to numerous \"information piece-workers\", have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all such systems must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in an appropriate manner, e.g. majority voting.\n  In this paper, we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm, inspired by belief propagation and low-rank matrix approximation, significantly outperforms majority voting and, in fact, is optimal through comparison to an oracle that knows the reliability of every worker. Further, we compare our approach with a more general class of algorithms which can dynamically assign tasks. By adaptively deciding which questions to ask to the next arriving worker, one might hope to reduce uncertainty more efficiently. We show that, perhaps surprisingly, the minimum price necessary to achieve a target reliability scales in the same manner under both adaptive and non-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under both scenarios. This strongly relies on the fact that workers are fleeting and can not be exploited. Therefore, architecturally, our results suggest that building a reliable worker-reputation system is essential to fully harnessing the potential of adaptive designs.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible to task people with small jobs, such as labeling images or looking up phone numbers, via a programmatic interface. MTurk tasks for processing datasets with humans are currently designed with significant reimplementation of common workflows and ad-hoc selection of parameters such as price to pay per task. We describe how we have integrated crowds into a declarative workflow engine called Qurk to reduce the burden on workflow designers. In this paper, we focus on how to use humans to compare items for sorting and joining data, two of the most common operations in DBMSs. We describe our basic query interface and the user interface of the tasks we post to MTurk. We also propose a number of optimizations, including task batching, replacing pairwise comparisons with numerical ratings, and pre-filtering tables before joining them, which dramatically reduce the overall cost of running sorts and joins on the crowd. In an experiment joining two sets of images, we reduce the overall cost from $67 in a naive implementation to about $3, without substantially affecting accuracy or latency. In an end-to-end experiment, we reduced cost by a factor of 14.5.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We present an algorithm for generating Poisson-disc patterns taking O(N) time to generate $N$ points. The method is based on a grid of regions which can contain no more than one point in the final pattern, and uses an explicit model of point arrival times under a uniform Poisson process.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We use network coding to improve the speed of distributed computation in the dynamic network model of Kuhn, Lynch and Oshman [STOC '10]. In this model an adversary adaptively chooses a new network topology in every round, making even basic distributed computations challenging.\n  Kuhn et al. show that n nodes, each starting with a d-bit token, can broadcast them to all nodes in time O(n^2) using b-bit messages, where b > d + log n. Their algorithms take the natural approach of {token forwarding}: in every round each node broadcasts some particular token it knows. They prove matching Omega(n^2) lower bounds for a natural class of token forwarding algorithms and an Omega(n log n) lower bound that applies to all token-forwarding algorithms.\n  We use network coding, transmitting random linear combinations of tokens, to break both lower bounds. Our algorithm's performance is quadratic in the message size b, broadcasting the n tokens in roughly d/b^2 * n^2 rounds. For b = d = O(log n) our algorithms use O(n^2/log n) rounds, breaking the first lower bound, while for larger message sizes we obtain linear-time algorithms. We also consider networks that change only every T rounds, and achieve an additional factor T^2 speedup. This contrasts with related lower and upper bounds of Kuhn et al. implying that for natural token-forwarding algorithms a speedup of T, but not more, can be obtained. Lastly, we give a general way to derandomize random linear network coding, that also leads to new deterministic information dissemination algorithms.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "This paper presents improved approximation algorithms for the problem of multiprocessor scheduling under uncertainty, or SUU, in which the execution of each job may fail probabilistically. This problem is motivated by the increasing use of distributed computing to handle large, computationally intensive tasks. In the SUU problem we are given n unit-length jobs and m machines, a directed acyclic graph G of precedence constraints among jobs, and unrelated failure probabilities q_{ij} for each job j when executed on machine i for a single timestep. Our goal is to find a schedule that minimizes the expected makespan, which is the expected time at which all jobs complete.\n  Lin and Rajaraman gave the first approximations for this NP-hard problem for the special cases of independent jobs, precedence constraints forming disjoint chains, and precedence constraints forming trees. In this paper, we present asymptotically better approximation algorithms. In particular, we give an O(loglog min(m,n))-approximation for independent jobs (improving on the previously best O(log n)-approximation). We also give an O(log(n+m) loglog min(m,n))-approximation algorithm for precedence constraints that form disjoint chains (improving on the previously best O(log(n)log(m)log(n+m)/loglog(n+m))-approximation by a (log n/loglog n)^2 factor when n = poly(m). Our algorithm for precedence constraints forming chains can also be used as a component for precedence constraints forming trees, yielding a similar improvement over the previously best algorithms for trees.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We examine the issue of separation and code design for networks that operate over finite fields. We demonstrate that source-channel (or source-network) separation holds for several canonical network examples like the noisy multiple access channel and the erasure degraded broadcast channel, when the whole network operates over a common finite field. This robustness of separation is predicated on the fact that noise and inputs are independent, and we examine the failure of separation when noise is dependent on inputs in multiple access channels.\n  Our approach is based on the sufficiency of linear codes. Using a simple and unifying framework, we not only re-establish with economy the optimality of linear codes for single-transmitter, single-receiver channels and for Slepian-Wolf source coding, but also establish the optimality of linear codes for multiple access and for erasure degraded broadcast channels. The linearity allows us to obtain simple optimal code constructions and to study capacity regions of the noisy multiple access and the degraded broadcast channel. The linearity of both source and network coding blurs the delineation between source and network codes. While our results point to the fact that separation of source coding and channel coding is optimal in some canonical networks, we show that decomposing networks into canonical subnetworks may not be effective. Thus, we argue that it may be the lack of decomposability of a network into canonical network modules, rather than the lack of separation between source and channel coding, that presents major challenges for coding over networks.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We consider the problem of establishing minimum-cost multicast connections over coded packet networks, i.e. packet networks where the contents of outgoing packets are arbitrary, causal functions of the contents of received packets. We consider both wireline and wireless packet networks as well as both static multicast (where membership of the multicast group remains constant for the duration of the connection) and dynamic multicast (where membership of the multicast group changes in time, with nodes joining and leaving the group).\n  For static multicast, we reduce the problem to a polynomial-time solvable optimization problem, and we present decentralized algorithms for solving it. These algorithms, when coupled with existing decentralized schemes for constructing network codes, yield a fully decentralized approach for achieving minimum-cost multicast. By contrast, establishing minimum-cost static multicast connections over routed packet networks is a very difficult problem even using centralized computation, except in the special cases of unicast and broadcast connections.\n  For dynamic multicast, we reduce the problem to a dynamic programming problem and apply the theory of dynamic programming to suggest how it may be solved.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We improve on random sampling techniques for approximately solving problems that involve cuts and flows in graphs. We give a near-linear-time construction that transforms any graph on n vertices into an O(n\\log n)-edge graph on the same vertices whose cuts have approximately the same value as the original graph's. In this new graph, for example, we can run the O(m^{3/2})-time maximum flow algorithm of Goldberg and Rao to find an s--t minimum cut in O(n^{3/2}) time. This corresponds to a (1+epsilon)-times minimum s--t cut in the original graph. In a similar way, we can approximate a sparsest cut to within O(log n) in O(n^2) time using a previous O(mn)-time algorithm. A related approach leads to a randomized divide and conquer algorithm producing an approximately maximum flow in O(m sqrt{n}) time.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "The multiway-cut problem is, given a weighted graph and k >= 2 terminal nodes, to find a minimum-weight set of edges whose removal separates all the terminals. The problem is NP-hard, and even NP-hard to approximate within 1+delta for some small delta > 0.\n  Calinescu, Karloff, and Rabani (1998) gave an algorithm with performance guarantee 3/2-1/k, based on a geometric relaxation of the problem. In this paper, we give improved randomized rounding schemes for their relaxation, yielding a 12/11-approximation algorithm for k=3 and a 1.3438-approximation algorithm in general.\n  Our approach hinges on the observation that the problem of designing a randomized rounding scheme for a geometric relaxation is itself a linear programming problem. The paper explores computational solutions to this problem, and gives a proof that for a general class of geometric relaxations, there are always randomized rounding schemes that match the integrality gap.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We consider the problem of coloring k-colorable graphs with the fewest possible colors. We present a randomized polynomial time algorithm that colors a 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log n), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any vertex. Besides giving the best known approximation ratio in terms of n, this marks the first non-trivial approximation result as a function of the maximum degree Delta. This result can be generalized to k-colorable graphs to obtain a coloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)} log^{1/2} n) colors. Our results are inspired by the recent work of Goemans and Williamson who used an algorithm for semidefinite optimization problems, which generalize linear programs, to obtain improved approximations for the MAX CUT and MAX 2-SAT problems. An intriguing outcome of our work is a duality relationship established between the value of the optimum solution to our semidefinite program and the Lovasz theta-function. We show lower bounds on the gap between the optimum solution of our semidefinite program and the actual chromatic number; by duality this also demonstrates interesting new facts about the theta-function.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "We significantly improve known time bounds for solving the minimum cut problem on undirected graphs. We use a ``semi-duality'' between minimum cuts and maximum spanning tree packings combined with our previously developed random sampling techniques. We give a randomized algorithm that finds a minimum cut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We also give a simpler randomized algorithm that finds all minimum cuts with high probability in O(n^2 log n) time. This variant has an optimal RNC parallelization. Both variants improve on the previous best time bound of O(n^2 log^3 n). Other applications of the tree-packing approach are new, nearly tight bounds on the number of near minimum cuts a graph may have and a new data structure for representing them in a space-efficient manner.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "The classic all-terminal network reliability problem posits a graph, each of whose edges fails independently with some given probability.\n        \u25b3 Less", "author": "David Karger"}, {"abstract": "There is much interest in integrating millimeter wave radios (mmWave) into wireless LANs and 5G cellular networks to benefit from their multiple GHz of available spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios require highly directional antennas. Since the antennas have pencil-beams, the transmitter and receiver need to align their antenna beams before they can communicate. Existing solutions scan the entire space to find the best alignment. Such a process has been shown to introduce up to seconds of delay, and is unsuitable for wireless networks where an access point has to quickly switch between users and accommodate mobile clients.\n  This paper presents Rapid-Link, a new protocol that can find the best mmWave beam alignment without scanning the space. Given all possible directions for setting the antenna beam, Rapid-Link provably finds the optimal direction in logarithmic number of measurements. Further, Rapid-Link works within the existing 802.11ad standard for mmWave LAN, and can support both clients and access points. We have implemented Rapid-Link in a mmWave radio and evaluated it empirically. Our results show that it reduces beam alignment delay by orders of magnitude. In particular, for highly directional mmWave devices operating under 802.11ad, the delay drops from over a second to 2.5 ms.\n        \u25b3 Less", "author": "Dina Katabi"}, {"abstract": "Many sensor applications are interested in computing a function over measurements (e.g., sum, average, max) as opposed to collecting all sensor data. Today, such data aggregation is done in a cluster-head. Sensor nodes transmit their values sequentially to a cluster-head node, which calculates the aggregation function and forwards it to the base station. In contrast, this paper explores the possibility of computing a desired function over the air. We devise a solution that enables sensors to transmit coherently over the wireless medium so that the cluster-head directly receives the value of the desired function. We present analysis and preliminary results that demonstrate that such a design yield a large improvement in network throughput.\n        \u25b3 Less", "author": "Dina Katabi"}, {"abstract": "Time-of-flight, i.e., the time incurred by a signal to travel from transmitter to receiver, is perhaps the most intuitive way to measure distances using wireless signals. It is used in major positioning systems such as GPS, RADAR, and SONAR. However, attempts at using time-of-flight for indoor localization have failed to deliver acceptable accuracy due to fundamental limitations in measuring time on Wi-Fi and other RF consumer technologies. While the research community has developed alternatives for RF-based indoor localization that do not require time-of-flight, those approaches have their own limitations that hamper their use in practice. In particular, many existing approaches need receivers with large antenna arrays while commercial Wi-Fi nodes have two or three antennas. Other systems require fingerprinting the environment to create signal maps. More fundamentally, none of these methods support indoor positioning between a pair of Wi-Fi devices without~third~party~support.\n  In this paper, we present a set of algorithms that measure the time-of-flight to sub-nanosecond accuracy on commercial Wi-Fi cards. We implement these algorithms and demonstrate a system that achieves accurate device-to-device localization, i.e. enables a pair of Wi-Fi devices to locate each other without any support from the infrastructure, not even the location of the access points.\n        \u25b3 Less", "author": "Dina Katabi"}, {"abstract": "We present the first sample-optimal sublinear time algorithms for the sparse Discrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our algorithms are analyzed for /average case/ signals. For signals whose spectrum is exactly sparse, our algorithms use O(k) samples and run in O(k log k) time, where k is the expected sparsity of the signal. For signals whose spectrum is approximately sparse, our algorithm uses O(k log n) samples and runs in O(k log^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of samples used by our algorithms matches the known lower bounds for the respective signal models.\n  By a known reduction, our algorithms give similar results for the one-dimensional sparse Discrete Fourier Transform when n is a power of a small composite number (e.g., n = 6^t).\n        \u25b3 Less", "author": "Dina Katabi"}, {"abstract": "We consider the problem of computing the k-sparse approximation to the discrete Fourier transform of an n-dimensional signal. We show:\n  * An O(k log n)-time randomized algorithm for the case where the input signal has at most k non-zero Fourier coefficients, and\n  * An O(k log n log(n/k))-time randomized algorithm for general input signals.\n  Both algorithms achieve o(n log n) time, and thus improve over the Fast Fourier Transform, for any k = o(n). They are the first known algorithms that satisfy this property. Also, if one assumes that the Fast Fourier Transform is optimal, the algorithm for the exactly k-sparse case is optimal for any k = n^{\u03a9(1)}.\n  We complement our algorithmic results by showing that any algorithm for computing the sparse Fourier transform of a general signal must use at least \u03a9(k log(n/k)/ log log n) signal samples, even if it is allowed to perform adaptive sampling.\n        \u25b3 Less", "author": "Dina Katabi"}, {"abstract": "Artificial Intelligence (AI) incorporating genetic and medical information have been applied in disease risk prediction, unveiling disease mechanism, and advancing therapeutics. However, AI training relies on highly sensitive and private data which significantly limit their applications and robustness evaluation. Moreover, the data access management after sharing across organization heavily relies on legal restriction, and there is no guarantee in preventing data leaking after sharing. Here, we present Genie, a secure AI platform which allows AI models to be trained on medical data securely. The platform combines the security of Intel Software Guarded eXtensions (SGX), transparency of blockchain technology, and verifiability of open algorithms and source codes. Genie shares insights of genetic and medical data without exposing anyone's raw data. All data is instantly encrypted upon upload and contributed to the models that the user chooses. The usage of the model and the value generated from the genetic and health data will be tracked via a blockchain, giving the data transparent and immutable ownership.\n        \u25b3 Less", "author": "Manolis Kellis"}, {"abstract": "Electronic health records (EHR) are rich heterogeneous collection of patient health information, whose broad adoption provides great opportunities for systematic health data mining. However, heterogeneous EHR data types and biased ascertainment impose computational challenges. Here, we present mixEHR, an unsupervised generative model integrating collaborative filtering and latent topic models, which jointly models the discrete distributions of data observation bias and actual data using latent disease-topic distributions. We apply mixEHR on 12.8 million phenotypic observations from the MIMIC dataset, and use it to reveal latent disease topics, interpret EHR results, impute missing data, and predict mortality in intensive care units. Using both simulation and real data, we show that mixEHR outperforms previous methods and reveals meaningful multi-disease insights.\n        \u25b3 Less", "author": "Manolis Kellis"}, {"abstract": "Several significant models have been developed that enable the study of diffusion of signals across biological, social and engineered networks. Within these established frameworks, the inverse problem of identifying the source of the propagated signal is challenging, owing to the numerous alternative possibilities for signal progression through the network. In real world networks, the challenge of determining sources is compounded as the true propagation dynamics are typically unknown, and when they have been directly measured, they rarely conform to the assumptions of any of the well-studied models. In this paper we introduce a method called Network Infusion (NI) that has been designed to circumvent these issues, making source inference practical for large, complex real world networks. The key idea is that to infer the source node in the network, full characterization of diffusion dynamics, in many cases, may not be necessary. This objective is achieved by creating a diffusion kernel that well-approximates standard diffusion models, but lends itself to inversion, by design, via likelihood maximization or error minimization. We apply NI for both single-source and multi-source diffusion, for both single-snapshot and multi-snapshot observations, and for both homogeneous and heterogeneous diffusion setups. We prove the mean-field optimality of NI for different scenarios, and demonstrate its effectiveness over several synthetic networks. Moreover, we apply NI to a real-data application, identifying news sources in the Digg social network, and demonstrate the effectiveness of NI compared to existing methods. Finally, we propose an integrative source inference framework that combines NI with a distance centrality-based method, which leads to a robust performance in cases where the underlying dynamics are unknown.\n        \u25b3 Less", "author": "Manolis Kellis"}, {"abstract": "We introduce Network Maximal Correlation (NMC) as a multivariate measure of nonlinear association among random variables. NMC is defined via an optimization that infers transformations of variables by maximizing aggregate inner products between transformed variables. For finite discrete and jointly Gaussian random variables, we characterize a solution of the NMC optimization using basis expansion of functions over appropriate basis functions. For finite discrete variables, we propose an algorithm based on alternating conditional expectation to determine NMC. Moreover we propose a distributed algorithm to compute an approximation of NMC for large and dense graphs using graph partitioning. For finite discrete variables, we show that the probability of discrepancy greater than any given level between NMC and NMC computed using empirical distributions decays exponentially fast as the sample size grows. For jointly Gaussian variables, we show that under some conditions the NMC optimization is an instance of the Max-Cut problem. We then illustrate an application of NMC in inference of graphical model for bijective functions of jointly Gaussian variables. Finally, we show NMC's utility in a data application of learning nonlinear dependencies among genes in a cancer dataset.\n        \u25b3 Less", "author": "Manolis Kellis"}, {"abstract": "Graph alignment refers to the problem of finding a bijective mapping across vertices of two graphs such that, if two nodes are connected in the first graph, their images are connected in the second graph. This problem arises in many fields such as computational biology, social sciences, and computer vision and is often cast as a quadratic assignment problem (QAP). Most standard graph alignment methods consider an optimization that maximizes the number of matches between the two graphs, ignoring the effect of mismatches. We propose a generalized graph alignment formulation that considers both matches and mismatches in a standard QAP formulation. This modification can have a major impact in aligning graphs with different sizes and heterogenous edge densities. Moreover, we propose two methods for solving the generalized graph alignment problem based on spectral decomposition of matrices. We compare the performance of proposed methods with some existing graph alignment algorithms including Natalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite programming-based method over various synthetic and real graph models. Our proposed method based on simultaneous alignment of multiple eigenvectors leads to consistently good performance in different graph models. In particular, in the alignment of regular graph structures which is one of the most difficult graph alignment cases, our proposed method significantly outperforms other methods.\n        \u25b3 Less", "author": "Manolis Kellis"}, {"abstract": "Information theory is rapidly approaching its 70th birthday. What are promising future directions for research in information theory? Where will information theory be having the most impact in 10-20 years? What new and emerging areas are ripe for the most impact, of the sort that information theory has had on the telecommunications industry over the last 60 years? How should the IEEE Information Theory Society promote high-risk new research directions and broaden the reach of information theory, while continuing to be true to its ideals and insisting on the intellectual rigor that makes its breakthroughs so powerful? These are some of the questions that an ad hoc committee (composed of the present authors) explored over the past two years. We have discussed and debated these questions, and solicited detailed inputs from experts in fields including genomics, biology, economics, and neuroscience. This report is the result of these discussions.\n        \u25b3 Less", "author": "Manolis Kellis"}, {"abstract": "We propose a novel diverse feature selection method based on determinantal point processes (DPPs). Our model enables one to flexibly define diversity based on the covariance of features (similar to orthogonal matching pursuit) or alternatively based on side information. We introduce our approach in the context of Bayesian sparse regression, employing a DPP as a variational approximation to the true spike and slab posterior distribution. We subsequently show how this variational DPP approximation generalizes and extends mean-field approximation, and can be learned efficiently by exploiting the fast sampling properties of DPPs. Our motivating application comes from bioinformatics, where we aim to identify a diverse set of genes whose expression profiles predict a tumor type where the diversity is defined with respect to a gene-gene interaction network. We also explore an application in spatial statistics. In both cases, we demonstrate that the proposed method yields significantly more diverse feature sets than classic sparse methods, without compromising accuracy.\n        \u25b3 Less", "author": "Manolis Kellis"}, {"abstract": "Through a combination of rheological characterization and temperature-variable imaging methods, a novel gelation pathway in dilute solutions of a semiconducting polymer to achieve interconnected, crystalline networks with hierarchical porosity is reported. Upon rapid cooling, solutions of regioregular poly(3-hexylthiophene) (RR-P3HT) in ortho-dichlorobenzene formed thermoreversible gels. Temperature-variable confocal microscopy revealed cooling-induced structural rearrangement to progress through viscoelastic phase separation. The phase separation process arrested prematurely during the formation of micron-sized solvent-rich \"holes\" within the RR-P3HT matrix due to intrachain crystallization. Cryogen-based scanning electron microscopy of RR P3HT gels revealed the existence of an interfibrillar network exhibiting nano-sized pores. Remarkably, these networks formed to equal gel strengths when a third component, either small molecule phenyl C61 butyric acid methyl ester (PCBM) or non-crystallizing regiorandom (Rra)-P3HT, was added to the solution. Organic solar cells in which the active layers were deposited from phase-separated solutions displayed 45% higher efficiency compared to reference cells.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "In this paper we generalize Drinfeld's twisted quantum affine algebras to construct twisted quantum algebras for all simply-laced generalized Cartan matrices and present their vertex representation realizations.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Property by design is one appealing idea in material synthesis but hard to achieve in practice. A recent successful example is the demonstration of van der Waals (vdW) heterostructures,1-3 in which atomic layers are stacked on each other and different ingredients can be combined beyond symmetry and lattice matching. This concept, usually described as a nanoscale Lego blocks, allows to build sophisticated structures layer by layer. However, this concept has been so far limited in two dimensional (2D) materials. Here we show a class of new material where different layers are coaxially (instead of planarly) stacked. As the structure is in one dimensional (1D) form, we name it \"1D vdW heterostructures\". We demonstrate a 5 nm diameter nanotube consisting of three different materials: an inner conductive carbon nanotube (CNT), a middle insulating hexagonal boron nitride nanotube (BNNT) and an outside semiconducting MoS2 nanotube. As the technique is highly applicable to other materials in the current 2D libraries,4-6 we anticipate our strategy to be a starting point for discovering a class of new semiconducting nanotube materials. A plethora of function-designable 1D heterostructures will appear after the combination of CNTs, BNNTs and semiconducting nanotubes.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Second-order nonlinear optical interactions, including second harmonic generation (SHG) and sum-frequency generation (SFG), can reveal a wealth of information about chemical, electronic, and vibrational dynamics at the nanoscale. Here, we demonstrate a powerful and flexible new approach, called phase-modulated degenerate parametric amplification (DPA). The technique, which allows for facile retrieval of both the amplitude and phase of the second-order nonlinear optical response, has many advantages over conventional or heterodyne-detected SHG, including the flexibility to detect the signal at either the second harmonic or fundamental field wavelength. We demonstrate the capabilities of this approach by imaging multi-grain flakes of single-layer MoS2. We identify the absolute crystal orientation of each MoS2 domain and resolve grain boundaries with high signal contrast and sub-diffraction-limited spatial resolution. This robust all-optical method can be used to characterize structure and dynamics in organic and inorganic systems, including biological tissue, soft materials, and metal and semiconductor nanostructures, and is particularly well-suited for imaging in media that are absorptive or highly scattering to visible and ultraviolet light.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "We report a rare atom-like interaction between excitons in monolayer WS2, measured using ultrafast absorption spectroscopy. At increasing excitation density, the exciton resonance energy exhibits a pronounced redshift followed by an anomalous blueshift. Using both material-realistic computation and phenomenological modeling, we attribute this observation to plasma effects and an attraction-repulsion crossover of the exciton-exciton interaction that mimics the Lennard-Jones potential between atoms. Our experiment demonstrates a strong analogy between excitons and atoms with respect to inter-particle interaction, which holds promise to pursue the predicted liquid and crystalline phases of excitons in two-dimensional materials.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Previously known to form only under high pressure synthetic conditions, here we report that the T'-type 214-structure cuprate based on the rare earth atom Tb is stabilized for ambient pressure synthesis through partial substitution of Pd for Cu. The new material is obtained in purest form for mixtures of nominal composition $Tb_{1.96}Cu_{0.80}Pd_{0.20}O_{4}$. The refined formula, in orthorhombic space group Pbca, with a = 5.5117(1) \u00c5, b = 5.5088(1) \u00c5, and c = 11.8818(1) \u00c5, is $Tb_{2}Cu_{0.83}Pd_{0.17}O_{4}$. An incommensurate structural modulation is seen along the a axis by electron diffraction and high resolution imaging. Magnetic susceptibility measurements reveal long range antiferromagnetic ordering at 7.9 K, with a less pronounced feature at 95 K; a magnetic moment reorientation transition is observed to onset at a field of approximately 1.1 Tesla at 3 K. The material is an n-type semiconductor.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "The ability to confine light into tiny spatial dimensions is important for applications such as microscopy, sensing and nanoscale lasers. While plasmons offer an appealing avenue to confine light, Landau damping in metals imposes a trade-off between optical field confinement and losses. We show that a graphene-insulator-metal heterostructure can overcome that trade-off, and demonstrate plasmon confinement down to the ultimate limit of the lengthscale of one atom. This is achieved by far-field excitation of plasmon modes squeezed into an atomically thin hexagonal boron nitride dielectric h-BN spacer between graphene and metal rods. A theoretical model which takes into account the non-local optical response of both graphene and metal is used to describe the results. These ultra-confined plasmonic modes, addressed with far-field light excitation, enables a route to new regimes of ultra-strong light-matter interactions.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "By using the Hectospec 6.5 m Multiple Mirror Telescope (MMT) and the 2.16 m telescope of National Astronomical Observatories, Chinese Academy of Sciences (NAOC), we obtained 188 high signal-to-noise ratio (S/N) spectra of HII regions in the nearby galaxy M101, which are the largest spectroscopic sample of HII regions for this galaxy so far. These spectra cover a wide range of regions on M101, which enables us to analyze two dimensional distributions of its physical properties. The physical parameters are derived from emission lines or stellar continuum, including stellar population age, electron temperature, oxygen abundance and etc. The oxygen abundances are derived using two empirical methods based on O3N2 and R$_{23}$ indicators, as well as the direct Te method when OIII$\\lambda4363$ is available. By applying the harmonic decomposition analysis to the velocity field, we obtained line-of-sight rotation velocity of 71 km s$^{-1}$ and a position angle of 36 degree. The stellar age profile shows an old stellar population in galaxy center and a relative young stellar population in outer regions, suggesting an old bulge and a young disk. Oxygen abundance profile exhibits a clear break at $\\sim$18 kpc, with a gradient of $-$0.0364 dex kpc$^{-1}$ in the inner region and $-$0.00686 dex kpc$^{-1}$ in the outer region. Our results agree with the \"inside-out\" disk growth scenario of M101.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Atomic-level structural changes in materials are important but challenging to study. Here, we demonstrate the dynamics and the possibility of manipulating a phosphorus dopant atom in graphene using scanning transmission electron microscopy (STEM). The mechanisms of various processes are explored and compared with those of other dopant species by first-principles calculations. This work paves the way for designing a more precise and optimized protocol for atomic engineering.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "The valley pseudospin in monolayer transition metal dichalcogenides (TMDs) has been proposed as a new way to manipulate information in various optoelectronic devices. This relies on a large valley polarization that remains stable over long timescales (hundreds of ns). However, time resolved measurements report valley lifetimes of only a few ps. This has been attributed to mechanisms such as phonon-mediated inter-valley scattering and a precession of the valley psedospin through electron-hole exchange. Here we use transient spin grating to directly measure the valley depolarization lifetime in monolayer MoSe$_{2}$. We find a fast valley decay rate that scales linearly with the excitation density at different temperatures. This establishes the presence of strong exciton-exciton Coulomb exchange interactions enhancing the valley depolarization. Our work highlights the microscopic processes inhibiting the efficient use of the exciton valley pseudospin in monolayer TMDs.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "The abundance of neutral hydrogen (HI) in satellite galaxies in the Local Group is important for studying the formation history of our Local Group. In this work, we generated mock HI satellite galaxies in the Local Group using the high mass resolution hydrodynamic \\textsc{apostle} simulation. The simulated HI mass function agrees with the ALFALFA survey very well above $10^6M_{\\odot}$, although there is a discrepancy below this scale because of the observed flux limit. After carefully checking various systematic elements in the observations, including fitting of line width, sky coverage, integration time, and frequency drift due to uncertainty in a galaxy's distance, we predicted the abundance of HI in galaxies in a future survey that will be conducted by FAST. FAST has a larger aperture and higher sensitivity than the Arecibo telescope. We found that the HI mass function could be estimated well around $10^5 M_{\\odot}$ if the integration time is 40 minutes. Our results indicate that there are 61 HI satellites in the Local Group, and 36 in the FAST field above $10^5 M_{\\odot}$. This estimation is one order of magnitude better than the current data, and will put a strong constraint on the formation history of the Local Group. Also more high resolution simulated samples are needed to achieve this target.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "We present a spectroscopic redshift catalog from the LAMOST Complete Spectroscopic Survey of Pointing Area (LaCoSSPAr) in the Southern Galactic Cap (SGC), which is designed to observe all sources (Galactic and extra-galactic) by using repeating observations with a limiting magnitude of $r=18.1~mag$ in two $20~deg^2$ fields. The project is mainly focusing on the completeness of LAMOST ExtraGAlactic Surveys (LEGAS) in the SGC, the deficiencies of source selection methods and the basic performance parameters of LAMOST telescope. In both fields, more than 95% of galaxies have been observed. A post-processing has been applied to LAMOST 1D spectrum to remove the majority of remaining sky background residuals. More than 10,000 spectra have been visually inspected to measure the redshift by using combinations of different emission/absorption features with uncertainty of $\u03c3_{z}/(1+z)<0.001$. In total, there are 1528 redshifts (623 absorption and 905 emission line galaxies) in Field A and 1570 redshifts (569 absorption and 1001 emission line galaxies) in Field B have been measured. The results show that it is possible to derive redshift from low SNR galaxies with our post-processing and visual inspection. Our analysis also indicates that up to 1/4 of the input targets for a typical extra-galactic spectroscopic survey might be unreliable. The multi-wavelength data analysis shows that the majority of mid-infrared-detected absorption (91.3%) and emission line galaxies (93.3%) can be well separated by an empirical criterion of $W2-W3=2.4$. Meanwhile, a fainter sequence paralleled to the main population of galaxies has been witnessed both in $M_r$/$W2-W3$ and $M_*$/$W2-W3$ diagrams, which could be the population of luminous dwarf galaxies but contaminated by the edge-on/highly inclined galaxies ($\\sim30\\%$).\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Recently, topological semimetals become hot topic in condensed matter physics, including Dirac semimetal, Weyl semimetal, and nodal line semimetal (NLSM). In this paper, a new type of node- line semimetal - type-II NLSM is proposed based on a two-band cubic lattice model. For type-II NLSM, the zero energy bulk states have a closed loop in momentum space but the (local) Weyl cones on nodal line become tilted. The effect of magnetic field and that of correlation on type-II NLSM are studied. In particular, after considering repulsive interaction and additional spin degrees of freedom, different types of long range magnetic orders appear in bulk states. In addition, the interaction-induced ferromagnetic order of surface states may exist. At critical point between type-I NLSM and type-II NLSM, arbitrary tiny interaction induces ferromagnetic order due to a flat band at Fermi surface.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "High-performance materials rely on small reorganization energies to facilitate both charge separation and charge transport. Here, we performed DFT calculations to predict small reorganization energies of rectangular silicene nanoclusters with hydrogen-passivated edges denoted by H-SiNC. We observe that across all geometries, H-SiNCs feature large electron affinities and highly stabilized anionic states, indicating their potential as n-type materials. Our findings suggest that fine-tuning the size of H-SiNCs along the zigzag and armchair directions may permit the design of novel n-type electronic materials and spinctronics devices that incorporate both high electron affinities and very low internal reorganization energies.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Porous materials provide a large surface to volume ratio, thereby providing a knob to alter fundamental properties in unprecedented ways. In thermal transport, porous nanomaterials can reduce thermal conductivity by not only enhancing phonon scattering from the boundaries of the pores and therefore decreasing the phonon mean free path, but also by reducing the phonon group velocity. Here we establish a structure-property relationship by measuring the porosity and thermal conductivity of individual electrolessly etched single crystalline silicon nanowires using a novel electron beam heating technique. Such porous silicon nanowires exhibit extremely low diffusive thermal conductivity (as low as 0.33 Wm-1K-1 at 300K for 43% porosity), even lower than that of amorphous silicon. The origin of such ultralow thermal conductivity is understood as a reduction in the phonon group velocity, experimentally verified by measuring the Young modulus, as well as the smallest structural size ever reported in crystalline Silicon (less than 5nm). Molecular dynamics simulations support the observation of a drastic reduction in thermal conductivity of silicon nanowires as a function of porosity. Such porous materials provide an intriguing platform to tune phonon transport, which can be useful in the design of functional materials towards electronics and nano-electromechanical systems.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "In this paper, we report the peculiar HI morphology of the cluster spiral galaxy NGC 6145, which has a 150 kpc HI filament on one side that is nearly parallel to its major axis. This filament is made up of several HI clouds and the diffuse HI gas between them, with no optical counterparts. We compare its HI distribution with other one-sided HI distributions in the literature, and find that the overall HI distribution is very different from the typical tidal and ram-pressure stripped HI shape, and its morphology is inconsistent with being a pure accretion event. Only about 30% of the total HI gas is anchored on the stellar disk, while most of HI gas forms the filament in the west. At a projected distance of 122 kpc, we find a massive elliptical companion (NGC 6146) with extended radio emission, whose axis points to an HI gap in NGC 6145. The velocity of the HI filament shows an overall light-of- sight motion of 80 to 180 km/s with respect to NGC 6145. Using the long-slit spectra of NGC 6145 along its major stellar axis, we find that some outer regions show enhanced star formation, while in contrast, almost no star formation activities are found in its center (less than 2 kpc). Pure accretion, tidal or ram-pressure stripping is not likely to produce the observed HI filament. An alternative explanation is the jet-stripping from NGC 6146, although direct evidence for a jet-cold gas interaction has not been found.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "The spectra of 413 star-forming (or HII) regions in M33 (NGC 598) were observed by using the multifiber spectrograph of Hectospec at the 6.5-m Multiple Mirror Telescope (MMT). By using this homogeneous spectra sample, we measured the intensities of emission lines and some physical parameters, such as electron temperatures, electron densities, and metallicities. Oxygen abundances were derived via the direct method (when available) and two empirical strong-line methods, namely, O3N2 and N2. In the high-metallicity end, oxygen abundances derived from O3N2 calibration were higher than those derived from N2 index, indicating an inconsistency between O3N2 and N2 calibrations. We presented a detailed analysis of the spatial distribution of gas-phase oxygen abundances in M33 and confirmed the existence of the axisymmetric global metallicity distribution widely assumed in literature. Local variations were also observed and subsequently associated with spiral structures to provide evidence of radial migration driven by arms. Our O/H gradient fitted out to 1.1 $R_{25}$ resulted in slopes of $-0.17\\pm0.03$, $-0.19\\pm0.01$, and $-0.16\\pm0.17$ dex $R_{25}^{-1}$ utilizing abundances from O3N2, N2 diagnostics, and direct method, respectively.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Coherent light-matter interaction can be used to manipulate the energy levels of atoms, molecules and solids. When light with frequency \u03c9 is detuned away from a resonance \u03c9o, repulsion between the photon-dressed (Floquet) states can lead to a shift of energy resonance. The dominant effect is the optical Stark shift (1/(\u03c90-\u03c9)), but there is an additional contribution from the so-called Bloch-Siegert shift (1/(\u03c9o+\u03c9)). Although it is common in atoms and molecules, the observation of Bloch-Siegert shift in solids has so far been limited only to artificial atoms since the shifts were small (<1 \u03bceV) and inseparable from the optical Stark shift. Here we observe an exceptionally large Bloch-Siegert shift (~10 meV) in monolayer WS2 under infrared optical driving by virtue of the strong light-matter interaction in this system. Moreover, we can disentangle the Bloch-Siegert shift entirely from the optical Stark shift, because the two effects are found to obey opposite selection rules at different valleys. By controlling the light helicity, we can confine the Bloch-Siegert shift to occur only at one valley, and the optical Stark shift at the other valley. Such a valley-exclusive Bloch-Siegert shift allows for enhanced control over the valleytronic properties in two-dimensional materials, and offers a new avenue to explore quantum optics in solids.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Coherent optical dressing of quantum materials offers technological advantages to control their electronic properties, such as the electronic valley degree of freedom in monolayer transition metal dichalcogenides (TMDs). Here, we observe a new type of optical Stark effect in monolayer WS2, one that is mediated by intervalley biexcitons under the blue-detuned driving with circularly polarized light. We found that such helical optical driving not only induces an exciton energy downshift at the excitation valley, but also causes an anomalous energy upshift at the opposite valley, which is normally forbidden by the exciton selection rules but now made accessible through the intervalley biexcitons. These findings reveal the critical, but hitherto neglected, role of biexcitons to couple the two seemingly independent valleys, and to enhance the optical control in valleytronics.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Two-dimensional (2-D) materials are of tremendous interest to integrated photonics given their singular optical characteristics spanning light emission, modulation, saturable absorption, and nonlinear optics. To harness their optical properties, these atomically thin materials are usually attached onto prefabricated devices via a transfer process. In this paper, we present a new route for 2-D material integration with planar photonics. Central to this approach is the use of chalcogenide glass, a multifunctional material which can be directly deposited and patterned on a wide variety of 2-D materials and can simultaneously function as the light guiding medium, a gate dielectric, and a passivation layer for 2-D materials. Besides claiming improved fabrication yield and throughput compared to the traditional transfer process, our technique also enables unconventional multilayer device geometries optimally designed for enhancing light-matter interactions in the 2-D layers. Capitalizing on this facile integration method, we demonstrate a series of high-performance glass-on-graphene devices including ultra-broadband on-chip polarizers, energy-efficient thermo-optic switches, as well as graphene-based mid-infrared (mid-IR) waveguide-integrated photodetectors and modulators.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Plasmons in graphene nanostructures show great promise for mid-infrared applications ranging from a few to tens of microns. However, mid-infrared plasmonic resonances in graphene nanostructures are usually weak and narrow-banded, limiting their potential in light manipulation and detection. Here we investigate the coupling among graphene plasmonic nanostructures and further show that by engineering the coupling, enhancement of light-graphene interaction strength and broadening of spectral width can be achieved simultaneously. Leveraging the concept of coupling, we demonstrate a hybrid 2-layer graphene nanoribbon array which shows 5 to 7% extinction within the entire 8 to 14 \u03bcm (~700 to 1250 cm-1) wavelength range, covering one of the important atmosphere \"infrared transmission windows\". Such coupled hybrid graphene plasmonic nanostructures may find applications in infrared sensing and free-space communications.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "In the effort to make 2D materials-based devices smaller, faster, and more efficient, it is important to control charge carrier at lengths approaching the nanometer scale. Traditional gating techniques based on capacitive coupling through a gate dielectric cannot generate strong and uniform electric fields at this scale due to divergence of the fields in dielectrics. This field divergence limits the gating strength, boundary sharpness, and pitch size of periodic structures, and restricts possible geometries of local gates (due to wire packaging), precluding certain device concepts, such as plasmonics and transformation optics based on metamaterials. Here we present a new gating concept based on a dielectric-free self-aligned electrolyte technique that allows spatially modulating charges with nanometer resolution. We employ a combination of a solid-polymer electrolyte gate and an ion-impenetrable e-beam-defined resist mask to locally create excess charges on top of the gated surface. Electrostatic simulations indicate high carrier density variations of $\u0394n =10^{14}\\text{cm}^{-2}$ across a length of 10 nm at the mask boundaries on the surface of a 2D conductor, resulting in a sharp depletion region and a strong in-plane electric field of $6\\times10^8 \\text{Vm}^{-1}$ across the so-created junction. We apply this technique to the 2D material graphene to demonstrate the creation of tunable p-n junctions for optoelectronic applications. We also demonstrate the spatial versatility and self-aligned properties of this technique by introducing a novel graphene thermopile photodetector.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Using a sample of ~6,000 local face-on star-forming galaxies (SFGs), we examine the correlations between the NUV-r colors both inside and outside the half-light radius, stellar mass M* and S\u00e9rsic index n in order to understand how the quenching of star formation is linked to galaxy structure. For these less dust-attenuated galaxies, NUV-r is found to be linearly correlated with Dn4000, supporting that NUV-r is a good photometric indicator of stellar age (or specific star formation rate). We find that: (1) At M*<10^{10.2}M_{\\sun}, the central NUV-r is on average only~ 0.25 mag redder than the outer NUV-r. The intrinsic value would be even smaller after accounting for dust correction. However, the central NUV-r becomes systematically much redder than the outer NUV-r for more massive galaxies at M*>10^{10.2}M_{\\sun}. (2) The central NUV-r shows no dependence on S\u00e9rsic index n at M*<10^{10.2}M_{\\sun}, while above this mass galaxies with a higher n tend to be redder in the central NUV-r color. These results suggest that galaxies with M*<10^{10.2}M_{\\sun} exhibit similar star formation activity from the inner R<R_{50} region to the R>R_{50} region. In contrast, a considerable fraction of the M*>10^{10.2}M_{\\sun} galaxies, especially those with a high n, have harbored a relatively inactive bulge component.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Ultrafast electron thermalization - the process leading to Auger recombination, carrier multiplication via impact ionization and hot carrier luminescence - occurs when optically excited electrons in a material undergo rapid electron-electron scattering to redistribute excess energy and reach electronic thermal equilibrium. Due to extremely short time and length scales, the measurement and manipulation of electron thermalization in nanoscale devices remains challenging even with the most advanced ultrafast laser techniques. Here, we overcome this challenge by leveraging the atomic thinness of two-dimensional van der Waals (vdW) materials in order to introduce a highly tunable electron transfer pathway that directly competes with electron thermalization. We realize this scheme in a graphene-boron nitride-graphene (G-BN-G) vdW heterostructure, through which optically excited carriers are transported from one graphene layer to the other. By applying an interlayer bias voltage or varying the excitation photon energy, interlayer carrier transport can be controlled to occur faster or slower than the intralayer scattering events, thus effectively tuning the electron thermalization pathways in graphene. Our findings, which demonstrate a novel means to probe and directly modulate electron energy transport in nanoscale materials, represent an important step toward designing and implementing novel optoelectronic and energy-harvesting devices with tailored microscopic properties.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Diverse parallel stitched two-dimensional heterostructures are synthesized, including metal-semiconductor (graphene-MoS2), semiconductor-semiconductor (WS2-MoS2), and insulator-semiconductor (hBN-MoS2), directly through selective sowing of aromatic molecules as the seeds in chemical vapor deposition (CVD) method. Our methodology enables the large-scale fabrication of lateral heterostructures with arbitrary patterns, and clean and precisely aligned interfaces, which offers tremendous potential for its application in integrated circuits.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "A small fraction($<10\\%$) of SDSS main sample galaxies(MGs) have not been targeted with spectroscopy due to the the fiber collision effect. These galaxies have been compiled into the input catalog of the LAMOST extra-galactic survey and named as the complementary galaxy sample. In this paper, we introduce the project and the status of the spectroscopies of the complementary galaxies in the first two years of the LAMOST spectral survey(till Sep. of 2014). Moreover, we present a sample of 1,102 galaxy pairs identified from the LAMOST complementary galaxies and SDSS MGs, which are defined as that the two members have a projected distance smaller than 100 kpc and the recessional velocity difference smaller than 500 $\\rm kms^{-1}$. Compared with the SDSS only selected galaxy pairs, the LAMOST-SDSS pairs take the advantages of not being biased toward large separations and therefor play as a useful supplement to the statistical studies of galaxy interaction and galaxy merging.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "It is unclear whether bulge growth is responsible for the flattening of the star formation main sequence (MS) at the high mass end. To investigate the role of bulges in shaping the MS, we compare the NUV$-r$ color between the central ($r<R_{50}$) and outer regions for a sample of 6401 local star-forming galaxies. The NUV$-r$ color is a good specific star formation rate indicator. We find that at $M_{\\ast}<10^{10.2}M_{\\sun}$, the central NUV$-r$ is on average only $\\sim$ 0.25 mag redder than the outer NUV$-r$. Above $M_{\\ast}=10^{10.2}M_{\\sun}$, the central NUV$-r$ becomes systematically much redder than the outer NUV$-r$ for more massive galaxies, indicating that the central bulge is more evolved at the massive end. When dividing the galaxies according to their S\u00e9rsic index $n$, we find that galaxies with $n$>2.0 tend to be redder in the central NUV$-r$ color than those with $n$<2.0, even at fixed B/T and $M_{\\ast}$. This suggests that star formation in bulges is more strongly dependent on $n$ (or central mass density) than on B/T. Finally, we find that the fraction of galaxies with $n$>2.0 rapidly increases with $M_{\\ast}$ at $M_{\\ast}>10^{10.2}M_{\\sun}$, which is consistent with the turning over of the MS at the same transition mass. We conclude that the increasing fraction of low-sSFR dense bulges in $M_{\\ast}>10^{10.2}M_{\\sun}$ galaxies, rather than increasing B/T, is responsible for the flattened slope of the $M_{\\ast}$$-$SFR relation at high masses.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "The Large sky Area Multi-Object Spectroscopic Telescope (LAMOST) General Survey is a spectroscopic survey that will eventually cover approximately half of the celestial sphere and collect 10 million spectra of stars, galaxies and QSOs. Objects both in the pilot survey and the first year general survey are included in the LAMOST First Data Release (DR1). The pilot survey started in October 2011 and ended in June 2012, and the data have been released to the public as the LAMOST Pilot Data Release in August 2012. The general survey started in September 2012, and completed its first year of operation in June 2013. The LAMOST DR1 includes a total of 1202 plates containing 2,955,336 spectra, of which 1,790,879 spectra have observed signal-to-noise S/N >10. All data with S/N>2 are formally released as LAMOST DR1 under the LAMOST data policy. This data release contains a total of 2,204,696 spectra, of which 1,944,329 are stellar spectra, 12,082 are galaxy spectra and 5,017 are quasars. The DR1 includes not only spectra, but also three stellar catalogues with measured parameters: AFGK-type stars with high quality spectra (1,061,918 entries), A-type stars (100,073 entries), and M stars (121,522 entries). This paper introduces the survey design, the observational and instrumental limitations, data reduction and analysis, and some caveats. Description of the FITS structure of spectral files and parameter catalogues is also provided.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "In this Letter, we investigate how galaxy mass assembly mode depends on stellar mass $M_{\\ast}$, using a large sample of $\\sim$10, 000 low redshift galaxies. Our galaxy sample is selected to have SDSS $R_{90}>5\\arcsec.0$, which allows the measures of both the integrated and the central NUV$-r$ color indices. We find that: in the $M_{\\ast}-($ NUV$-r$) green valley, the $M_{\\ast}<10^{10}~M_{\\sun}$ galaxies mostly have positive or flat color gradients, while most of the $M_{\\ast}>10^{10.5}~M_{\\sun}$ galaxies have negative color gradients. When their central $D_{n}4000$ index values exceed 1.6, the $M_{\\ast}<10^{10.0}~M_{\\sun}$ galaxies have moved to the UV red sequence, whereas a large fraction of the $M_{\\ast}>10^{10.5}~M_{\\sun}$ galaxies still lie on the UV blue cloud or the green valley region. We conclude that the main galaxy assembly mode is transiting from \"the outside-in\" mode to \"the inside-out\" mode at $M_{\\ast}< 10^{10}~M_{\\sun}$ and at $M_{\\ast}> 10^{10.5}~M_{\\sun}$. We argue that the physical origin of this is the compromise between the internal and the external process that driving the star formation quenching in galaxies. These results can be checked with the upcoming large data produced by the on-going IFS survey projects, such as CALIFA, MaNGA and SAMI in the near future.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Pure-jump processes have been increasingly popular in modeling high-frequency financial data, partially due to their versatility and flexibility. In the meantime, several statistical tests have been proposed in the literature to check the validity of using pure-jump models. However, these tests suffer from several drawbacks, such as requiring rather stringent conditions and having slow rates of convergence. In this paper, we propose a different test to check whether the underlying process of high-frequency data can be modeled by a pure-jump process. The new test is based on the realized characteristic function, and enjoys a much faster convergence rate of order $O(n^{1/2})$ (where $n$ is the sample size) versus the usual $o(n^{1/4})$ available for existing tests; it is applicable much more generally than previous tests; for example, it is robust to jumps of infinite variation and flexible modeling of the diffusion component. Simulation studies justify our findings and the test is also applied to some real high-frequency financial data.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "As a new two-dimensional layered material, black phosphorus (BP) is a promising material for nanoelectronics and nano-optoelectronics. We use Raman spectroscopy and first-principles theory to report our findings related to low-frequency (LF) interlayer breathing modes (<100 cm-1) in few-layer BP for the first time. The breathing modes are assigned to Ag symmetry by the laser polarization dependence study and group theory analysis. Compared to the high-frequency (HF) Raman modes, the LF breathing modes are much more sensitive to interlayer coupling and thus their frequencies show much stronger dependence on the number of layers. Hence, they could be used as effective means to probe both the crystalline orientation and thickness for few-layer BP. Furthermore, the temperature dependence study shows that the breathing modes have a harmonic behavior, in contrast to HF Raman modes which are known to exhibit anharmonicity.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "A joint measurement is presented of the branching fractions $B^0_s\\to\u03bc^+\u03bc^-$ and $B^0\\to\u03bc^+\u03bc^-$ in proton-proton collisions at the LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis produces the first observation of the $B^0_s\\to\u03bc^+\u03bc^-$ decay, with a statistical significance exceeding six standard deviations, and the best measurement of its branching fraction so far. Furthermore, evidence for the $B^0\\to\u03bc^+\u03bc^-$ decay is obtained with a statistical significance of three standard deviations. The branching fraction measurements are statistically compatible with SM predictions and impose stringent constraints on several theories beyond the SM.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "A single-term density functional model for nondynamic and strong correlation is presented, based on single-determinant Kohn-Sham density functional theory. It is derived from modeling the adiabatic connection and contains only two nonlinear empirical parameters. Preliminary tests show that the model recovers majority of nondynamic correlation during a molecular dissociation and at the same time performs reasonably for atomization energies. It demonstrates the feasibility of developing DFT functionals for nondynamic and strong correlation within the single-determinant KS scheme.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Variable selection is of increasing importance to address the difficulties of high dimensionality in many scientific areas. In this paper, we demonstrate a property for distance covariance, which is incorporated in a novel feature screening procedure together with the use of distance correlation. The approach makes no distributional assumptions for the variables and does not require the specification of a regression model, and hence is especially attractive in variable selection given an enormous number of candidate attributes without much information about the true model with the response. The method is applied to two genetic risk problems, where issues including uncertainty of variable selection via cross validation, subgroup of hard-to-classify cases and the application of a reject option are discussed.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Recently emerging large-area single-layer MoS2 grown by chemical vapor deposition has triggered great interest due to its exciting potential for applications in advanced electronic and optoelectronic devices. Unlike gapless graphene, MoS2 has an intrinsic band gap in the visible which crosses over from an indirect to a direct gap when reduced to a single atomic layer. In this article, we report a comprehensive study of fundamental optical properties of MoS2 revealed by optical spectroscopy of Raman, photoluminescence, and vacuum ultraviolet spectroscopic ellipsometry. A band gap of 1.42 eV is determined by the absorption threshold of bulk MoS2 that shifts to 1.83 eV in monolayer MoS2. We extracted the high precision dielectric function up to 9.0 eV which leads to the identification of many unique interband transitions at high symmetry points in the MoS2 momentum space. The positions of the A and B excitons in single layers are found to shift upwards in energy compared with those of the bulk form and have smaller separation. A very strong optical critical point predicted to correspond to a quasi-particle gap is observed at 2.86 eV, which is attributed to optical transitions along the parallel bands between the M and gama points in the reduced Brillouin zone. The absence of the bulk MoS2 spin-orbit interaction peak at ~ 3.0 eV in monolayer MoS2 is, as predicted, the consequence of the coalescence of nearby excitons. A higher energy optical transition at 3.98 eV, commonly occurred in bulk semiconductors, is associated with a combination of several critical points.These optical transitions herein reported enhance our understanding of monolayer MoS2 as well as of two-dimensional systems in general, and thus provide informative guidelines for MoS2 optical device designs and theoretical considerations.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "In this Letter, we analyse the radial UV-optical color distributions in a sample of low redshift green valley (GV) galaxies, with the Galaxy Evolution Explorer (GALEX)+Sloan Digital Sky Survey (SDSS) images, to investigate how the residual recent star formation distribute in these galaxies. We find that the dust-corrected $u-r$ colors of early-type galaxies (ETGs) are flat out to $R_{90}$, while the colors turn blue monotonously when $r>0.5R_{50}$ for late-type galaxies (LTGs). More than a half of the ETGs are blue-cored and have remarkable positive NUV$-r$ color gradients, suggesting that their star formation are centrally concentrated; the rest have flat color distributions out to $R_{90}$. The centrally concentrated star formation activity in a large portion of ETGs is confirmed by the SDSS spectroscopy, showing that $\\sim$50 % ETGs have EW(H$\\rm \u03b1$)$>6.0$ \u00c5. For the LTGs, 95% of them show uniform radial color profiles, which can be interpreted as a red bulge plus an extended blue disk. The links between the two kinds of ETGs, e.g., those objects having remarkable \"blue-cored\" and those having flat color gradients, are less known and require future investigations. It is suggested that the LTGs follow a general picture that quenching first occur in the core regions, and then finally extend to the rest of the galaxy. Our results can be re-examined and have important implications for the IFU surveys, such as MaNGA and SAMI.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Breaking space-time symmetries in two-dimensional crystals (2D) can dramatically influence their macroscopic electronic properties. Monolayer transition-metal dichalcogenides (TMDs) are prime examples where the intrinsically broken crystal inversion symmetry permits the generation of valley-selective electron populations, even though the two valleys are energetically degenerate, locked by time-reversal symmetry. Lifting the valley degeneracy in these materials is of great interest because it would allow for valley-specific band engineering and offer additional control in valleytronic applications. While applying a magnetic field should in principle accomplish this task, experiments to date have observed no valley-selective energy level shifts in fields accessible in the laboratory. Here we show the first direct evidence of lifted valley degeneracy in the monolayer TMD WS2. By applying intense circularly polarized light, which breaks time-reversal symmetry, we demonstrate that the exciton level in each valley can be selectively tuned by as much as 18 meV via the optical Stark effect. These results offer a novel way to control valley degree of freedom, and may provide a means to realize new valley-selective Floquet topological phases in 2D TMDs.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Layered transition metal dichalcogenides display a wide range of attractive physical and chemical properties and are potentially important for various device applications. Here we report the electronic transport and device properties of monolayer molybdenum disulphide (MoS2) grown by chemical vapour deposition (CVD). We show that these devices have the potential to suppress short channel effects and have high critical breakdown electric field. However, our study reveals that the electronic properties of these devices are at present, severely limited by the presence of a significant amount of band tail trapping states. Through capacitance and ac conductance measurements, we systematically quantify the density-of-states and response time of these states. Due to the large amount of trapped charges, the measured effective mobility also leads to a large underestimation of the true band mobility and the potential of the material. Continual engineering efforts on improving the sample quality are needed for its potential applications.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Due to the rapid growth of internet broadband access and proliferation of modern mobile devices, various types of multimedia (e.g. text, images, audios and videos) have become ubiquitously available anytime. Mobile device users usually store and use multimedia contents based on their personal interests and preferences. Mobile device challenges such as storage limitation have however introduced the problem of mobile multimedia overload to users. In order to tackle this problem, researchers have developed various techniques that recommend multimedia for mobile users. In this survey paper, we examine the importance of mobile multimedia recommendation systems from the perspective of three smart communities, namely, mobile social learning, mobile event guide and context-aware services. A cautious analysis of existing research reveals that the implementation of proactive, sensor-based and hybrid recommender systems can improve mobile multimedia recommendations. Nevertheless, there are still challenges and open issues such as the incorporation of context and social properties, which need to be tackled in order to generate accurate and trustworthy mobile multimedia recommendations.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Interactions between two excitons can result in the formation of bound quasiparticles, known as biexcitons. Their properties are determined by the constituent excitons, with orbital and spin states resembling those of atoms. Monolayer transition metal dichalcogenides (TMDs) present a unique system where excitons acquire a new degree of freedom, the valley pseudospin, from which a novel intervalley biexciton can be created. These biexcitons comprise two excitons from different valleys, which are distinct from biexcitons in conventional semiconductors and have no direct analogue in atomic and molecular systems. However, their valley properties are not accessible to traditional transport and optical measurements. Here, we report the observation of intervalley biexcitons in the monolayer TMD MoS2 using ultrafast pump-probe spectroscopy. By applying broadband probe pulses with different helicities, we identify two species of intervalley biexcitons with large binding energies of 60 meV and 40 meV. In addition, we also reveal effects beyond biexcitonic pairwise interactions in which the exciton energy redshifts at increasing exciton densities, indicating the presence of many-body interactions among them.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "In the era of big data, a large amount of noisy and incomplete data can be collected from multiple sources for prediction tasks. Combining multiple models or data sources helps to counteract the effects of low data quality and the bias of any single model or data source, and thus can improve the robustness and the performance of predictive models. Out of privacy, storage and bandwidth considerations, in certain circumstances one has to combine the predictions from multiple models or data sources to obtain the final predictions without accessing the raw data. Consensus-based prediction combination algorithms are effective for such situations. However, current research on prediction combination focuses on the single label setting, where an instance can have one and only one label. Nonetheless, data nowadays are usually multilabeled, such that more than one label have to be predicted at the same time. Direct applications of existing prediction combination methods to multilabel settings can lead to degenerated performance. In this paper, we address the challenges of combining predictions from multiple multilabel classifiers and propose two novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and MLCM-a (MLCM for microAUC). These algorithms can capture label correlations that are common in multilabel classifications, and optimize corresponding performance metrics. Experimental results on popular multilabel classification tasks verify the theoretical analysis and effectiveness of the proposed methods.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Due to their exceptional mechanical and optical properties, dielectric silicon nitride (SiN) micromembrane resonators have become the centerpiece of many optomechanical experiments. Efficient capacitive coupling of the membrane to an electrical system would facilitate exciting hybrid optoelectromechanical devices. However, capacitive coupling of such dielectric membranes is rather weak. Here we add a single layer of graphene on SiN micromembranes and compare electromechanical coupling and mechanical properties to bare dielectric membranes and to membranes metallized with an aluminium layer. The electrostatic coupling of graphene coated membranes is found to be equal to a perfectly conductive membrane. Our results show that a single layer of graphene substantially enhances the electromechanical capacitive coupling without significantly adding mass, decreasing the superior mechanical quality factor or affecting the optical properties of SiN micromembrane resonators.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "2D nanoelectronics based on single-layer MoS2 offers great advantages for both conventional and ubiquitous applications. This paper discusses the large-scale CVD growth of single-layer MoS2 and fabrication of devices and circuits for the first time. Both digital and analog circuits are fabricated to demonstrate its capability for mixed-signal applications.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Two-dimensional (2D) materials, such as molybdenum disulfide (MoS2), have been shown to exhibit excellent electrical and optical properties. The semiconducting nature of MoS2 allows it to overcome the shortcomings of zero-bandgap graphene, while still sharing many of graphene's advantages for electronic and optoelectronic applications. Discrete electronic and optoelectronic components, such as field-effect transistors, sensors and photodetectors made from few-layer MoS2 show promising performance as potential substitute of Si in conventional electronics and of organic and amorphous Si semiconductors in ubiquitous systems and display applications. An important next step is the fabrication of fully integrated multi-stage circuits and logic building blocks on MoS2 to demonstrate its capability for complex digital logic and high-frequency ac applications. This paper demonstrates an inverter, a NAND gate, a static random access memory, and a five-stage ring oscillator based on a direct-coupled transistor logic technology. The circuits comprise between two to twelve transistors seamlessly integrated side-by-side on a single sheet of bilayer MoS2. Both enhancement-mode and depletion-mode transistors were fabricated thanks to the use of gate metals with different work functions.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "The chemical functionalization of graphene enables control over electronic properties and sensor recognition sites. However, its study is confounded by an unusually strong influence of the underlying substrate. In this paper, we show a stark difference in the rate of electron transfer chemistry with aryl diazonium salts on monolayer graphene supported on a broad range of substrates. Reactions proceed rapidly when graphene is on SiO_2 and Al_2O_3 (sapphire), but negligibly on alkyl-terminated and hexagonal boron nitride (hBN) surfaces. The effect is contrary to expectations based on doping levels and can instead be described using a reactivity model accounting for substrate-induced electron-hole puddles in graphene. Raman spectroscopic mapping is used to characterize the effect of the substrates on graphene. Reactivity imprint lithography (RIL) is demonstrated as a technique for spatially patterning chemical groups on graphene by patterning the underlying substrate, and is applied to the covalent tethering of proteins on graphene.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "It is generally accepted that the asset price processes contain jumps. In fact, pure jump models have been widely used to model asset prices and/or stochastic volatilities. The question is: is there any statistical evidence from the high-frequency financial data to support using pure jump models alone? The purpose of this paper is to develop such a statistical test against the necessity of a diffusion component. The test is very simple to use and yet effective. Asymptotic properties of the proposed test statistic will be studied. Simulation studies and some real-life examples are included to illustrate our results.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "We analyze the role of bars in the build-up of central mass concentrations in massive, disk galaxies. Our parent sample consists of 3757 face-on disk galaxies with redshifts between 0.01 and 0.05, selected from the seventh Data Release of the Sloan Digital Sky Survey. 1555 galaxies with bars are identified using position angle and ellipticity profiles of the $i$-band light. We compare the ratio of the specific star formation rate measured in the 1-3 kpc central region of the galaxy to that measured for the whole galaxy. Galaxies with strong bars have centrally enhanced star formation; the degree of enhancement depends primarily on the ellipticity of the bar, and not on the size of the bar or on the mass or structure of the host galaxy. The fraction of galaxies with strong bars is highest at stellar masses greater than $3 \\times 10^{10} M_{\\odot}$, stellar surface densities less than $3 \\times 10^8 M_{\\odot}$ and concentration indices less than 2.5. In this region of parameter space, galaxies with strong bars either have enhanced central star formation rates, or star formation that is {\\em suppressed} compared to the mean. This suggests that bars may play a role in the eventual quenching of star formation in galaxies. Only 50% of galaxies with strongly concentrated star formation have strong bars, indicating that other processes such as galaxy interactions also induce central star-bursts. We also find that the ratio of the size of the bar to that of the disk depends mainly on the colour of the galaxy, suggesting that the growth and destruction of bars are regulated by gas accretion, as suggested by simulations.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "In this letter, we analyze the carrier transit delay in graphene field-effect transistors (GFETs). GFETs are fabricated at the wafer-scale on sapphire substrate. For a device with a gate length of 210 nm, a current gain cut-off frequency fT of 18 GHz and 22 GHz is obtained before and after de-embedding. The extraction of the internal (Cgs,i, Cgd,i) and external capacitances (Cgs,ex and Cgd,ex) from the scaling behavior of the gate capacitances Cgs and Cgd allows the intrinsic (\u03c4_int), extrinsic (\u03c4_ext) and parasitic delays (\u03c4_par) to be obtained. In addition, the extraction of the intrinsic delay provides a new way to directly estimate carrier velocity from the experimental data while the breakdown of the total delay into intrinsic, extrinsic, and parasitic components can offer valuable information for optimizing RF GFETs structures.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "We present experimental measurements of the electronic contribution to the Raman spectra of individual metallic single-walled carbon nanotubes (MSWNTs). Photoexcited carriers are inelastically scattered by a continuum of low-energy electron-hole pairs created across the graphenelike linear electronic subbands of the MSWNTs. The optical resonances in MSWNTs give rise to well-defined electronic Raman peaks. This resonant electronic Raman scattering is a unique feature of the electronic structure of these one-dimensional quasimetals.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "BigBOSS is a Stage IV ground-based dark energy experiment to study baryon acoustic oscillations (BAO) and the growth of structure with a wide-area galaxy and quasar redshift survey over 14,000 square degrees. It has been conditionally accepted by NOAO in response to a call for major new instrumentation and a high-impact science program for the 4-m Mayall telescope at Kitt Peak. The BigBOSS instrument is a robotically-actuated, fiber-fed spectrograph capable of taking 5000 simultaneous spectra over a wavelength range from 340 nm to 1060 nm, with a resolution R = 3000-4800.\n  Using data from imaging surveys that are already underway, spectroscopic targets are selected that trace the underlying dark matter distribution. In particular, targets include luminous red galaxies (LRGs) up to z = 1.0, extending the BOSS LRG survey in both redshift and survey area. To probe the universe out to even higher redshift, BigBOSS will target bright [OII] emission line galaxies (ELGs) up to z = 1.7. In total, 20 million galaxy redshifts are obtained to measure the BAO feature, trace the matter power spectrum at smaller scales, and detect redshift space distortions. BigBOSS will provide additional constraints on early dark energy and on the curvature of the universe by measuring the Ly-alpha forest in the spectra of over 600,000 2.2 < z < 3.5 quasars.\n  BigBOSS galaxy BAO measurements combined with an analysis of the broadband power, including the Ly-alpha forest in BigBOSS quasar spectra, achieves a FOM of 395 with Planck plus Stage III priors. This FOM is based on conservative assumptions for the analysis of broad band power (kmax = 0.15), and could grow to over 600 if current work allows us to push the analysis to higher wave numbers (kmax = 0.3). BigBOSS will also place constraints on theories of modified gravity and inflation, and will measure the sum of neutrino masses to 0.024 eV accuracy.\n        \u25b3 Less", "author": "Jing Kong"}, {"abstract": "Distributed transactions on high-overhead TCP/IP-based networks were conventionally considered to be prohibitively expensive and thus were avoided at all costs. To that end, the primary goal of almost any existing partitioning scheme is to minimize the number of cross-partition transactions. However, with the next generation of fast RDMA-enabled networks, this assumption is no longer valid. In fact, recent work has shown that distributed databases can scale even when the majority of transactions are cross-partition.\n  In this paper, we first make the case that the new bottleneck which hinders truly scalable transaction processing in modern RDMA-enabled databases is data contention, and that optimizing for data contention leads to different partitioning layouts than optimizing for the number of distributed transactions. We then present Chiller, a new approach to data partitioning and transaction execution, which minimizes data contention for both local and distributed transactions. Finally, we evaluate Chiller using TPC-C and a real-world workload, and show that our partitioning and execution strategy outperforms traditional partitioning techniques which try to avoid distributed transactions, by up to a factor of 2 under the same conditions.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Visual representations of data (visualizations) are tools of great importance and widespread use in data analytics as they provide users visual insight to patterns in the observed data in a simple and effective way. However, since visualizations tools are applied to sample data, there is a a risk of visualizing random fluctuations in the sample rather than a true pattern in the data. This problem is even more significant when visualization is used to identify interesting patterns among many possible possibilities, or to identify an interesting deviation in a pair of observations among many possible pairs, as commonly done in visual recommendation systems.\n  We present VizRec, a framework for improving the performance of visual recommendation systems by quantifying the statistical significance of recommended visualizations. The proposed methodology allows to control the probability of misleading visual recommendations using both classical statistical testing procedures and a novel application of the Vapnik Chervonenkis (VC) dimension method which is a fundamental concept in statistical learning theory.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Over the past decades, researchers and ML practitioners have come up with better and better ways to build, understand and improve the quality of ML models, but mostly under the key assumption that the training data is distributed identically to the testing data. In many real-world applications, however, some potential training examples are unknown to the modeler, due to sample selection bias or, more generally, covariate shift, i.e., a distribution shift between the training and deployment stage. The resulting discrepancy between training and testing distributions leads to poor generalization performance of the ML model and hence biased predictions. We provide novel algorithms that estimate the number and properties of these unknown training examples---unknown unknowns. This information can then be used to correct the training set, prior to seeing any test data. The key idea is to combine species-estimation techniques with data-driven methods for estimating the feature values for the unknown unknowns. Experiments on a variety of ML models and datasets indicate that taking the unknown examples into account can yield a more robust ML model that generalizes better.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Data visualization should be accessible for all analysts with data, not just the few with technical expertise. Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other ML-based systems.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "As machine learning (ML) systems become democratized, it becomes increasingly important to help users easily debug their models. However, current data tools are still primitive when it comes to helping users trace model performance problems all the way to the data. We focus on the particular problem of slicing data to identify subsets of the validation data where the model performs poorly. This is an important problem in model validation because the overall model performance can fail to reflect that of the smaller subsets, and slicing allows users to analyze the model performance on a more granular-level. Unlike general techniques (e.g., clustering) that can find arbitrary slices, our goal is to find interpretable slices (which are easier to take action compared to arbitrary subsets) that are problematic and large. We propose Slice Finder, which is an interactive framework for identifying such slices using statistical techniques. Applications include diagnosing model fairness and fraud detection, where identifying slices that are interpretable to humans is crucial.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "As neural networks become widely deployed in different applications and on different hardware, it has become increasingly important to optimize inference time and model size along with model accuracy. Most current techniques optimize model size, model accuracy and inference time in different stages, resulting in suboptimal results and computational inefficiency. In this work, we propose a new technique called Smallify that optimizes all three of these metrics at the same time. Specifically we present a new method to simultaneously optimize network size and model performance by neuron-level pruning during training. Neuron-level pruning not only produces much smaller networks but also produces dense weight matrices that are amenable to efficient inference. By applying our technique to convolutional as well as fully connected models, we show that Smallify can reduce network size by 35X with a 6X improvement in inference time with similar accuracy as models found by traditional training techniques.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Existing benchmarks for analytical database systems such as TPC-DS and TPC-H are designed for static reporting scenarios. The main metric of these benchmarks is the performance of running individual SQL queries over a synthetic database. In this paper, we argue that such benchmarks are not suitable for evaluating database workloads originating from interactive data exploration (IDE) systems where most queries are ad-hoc, not based on predefined reports, and built incrementally. As a main contribution, we present a novel benchmark called IDEBench that can be used to evaluate the performance of database systems for IDE workloads. As opposed to traditional benchmarks for analytical database systems, our goal is to provide more meaningful workloads and datasets that can be used to benchmark IDE query engines, with a particular focus on metrics that capture the trade-off between query performance and quality of the result. As a second contribution, this paper evaluates and discusses the performance results of selected IDE query engines using our benchmark. The study includes two commercial systems, as well as two research prototypes (IDEA, approXimateDB/XDB), and one traditional analytical database system (MonetDB).\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Index structures are one of the most important tools that DBAs leverage in order to improve the performance of analytics and transactional workloads. However, with the explosion of data that is constantly being generated in a wide variety of domains including autonomous vehicles, Internet of Things (IoT) devices, and E-commerce sites, building several indexes can often become prohibitive and consume valuable system resources. In fact, a recent study has shown that indexes created as part of the TPC-C benchmark can account for 55% of the total memory available in a state-of-the-art in-memory DBMS. This overhead consumes valuable and expensive main memory, and limits the amount of space that a database has available to store new data or process existing data.\n  In this paper, we present a novel approximate index structure called A-Tree. At the core of our index is a tunable error parameter that allows a DBA to balance lookup performance and space consumption. To navigate this tradeoff, we provide a cost model that helps the DBA choose an appropriate error parameter given either (1) a lookup latency requirement (e.g., 500ns) or (2) a storage budget (e.g., 100MB). Using a variety of real-world datasets, we show that our index structure is able to provide performance that is comparable to full index structures while reducing the storage footprint by orders of magnitude.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Going deeper and wider in neural architectures improves the accuracy, while the limited GPU DRAM places an undesired restriction on the network design domain. Deep Learning (DL) practitioners either need change to less desired network architectures, or nontrivially dissect a network across multiGPUs. These distract DL practitioners from concentrating on their original machine learning tasks. We present SuperNeurons: a dynamic GPU memory scheduling runtime to enable the network training far beyond the GPU DRAM capacity. SuperNeurons features 3 memory optimizations, \\textit{Liveness Analysis}, \\textit{Unified Tensor Pool}, and \\textit{Cost-Aware Recomputation}, all together they effectively reduce the network-wide peak memory usage down to the maximal memory usage among layers. We also address the performance issues in those memory saving techniques. Given the limited GPU DRAM, SuperNeurons not only provisions the necessary memory for the training, but also dynamically allocates the memory for convolution workspaces to achieve the high performance. Evaluations against Caffe, Torch, MXNet and TensorFlow have demonstrated that SuperNeurons trains at least 3.2432 deeper network than current ones with the leading performance. Particularly, SuperNeurons can train ResNet2500 that has $10^4$ basic network layers on a 12GB K40c.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Recent tools for interactive data exploration significantly increase the chance that users make false discoveries. The crux is that these tools implicitly allow the user to test a large body of different hypotheses with just a few clicks thus incurring in the issue commonly known in statistics as the multiple hypothesis testing error. In this paper, we propose solutions to integrate multiple hypothesis testing control into interactive data exploration tools. A key insight is that existing methods for controlling the false discovery rate (such as FDR) are not directly applicable for interactive data exploration. We therefore discuss a set of new control procedures that are better suited and integrated them in our system called Aware. By means of extensive experiments using both real-world and synthetic data sets we demonstrate how Aware can help experts and novice users alike to efficiently control false discoveries.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Data cleaning, whether manual or algorithmic, is rarely perfect leaving a dataset with an unknown number of false positives and false negatives after cleaning. In many scenarios, quantifying the number of remaining errors is challenging because our data integrity rules themselves may be incomplete, or the available gold-standard datasets may be too small to extrapolate. As the use of inherently fallible crowds becomes more prevalent in data cleaning problems, it is important to have estimators to quantify the extent of such errors. We propose novel species estimators to estimate the number of distinct remaining errors in a dataset after it has been cleaned by a set of crowd workers -- essentially, quantifying the utility of hiring additional workers to clean the dataset. This problem requires new estimators that are robust to false positives and false negatives, and we empirically show on three real-world datasets that existing species estimators are unstable for this problem, while our proposed techniques quickly converge.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Reusing intermediates in databases to speed-up analytical query processing has been studied in the past. Existing solutions typically require intermediate results of individual operators to be materialized into temporary tables to be considered for reuse in subsequent queries. However, these approaches are fundamentally ill-suited for use in modern main memory databases. The reason is that modern main memory DBMSs are typically limited by the bandwidth of the memory bus, thus query execution is heavily optimized to keep tuples in the CPU caches and registers. To that end, adding additional materialization operations into a query plan not only add additional traffic to the memory bus but more importantly prevent the important cache- and register-locality opportunities resulting in high performance penalties.\n  In this paper we study a novel reuse model for intermediates, which caches internal physical data structures materialized during query processing (due to pipeline breakers) and externalizes them so that they become reusable for upcoming operations. We focus on hash tables, the most commonly used internal data structure in main memory databases to perform join and aggregation operations. As queries arrive, our reuse-aware optimizer reasons about the reuse opportunities for hash tables, employing cost models that take into account hash table statistics together with the CPU and data movement costs within the cache hierarchy. Experimental results, based on our HashStash prototype demonstrate performance gains of $2\\times$ for typical analytical workloads with no additional overhead for materializing intermediates.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "The common wisdom is that distributed transactions do not scale. But what if distributed transactions could be made scalable using the next generation of networks and a redesign of distributed databases? There would be no need for developers anymore to worry about co-partitioning schemes to achieve decent performance. Application development would become easier as data placement would no longer determine how scalable an application is. Hardware provisioning would be simplified as the system administrator can expect a linear scale-out when adding more machines rather than some complex sub-linear function, which is highly application specific.\n  In this paper, we present the design of our novel scalable database system NAM-DB and show that distributed transactions with the very common Snapshot Isolation guarantee can indeed scale using the next generation of RDMA-enabled network technology without any inherent bottlenecks. Our experiments with the TPC-C benchmark show that our system scales linearly to over 6.5 million new-order (14.5 million total) distributed transactions per second on 56 machines.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "In recent years, crowdsourcing is increasingly applied as a means to enhance data quality. Although the crowd generates insightful information especially for complex problems such as entity resolution (ER), the output quality of crowd workers is often noisy. That is, workers may unintentionally generate false or contradicting data even for simple tasks. The challenge that we address in this paper is how to minimize the cost for task requesters while maximizing ER result quality under the assumption of unreliable input from the crowd. For that purpose, we first establish how to deduce a consistent ER solution from noisy worker answers as part of the data interpretation problem. We then focus on the next-crowdsource problem which is to find the next task that maximizes the information gain of the ER result for the minimal additional cost. We compare our robust data interpretation strategies to alternative state-of-the-art approaches that do not incorporate the notion of fault-tolerance, i.e., the robustness to noise. In our experimental evaluation we show that our approaches yield a quality improvement of at least 20% for two real-world datasets. Furthermore, we examine task-to-worker assignment strategies as well as task parallelization techniques in terms of their cost and quality trade-offs in this paper. Based on both synthetic and crowdsourced datasets, we then draw conclusions on how to minimize cost while maintaining high quality ER results.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Materialized views (MVs), stored pre-computed results, are widely used to facilitate fast queries on large datasets. When new records arrive at a high rate, it is infeasible to continuously update (maintain) MVs and a common solution is to defer maintenance by batching updates together. Between batches the MVs become increasingly stale with incorrect, missing, and superfluous rows leading to increasingly inaccurate query results. We propose Stale View Cleaning (SVC) which addresses this problem from a data cleaning perspective. In SVC, we efficiently clean a sample of rows from a stale MV, and use the clean sample to estimate aggregate query results. While approximate, the estimated query results reflect the most recent data. As sampling can be sensitive to long-tailed distributions, we further explore an outlier indexing technique to give increased accuracy when the data distributions are skewed. SVC complements existing deferred maintenance approaches by giving accurate and bounded query answers between maintenance. We evaluate our method on a generated dataset from the TPC-D benchmark and a real video distribution application. Experiments confirm our theoretical results: (1) cleaning an MV sample is more efficient than full view maintenance, (2) the estimated results are more accurate than using the stale MV, and (3) SVC is applicable for a wide variety of MVs.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) is the integrated data set complete and (2) what is the impact of any unknown (i.e., unobserved) data on query results?\n  In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Next generation high-performance RDMA-capable networks will require a fundamental rethinking of the design and architecture of modern distributed DBMSs. These systems are commonly designed and optimized under the assumption that the network is the bottleneck: the network is slow and \"thin\", and thus needs to be avoided as much as possible. Yet this assumption no longer holds true. With InfiniBand FDR 4x, the bandwidth available to transfer data across network is in the same ballpark as the bandwidth of one memory channel, and it increases even further with the most recent EDR standard. Moreover, with the increasing advances of RDMA, the latency improves similarly fast. In this paper, we first argue that the \"old\" distributed database design is not capable of taking full advantage of the network. Second, we propose architectural redesigns for OLTP, OLAP and advanced analytical frameworks to take better advantage of the improved bandwidth, latency and RDMA capabilities. Finally, for each of the workload categories, we show that remarkable performance improvements can be achieved.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Stream processing addresses the needs of real-time applications. Transaction processing addresses the coordination and safety of short atomic computations. Heretofore, these two modes of operation existed in separate, stove-piped systems. In this work, we attempt to fuse the two computational paradigms in a single system called S-Store. In this way, S-Store can simultaneously accommodate OLTP and streaming applications. We present a simple transaction model for streams that integrates seamlessly with a traditional OLTP system. We chose to build S-Store as an extension of H-Store, an open-source, in-memory, distributed OLTP database system. By implementing S-Store in this way, we can make use of the transaction processing facilities that H-Store already supports, and we can concentrate on the additional implementation features that are needed to support streaming. Similar implementations could be done using other main-memory OLTP platforms. We show that we can actually achieve higher throughput for streaming workloads in S-Store than an equivalent deployment in H-Store alone. We also show how this can be achieved within H-Store with the addition of a modest amount of new functionality. Furthermore, we compare S-Store to two state-of-the-art streaming systems, Spark Streaming and Storm, and show how S-Store matches and sometimes exceeds their performance while providing stronger transactional guarantees.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "The proliferation of massive datasets combined with the development of sophisticated analytical techniques have enabled a wide variety of novel applications such as improved product recommendations, automatic image tagging, and improved speech-driven interfaces. These and many other applications can be supported by Predictive Analytic Queries (PAQs). A major obstacle to supporting PAQs is the challenging and expensive process of identifying and training an appropriate predictive model. Recent efforts aiming to automate this process have focused on single node implementations and have assumed that model training itself is a black box, thus limiting the effectiveness of such approaches on large-scale problems. In this work, we build upon these recent efforts and propose an integrated PAQ planning architecture that combines advanced model search techniques, bandit resource allocation via runtime algorithm introspection, and physical optimization via batching. The result is TuPAQ, a component of the MLbase system, which solves the PAQ planning problem with comparable quality to exhaustive strategies but an order of magnitude more efficiently than the standard baseline approach, and can scale to models trained on terabytes of data across hundreds of machines.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "In the SIGMOD 2013 conference, we published a paper extending our earlier work on crowdsourced entity resolution to improve crowdsourced join processing by exploiting transitive relationships [Wang et al. 2013]. The VLDB 2014 conference has a paper that follows up on our previous work [Vesdapunt et al., 2014], which points out and corrects a mistake we made in our SIGMOD paper. Specifically, in Section 4.2 of our SIGMOD paper, we defined the \"Expected Optimal Labeling Order\" (EOLO) problem, and proposed an algorithm for solving it. We incorrectly claimed that our algorithm is optimal. In their paper, Vesdapunt et al. show that the problem is actually NP-Hard, and based on that observation, propose a new algorithm to solve it. In this note, we would like to put the Vesdapunt et al. results in context, something we believe that their paper does not adequately do.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "The development of crowdsourced query processing systems has recently attracted a significant attention in the database community. A variety of crowdsourced queries have been investigated. In this paper, we focus on the crowdsourced join query which aims to utilize humans to find all pairs of matching objects from two collections. As a human-only solution is expensive, we adopt a hybrid human-machine approach which first uses machines to generate a candidate set of matching pairs, and then asks humans to label the pairs in the candidate set as either matching or non-matching. Given the candidate pairs, existing approaches will publish all pairs for verification to a crowdsourcing platform. However, they neglect the fact that the pairs satisfy transitive relations. As an example, if $o_1$ matches with $o_2$, and $o_2$ matches with $o_3$, then we can deduce that $o_1$ matches with $o_3$ without needing to crowdsource $(o_1, o_3)$. To this end, we study how to leverage transitive relations for crowdsourced joins. We propose a hybrid transitive-relations and crowdsourcing labeling framework which aims to crowdsource the minimum number of pairs to label all the candidate pairs. We prove the optimal labeling order in an ideal setting and propose a heuristic labeling order in practice. We devise a parallel labeling algorithm to efficiently crowdsource the pairs following the order. We evaluate our approaches in both simulated environment and a real crowdsourcing platform. Experimental results show that our approaches with transitive relations can save much more money and time than existing methods, with a little loss in the result quality.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "There is a fundamental discrepancy between the targeted and actual users of current analytics frameworks. Most systems are designed for the data and infrastructure of the Googles and Facebooks of the world---petabytes of data distributed across large cloud deployments consisting of thousands of cheap commodity machines. Yet, the vast majority of users operate clusters ranging from a few to a few dozen nodes, analyze relatively small datasets of up to a few terabytes, and perform primarily compute-intensive operations. Targeting these users fundamentally changes the way we should build analytics systems.\n  This paper describes the design of Tupleware, a new system specifically aimed at the challenges faced by the typical user. Tupleware's architecture brings together ideas from the database, compiler, and programming languages communities to create a powerful end-to-end solution for data analysis. We propose novel techniques that consider the data, computations, and hardware together to achieve maximum performance on a case-by-case basis. Our experimental evaluation quantifies the impact of our novel techniques and shows orders of magnitude performance improvement over alternative systems.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "MLI is an Application Programming Interface designed to address the challenges of building Machine Learn- ing algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Entity resolution is central to data integration and data cleaning. Algorithmic approaches have been improving in quality, but remain far from perfect. Crowdsourcing platforms offer a more accurate but expensive (and slow) way to bring human insight into the process. Previous work has proposed batching verification tasks for presentation to human workers but even with batching, a human-only approach is infeasible for data sets of even moderate size, due to the large numbers of matches to be tested. Instead, we propose a hybrid human-machine approach in which machines are used to do an initial, coarse pass over all the data, and people are used to verify only the most likely matching pairs. We show that for such a hybrid system, generating the minimum number of verification tasks of a given size is NP-Hard, but we develop a novel two-tiered heuristic approach for creating batched tasks. We describe this method, and present the results of extensive experiments on real data sets using a popular crowdsourcing platform. The experiments show that our hybrid approach achieves both good efficiency and high accuracy compared to machine-only or human-only alternatives.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Replicating data across multiple data centers not only allows moving the data closer to the user and, thus, reduces latency for applications, but also increases the availability in the event of a data center failure. Therefore, it is not surprising that companies like Google, Yahoo, and Netflix already replicate user data across geographically different regions.\n  However, replication across data centers is expensive. Inter-data center network delays are in the hundreds of milliseconds and vary significantly. Synchronous wide-area replication is therefore considered to be unfeasible with strong consistency and current solutions either settle for asynchronous replication which implies the risk of losing data in the event of failures, restrict consistency to small partitions, or give up consistency entirely. With MDCC (Multi-Data Center Consistency), we describe the first optimistic commit protocol, that does not require a master or partitioning, and is strongly consistent at a cost similar to eventually consistent protocols. MDCC can commit transactions in a single round-trip across data centers in the normal operational case. We further propose a new programming model which empowers the application developer to handle longer and unpredictable latencies caused by inter-data center communication. Our evaluation using the TPC-W benchmark with MDCC deployed across 5 geographically diverse data centers shows that MDCC is able to achieve throughput and latency similar to eventually consistent quorum protocols and that MDCC is able to sustain a data center outage without a significant impact on response times while guaranteeing strong consistency.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Hybrid human/computer systems promise to greatly expand the usefulness of query processing by incorporating the crowd for data gathering and other tasks. Such systems raise many database system implementation questions. Perhaps most fundamental is that the closed world assumption underlying relational query semantics does not hold in such systems. As a consequence the meaning of even simple queries can be called into question. Furthermore query progress monitoring becomes difficult due to non-uniformities in the arrival of crowdsourced data and peculiarities of how people work in crowdsourcing systems. To address these issues, we develop statistical tools that enable users and systems developers to reason about tradeoffs between time/cost and completeness. These tools can also help drive query execution and crowdsourcing strategies. We evaluate our techniques using experiments on a popular crowdsourcing platform.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "Newly-released web applications often succumb to a \"Success Disaster,\" where overloaded database machines and resulting high response times destroy a previously good user experience. Unfortunately, the data independence provided by a traditional relational database system, while useful for agile development, only exacerbates the problem by hiding potentially expensive queries under simple declarative expressions. As a result, developers of these applications are increasingly abandoning relational databases in favor of imperative code written against distributed key/value stores, losing the many benefits of data independence in the process. Instead, we propose PIQL, a declarative language that also provides scale independence by calculating an upper bound on the number of key/value store operations that will be performed for any query. Coupled with a service level objective (SLO) compliance prediction model and PIQL's scalable database architecture, these bounds make it easy for developers to write success-tolerant applications that support an arbitrarily large number of users while still providing acceptable performance. In this paper, we present the PIQL query processing system and evaluate its scale independence on hundreds of machines using two benchmarks, TPC-W and SCADr.\n        \u25b3 Less", "author": "Tim Kraska"}, {"abstract": "In Programming by Example, a system attempts to infer a program from input and output examples, generally by searching for a composition of certain base functions. Performing a naive brute force search is infeasible for even mildly involved tasks. We note that the examples themselves often present clues as to which functions to compose, and how to rank the resulting programs. In text processing, which is our domain of interest, clues arise from simple textual features: for example, if parts of the input and output strings are permutations of one another, this suggests that sorting may be useful. We describe a system that learns the reliability of such clues, allowing for faster search and a principled ranking over programs. Experiments on a prototype of this system show that this learning scheme facilitates efficient inference on a range of text processing tasks.\n        \u25b3 Less", "author": "Butler Lampson"}, {"abstract": "The DESI Legacy Imaging Surveys are a combination of three public projects (the Dark Energy Camera Legacy Survey, the Beijing-Arizona Sky Survey, and the Mayall z-band Legacy Survey) that will jointly image ~14,000 square degrees of the extragalactic sky visible from the northern hemisphere in three optical bands (g, r, and z) using telescopes at the Kitt Peak National Observatory and the Cerro Tololo Inter-American Observatory. The combined survey footprint is split into two contiguous areas by the Galactic plane. The optical imaging is conducted using a unique strategy of dynamic observing that results in a survey of nearly uniform depth. In addition to calibrated images, the project is delivering an inference-based catalog which includes photometry from the grz optical bands and from four mid-infrared bands (at 3.4um, 4.6um, 12um and 22um) observed by the Wide-field Infrared Survey Explorer (WISE) satellite during its full operational lifetime. The project plans two public data releases each year. All the software used to generate the catalogs is also released with the data. This paper provides an overview of the Legacy Surveys project.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) has been in operation since July 2014. This paper describes the second data release from this phase, and the fourteenth from SDSS overall (making this, Data Release Fourteen or DR14). This release makes public data taken by SDSS-IV in its first two years of operation (July 2014-2016). Like all previous SDSS releases, DR14 is cumulative, including the most recent reductions and calibrations of all data taken by SDSS since the first phase began operations in 2000. New in DR14 is the first public release of data from the extended Baryon Oscillation Spectroscopic Survey (eBOSS); the first data from the second phase of the Apache Point Observatory (APO) Galactic Evolution Experiment (APOGEE-2), including stellar parameter estimates from an innovative data driven machine learning algorithm known as \"The Cannon\"; and almost twice as many data cubes from the Mapping Nearby Galaxies at APO (MaNGA) survey as were in the previous release (N = 2812 in total). This paper describes the location and format of the publicly available data from SDSS-IV surveys. We provide references to the important technical papers describing how these data have been taken (both targeting and observation details) and processed for scientific use. The SDSS website (www.sdss.org) has been updated for this release, and provides links to data downloads, as well as tutorials and examples of data use. SDSS-IV is planning to continue to collect astronomical data until 2020, and will be followed by SDSS-V.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We report results from the first search for sterile neutrinos mixing with active neutrinos through a reduction in the rate of neutral-current interactions over a baseline of 810\\,km between the NOvA detectors. Analyzing a 14-kton detector equivalent exposure of 6.05$\\times$10$^{20}$ protons-on-target in the NuMI beam at Fermilab, we observe 95 neutral-current candidates at the Far Detector compared with $83.5 \\pm 9.7 \\mbox{(stat.)} \\pm 9.4 \\mbox{(syst.)}$ events predicted assuming mixing only occurs between active neutrino species. No evidence for $\u03bd_\u03bc \\rightarrow \u03bd_{s}$ transitions is found. Interpreting these results within a 3+1 model, we place constraints on the mixing angles $\u03b8_{24}<20.8^{\\circ}$ and $\u03b8_{34}<31.2^{\\circ}$ at the 90% C.L. for $0.05~eV^2\\leq \u0394m^2_{41}\\leq 0.5~eV^2$, the range of mass splittings that produce no significant oscillations over the Near Detector baseline.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We explore the H-alpha emission in the massive quiescent galaxies observed by the KMOS-3D survey at 0.7 < z < 2.7. The H-alpha line is robustly detected in 20 out of 120 UVJ-selected quiescent galaxies, and we classify the emission mechanism using the H-alpha line width and the [NII]/H-alpha line ratio. We find that AGN are likely to be responsible for the line emission in more than half of the cases. We also find robust evidence for star formation activity in nine quiescent galaxies, which we explore in detail. The H-alpha kinematics reveal rotating disks in five of the nine galaxies. The dust-corrected H-alpha star formation rates are low (0.2 - 7 Msun/yr), and place these systems significantly below the main sequence. The 24micron-based infrared luminosities, instead, overestimate the star formation rates. These galaxies present a lower gas-phase metallicity compared to star-forming objects with similar stellar mass, and many of them have close companions. We therefore conclude that the low-level star formation activity in these nine quiescent galaxies is likely to be fueled by inflowing gas or minor mergers, and could be a sign of rejuvenation events.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "Results are reported from an improved measurement of $\u03bd_\u03bc\\rightarrow \u03bd_e$ transitions by the NOvA experiment. Using an exposure equivalent to $6.05\\times10^{20}$ protons-on-target 33 $\u03bd_e$ candidates were observed with a background of $8.2\\pm0.8$ (syst.). Combined with the latest NOvA $\u03bd_\u03bc$ disappearance data and external constraints from reactor experiments on $\\sin^22\u03b8_{13}$, the hypothesis of inverted mass hierarchy with $\u03b8_{23}$ in the lower octant is disfavored at greater than $93\\%$ C.L. for all values of $\u03b4_{CP}$.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We describe the Sloan Digital Sky Survey IV (SDSS-IV), a project encompassing three major spectroscopic programs. The Apache Point Observatory Galactic Evolution Experiment 2 (APOGEE-2) is observing hundreds of thousands of Milky Way stars at high resolution and high signal-to-noise ratio in the near-infrared. The Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey is obtaining spatially-resolved spectroscopy for thousands of nearby galaxies (median redshift of z = 0.03). The extended Baryon Oscillation Spectroscopic Survey (eBOSS) is mapping the galaxy, quasar, and neutral gas distributions between redshifts z = 0.6 and 3.5 to constrain cosmology using baryon acoustic oscillations, redshift space distortions, and the shape of the power spectrum. Within eBOSS, we are conducting two major subprograms: the SPectroscopic IDentification of eROSITA Sources (SPIDERS), investigating X-ray AGN and galaxies in X-ray clusters, and the Time Domain Spectroscopic Survey (TDSS), obtaining spectra of variable sources. All programs use the 2.5-meter Sloan Foundation Telescope at Apache Point Observatory; observations there began in Summer 2014. APOGEE-2 also operates a second near-infrared spectrograph at the 2.5-meter du Pont Telescope at Las Campanas Observatory, with observations beginning in early 2017. Observations at both facilities are scheduled to continue through 2020. In keeping with previous SDSS policy, SDSS-IV provides regularly scheduled public data releases; the first one, Data Release 13, was made available in July 2016.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "This Letter reports new results on muon neutrino disappearance from NOvA, using a 14 kton detector equivalent exposure of $6.05\\times10^{20}$ protons-on-target from the NuMI beam at the Fermi National Accelerator Laboratory. The measurement probes the muon-tau symmetry hypothesis that requires maximal mixing ($\u03b8_{23} = \u03c0/4$). Assuming the normal mass hierarchy, we find $\u0394m^2 = (2.67 \\pm 0.11)\\times 10^{-3}$ eV$^2$ and $\\sin^2 \u03b8_{23}$ at the two statistically degenerate values $0.404^{+0.030}_{-0.022}$ and $0.624^{+0.022}_{-0.030}$, both at the 68% confidence level. Our data disfavor the maximal mixing scenario with 2.6 $\u03c3$ significance.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "DESI (Dark Energy Spectropic Instrument) is a Stage IV ground-based dark energy experiment that will study baryon acoustic oscillations and the growth of structure through redshift-space distortions with a wide-area galaxy and quasar redshift survey. The DESI instrument is a robotically-actuated, fiber-fed spectrograph capable of taking up to 5,000 simultaneous spectra over a wavelength range from 360 nm to 980 nm. The fibers feed ten three-arm spectrographs with resolution $R= \u03bb/\u0394\u03bb$ between 2000 and 5500, depending on wavelength. The DESI instrument will be used to conduct a five-year survey designed to cover 14,000 deg$^2$. This powerful instrument will be installed at prime focus on the 4-m Mayall telescope in Kitt Peak, Arizona, along with a new optical corrector, which will provide a three-degree diameter field of view. The DESI collaboration will also deliver a spectroscopic pipeline and data management system to reduce and archive all data for eventual public use.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "DESI (Dark Energy Spectroscopic Instrument) is a Stage IV ground-based dark energy experiment that will study baryon acoustic oscillations (BAO) and the growth of structure through redshift-space distortions with a wide-area galaxy and quasar redshift survey. To trace the underlying dark matter distribution, spectroscopic targets will be selected in four classes from imaging data. We will measure luminous red galaxies up to $z=1.0$. To probe the Universe out to even higher redshift, DESI will target bright [O II] emission line galaxies up to $z=1.7$. Quasars will be targeted both as direct tracers of the underlying dark matter distribution and, at higher redshifts ($ 2.1 < z < 3.5$), for the Ly-$\u03b1$ forest absorption features in their spectra, which will be used to trace the distribution of neutral hydrogen. When moonlight prevents efficient observations of the faint targets of the baseline survey, DESI will conduct a magnitude-limited Bright Galaxy Survey comprising approximately 10 million galaxies with a median $z\\approx 0.2$. In total, more than 30 million galaxy and quasar redshifts will be obtained to measure the BAO feature and determine the matter power spectrum, including redshift space distortions.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) began observations in July 2014. It pursues three core programs: APOGEE-2, MaNGA, and eBOSS. In addition, eBOSS contains two major subprograms: TDSS and SPIDERS. This paper describes the first data release from SDSS-IV, Data Release 13 (DR13), which contains new data, reanalysis of existing data sets and, like all SDSS data releases, is inclusive of previously released data. DR13 makes publicly available 1390 spatially resolved integral field unit observations of nearby galaxies from MaNGA, the first data released from this survey. It includes new observations from eBOSS, completing SEQUELS. In addition to targeting galaxies and quasars, SEQUELS also targeted variability-selected objects from TDSS and X-ray selected objects from SPIDERS. DR13 includes new reductions of the SDSS-III BOSS data, improving the spectrophotometric calibration and redshift classification. DR13 releases new reductions of the APOGEE-1 data from SDSS-III, with abundances of elements not previously included and improved stellar parameters for dwarf stars and cooler stars. For the SDSS imaging data, DR13 provides new, more robust and precise photometric calibrations. Several value-added catalogs are being released in tandem with DR13, in particular target catalogs relevant for eBOSS, TDSS, and SPIDERS, and an updated red-clump catalog for APOGEE. This paper describes the location and format of the data now publicly available, as well as providing references to the important technical papers that describe the targeting, observing, and data reduction. The SDSS website, http://www.sdss.org, provides links to the data, tutorials and examples of data access, and extensive documentation of the reduction and analysis procedures. DR13 is the first of a scheduled set that will contain new data and analyses from the planned ~6-year operations of SDSS-IV.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "Spatially-dense pressure measurements are needed on curved surfaces in marine environments to provide marine vehicles with the detailed, real-time measurements of the near-field flow necessary to improve performance through flow control. To address this challenge, a waterproof and conformal pressure sensor array comprising carbon black-doped-silicone closed-cell foam (CBPDMS foam) was developed for use in marine applications. The response of the CBPDMS foam sensor arrays was characterized using periodic hydrodynamic pressure stimuli from vertical plunging, from which a piecewise polynomial calibration was developed to describe the sensor response. Inspired by the distributed pressure and velocity sensing capabilities of the fish lateral line, the CBPDMS foam sensor arrays have significant advantages over existing commercial sensors for distributed flow reconstruction and control. Experimental results have shown the sensor arrays to have sensitivity on the order of 5 Pascal, dynamic range of 50-500 Pascal; are contained in a waterproof and completely flexible package, and have material cost less than $10 per sensor.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We present measurements of the [NII]/Ha ratio as a probe of gas-phase oxygen abundance for a sample of 419 star-forming galaxies at z=0.6-2.7 from the KMOS3D near-IR multi-IFU survey. The mass-metallicity relation (MZR) is determined consistently with the same sample selection, metallicity tracer, and methodology over the wide redshift range probed by the survey. We find good agreement with long-slit surveys in the literature, except for the low-mass slope of the relation at z~2.3, where this sample is less biased than previous samples based on optical spectroscopic redshifts. In this regime we measure a steeper slope than some literature results. Excluding the AGN contribution from the MZR reduces sensitivity at the high mass end, but produces otherwise consistent results. There is no significant dependence of the [NII]/Ha ratio on SFR or environment at fixed redshift and stellar mass. The IFU data allow spatially resolved measurements of [NII]/Ha, from which we can infer abundance gradients for 180 galaxies, thus tripling the current sample in the literature. The observed gradients are on average flat, with only 15 gradients statistically offset from zero at >3sigma. We have modelled the effect of beam-smearing, assuming a smooth intrinsic radial gradient and known seeing, inclination and effective radius for each galaxy. Our seeing-limited observations can recover up to 70% of the intrinsic gradient for the largest, face-on disks, but only 30% for the smaller, more inclined galaxies. We do not find significant trends between observed or corrected gradients and any stellar population, dynamical or structural galaxy parameters, mostly in agreement with existing studies with much smaller sample sizes. In cosmological simulations, strong feedback is generally required to produce flat gradients at high redshift.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The Large Synoptic Survey Telescope (LSST) is a large-aperture, wide-field, ground-based survey system that will image the sky in six optical bands from 320 to 1050 nm, uniformly covering approximately $18,000$deg$^2$ of the sky over 800 times. The LSST is currently under construction on Cerro Pach\u00f3n in Chile, and expected to enter operations in 2022. Once operational, the LSST will explore a wide range of astrophysical questions, from discovering \"killer\" asteroids to examining the nature of Dark Energy.\n  The LSST will generate on average 15 TB of data per night, and will require a comprehensive Data Management system to reduce the raw data to scientifically useful catalogs and images with minimum human intervention. These reductions will result in a real-time alert stream, and eleven data releases over the 10-year duration of LSST operations. To enable this processing, the LSST project is developing a new, general-purpose, high-performance, scalable, well documented, open source data processing software stack for O/IR surveys. Prototypes of this stack are already capable of processing data from existing cameras (e.g., SDSS, DECam, MegaCam), and form the basis of the Hyper-Suprime Cam (HSC) Survey data reduction pipeline.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We describe the algorithm used to select the Luminous Red Galaxy (LRG) sample for the extended Baryon Oscillation Spectroscopic Survey (eBOSS) of the Sloan Digital Sky Survey IV (SDSS-IV) using photometric data from both the SDSS and the Wide-Field Infrared Survey Explorer (WISE). LRG targets are required to meet a set of color selection criteria and have z-band and i-band MODEL magnitudes z < 19.95 and 19.9 < i < 21.8, respectively. Our algorithm selects roughly 50 LRG targets per square degree, the great majority of which lie in the redshift range 0.6 < z < 1.0 (median redshift 0.71). We demonstrate that our methods are highly effective at eliminating stellar contamination and lower-redshift galaxies. We perform a number of tests using spectroscopic data from SDSS-III/BOSS to determine the redshift reliability of our target selection and its ability to meet the science requirements of eBOSS. The SDSS spectra are of high enough signal-to-noise ratio that at least 89% of the target sample yields secure redshift measurements. We also present tests of the uniformity and homogeneity of the sample, demonstrating that it should be clean enough for studies of the large-scale structure of the universe at higher redshifts than SDSS-III/BOSS LRGs reached.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The Extended Baryon Oscillation Spectroscopic Survey (eBOSS) will conduct novel cosmological observations using the BOSS spectrograph at Apache Point Observatory. Observations will be simultaneous with the Time Domain Spectroscopic Survey (TDSS) designed for variability studies and the Spectroscopic Identification of eROSITA Sources (SPIDERS) program designed for studies of X-ray sources. eBOSS will use four different tracers to measure the distance-redshift relation with baryon acoustic oscillations (BAO). Using more than 250,000 new, spectroscopically confirmed luminous red galaxies at a median redshift z=0.72, we project that eBOSS will yield measurements of $d_A(z)$ to an accuracy of 1.2% and measurements of H(z) to 2.1% when combined with the z>0.6 sample of BOSS galaxies. With ~195,000 new emission line galaxy redshifts, we expect BAO measurements of $d_A(z)$ to an accuracy of 3.1% and H(z) to 4.7% at an effective redshift of z= 0.87. A sample of more than 500,000 spectroscopically-confirmed quasars will provide the first BAO distance measurements over the redshift range 0.9<z<2.2, with expected precision of 2.8% and 4.2% on $d_A(z)$ and H(z), respectively. Finally, with 60,000 new quasars and re-observation of 60,000 quasars known from BOSS, we will obtain new Lyman-alpha forest measurements at redshifts z>2.1; these new data will enhance the precision of $d_A(z)$ and H(z) by a factor of 1.44 relative to BOSS. Furthermore, eBOSS will provide improved tests of General Relativity on cosmological scales through redshift-space distortion measurements, improved tests for non-Gaussianity in the primordial density field, and new constraints on the summed mass of all neutrino species. Here, we provide an overview of the cosmological goals, spectroscopic target sample, demonstration of spectral quality from early data, and projected cosmological constraints from eBOSS.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "As part of the Sloan Digital Sky Survey IV the extended Baryon Oscillation Spectroscopic Survey (eBOSS) will improve measurements of the cosmological distance scale by applying the Baryon Acoustic Oscillation (BAO) method to quasar samples. eBOSS will adopt two approaches to target quasars over 7500 sq. deg. First, a \"CORE\" quasar sample will combine optical selection in ugriz using a likelihood-based routine called XDQSOz, with a mid-IR-optical color-cut. eBOSS CORE selection (to g < 22 OR r < 22) should return ~ 70 quasars per sq. deg. at redshifts 0.9 < z < 2.2 and ~7 z > 2.1 quasars per sq. deg. Second, a selection based on variability in multi-epoch imaging from the Palomar Transient Factory should recover an additional ~3-4 z > 2.1 quasars per sq. deg. to g < 22.5. A linear model of how imaging systematics affect target density recovers the angular distribution of eBOSS CORE quasars over 96.7% (76.7%) of the SDSS North (South) Galactic Cap area. The eBOSS CORE quasar sample should thus be sufficiently dense and homogeneous over 0.9 < z < 2.2 to yield the first few-percent-level BAO constraint near z~1.5. eBOSS quasars at z > 2.1 will be used to improve BAO measurements in the Lyman-alpha Forest. Beyond its key cosmological goals, eBOSS should be the next-generation quasar survey, comprising > 500,000 new quasars and > 500,000 uniformly selected spectroscopically confirmed 0.9 < z < 2.2 quasars. At the conclusion of eBOSS, the SDSS will have provided unique spectra of over 800,000 quasars.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We present a new, fully generative model of optical telescope image sets, along with a variational procedure for inference. Each pixel intensity is treated as a Poisson random variable, with a rate parameter dependent on latent properties of stars and galaxies. Key latent properties are themselves random, with scientific prior distributions constructed from large ancillary data sets. We check our approach on synthetic images. We also run it on images from a major sky survey, where it exceeds the performance of the current state-of-the-art method for locating celestial bodies and measuring their colors.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The third generation of the Sloan Digital Sky Survey (SDSS-III) took data from 2008 to 2014 using the original SDSS wide-field imager, the original and an upgraded multi-object fiber-fed optical spectrograph, a new near-infrared high-resolution spectrograph, and a novel optical interferometer. All the data from SDSS-III are now made public. In particular, this paper describes Data Release 11 (DR11) including all data acquired through 2013 July, and Data Release 12 (DR12) adding data acquired through 2014 July (including all data included in previous data releases), marking the end of SDSS-III observing. Relative to our previous public release (DR10), DR12 adds one million new spectra of galaxies and quasars from the Baryon Oscillation Spectroscopic Survey (BOSS) over an additional 3000 sq. deg of sky, more than triples the number of H-band spectra of stars as part of the Apache Point Observatory (APO) Galactic Evolution Experiment (APOGEE), and includes repeated accurate radial velocity measurements of 5500 stars from the Multi-Object APO Radial Velocity Exoplanet Large-area Survey (MARVELS). The APOGEE outputs now include measured abundances of 15 different elements for each star. In total, SDSS-III added 2350 sq. deg of ugriz imaging; 155,520 spectra of 138,099 stars as part of the Sloan Exploration of Galactic Understanding and Evolution 2 (SEGUE-2) survey; 2,497,484 BOSS spectra of 1,372,737 galaxies, 294,512 quasars, and 247,216 stars over 9376 sq. deg; 618,080 APOGEE spectra of 156,593 stars; and 197,040 MARVELS spectra of 5,513 stars. Since its first light in 1998, SDSS has imaged over 1/3 of the Celestial sphere in five bands and obtained over five million astronomical spectra.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We present the correlations between stellar mass, star formation rate (SFR) and [NII]/Ha flux ratio as indicator of gas-phase metallicity for a sample of 222 galaxies at 0.8 < z < 2.6 and log(M*/Msun)=9.0-11.5 from the LUCI, SINS/zC-SINF and KMOS3D surveys. This sample provides a unique analysis of the mass-metallicity relation (MZR) over an extended redshift range using consistent data analysis techniques and strong-line metallicity indicator. We find a constant slope at the low-mass end of the relation and can fully describe its redshift evolution through the evolution of the characteristic turnover mass where the relation begins to flatten at the asymptotic metallicity. At fixed mass and redshift, our data do not show a correlation between the [NII]/Ha ratio and SFR, which disagrees with the 0.2-0.3dex offset in [NII]/Ha predicted by the \"fundamental relation\" between stellar mass, SFR and metallicity discussed in recent literature. However, the overall evolution towards lower [NII]/Ha at earlier times does broadly agree with these predictions.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The Sloan Digital Sky Survey (SDSS) has been in operation since 2000 April. This paper presents the tenth public data release (DR10) from its current incarnation, SDSS-III. This data release includes the first spectroscopic data from the Apache Point Observatory Galaxy Evolution Experiment (APOGEE), along with spectroscopic data from the Baryon Oscillation Spectroscopic Survey (BOSS) taken through 2012 July. The APOGEE instrument is a near-infrared R~22,500 300-fiber spectrograph covering 1.514--1.696 microns. The APOGEE survey is studying the chemical abundances and radial velocities of roughly 100,000 red giant star candidates in the bulge, bar, disk, and halo of the Milky Way. DR10 includes 178,397 spectra of 57,454 stars, each typically observed three or more times, from APOGEE. Derived quantities from these spectra (radial velocities, effective temperatures, surface gravities, and metallicities) are also included.DR10 also roughly doubles the number of BOSS spectra over those included in the ninth data release. DR10 includes a total of 1,507,954 BOSS spectra, comprising 927,844 galaxy spectra; 182,009 quasar spectra; and 159,327 stellar spectra, selected over 6373.2 square degrees.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "We report constraints on antineutrino oscillation parameters that were obtained by using the two MINOS detectors to measure the 7% muon antineutrino component of the NuMI neutrino beam. In the Far Detector, we select 130 events in the charged-current muon antineutrino sample, compared to a prediction of 136.4 +/- 11.7(stat) ^{+10.2}_{-8.9}(syst) events under the assumption |dm2bar|=2.32x10^-3 eV^2, snthetabar=1.0. Assuming no oscillations occur at the Near Detector baseline, a fit to the two-flavor oscillation approximation constrains |dm2bar|<3.37x10^-3 eV^2 at the 90% confidence level with snthetabar=1.0.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "Multiwavelength observations of the high-frequency-peaked blazar 1ES2344+514 were performed from 2007 October to 2008 January. The campaign represents the first contemporaneous data on the object at very high energy (VHE, E >100 GeV) \u03b3-ray, X-ray, and UV energies. Observations with VERITAS in VHE \u03b3-rays yield a strong detection of 20 \u03c3 with 633 excess events in a total exposure of 18.1 hours live-time. A strong VHE \u03b3-ray flare on 2007 December 7 is measured at F(>300 GeV) = (6.76 \\pm 0.62) \\times 10-11 ph cm-2 s-1, corresponding to 48% of the Crab Nebula flux. Excluding this flaring episode, nightly variability at lower fluxes is observed with a time-averaged mean of F(>300 GeV) = (1.06 \\pm 0.09) \\times 10-11 ph cm-2 s-1 (7.6% of the Crab Nebula flux). The differential photon spectrum between 390 GeV and 8.3 TeV for the time-averaged observations excluding 2007 December 7 is well described by a power law with a photon index of \u0393 = 2.78 \\pm 0.09stat \\pm 0.15syst. Over the full period of VERITAS observations contemporaneous X-ray and UV data were taken with Swift and RXTE. The measured 2-10 keV flux ranged by a factor of ~7 during the campaign. On 2007 December 8 the highest ever observed X-ray flux from 1ES 2344+514 was measured by Swift XRT at a flux of F(2-10 keV) = (6.28 \\pm 0.31) \\times 10-11 erg cm-2 s-1. Evidence for a correlation between the X-ray flux and VHE \u03b3-ray flux on nightly time-scales is indicated with a Pearson correlation coefficient of r = 0.60 \\pm 0.11. Contemporaneous spectral energy distributions (SEDs) of 1ES 2344+514 are presented for two distinct flux states. A one-zone synchrotron self-Compton (SSC) model describes both SEDs using parameters consistent with previous SSC modeling of 1ES 2344+514 from non-contemporaneous observations.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The Sloan Digital Sky Survey (SDSS) started a new phase in August 2008, with new instrumentation and new surveys focused on Galactic structure and chemical evolution, measurements of the baryon oscillation feature in the clustering of galaxies and the quasar Ly alpha forest, and a radial velocity search for planets around ~8000 stars. This paper describes the first data release of SDSS-III (and the eighth counting from the beginning of the SDSS). The release includes five-band imaging of roughly 5200 deg^2 in the Southern Galactic Cap, bringing the total footprint of the SDSS imaging to 14,555 deg^2, or over a third of the Celestial Sphere. All the imaging data have been reprocessed with an improved sky-subtraction algorithm and a final, self-consistent photometric recalibration and flat-field determination. This release also includes all data from the second phase of the Sloan Extension for Galactic Understanding and Evolution (SEGUE-2), consisting of spectroscopy of approximately 118,000 stars at both high and low Galactic latitudes. All the more than half a million stellar spectra obtained with the SDSS spectrograph have been reprocessed through an improved stellar parameters pipeline, which has better determination of metallicity for high metallicity stars.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "The Laser Interferometer Gravitational Wave Observatory (LIGO) is a network of three detectors built to detect local perturbations in the space-time metric from astrophysical sources. These detectors, two in Hanford, WA and one in Livingston, LA, are power-recycled Fabry-Perot Michelson interferometers. In their fifth science run (S5), between November 2005 and October 2007, these detectors accumulated one year of triple coincident data while operating at their designed sensitivity. In this paper, we describe the calibration of the instruments in the S5 data set, including measurement techniques and uncertainty estimation.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "MINOS is a long baseline neutrino oscillation experiment that uses two detectors separated by 734 km. The readout systems used for the two detectors are different and have to be independently calibrated. To verify and make a direct comparison of the calibrated response of the two readout systems, test beam data were acquired using a smaller calibration detector. This detector was simultaneously instrumented with both readout systems and exposed to the CERN PS T7 test beam. Differences in the calibrated response of the two systems are shown to arise from differences in response non-linearity, photomultiplier crosstalk, and threshold effects at the few percent level. These differences are reproduced by the Monte Carlo (MC) simulation to better than 1% and a scheme that corrects for these differences by calibrating the MC to match the data in each detector separately is presented. The overall difference in calorimetric response between the two readout systems is shown to be consistent with zero to a precision of 1.3% in data and 0.3% in MC with no significant energy dependence.\n        \u25b3 Less", "author": "Jeffrey Lang"}, {"abstract": "This paper investigates a variant of the work-stealing algorithm that we call the localized work-stealing algorithm. The intuition behind this variant is that because of locality, processors can benefit from working on their own work. Consequently, when a processor is free, it makes a steal attempt to get back its own work. We call this type of steal a steal-back. We show that the expected running time of the algorithm is $T_1/P+O(T_\\infty P)$, and that under the \"even distribution of free agents assumption\", the expected running time of the algorithm is $T_1/P+O(T_\\infty\\lg P)$. In addition, we obtain another running-time bound based on ratios between the sizes of serial tasks in the computation. If $M$ denotes the maximum ratio between the largest and the smallest serial tasks of a processor after removing a total of $O(P)$ serial tasks across all processors from consideration, then the expected running time of the algorithm is $T_1/P+O(T_\\infty M)$.\n        \u25b3 Less", "author": "Charles Leiserson"}, {"abstract": "Inspired by applications in parallel computing, we analyze the setting of work stealing in multithreaded computations. We obtain tight upper bounds on the number of steals when the computation can be modeled by rooted trees. In particular, we show that if the computation with $n$ processors starts with one processor having a complete $k$-ary tree of height $h$ (and the remaining $n-1$ processors having nothing), the maximum possible number of steals is $\\sum_{i=1}^n(k-1)^i\\binom{h}{i}$.\n        \u25b3 Less", "author": "Charles Leiserson"}, {"abstract": "It is our view that the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks. This paper is a position paper defining the problem and announcing our intention to launch an open effort to define this standard.\n        \u25b3 Less", "author": "Charles Leiserson"}, {"abstract": "Negative probability values have been widely employed as an indicator of the nonclassicality of quantum systems. Known as a quasiprobability distribution, they are regarded as a useful tool that provides significant insight into the underlying fundamentals of quantum theory when compared to the classical statistics. However, in this approach, an operational interpretation of these negative values with respect to the definition of probability---the relative frequency of occurred event---is missing. An alternative approach is therefore considered where the quasiprobability operationally reveals the negativity of measured quantities. We here present an experimental realization of the operational quasiprobability, which consists of sequential measurements in time. To this end, we implement two sets of polarization measurements of single photons. We find that the measured negativity can be interpreted in the context of selecting measurements, and it reflects the nonclassical nature of photons. Our results suggest a new operational way to unravel the nonclassicality of photons in the context of measurement selection.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "Public safety networks avail to disseminate information during emergency situations through its dedicated servers. Public safety networks accommodate public safety communication (PSC) applications to track the location of its utilizers and enable to sustain transmissions even in the crucial scenarios. Despite that, if the traditional setups responsible for PSCs are unavailable, it becomes prodigiously arduous to handle any of the safety applications, which may cause havoc in the society. Dependence on a secondary network may assist to solve such an issue. But, the secondary networks should be facilely deployable and must not cause exorbitant overheads in terms of cost and operation. For this, LoRaWAN can be considered as an ideal solution as it provides low power and long-range communication. However, an excessive utilization of the secondary network may result in high depletion of its own resources and can lead to a complete shutdown of services, which is a quandary at hand. As a solution, this paper proposes a novel network model via a combination of LoRaWAN and traditional public safety networks, and uses a self-enforcing agreement based game theory for allocating resources efficiently amongst the available servers. The proposed approach adopts memory and energy constraints as agreements, which are satisfied through Nash equilibrium. The numerical results show that the proposed approach is capable of efficiently allocating the resources with sufficiently high gains for resource conservation, network sustainability, resource restorations and probability to continue at the present conditions even in the complete absence of traditional Access Points (APs) compared with a baseline scenario with no failure of nodes.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "OB associations are the prevailing star forming sites in the Galaxy. Up to now, the process of how OB associations were formed remained a mystery. A possible process is self-regulating star formation driven by feedback from massive stars. However, although a number of observational studies uncovered various signposts of feedback-driven star formation, the effectiveness of such feedback has been questioned. Stellar and gas kinematics is a promising tool to capture the relative motion of newborn stars and gas away from ionizing sources. We present high-resolution spectroscopy of stars and gas in the young open cluster NGC 1893. Our findings show that newborn stars and the tadpole nebula Sim 130 are moving away from the central cluster containing two O-type stars, and that the timescale of sequential star formation is about 1 Myr within a 9 parsec distance. The newborn stars formed by feedback from massive stars account for at least 18 per cent of the total stellar population in the cluster, suggesting that this process can play an important role in the formation of OB associations. These results support the self-regulating star formation model.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "Possible mechanisms of overlimiting current in unsupported electrolytes, exceeding diffusion limitation, have been intensely studied for their fundamental significance and applications to desalination, separations, sensing, and energy storage. In bulk membrane systems, the primary physical mechanism is electro-convection, driven by electro-osmotic instability on the membrane surface. It has recently been predicted that confinement by charged surfaces in microchannels or porous media favors two new mechanisms, electro-osmotic flow (EOF) and surface conduction (SC), driven by large electric fields in the depleted region acting on the electric double layers on the sidewalls. Here, we provide the first direct evidence for the transition from SC to EOF above a critical channel height, using in situ particle tracking and current-voltage measurements in a micro/nanofluidic device. The dependence of the over-limiting conductance on channel depth (d) is consistent with theoretical predictions, scaling as d^-1 for SC and d^4/5 for EOF with a transition around d=8um. This complete picture of surface-driven over-limiting current can guide engineering applications of ion concentration polarization phenomena in microfluidics and porous media.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "Nuclear masses ranging from O to Ti isotopes are systematically investigated with relativistic continuum Hartree-Bogoliubov (RCHB) theory, which can provide a proper treatment of pairing correlations in the presence of the continuum. From O to Ti isotopes, there are 402 nuclei predicted to be bound by the density functional PC-PK1. For the 234 nuclei with mass measured, the root mean square (rms) deviation is 2.23 MeV. It is found that the proton drip-lines predicted with various mass models are roughly the same and basically agree with the observation. The neutron drip-lines predicted, however, are quite different. Due to the continuum couplings, the neutron drip-line nuclei predicted are extended further neutron-rich than other mass models. By comparison with finite-range droplet model (FRDM), the neutron drip-line nucleus predicted by RCHB theory has respectively 2(O), 10(Ne), 10(Na), 6(Mg), 8(Al), 6(Si), 8(P), 6(S), 14(K), 10(Ca), 10(Sc), and 12(Ti) more neutrons.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "We present an analysis of the papers published in the journals Nature and Science in the years from 2006 to 2010. During this period, a total of 7788 papers were published in the two journals. This includes 544 astronomy papers that comprise 7.0% of the papers in `all' research fields and 18.9% of those in the fields of `physical sciences'. The sub-fields of research of the astronomy papers are distributed, in descending order of number of papers, in Solar System, stellar astronomy, galaxies and the universe, the Milky Way Galaxy, and exoplanets. The observational facilities used for the studies are mainly ground-based telescopes (31.1%), spacecrafts (27.0%), and space telescopes (22.8%), while 16.0% of papers did not use any noticeable facilities and 1.7% used other facilities. Korean scientists have published 86 papers (33 in Nature and 53 in Science), which is 1.10% of all the papers (N=7788) in the two journals. The share of papers by Korean astronomers among the scientific papers by Koreans is 8.14%, slightly higher than the contribution of astronomy papers (7.0%) in both journals.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "Detecting the edges of objects within images is critical for quality image processing. We present an edge-detecting technique that uses morphological amoebas that adjust their shape based on variation in image contours. We evaluate the method both quantitatively and qualitatively for edge detection of images, and compare it to classic morphological methods. Our amoeba-based edge-detection system performed better than the classic edge detectors.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "Graphene has shown great application opportunities in future nanoelectronic devices due to its outstanding electronic properties. Moreover, its impressive optical properties have been attracting the interest of researchers, and, recently, the photovoltaic effects of a heterojunction structure embedded with few layer graphene (FLG) have been demonstrated. Here, we report the photovoltaic response of graphene-semiconductor junctions and the controlled open-circuit voltage (Voc) with varying numbers of graphene layers. After unavoidably adsorbed contaminants were removed from the FLGs, by means of in situ annealing, prepared by layer-by-layer transfer of the chemically grown graphene layer, the work functions of FLGs showed a sequential increase as the graphene layers increase, despite of random interlayer-stacking, resulting in the modulation of photovoltaic behaviors of FLGs/Si interfaces. The surface photovoltaic effects observed here show an electronic realignment in the depth direction in the FLG heterojunction systems, indicating future potential toward solar devices utilizing the excellent transparency and flexibility of FLG.\n        \u25b3 Less", "author": "Jae Lim"}, {"abstract": "Optimistic concurrency control (OCC) can exploit the strengths of parallel hardware to provide excellent performance for uncontended transactions, and is popular in high-performance in-memory databases and transactional systems. But at high contention levels, OCC is susceptible to frequent aborts, leading to wasted work and degraded performance. Contention managers, mixed optimistic/pessimistic concurrency control algorithms, and novel optimistic-inspired concurrency control algorithms, such as TicToc, aim to address this problem, but these mechanisms introduce sometimes-high overheads of their own. We show that in real-world benchmarks, traditional OCC can outperform these alternative mechanisms by simply adding fine-grained version timestamps (using different timestamps for disjoint components of each record). With fine-grained timestamps, OCC gets 1.14x TicToc's throughput in TPC-C at 128 cores (previous work reported TicToc having 1.8x higher throughput than OCC at 80 hyperthreads). Our study shows that timestamp granularity has a greater impact than previously thought on the performance of transaction processing systems, and should not be overlooked in the push for faster concurrency control schemes.\n        \u25b3 Less", "author": "Barbara Liskov"}, {"abstract": "Due to the difficulty in detecting and manipulating magnetic states of antiferromagnetic materials, studying their switching dynamics using electrical methods remains a challenging task. In this work, by employing heavy metal/rare earth-transition metal alloy bilayers, we experimentally studied current-induced domain wall dynamics in an antiferromagnetically coupled system. We show that the current-induced domain wall mobility reaches a maximum close to the angular momentum compensation. With experiment and modelling, we further reveal the internal structures of domain walls and the underlying mechanisms for their fast motion. We show that the chirality of the ferrimagnetic domain walls remains the same across the compensation points, suggesting that spin orientations of specific sublattices rather than net magnetization determine Dzyaloshinskii-Moriya interaction in heavy metal/ferrimagnet bilayers. The high current-induced domain wall mobility and the robust domain wall chirality in compensated ferrimagnetic material opens new opportunities for high-speed spintronic devices.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "Recent studies on the magneto-transport properties of topological insulators (TI) have attracted great attention due to the rich spin-orbit physics and promising applications in spintronic devices. Particularly the strongly spin-moment coupled electronic states have been extensively pursued to realize efficient spin-orbit torque (SOT) switching. However, so far current-induced magnetic switching with TI has only been observed at cryogenic temperatures. It remains a controversial issue whether the topologically protected electronic states in TI could benefit spintronic applications at room temperature. In this work, we report full SOT switching in a TI/ferromagnet bilayer heterostructure with perpendicular magnetic anisotropy at room temperature. The low switching current density provides a definitive proof on the high SOT efficiency from TI. The effective spin Hall angle of TI is determined to be several times larger than commonly used heavy metals. Our results demonstrate the robustness of TI as an SOT switching material and provide a direct avenue towards applicable TI-based spintronic devices.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "Despite the potential advantages of information storage in antiferromagnetically coupled materials, it remains unclear whether one can control the magnetic moment orientation efficiently because of the cancelled magnetic moment. Here, we report spin-orbit torque induced magnetization switching of ferrimagnetic Co1-xTbx films with perpendicular magnetic anisotropy. Current induced switching is demonstrated in all of the studied film compositions, including those near the magnetization compensation point. The spin-orbit torque induced effective field is further quantified in the domain wall motion regime. A divergent behavior that scales with the inverse of magnetic moment is confirmed close to the compensation point, which is consistent with angular momentum conservation. Moreover, we also quantify the Dzyaloshinskii-Moriya interaction energy in the Ta/Co1-xTbx system and we find that the energy density increases as a function of the Tb concentration. The demonstrated spin-orbit torque switching, in combination with the fast magnetic dynamics and minimal net magnetization of ferrimagnetic alloys, promises spintronic devices that are faster and with higher density than traditional ferromagnetic systems.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "We demonstrate that the charge-spin conversion efficiency of topological insulators (TI) can be experimentally determined by injecting spin-polarized tunneling electrons into a TI. Through a comparative study between bismuth selenide and bismuth antimony telluride, we verified the topological-surface-state origin of the observed giant spin signals. By injecting energetic electrons into bismuth selenide, we further studied the energy dependence of the effective spin polarization at the TI surface. The experimentally verified large spin polarization, as well as our calculations, provides new insights into optimizing TI materials for near room-temperature spintronic applications.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "Two promising strategies for achieving efficient control of magnetization in future magnetic memory and non-volatile spin logic devices are spin transfer torque from spin polarized currents and voltage-controlled magnetic anisotropy (VCMA). Spin transfer torque is in widespread development as the write mechanism for next-generation magnetic memory, while VCMA offers the potential of even better energy performance due to smaller Ohmic losses. Here we introduce a 3-terminal magnetic tunnel junction (MTJ) device that combines both of these mechanisms to achieve new functionality: gate-voltage-modulated spin torque switching. This gating makes possible both more energy-efficient switching and also improved architectures for memory and logic applications, including a simple approach for making magnetic memories with a maximum-density cross-point geometry that does not require a control transistor for every MTJ.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "We show that direct current in a tantalum microstrip can induce steady-state magnetic oscillations in an adjacent nanomagnet through spin torque from the spin Hall effect (SHE). The oscillations are detected electrically via a magnetic tunnel junction (MTJ) contacting the nanomagnet. The oscillation frequency can be controlled using the MTJ bias to tune the magnetic anisotropy. In this 3-terminal device the SHE torque and the MTJ bias therefore provide independent controls of the oscillation amplitude and frequency, enabling new approaches for developing tunable spin torque nano-oscillators.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "We report a giant spin Hall effect (SHE) in \u03b2-W thin films. Using spin torque induced ferromagnetic resonance with a \u03b2-W/CoFeB bilayer microstrip we determine the spin Hall angle to be |\u03b8|=0.30\\pm0.02, large enough for an in-plane current to efficiently reverse the orientation of an in-plane magnetized CoFeB free layer of a nanoscale magnetic tunnel junction adjacent to a thin \u03b2-W layer. From switching data obtained with such 3-terminal devices we independently determine |\u03b8|=0.33\\pm0.06. We also report variation of the spin Hall switching efficiency with W layers of different resistivities and hence of variable (\u03b1 and \u03b2) phase composition.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "A pure spin current generated within a nonlocal spin valve can exert a spin transfer torque on a nanomagnet. This nonlocal torque enables new design schemes for magnetic memory devices that do not require the application of large voltages across tunnel barriers that can suffer electrical breakdown. Here we report a quantitative measurement of this nonlocal spin torque using spin-torque-driven ferromagnetic resonance. Our measurement agrees well with the prediction of an effective circuit model for spin transport. Based on this model, we suggest strategies for optimizing the strength of nonlocal torque.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "We report a giant spin Hall effect (SHE) in \u03b2-Ta that generates spin currents intense enough to induce efficient spin-transfer-torque switching of ferromagnets, thereby providing a new approach for controlling magnetic devices that can be superior to existing technologies. We quantify this SHE by three independent methods and demonstrate spin-torque (ST) switching of both out-of-plane and in-plane magnetized layers. We implement a three-terminal device that utilizes current passing through a low impedance Ta-ferromagnet bilayer to effect switching of a nanomagnet, with a higher-impedance magnetic tunnel junction for read-out. The efficiency and reliability of this device, together with its simplicity of fabrication, suggest that this three-terminal SHE-ST design can eliminate the main obstacles currently impeding the development of magnetic memory and non-volatile spin logic technologies.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "Several different experimental techniques have been used in efforts to measure the spin Hall conductivity and the spin Hall angle in Pt samples at room temperature, with results that disagree by more than a factor of 20, with spin Hall conductivities from 2.4 x 10^4 to 5.1 x 10^5 [hbar/(2e)] (Ohm-m)^-1 and spin Hall angles from 0.0037 to 0.08. We review this work, and analyze possible reasons for the discrepancies. We explain that the smallest values for the spin Hall angle that have been reported, based on measurements of lateral permalloy/copper/platinum devices, are incorrect because the original analyses did not properly take into account that copper layers in these devices will shunt charge current flowing through adjacent platinum wires, thereby greatly reducing the size of the spin-Hall-related signals. We suggest that differences between the results for the spin Hall angle found by other experimental techniques are primarily a consequence of different assumptions about the value of the spin diffusion length in Pt. We present a new measurement of the spin diffusion length in Pt within sputtered Pt/permalloy bilayer thin films at room temperature, finding 1.4 \\pm 0.3 nm, a much smaller value than has generally been assumed previously. With this value for the spin diffusion length, the previously-discordant results can be brought into much better agreement, with the result that the spin Hall conductivities are (1.4 - 3.4) x 10^5 [hbar/(2e)] (Ohm-m)^-1 and the spin Hall angles are greater than 0.05. These values are sufficiently large that the spin Hall effect in Pt can be used to generate spin currents and spin transfer torques strong enough for efficient manipulation of magnetic moments in adjacent ferromagnetic layers.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "The spin Hall effect (SHE) generates spin currents within nonmagnetic materials. Previously, studies of the SHE have been motivated primarily to understand its fundamental origin and magnitude. Here we demonstrate, using measurement and modeling, that in a Pt/Co bilayer with perpendicular magnetic anisotropy the SHE can produce a spin transfer torque that is strong enough to efficiently rotate and reversibly switch the Co magnetization, thereby providing a new strategy both to understand the SHE and to manipulate magnets. We suggest that the SHE torque can have a similarly strong influence on current-driven magnetic domain wall motion in Pt/ferromagnet multilayers. We estimate that in optimized devices the SHE torque can switch magnetic moments using currents comparable to those in magnetic tunnel junctions operated by conventional spin-torque switching, meaning that the SHE can enable magnetic memory and logic devices with similar performance but simpler architecture than the current state of the art.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "We demonstrate that the spin Hall effect in a thin film with strong spin-orbit scattering can excite magnetic precession in an adjacent ferromagnetic film. The flow of alternating current through a Pt/NiFe bilayer generates an oscillating transverse spin current in the Pt, and the resultant transfer of spin angular momentum to the NiFe induces ferromagnetic resonance (FMR) dynamics. The Oersted field from the current also generates an FMR signal but with a different symmetry. The ratio of these two signals allows a quantitative determination of the spin current and the spin Hall angle.\n        \u25b3 Less", "author": "Luqiao Liu"}, {"abstract": "We present the results of dust emission polarization measurements of Ophiuchus-B (Oph-B) carried out using the Submillimetre Common-User Bolometer Array 2 (SCUBA-2) camera with its associated polarimeter (POL-2) on the James Clerk Maxwell Telescope (JCMT) in Hawaii. This work is part of the B-fields In Star-forming Region Observations (BISTRO) survey initiated to understand the role of magnetic fields in star formation for nearby star-forming molecular clouds. We present a first look at the geometry and strength of magnetic fields in Oph-B. The field geometry is traced over $\\sim$0.2 pc, with clear detection of both of the sub-clumps of Oph-B. The field pattern appears significantly disordered in sub-clump Oph-B1. The field geometry in Oph-B2 is more ordered, with a tendency to be along the major axis of the clump, parallel to the filamentary structure within which it lies. The degree of polarization decreases systematically towards the dense core material in the two sub-clumps. The field lines in the lower density material along the periphery are smoothly joined to the large scale magnetic fields probed by NIR polarization observations. We estimated a magnetic field strength of 630$\\pm$410 $\u03bc$G in the Oph-B2 sub-clump using a Davis-Chandeasekhar-Fermi analysis. With this magnetic field strength, we find a mass-to-flux ratio $\u03bb$= 1.6$\\pm$1.1, which suggests that the Oph-B2 clump is slightly magnetically supercritical.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "We present 850 $\u03bc$m imaging polarimetry data of the $\u03c1$ Oph-A core taken with the Submillimeter Common-User Bolometer Array-2 (SCUBA-2) and its polarimeter (POL-2), as part of our ongoing survey project, BISTRO (B-fields In STar forming RegiOns). The polarization vectors are used to identify the orientation of the magnetic field projected on the plane of the sky at a resolution of 0.01 pc. We identify 10 subregions with distinct polarization fractions and angles in the 0.2 pc $\u03c1$ Oph A core; some of them can be part of a coherent magnetic field structure in the $\u03c1$ Oph region. The results are consistent with previous observations of the brightest regions of $\u03c1$ Oph-A, where the degrees of polarization are at a level of a few percents, but our data reveal for the first time the magnetic field structures in the fainter regions surrounding the core where the degree of polarization is much higher ($> 5 \\%$). A comparison with previous near-infrared polarimetric data shows that there are several magnetic field components which are consistent at near-infrared and submillimeter wavelengths. Using the Davis-Chandrasekhar-Fermi method, we also derive magnetic field strengths in several sub-core regions, which range from approximately 0.2 to 5 mG. We also find a correlation between the magnetic field orientations projected on the sky with the core centroid velocity components.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R&D activities required to prepare for this software upgrade.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "The detection of GW170817 in both gravitational waves and electromagnetic waves heralds the age of gravitational-wave multi-messenger astronomy. On 17 August 2017 the Advanced LIGO and Virgo detectors observed GW170817, a strong signal from the merger of a binary neutron-star system. Less than 2 seconds after the merger, a gamma-ray burst (GRB 170817A) was detected within a region of the sky consistent with the LIGO-Virgo-derived location of the gravitational-wave source. This sky region was subsequently observed by optical astronomy facilities, resulting in the identification of an optical transient signal within $\\sim 10$ arcsec of the galaxy NGC 4993. These multi-messenger observations allow us to use GW170817 as a standard siren, the gravitational-wave analog of an astronomical standard candle, to measure the Hubble constant. This quantity, which represents the local expansion rate of the Universe, sets the overall scale of the Universe and is of fundamental importance to cosmology. Our measurement combines the distance to the source inferred purely from the gravitational-wave signal with the recession velocity inferred from measurements of the redshift using electromagnetic data. This approach does not require any form of cosmic \"distance ladder;\" the gravitational wave analysis can be used to estimate the luminosity distance out to cosmological scales directly, without the use of intermediate astronomical distance measurements. We determine the Hubble constant to be $70.0^{+12.0}_{-8.0} \\, \\mathrm{km} \\, \\mathrm{s}^{-1} \\, \\mathrm{Mpc}^{-1}$ (maximum a posteriori and 68% credible interval). This is consistent with existing measurements, while being completely independent of them. Additional standard-siren measurements from future gravitational-wave sources will provide precision constraints of this important cosmological parameter.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "We present the first results from the B-fields In STar-forming Region Observations (BISTRO) survey, using the Sub-millimetre Common-User Bolometer Array 2 (SCUBA-2) camera, with its associated polarimeter (POL-2), on the James Clerk Maxwell Telescope (JCMT) in Hawaii. We discuss the survey's aims and objectives. We describe the rationale behind the survey, and the questions which the survey will aim to answer. The most important of these is the role of magnetic fields in the star formation process on the scale of individual filaments and cores in dense regions. We describe the data acquisition and reduction processes for POL-2, demonstrating both repeatability and consistency with previous data. We present a first-look analysis of the first results from the BISTRO survey in the OMC 1 region. We see that the magnetic field lies approximately perpendicular to the famous 'integral filament' in the densest regions of that filament. Furthermore, we see an 'hour-glass' magnetic field morphology extending beyond the densest region of the integral filament into the less-dense surrounding material, and discuss possible causes for this. We also discuss the more complex morphology seen along the Orion Bar region. We examine the morphology of the field along the lower-density north-eastern filament. We find consistency with previous theoretical models that predict magnetic fields lying parallel to low-density, non-self-gravitating filaments, and perpendicular to higher-density, self-gravitating filaments.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "Unlike other industries in which intellectual property is patentable, the financial industry relies on trade secrecy to protect its business processes and methods, which can obscure critical financial risk exposures from regulators and the public. We develop methods for sharing and aggregating such risk exposures that protect the privacy of all parties involved and without the need for a trusted third party. Our approach employs secure multi-party computation techniques from cryptography in which multiple parties are able to compute joint functions without revealing their individual inputs. In our framework, individual financial institutions evaluate a protocol on their proprietary data which cannot be inverted, leading to secure computations of real-valued statistics such a concentration indexes, pairwise correlations, and other single- and multi-point statistics. The proposed protocols are computationally tractable on realistic sample sizes. Potential financial applications include: the construction of privacy-preserving real-time indexes of bank capital and leverage ratios; the monitoring of delegated portfolio investments; financial audits; and the publication of new indexes of proprietary trading strategies.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "The quantitative aspirations of economists and financial analysts have for many years been based on the belief that it should be possible to build models of economic systems - and financial markets in particular - that are as predictive as those in physics. While this perspective has led to a number of important breakthroughs in economics, \"physics envy\" has also created a false sense of mathematical precision in some cases. We speculate on the origins of physics envy, and then describe an alternate perspective of economic behavior based on a new taxonomy of uncertainty. We illustrate the relevance of this taxonomy with two concrete examples: the classical harmonic oscillator with some new twists that make physics look more like economics, and a quantitative equity market-neutral strategy. We conclude by offering a new interpretation of tail events, proposing an \"uncertainty checklist\" with which our taxonomy can be implemented, and considering the role that quants played in the current financial crisis.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "We construct a financial \"Turing test\" to determine whether human subjects can differentiate between actual vs. randomized financial returns. The experiment consists of an online video-game (http://arora.ccs.neu.edu) where players are challenged to distinguish actual financial market returns from random temporal permutations of those returns. We find overwhelming statistical evidence (p-values no greater than 0.5%) that subjects can consistently distinguish between the two types of time series, thereby refuting the widespread belief that financial markets \"look random.\" A key feature of the experiment is that subjects are given immediate feedback regarding the validity of their choices, allowing them to learn and adapt. We suggest that such novel interfaces can harness human capabilities to process and extract information from financial data in ways that computers cannot.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "We propose to study market efficiency from a computational viewpoint. Borrowing from theoretical computer science, we define a market to be \\emph{efficient with respect to resources $S$} (e.g., time, memory) if no strategy using resources $S$ can make a profit. As a first step, we consider memory-$m$ strategies whose action at time $t$ depends only on the $m$ previous observations at times $t-m,...,t-1$. We introduce and study a simple model of market evolution, where strategies impact the market by their decision to buy or sell. We show that the effect of optimal strategies using memory $m$ can lead to \"market conditions\" that were not present initially, such as (1) market bubbles and (2) the possibility for a strategy using memory $m' > m$ to make a bigger profit than was initially possible. We suggest ours as a framework to rationalize the technological arms race of quantitative trading firms.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "The Advanced Technology Large-Aperture Space Telescope (ATLAST) is a set of mission concepts for the next generation of UVOIR space observatory with a primary aperture diameter in the 8-m to 16-m range that will allow us to perform some of the most challenging observations to answer some of our most compelling questions, including \"Is there life elsewhere in the Galaxy?\" We have identified two different telescope architectures, but with similar optical designs, that span the range in viable technologies. The architectures are a telescope with a monolithic primary mirror and two variations of a telescope with a large segmented primary mirror. This approach provides us with several pathways to realizing the mission, which will be narrowed to one as our technology development progresses. The concepts invoke heritage from HST and JWST design, but also take significant departures from these designs to minimize complexity, mass, or both.\n  Our report provides details on the mission concepts, shows the extraordinary scientific progress they would enable, and describes the most important technology development items. These are the mirrors, the detectors, and the high-contrast imaging technologies, whether internal to the observatory, or using an external occulter. Experience with JWST has shown that determined competitors, motivated by the development contracts and flight opportunities of the new observatory, are capable of achieving huge advances in technical and operational performance while keeping construction costs on the same scale as prior great observatories.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "We consider communication between two parties using a bipartite quantum operation, which constitutes the most general quantum mechanical model of two-party communication. We primarily focus on the simultaneous forward and backward communication of classical messages. For the case in which the two parties share unlimited prior entanglement, we give inner and outer bounds on the achievable rate region that generalize classical results due to Shannon. In particular, using a protocol of Bennett, Harrow, Leung, and Smolin, we give a one-shot expression in terms of the Holevo information for the entanglement-assisted one-way capacity of a two-way quantum channel. As applications, we rederive two known additivity results for one-way channel capacities: the entanglement-assisted capacity of a general one-way channel, and the unassisted capacity of an entanglement-breaking one-way channel.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "In this review article we explore several recent advances in the quantitative modeling of financial markets. We begin with the Efficient Markets Hypothesis and describe how this controversial idea has stimulated a number of new directions of research, some focusing on more elaborate mathematical models that are captable of rationalizing the empirical facrts, others taking a completely different different tack in rejecting rationality altogether. One of the most promising directions is to view financial markets from a biological perspective and, specifically, with an evolutionary framework in which markets, instruments, institutions, and investors interact and evolve dynamically according to the \"law\" of economic selection. Under this view, financial agents compete and adapt, but they do not necessarily do so in an optimal fashion. Evolutionary and ecological models of financial markets is truly a new frontier whose exploration has just begun.\n        \u25b3 Less", "author": "Andrew W. Lo"}, {"abstract": "Multi-object manipulation problems in continuous state and action spaces can be solved by planners that search over sampled values for the continuous parameters of operators. The efficiency of these planners depends critically on the effectiveness of the samplers used, but effective sampling in turn depends on details of the robot, environment, and task. Our strategy is to learn functions called specializers that generate values for continuous operator parameters, given a state description and values for the discrete parameters. Rather than trying to learn a single specializer for each operator from large amounts of data on a single task, we take a modular meta-learning approach. We train on multiple tasks and learn a variety of specializers that, on a new task, can be quickly adapted using relatively little data -- thus, our system \"learns quickly to plan quickly\" using these specializers. We validate our approach experimentally in simulated 3D pick-and-place tasks with continuous state and action spaces.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "In this paper, we propose a learning algorithm that speeds up the search in task and motion planning problems. Our algorithm proposes solutions to three different challenges that arise in learning to improve planning efficiency: what to predict, how to represent a planning problem instance, and how to transfer knowledge from one problem instance to another. We propose a method that predicts constraints on the search space based on a generic representation of a planning problem instance, called score-space, where we represent a problem instance in terms of the performance of a set of solutions attempted so far. Using this representation, we transfer knowledge, in the form of constraints, from previous problems based on the similarity in score space. We design a sequential algorithm that efficiently predicts these constraints, and evaluate it in three different challenging task and motion planning problems. Results indicate that our approach performs orders of magnitudes faster than an unguided planner\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "Many prediction problems, such as those that arise in the context of robotics, have a simplifying underlying structure that could accelerate learning. In this paper, we present a strategy for learning a set of neural network modules that can be combined in different ways. We train different modular structures on a set of related tasks and generalize to new tasks by composing the learned modules in new ways. We show this improves performance in two robotics-related problems.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "In many robotic applications, an autonomous agent must act within and explore a partially observed environment that is unobserved by its human teammate. We consider such a setting in which the agent can, while acting, transmit declarative information to the human that helps them understand aspects of this unseen environment. In this work, we address the algorithmic question of how the agent should plan out what actions to take and what information to transmit. Naturally, one would expect the human to have preferences, which we model information-theoretically by scoring transmitted information based on the change it induces in weighted entropy of the human's belief state. We formulate this setting as a belief MDP and give a tractable algorithm for solving it approximately. Then, we give an algorithm that allows the agent to learn the human's preferences online, through exploration. We validate our approach experimentally in simulated discrete and continuous partially observed search-and-recover domains. Visit http://tinyurl.com/chitnis-corl-18 for a supplementary video.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "In many applications that involve processing high-dimensional data, it is important to identify a small set of entities that account for a significant fraction of detections. Rather than formalize this as a clustering problem, in which all detections must be grouped into hard or soft categories, we formalize it as an instance of the frequent items or heavy hitters problem, which finds groups of tightly clustered objects that have a high density in the feature space. We show that the heavy hitters formulation generates solutions that are more accurate and effective than the clustering formulation. In addition, we present a novel online algorithm for heavy hitters, called HAC, which addresses problems in continuous space, and demonstrate its effectiveness on real video and household domains.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "The objective of this work is to augment the basic abilities of a robot by learning to use new sensorimotor primitives to enable the solution of complex long-horizon problems. Solving long-horizon problems in complex domains requires flexible generative planning that can combine primitive abilities in novel combinations to solve problems as they arise in the world. In order to plan to combine primitive actions, we must have models of the preconditions and effects of those actions: under what circumstances will executing this primitive achieve some particular effect in the world?\n  We use, and develop novel improvements on, state-of-the-art methods for active learning and sampling. We use Gaussian process methods for learning the conditions of operator effectiveness from small numbers of expensive training examples collected by experimentation on a robot. We develop adaptive sampling methods for generating diverse elements of continuous sets (such as robot configurations and object poses) during planning for solving a new task, so that planning is as efficient as possible. We demonstrate these methods in an integrated system, combining newly learned models with an efficient continuous-space robot task and motion planner to learn to solve long horizon problems more efficiently than was previously possible.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "In partially observed environments, it can be useful for a human to provide the robot with declarative information that represents probabilistic relational constraints on properties of objects in the world, augmenting the robot's sensory observations. For instance, a robot tasked with a search-and-rescue mission may be informed by the human that two victims are probably in the same room. An important question arises: how should we represent the robot's internal knowledge so that this information is correctly processed and combined with raw sensory information? In this paper, we provide an efficient belief state representation that dynamically selects an appropriate factoring, combining aspects of the belief when they are correlated through information and separating them when they are not. This strategy works in open domains, in which the set of possible objects is not known in advance, and provides significant improvements in inference time over a static factoring, leading to more efficient planning for complex partially observed tasks. We validate our approach experimentally in two open-domain planning problems: a 2D discrete gridworld task and a 3D continuous cooking task. A supplementary video can be found at http://tinyurl.com/chitnis-iros-18.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "Many planning applications involve complex relationships defined on high-dimensional, continuous variables. For example, robotic manipulation requires planning with kinematic, collision, and motion constraints involving robot configurations, object transforms, and robot trajectories. These constraints typically require specialized procedures to sample satisfying values. We extend the STRIPS planning language to support a generic, declarative specification for these procedures while treating their implementation as blackboxes. We provide several domain-independent algorithms that reduce STRIPStream problems to a sequence of finite-domain STRIPS planning problems. Additionally, we describe cost-sensitive planning within this framework. Finally, we evaluate our algorithms on three robotic task and motion planning domains.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "This paper presents a general-purpose formulation of a large class of discrete-time planning problems, with hybrid state and control-spaces, as factored transition systems. Factoring allows state transitions to be described as the intersection of several constraints each affecting a subset of the state and control variables. Robotic manipulation problems with many movable objects involve constraints that only affect several variables at a time and therefore exhibit large amounts of factoring. We develop a theoretical framework for solving factored transition systems with sampling-based algorithms. The framework characterizes conditions on the submanifold in which solutions lie, leading to a characterization of robust feasibility that incorporates dimensionality-reducing constraints. It then connects those conditions to corresponding conditional samplers that can be composed to produce values on this submanifold. We present two domain-independent, probabilistically complete planning algorithms that take, as input, a set of conditional samplers. We demonstrate the empirical efficiency of these algorithms on a set of challenging task and motion planning problems involving picking, placing, and pushing.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "In robotics, it is essential to be able to plan efficiently in high-dimensional continuous state-action spaces for long horizons. For such complex planning problems, unguided uniform sampling of actions until a path to a goal is found is hopelessly inefficient, and gradient-based approaches often fall short when the optimization manifold of a given problem is not smooth. In this paper we present an approach that guides the search of a state-space planner, such as A*, by learning an action-sampling distribution that can generalize across different instances of a planning problem. The motivation is that, unlike typical learning approaches for planning for continuous action space that estimate a policy, an estimated action sampler is more robust to error since it has a planner to fall back on. We use a Generative Adversarial Network (GAN), and address an important issue: search experience consists of a relatively large number of actions that are not on a solution path and a relatively small number of actions that actually are on a solution path. We introduce a new technique, based on an importance-ratio estimation method, for using samples from a non-target distribution to make GAN learning more data-efficient. We provide theoretical guarantees and empirical evaluation in three challenging continuous robot planning problems to illustrate the effectiveness of our algorithm.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "As drones and autonomous cars become more widespread it is becoming increasingly important that robots can operate safely under realistic conditions. The noisy information fed into real systems means that robots must use estimates of the environment to plan navigation. Efficiently guaranteeing that the resulting motion plans are safe under these circumstances has proved difficult. We examine how to guarantee that a trajectory or policy is safe with only imperfect observations of the environment. We examine the implications of various mathematical formalisms of safety and arrive at a mathematical notion of safety of a long-term execution, even when conditioned on observational information. We present efficient algorithms that can prove that trajectories or policies are safe with much tighter bounds than in previous work. Notably, the complexity of the environment does not affect our methods ability to evaluate if a trajectory or policy is safe. We then use these safety checking methods to design a safe variant of the RRT planning algorithm.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "Many robotic planning applications involve continuous actions with highly non-linear constraints, which cannot be modeled using modern planners that construct a propositional representation. We introduce STRIPStream: an extension of the STRIPS language which can model these domains by supporting the specification of blackbox generators to handle complex constraints. The outputs of these generators interact with actions through possibly infinite streams of objects and static predicates. We provide two algorithms which both reduce STRIPStream problems to a sequence of finite-domain planning problems. The representation and algorithms are entirely domain independent. We demonstrate our framework on simple illustrative domains, and then on a high-dimensional, continuous robotic task and motion planning domain.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "Mobile manipulation problems involving many objects are challenging to solve due to the high dimensionality and multi-modality of their hybrid configuration spaces. Planners that perform a purely geometric search are prohibitively slow for solving these problems because they are unable to factor the configuration space. Symbolic task planners can efficiently construct plans involving many variables but cannot represent the geometric and kinematic constraints required in manipulation. We present the FFRob algorithm for solving task and motion planning problems. First, we introduce Extended Action Specification (EAS) as a general purpose planning representation that supports arbitrary predicates as conditions. We adapt existing heuristic search ideas for solving \\proc{strips} planning problems, particularly delete-relaxations, to solve EAS problem instances. We then apply the EAS representation and planners to manipulation problems resulting in FFRob. FFRob iteratively discretizes task and motion planning problems using batch sampling of manipulation primitives and a multi-query roadmap structure that can be conditionalized to evaluate reachability under different placements of movable objects. This structure enables the EAS planner to efficiently compute heuristics that incorporate geometric and kinematic planning constraints to give a tight estimate of the distance to the goal. Additionally, we show FFRob is probabilistically complete and has finite expected runtime. Finally, we empirically demonstrate FFRob's effectiveness on complex and diverse task and motion planning tasks including rearrangement planning and navigation among movable objects.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "We investigate learning heuristics for domain-specific planning. Prior work framed learning a heuristic as an ordinary regression problem. However, in a greedy best-first search, the ordering of states induced by a heuristic is more indicative of the resulting planner's performance than mean squared error. Thus, we instead frame learning a heuristic as a learning to rank problem which we solve using a RankSVM formulation. Additionally, we introduce new methods for computing features that capture temporal interactions in an approximate plan. Our experiments on recent International Planning Competition problems show that the RankSVM learned heuristics outperform both the original heuristics and heuristics learned through ordinary regression.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "We introduce a framework for model learning and planning in stochastic domains with continuous state and action spaces and non-Gaussian transition models. It is efficient because (1) local models are estimated only when the planner requires them; (2) the planner focuses on the most relevant states to the current planning problem; and (3) the planner focuses on the most informative and/or high-value actions. Our theoretical analysis shows the validity and asymptotic optimality of the proposed approach. Empirically, we demonstrate the effectiveness of our algorithm on a simulated multi-modal pushing problem.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "In this paper we address planning problems in high-dimensional hybrid configuration spaces, with a particular focus on manipulation planning problems involving many objects. We present the hybrid backward-forward (HBF) planning algorithm that uses a backward identification of constraints to direct the sampling of the infinite action space in a forward search from the initial state towards a goal configuration. The resulting planner is probabilistically complete and can effectively construct long manipulation plans requiring both prehensile and nonprehensile actions in cluttered environments.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "To accomplish tasks in human-centric indoor environments, robots need to represent and understand the world in terms of objects and their attributes. We refer to this attribute-based representation as a world model, and consider how to acquire it via noisy perception and maintain it over time, as objects are added, changed, and removed in the world. Previous work has framed this as multiple-target tracking problem, where objects are potentially in motion at all times. Although this approach is general, it is computationally expensive. We argue that such generality is not needed in typical world modeling tasks, where objects only change state occasionally. More efficient approaches are enabled by restricting ourselves to such semi-static environments.\n  We consider a previously-proposed clustering-based world modeling approach that assumed static environments, and extend it to semi-static domains by applying a dependent Dirichlet-process (DDP) mixture model. We derive a novel MAP inference algorithm under this model, subject to data association constraints. We demonstrate our approach improves computational performance in semi-static environments.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "We apply decision theoretic techniques to construct non-player characters that are able to assist a human player in collaborative games. The method is based on solving Markov decision processes, which can be difficult when the game state is described by many variables. To scale to more complex games, the method allows decomposition of a game task into subtasks, each of which can be modelled by a Markov decision process. Intention recognition is used to infer the subtask that the human is currently performing, allowing the helper to assist the human in performing the correct task. Experiments show that the method can be effective, giving near-human level performance in helping a human in a collaborative game.\n        \u25b3 Less", "author": "Tom\u00e1s Lozano-P\u00e9rez"}, {"abstract": "We summarize the red channel (2-5 micron) of the Planetary Systems Imager (PSI), a proposed second-generation instrument for the TMT. Cold exoplanets emit the majority of their light in the thermal infrared, which means these exoplanets can be detected at a more modest contrast than at other wavelengths. PSI-Red will be able to detect and characterize a wide variety of exoplanets, including radial-velocity planets on wide orbits, accreting protoplanets in nearby star-forming regions, and reflected-light planets around the nearest stars. PSI-Red will feature an imager, a low-resolution lenslet integral field spectrograph, a medium-resolution lenslet+slicer integral field spectrograph, and a fiber-fed high-resolution spectrograph.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "This paper presents a novel nonlinear disturbance rejection control for hydraulic robots. This method requires two third-order filters as well as inverse dynamics in order to estimate the disturbances. All the parameters for the third-order filters are pre-defined. The proposed method is nonlinear, which does not require the linearization of the rigid body dynamics. The estimated disturbances are used by the nonlinear controller in order to achieve disturbance attenuation. The performance of the proposed approach is compared with existing approaches. Finally, the tracking performance and robustness of the proposed approach is validated extensively on real hardware by performing different tasks under either internal or both internal and external disturbances. The experimental results demonstrate the robustness and superior tracking performance of the proposed approach.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "The Greenland Telescope project has recently participated in an experiment to image the supermassive black hole shadow at the center of M87 using Very Long Baseline Interferometry technique in April of 2018. The antenna consists of the 12-m ALMA North American prototype antenna that was modified to support two auxiliary side containers and to withstand an extremely cold environment. The telescope is currently at Thule Air Base in Greenland with the long-term goal to move the telescope over the Greenland ice sheet to Summit Station. The GLT currently has a single cryostat which houses three dual polarization receivers that cover 84-96 GHz, 213-243 GHz and 271-377 GHz bands. A hydrogen maser frequency source in conjunction with high frequency synthesizers are used to generate the local oscillator references for the receivers. The intermediate frequency outputs of each receiver cover 4-8 GHz and are heterodyned to baseband for digitization within a set of ROACH-2 units then formatted for recording onto Mark-6 data recorders. A separate set of ROACH-2 units operating in parallel provides the function of auto-correlation for real-time spectral analysis. Due to the stringent instrumental stability requirements for interferometry a diagnostic test system was incorporated into the design. Tying all of the above equipment together is the fiber optic system designed to operate in a low temperature environment and scalable to accommodate a larger distance between the control module and telescope for Summit Station. A report on the progress of the above electronics instrumentation system will be provided.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "Data cleaning consumes about 80% of the time spent on data analysis for clinical research projects. This is a much bigger problem in the era of big data and machine learning in the field of medicine where large volumes of data are being generated. We report an initial effort towards automated patient data cleaning using deep learning: the standardization of organ labeling in radiation therapy. Organs are often labeled inconsistently at different institutions (sometimes even within the same institution) and at different time periods, which poses a problem for clinical research, especially for multi-institutional collaborative clinical research where the acquired patient data is not being used effectively. We developed a convolutional neural network (CNN) to automatically identify each organ in the CT image and then label it with the standardized nomenclature presented at AAPM Task Group 263. We tested this model on the CT images of 54 patients with prostate and 100 patients with head and neck cancer who previously received radiation therapy. The model achieved 100% accuracy in detecting organs and assigning standardized labels for the patients tested. This work shows the feasibility of using deep learning in patient data cleaning that enables standardized datasets to be generated for effective intra- and interinstitutional collaborative clinical research.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "Deep learning has started to revolutionize several different industries, and the applications of these methods in medicine are now becoming more commonplace. This study focuses on investigating the feasibility of tracking patients and clinical staff wearing Bluetooth Low Energy (BLE) tags in a radiation oncology clinic using artificial neural networks (ANNs) and convolutional neural networks (CNNs). The performance of these networks was compared to relative received signal strength indicator (RSSI) thresholding and triangulation. By utilizing temporal information, a combined CNN+ANN network was capable of correctly identifying the location of the BLE tag with an accuracy of 99.9%. It outperformed a CNN model (accuracy = 94%), a thresholding model employing majority voting (accuracy = 95%), and a triangulation classifier utilizing majority voting (accuracy = 95%). Future studies will seek to deploy this affordable real time location system in hospitals to improve clinical workflow, efficiency, and patient safety.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, that is better suited to low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy where, rather than one way transfer between a static pre-defined teacher and a student, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on CIFAR-100 recognition and Market-1501 person re-identification benchmarks. Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "We propose a route toward realizing fractionalized topological phases of matter (i.e. with intrinsic topological order) by literally building on un-fractionalized phases. Our approach employs a Kondo lattice model in which a gapped electronic system of non-interacting fermions is coupled to non-interacting local moments via the exchange interaction. Using general entanglement-based arguments and explicit lattice models, we show that in this way gapped spin liquids can be induced in the spin system. We demonstrate the power of this topological bootstrap concept with two examples: (1) a chiral spin liquid induced by a Chern insulator and (2) a Z2 spin liquid induced by a superconductor. In particular, in the latter example, the toric code is realized as an exactly solvable example of topological bootstrap. Our approach can be generalized to all lattices, higher dimensions, and non-abelian topological orders.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "The Thirty Meter Telescope (TMT) first light instrument IRIS (Infrared Imaging Spectrograph) will complete its preliminary design phase in 2016. The IRIS instrument design includes a near-infrared (0.85 - 2.4 micron) integral field spectrograph (IFS) and imager that are able to conduct simultaneous diffraction-limited observations behind the advanced adaptive optics system NFIRAOS. The IRIS science cases have continued to be developed and new science studies have been investigated to aid in technical performance and design requirements. In this development phase, the IRIS science team has paid particular attention to the selection of filters, gratings, sensitivities of the entire system, and science cases that will benefit from the parallel mode of the IFS and imaging camera. We present new science cases for IRIS using the latest end-to-end data simulator on the following topics: Solar System bodies, the Galactic center, active galactic nuclei (AGN), and distant gravitationally-lensed galaxies. We then briefly discuss the necessity of an advanced data management system and data reduction pipeline.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "We present a high spatial resolution optical and infrared study of the circumnuclear region in Arp 220, a late-stage galaxy merger. Narrowband imaging using HST/WFC3 has resolved the previously observed peak in H$\u03b1$+[NII] emission into a bubble-shaped feature. This feature measures 1.6\" in diameter, or 600 pc, and is only 1\" northwest of the western nucleus. The bubble is aligned with the western nucleus and the large-scale outflow axis seen in X-rays. We explore several possibilities for the bubble origin, including a jet or outflow from a hidden active galactic nucleus (AGN), outflows from high levels of star formation within the few hundred pc nuclear gas disk, or an ultraluminous X-ray source. An obscured AGN or high levels of star formation within the inner $\\sim$100 pc of the nuclei are favored based on the alignment of the bubble and energetics arguments.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "IRIS (InfraRed Imaging Spectrograph) is a first light near-infrared diffraction limited imager and integral field spectrograph being designed for the future Thirty Meter Telescope (TMT). IRIS is optimized to perform astronomical studies across a significant fraction of cosmic time, from our Solar System to distant newly formed galaxies (Barton et al. [1]). We present a selection of the innovative science cases that are unique to IRIS in the era of upcoming space and ground-based telescopes. We focus on integral field spectroscopy of directly imaged exoplanet atmospheres, probing fundamental physics in the Galactic Center, measuring 10^4 to 10^10 Msun supermassive black hole masses, resolved spectroscopy of young star-forming galaxies (1 < z < 5) and first light galaxies (6 < z < 12), and resolved spectroscopy of strong gravitational lensed sources to measure dark matter substructure. For each of these science cases we use the IRIS simulator (Wright et al. [2], Do et al. [3]) to explore IRIS capabilities. To highlight the unique IRIS capabilities, we also update the point and resolved source sensitivities for the integral field spectrograph (IFS) in all five broadband filters (Z, Y, J, H, K) for the finest spatial scale of 0.004\" per spaxel. We briefly discuss future development plans for the data reduction pipeline and quicklook software for the IRIS instrument suite.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "The T2K experiment is a long-baseline neutrino oscillation experiment. Its main goal is to measure the last unknown lepton sector mixing angle \u03b8_{13} by observing \u03bd_e appearance in a \u03bd_\u03bc beam. It also aims to make a precision measurement of the known oscillation parameters, \u0394m^{2}_{23} and sin^{2} 2\u03b8_{23}, via \u03bd_\u03bc disappearance studies. Other goals of the experiment include various neutrino cross section measurements and sterile neutrino searches. The experiment uses an intense proton beam generated by the J-PARC accelerator in Tokai, Japan, and is composed of a neutrino beamline, a near detector complex (ND280), and a far detector (Super-Kamiokande) located 295 km away from J-PARC. This paper provides a comprehensive review of the instrumentation aspect of the T2K experiment and a summary of the vital information for each subsystem.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "In this paper, we proved a special case of the DDVV Conjecture.\n        \u25b3 Less", "author": "Timothy Lu"}, {"abstract": "In the paper titled \"The SNOW Theorem\" the authors proposed four desirable properties in transaction processing systems for achieving low-latency of READ transactions, with asynchronous and reliable communications, and referred to them collectively as the SNOW properties: The underlying properties, in the context of an execution, are (i) strict serializability (S) property where READ and WRITE transactions seem to occur atomically, (ii) non-blocking (N) property implies for read operations, (iii) one version and one round (O) property, where reads operations completes in one-round of client-server communication and only one version of the object value is sent, and (iv) concurrent WRITE transactions (W) property, which means WRITE transactions can occur. Then they argued that it is impossible to implement all the four properties, in the same system, even with at least three clients. They referred to their result as the SNOW theorem, and they posed the two-client setting as an open question. Here we revisit the results of the work and present several new results. In our first result, we resolve the two-client scenario: We prove that even with two clients, without client-to-client messaging, it is impossible to design an transaction processing system which satisfies the SNOW properties. Second, we provide a rigorous proof of the SNOW theorem for systems with at least three clients, i.e., we show that it is impossible to implement a transaction processing system, consisting of at least three clients, even with client-to-client messaging, that satisfies the SNOW properties. Next we derive a useful property for executions of algorithms that implement objects of data types considered in our work that helps us show the S property of algorithms presented in the paper. Then we present two new algorithms that satisfies S, N, W and wreaked versions O property.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We consider multi-armed bandit problems in social groups wherein each individual has bounded memory and shares the common goal of learning the best arm/option. We say an individual learns the best option if eventually (as $t\\to \\infty$) it pulls only the arm with the highest expected reward. While this goal is provably impossible for an isolated individual due to bounded memory, we show that, in social groups, this goal can be achieved easily with the aid of social persuasion (i.e., communication) as long as the communication networks/graphs satisfy some mild conditions. To deal with the interplay between the randomness in the rewards and in the social interaction, we employ the {\\em mean-field approximation} method. Considering the possibility that the individuals in the networks may not be exchangeable when the communication networks are not cliques, we go beyond the classic mean-field techniques and apply a refined version of mean-field approximation:\n  (1) Using coupling we show that, if the communication graph is connected and is either regular or has doubly-stochastic degree-weighted adjacency matrix, with probability $\\to 1$ as the social group size $N \\to \\infty$, every individual in the social group learns the best option.\n  (2) If the minimum degree of the graph diverges as $N \\to \\infty$, over an arbitrary but given finite time horizon, the sample paths describing the opinion evolutions of the individuals are asymptotically independent. In addition, the proportions of the population with different opinions converge to the unique solution of a system of ODEs. In the solution of the obtained ODEs, the proportion of the population holding the correct opinion converges to $1$ exponentially fast in time.\n  Notably, our results hold even if the communication graphs are highly sparse.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "This paper is part of a project on developing an algorithmic theory of brain networks, based on stochastic Spiking Neural Network (SNN) models. Inspired by tasks that seem to be solved in actual brains, we are defining abstract problems to be solved by these networks. In our work so far, we have developed models and algorithms for the Winner-Take-All problem from computational neuroscience [LMP17a,Mus18], and problems of similarity detection and neural coding [LMP17b]. We plan to consider many other problems and networks, including both static networks and networks that learn.\n  This paper is about basic theory for the stochastic SNN model. In particular, we define a simple version of the model. This version assumes that the neurons' only state is a Boolean, indicating whether the neuron is firing or not. In later work, we plan to develop variants of the model with more elaborate state. We also define an external behavior notion for SNNs, which can be used for stating requirements to be satisfied by the networks.\n  We then define a composition operator for SNNs. We prove that our external behavior notion is \"compositional\", in the sense that the external behavior of a composed network depends only on the external behaviors of the component networks. We also define a hiding operator that reclassifies some output behavior of an SNN as internal. We give basic results for hiding.\n  Finally, we give a formal definition of a problem to be solved by an SNN, and give basic results showing how composition and hiding of networks affect the problems that they solve. We illustrate our definitions with three examples: building a circuit out of gates, building an \"Attention\" network out of a \"Winner-Take-All\" network and a \"Filter\" network, and a toy example involving combining two networks in a cyclic fashion.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "Atomicity or strong consistency is one of the fundamental, most intuitive, and hardest to provide primitives in distributed shared memory emulations. To ensure survivability, scalability, and availability of a storage service in the presence of failures, traditional approaches for atomic memory emulation, in message passing environments, replicate the objects across multiple servers. Compared to replication based algorithms, erasure code-based atomic memory algorithms has much lower storage and communication costs, but usually, they are harder to design. The difficulty of designing atomic memory algorithms further grows, when the set of servers may be changed to ensure survivability of the service over software and hardware upgrades, while avoiding service interruptions. Atomic memory algorithms for performing server reconfiguration, in the replicated systems, are very few, complex, and are still part of an active area of research; reconfigurations of erasure-code based algorithms are non-existent.\n  In this work, we present ARES, an algorithmic framework that allows reconfiguration of the underlying servers, and is particularly suitable for erasure-code based algorithms emulating atomic objects. ARES introduces new configurations while keeping the service available. To use with ARES we also propose a new, and to our knowledge, the first two-round erasure code based algorithm TREAS, for emulating multi-writer, multi-reader (MWMR) atomic objects in asynchronous, message-passing environments, with near-optimal communication and storage costs. Our algorithms can tolerate crash failures of any client and some fraction of servers, and yet, guarantee safety and liveness property. Moreover, by bringing together the advantages of ARES and TREAS, we propose an optimized algorithm where new configurations can be installed without the objects values passing through the reconfiguration clients.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We study the problem of distributed task allocation inspired by the behavior of social insects, which perform task allocation in a setting of limited capabilities and noisy environment feedback. We assume that each task has a demand that should be satisfied but not exceeded, i.e., there is an optimal number of ants that should be working on this task at a given time. The goal is to assign a near-optimal number of workers to each task in a distributed manner and without explicit access to the values of the demands nor the number of ants working on the task.\n  We seek to answer the question of how the quality of task allocation depends on the accuracy of assessing whether too many (overload) or not enough (lack) ants are currently working on a given task. Concretely, we address the open question of solving task allocation in the model where each ant receives feedback that depends on the deficit defined as the (possibly negative) difference between the optimal demand and the current number of workers in the task. The feedback is modeled as a random variable that takes value lack or overload with probability given by a sigmoid of the deficit. Each ants receives the feedback independently, but the higher the overload or lack of workers for a task, the more likely it is that all the ants will receive the same, correct feedback from this task; the closer the deficit is to zero, the less reliable the feedback becomes. We measure the performance of task allocation algorithms using the notion of regret, defined as the absolute value of the deficit summed over all tasks and summed over time.\n  We propose a simple, constant-memory, self-stabilizing, distributed algorithm that quickly converges from any initial distribution to a near-optimal assignment. We also show that our algorithm works not only under stochastic noise but also in an adversarial noise setting.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "In this paper, we study local and global broadcast in the dual graph model, which describes communication in a radio network with both reliable and unreliable links. Existing work proved that efficient solutions to these problems are impossible in the dual graph model under standard assumptions. In real networks, however, simple back-off strategies tend to perform well for solving these basic communication tasks. We address this apparent paradox by introducing a new set of constraints to the dual graph model that better generalize the slow/fast fading behavior common in real networks. We prove that in the context of these new constraints, simple back-off strategies now provide efficient solutions to local and global broadcast in the dual graph model. We also precisely characterize how this efficiency degrades as the new constraints are reduced down to non-existent, and prove new lower bounds that establish this degradation as near optimal for a large class of natural algorithms. We conclude with a preliminary investigation of the performance of these strategies when we include additional generality to the model. These results provide theoretical foundations for the practical observation that simple back-off algorithms tend to work well even amid the complicated link dynamics of real radio networks.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We consider multi-armed bandit problems in social groups wherein each individual has bounded memory and shares the common goal of learning the best arm/option. We say an individual learns the best option if eventually (as $t \\to \\infty$) it pulls only the arm with the highest average reward. While this goal is provably impossible for an isolated individual, we show that, in social groups, this goal can be achieved easily with the aid of social persuasion, i.e., communication. Specifically, we study the learning dynamics wherein an individual sequentially decides on which arm to pull next based on not only its private reward feedback but also the suggestions provided by randomly chosen peers. Our learning dynamics are hard to analyze via explicit probabilistic calculations due to the stochastic dependency induced by social interaction. Instead, we employ the mean-field approximation method from statistical physics and we show:\n  (1) With probability $\\to 1$ as the social group size $N \\to \\infty $, every individual in the social group learns the best option.\n  (2) Over an arbitrary finite time horizon $[0, T]$, with high probability (in $N$), the fraction of individuals that prefer the best option grows to 1 exponentially fast as $t$ increases ($t\\in [0, T]$).\n  A major innovation of our mean-filed analysis is a simple yet powerful technique to deal with absorbing states in the interchange of limits $N \\to \\infty$ and $t \\to \\infty $. The mean-field approximation method allows us to approximate the probabilistic sample paths of our learning dynamics by a deterministic and smooth trajectory that corresponds to the unique solution of a well-behaved system of ordinary differential equations (ODEs). Such an approximation is desired because the analysis of a system of ODEs is relatively easier than that of the original stochastic system.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We study distributed algorithms implemented in a simplified biologically inspired model for stochastic spiking neural networks. We focus on tradeoffs between computation time and network complexity, along with the role of randomness in efficient neural computation.\n  It is widely accepted that neural computation is inherently stochastic. In recent work, we explored how this stochasticity could be leveraged to solve the `winner-take-all' leader election task. Here, we focus on using randomness in neural algorithms for similarity testing and compression. In the most basic setting, given two $n$-length patterns of firing neurons, we wish to distinguish if the patterns are equal or $\u03b5$-far from equal.\n  Randomization allows us to solve this task with a very compact network, using $O \\left (\\frac{\\sqrt{n}\\log n}\u03b5\\right)$ auxiliary neurons, which is sublinear in the input size. At the heart of our solution is the design of a $t$-round neural random access memory, or indexing network, which we call a neuro-RAM. This module can be implemented with $O(n/t)$ auxiliary neurons and is useful in many applications beyond similarity testing.\n  Using a VC dimension-based argument, we show that the tradeoff between runtime and network size in our neuro-RAM is nearly optimal. Our result has several implications -- since our neuro-RAM can be implemented with deterministic threshold gates, it shows that, in contrast to similarity testing, randomness does not provide significant computational advantages for this problem. It also establishes a separation between feedforward networks whose gates spike with sigmoidal probability functions, and well-studied deterministic sigmoidal networks, whose gates output real number sigmoidal values, and which can implement a neuro-RAM much more efficiently.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We adapt a recent algorithm by Ghaffari [SODA'16] for computing a Maximal Independent Set in the LOCAL model, so that it works in the significantly weaker BEEP model. For networks with maximum degree $\u0394$, our algorithm terminates locally within time $O((\\log \u0394+ \\log (1/\u03b5)) \\cdot \\log(1/\u03b5))$, with probability at least $1 - \u03b5$. The key idea of the modification is to replace explicit messages about transmission probabilities with estimates based on the number of received messages.\n  After the successful introduction (and implicit use) of local analysis, e.g., by Barenboim et al. [JACM'16], Chung et al. [PODC'14], Ghaffari [SODA'16], and Halldorsson et al. [PODC'15], we study this concept in the BEEP model for the first time.\n  By doing so, we improve over local bounds that are implicitly derived from previous work (that uses traditional global analysis) on computing a Maximal Independent Set in the \\beep model for a large range of values of the parameter $\u0394$. At the same time, we show that our algorithm in the \\beep model only needs to pay a $\\log(1/\u03b5)$ factor in the runtime compared to the best known MIS algorithm in the much more powerful \\local model. We demonstrate that this overhead is negligible, as communication via beeps can be implemented using significantly less resources than communication in the LOCAL model. In particular, when looking at implementing these models, one round of the \\local model needs at least $O(\u0394)$ time units, while one round in the BEEP model needs $O(\\log\u0394)$ time units, an improvement that diminishes the loss of a $\\log(1/\u03b5)$ factor in most settings.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "Motivated by emerging applications to the edge computing paradigm, we introduce a two-layer erasure-coded fault-tolerant distributed storage system offering atomic access for read and write operations. In edge computing, clients interact with an edge-layer of servers that is geographically near; the edge-layer in turn interacts with a back-end layer of servers. The edge-layer provides low latency access and temporary storage for client operations, and uses the back-end layer for persistent storage. Our algorithm, termed Layered Data Storage (LDS) algorithm, offers several features suitable for edge-computing systems, works under asynchronous message-passing environments, supports multiple readers and writers, and can tolerate $f_1 < n_1/2$ and $f_2 < n_2/3$ crash failures in the two layers having $n_1$ and $n_2$ servers, respectively. We use a class of erasure codes known as regenerating codes for storage of data in the back-end layer. The choice of regenerating codes, instead of popular choices like Reed-Solomon codes, not only optimizes the cost of back-end storage, but also helps in optimizing communication cost of read operations, when the value needs to be recreated all the way from the back-end. The two-layer architecture permits a modular implementation of atomicity and erasure-code protocols; the implementation of erasure-codes is mostly limited to interaction between the two layers. We prove liveness and atomicity of LDS, and also compute performance costs associated with read and write operations. Further, in a multi-object system running $N$ independent instances of LDS, where only a small fraction of the objects undergo concurrent accesses at any point during the execution, the overall storage cost is dominated by that of persistent storage in the back-end layer, and is given by $\u0398(N)$.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We initiate a line of investigation into biological neural networks from an algorithmic perspective. We develop a simplified but biologically plausible model for distributed computation in stochastic spiking neural networks and study tradeoffs between computation time and network complexity in this model. Our aim is to abstract real neural networks in a way that, while not capturing all interesting features, preserves high-level behavior and allows us to make biologically relevant conclusions.\n  In this paper, we focus on the important `winner-take-all' (WTA) problem, which is analogous to a neural leader election unit: a network consisting of $n$ input neurons and $n$ corresponding output neurons must converge to a state in which a single output corresponding to a firing input (the `winner') fires, while all other outputs remain silent. Neural circuits for WTA rely on inhibitory neurons, which suppress the activity of competing outputs and drive the network towards a converged state with a single firing winner. We attempt to understand how the number of inhibitors used affects network convergence time.\n  We show that it is possible to significantly outperform naive WTA constructions through a more refined use of inhibition, solving the problem in $O(\u03b8)$ rounds in expectation with just $O(\\log^{1/\u03b8} n)$ inhibitors for any $\u03b8$. An alternative construction gives convergence in $O(\\log^{1/\u03b8} n)$ rounds with $O(\u03b8)$ inhibitors. We compliment these upper bounds with our main technical contribution, a nearly matching lower bound for networks using $\\ge \\log\\log n$ inhibitors. Our lower bound uses familiar indistinguishability and locality arguments from distributed computing theory. It lets us derive a number of interesting conclusions about the structure of any network solving WTA with good probability, and the use of randomness and inhibition within such a network.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "The focus of this paper is to understand storage costs of emulating an atomic shared memory over an asynchronous, distributed message passing system. Previous literature has developed several shared memory emulation algorithms based on replication and erasure coding techniques. In this paper, we present information-theoretic lower bounds on the storage costs incurred by shared memory emulation algorithms. Our storage cost lower bounds are universally applicable, that is, we make no assumption on the structure of the algorithm or the method of encoding the data.\n  We consider an arbitrary algorithm $A$ that implements an atomic multi-writer single-reader (MWSR) shared memory variable whose values come from a finite set $\\mathcal{V}$ over a system of $N$ servers connected by point-to-point asynchronous links. We require that in every fair execution of algorithm $A$ where the number of server failures is smaller than a parameter $f$, every operation invoked at a non-failing client terminates. We define the storage cost of a server in algorithm $A$ as the logarithm (to base 2) of number of states it can take on; the total-storage cost of algorithm $A$ is the sum of the storage cost of all servers.\n  Our results are as follows. (i) We show that if algorithm $A$ does not use server gossip, then the total storage cost is lower bounded by $2 \\frac{N}{N-f+1}\\log_2|\\mathcal{V}|-o(\\log_2|\\mathcal{V}|)$. (ii) The total storage cost is at least $2 \\frac{N}{N-f+2} \\log_{2}|\\mathcal{V}|-o(\\log_{2}|\\mathcal{V}|)$ even if the algorithm uses server gossip. (iii) We consider algorithms where the write protocol sends information about the value in at most one phase. We show that the total storage cost is at least $\u03bd^* \\frac{N}{N-f+\u03bd^*-1} \\log_2( |\\mathcal{V}|)- o(\\log_2(|\\mathcal{V}|),$ where $\u03bd^*$ is the minimum of $f+1$ and the number of active write operations of an execution.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "Erasure codes offer an efficient way to decrease storage and communication costs while implementing atomic memory service in asynchronous distributed storage systems. In this paper, we provide erasure-code-based algorithms having the additional ability to perform background repair of crashed nodes. A repair operation of a node in the crashed state is triggered externally, and is carried out by the concerned node via message exchanges with other active nodes in the system. Upon completion of repair, the node re-enters active state, and resumes participation in ongoing and future read, write, and repair operations. To guarantee liveness and atomicity simultaneously, existing works assume either the presence of nodes with stable storage, or presence of nodes that never crash during the execution. We demand neither of these; instead we consider a natural, yet practical network stability condition $N1$ that only restricts the number of nodes in the crashed/repair state during broadcast of any message.\n  We present an erasure-code based algorithm $RADON_C$ that is always live, and guarantees atomicity as long as condition $N1$ holds. In situations when the number of concurrent writes is limited, $RADON_C$ has significantly improved storage and communication cost over a replication-based algorithm $RADON_R$, which also works under $N1$. We further show how a slightly stronger network stability condition $N2$ can be used to construct algorithms that never violate atomicity. The guarantee of atomicity comes at the expense of having an additional phase during the read and write operations.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "Erasure codes are increasingly being studied in the context of implementing atomic memory objects in large scale asynchronous distributed storage systems. When compared with the traditional replication based schemes, erasure codes have the potential of significantly lowering storage and communication costs while simultaneously guaranteeing the desired resiliency levels. In this work, we propose the Storage-Optimized Data-Atomic (SODA) algorithm for implementing atomic memory objects in the multi-writer multi-reader setting. SODA uses Maximum Distance Separable (MDS) codes, and is specifically designed to optimize the total storage cost for a given fault-tolerance requirement. For tolerating $f$ server crashes in an $n$-server system, SODA uses an $[n, k]$ MDS code with $k=n-f$, and incurs a total storage cost of $\\frac{n}{n-f}$. SODA is designed under the assumption of reliable point-to-point communication channels. The communication cost of a write and a read operation are respectively given by $O(f^2)$ and $\\frac{n}{n-f}(\u03b4_w+1)$, where $\u03b4_w$ denotes the number of writes that are concurrent with the particular read. In comparison with the recent CASGC algorithm, which also uses MDS codes, SODA offers lower storage cost while pays more on the communication cost.\n  We also present a modification of SODA, called SODA$_{\\text{err}}$, to handle the case where some of the servers can return erroneous coded elements during a read operation. Specifically, in order to tolerate $f$ server failures and $e$ error-prone coded elements, the SODA$_{\\text{err}}$ algorithm uses an $[n, k]$ MDS code such that $k=n-2e-f$. SODA$_{\\text{err}}$ also guarantees liveness and atomicity, while maintaining an optimized total storage cost of $\\frac{n}{n-f-2e}$.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We present dynamic I/O automata (DIOA), a compositional model of dynamic systems. In DIOA, automata can be created and destroyed dynamically, as computation proceeds, and an automaton can dynamically change its signature, i.e., the set of actions in which it can participate.\n  DIOA features operators for parallel composition, action hiding, action renaming, a notion of automaton creation, and a notion of behavioral subtyping by means of trace inclusion. DIOA can model mobility, using signature modification, and is hierarchical: a dynamically changing system of interacting automata is itself modeled as a single automaton.\n  We also show that parallel composition, action hiding, action renaming, and (subject to some technical conditions) automaton creation are all monotonic with respect to trace inclusion: if one component is replaced by another whose traces are a subset of the former, then the set of traces of the system as a whole can only be reduced.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "Many ant species employ distributed population density estimation in applications ranging from quorum sensing [Pra05], to task allocation [Gor99], to appraisal of enemy colony strength [Ada90]. It has been shown that ants estimate density by tracking encounter rates -- the higher the population density, the more often the ants bump into each other [Pra05,GPT93].\n  We study distributed density estimation from a theoretical perspective. We show that a group of anonymous agents randomly walking on a grid are able to estimate their density $d$ to within a multiplicative factor $1 \\pm \u03b5$ with probability $1-\u03b4$ in just $\\tilde O \\left (\\frac{\\log(1/\u03b4)\\log(1/d\u03b5)}{d\u03b5^2} \\right )$ steps by measuring their encounter rates with other agents. Despite dependencies inherent in the fact that nearby agents may collide repeatedly (and, worse, cannot recognize when this happens), this bound nearly matches what is required to estimate $d$ by independently sampling grid locations.\n  From a biological perspective, our work helps shed light on how ants and other social insects can obtain relatively accurate density estimates via encounter rates. From a technical perspective, our analysis provides new tools for understanding complex dependencies in the collision probabilities of multiple random walks. We bound the strength of these dependencies using $local\\ mixing\\ properties$ of the underlying graph. Our results extend beyond the grid to more general graphs and we discuss applications to social network size estimation, density estimation by robot swarms, and random walked-based sampling of sensor networks.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "This paper studies the theory of the additive wireless network model, in which the received signal is abstracted as an addition of the transmitted signals. Our central observation is that the crucial challenge for computing in this model is not high contention, as assumed previously, but rather guaranteeing a bounded amount of \\emph{information} in each neighborhood per round, a property that we show is achievable using a new random coding technique.\n  Technically, we provide efficient algorithms for fundamental distributed tasks in additive networks, such as solving various symmetry breaking problems, approximating network parameters, and solving an \\emph{asymmetry revealing} problem such as computing a maximal input.\n  The key method used is a novel random coding technique that allows a node to successfully decode the received information, as long as it does not contain too many distinct values. We then design our algorithms to produce a limited amount of information in each neighborhood in order to leverage our enriched toolbox for computing in additive networks.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We present the first algorithm that implements an abstract MAC (absMAC) layer in the Signal-to-Interference-plus-Noise-Ratio (SINR) wireless network model. We first prove that efficient SINR implementations are not possible for the standard absMAC specification. We modify that specification to an \"approximate\" version that better suits the SINR model. We give an efficient algorithm to implement the modified specification, and use it to derive efficient algorithms for higher-level problems of global broadcast and consensus.\n  In particular, we show that the absMAC progress property has no efficient implementation in terms of the SINR strong connectivity graph $G_{1-\u03b5}$, which contains edges between nodes of distance at most $(1-\u03b5)$ times the transmission range, where $\u03b5>0$ is a small constant that can be chosen by the user. This progress property bounds the time until a node is guaranteed to receive some message when at least one of its neighbors is transmitting.\n  To overcome this limitation, we introduce the slightly weaker notion of approximate progress into the absMAC specification. We provide a fast implementation of the modified specification, based on decomposing a known algorithm into local and global parts. We analyze our algorithm in terms of local parameters such as node degrees, rather than global parameters such as the overall number of nodes. A key contribution is our demonstration that such a local analysis is possible even in the presence of global interference.\n  Our absMAC algorithm leads to several new, efficient algorithms for solving higher-level problems in the SINR model. Namely, by combining our algorithm with known high-level algorithms, we obtain an improved algorithm for global single-message broadcast in the SINR model, and the first efficient algorithm for multi-message broadcast in that model.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We introduce the study of the ant colony house-hunting problem from a distributed computing perspective. When an ant colony's nest becomes unsuitable due to size constraints or damage, the colony must relocate to a new nest. The task of identifying and evaluating the quality of potential new nests is distributed among all ants. The ants must additionally reach consensus on a final nest choice and the full colony must be transported to this single new nest. Our goal is to use tools and techniques from distributed computing theory in order to gain insight into the house-hunting process.\n  We develop a formal model for the house-hunting problem inspired by the behavior of the Temnothorax genus of ants. We then show a \u03a9(log n) lower bound on the time for all n ants to agree on one of k candidate nests. We also present two algorithms that solve the house-hunting problem in our model. The first algorithm solves the problem in optimal O(log n) time but exhibits some features not characteristic of natural ant behavior. The second algorithm runs in O(k log n) time and uses an extremely simple and natural rule for each ant to decide on the new nest.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "The FLP result shows that crash-tolerant consensus is impossible to solve in asynchronous systems, and several solutions have been proposed for crash-tolerant consensus under alternative (stronger) models. One popular approach is to augment the asynchronous system with appropriate failure detectors, which provide (potentially unreliable) information about process crashes in the system, to circumvent the FLP impossibility.\n  In this paper, we demonstrate the exact mechanism by which (sufficiently powerful) asynchronous failure detectors enable solving crash-tolerant consensus. Our approach, which borrows arguments from the FLP impossibility proof and the famous result from CHT, which shows that $\u03a9$ is a weakest failure detector to solve consensus, also yields a natural proof to $\u03a9$ as a weakest asynchronous failure detector to solve consensus. The use of I/O automata theory in our approach enables us to model execution in a more detailed fashion than CHT and also addresses the latent assumptions and assertions in the original result in CHT.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "This paper considers the communication and storage costs of emulating atomic (linearizable) multi-writer multi-reader shared memory in distributed message-passing systems. The paper contains three main contributions: (1) We present a atomic shared-memory emulation algorithm that we call Coded Atomic Storage (CAS). This algorithm uses erasure coding methods. In a storage system with $N$ servers that is resilient to $f$ server failures, we show that the communication cost of CAS is $\\frac{N}{N-2f}$. The storage cost of CAS is unbounded. (2) We present a modification of the CAS algorithm known as CAS with Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer $\u03b4$ and has a bounded storage cost. We show that in every execution where the number of write operations that are concurrent with a read operation is no bigger than $\u03b4$, the CASGC algorithm with parameter $\u03b4$ satisfies atomicity and liveness. We explicitly characterize the storage cost of CASGC, and show that it has the same communication cost as CAS. (3) We describe an algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS) algorithm that achieves a smaller communication cost than CAS and CASGC. In particular, CCOAS incurs read and write communication costs of $\\frac{N}{N-f}$ measured in terms of number of object values. We also discuss drawbacks of CCOAS as compared with CAS and CASGC.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We consider the ANTS problem [Feinerman et al.] in which a group of agents collaboratively search for a target in a two-dimensional plane. Because this problem is inspired by the behavior of biological species, we argue that in addition to studying the {\\em time complexity} of solutions it is also important to study the {\\em selection complexity}, a measure of how likely a given algorithmic strategy is to arise in nature due to selective pressures. In more detail, we propose a new selection complexity metric $\u03c7$, defined for algorithm ${\\cal A}$ such that $\u03c7({\\cal A}) = b + \\log \\ell$, where $b$ is the number of memory bits used by each agent and $\\ell$ bounds the fineness of available probabilities (agents use probabilities of at least $1/2^\\ell$). In this paper, we study the trade-off between the standard performance metric of speed-up, which measures how the expected time to find the target improves with $n$, and our new selection metric.\n  In particular, consider $n$ agents searching for a treasure located at (unknown) distance $D$ from the origin (where $n$ is sub-exponential in $D$). For this problem, we identify $\\log \\log D$ as a crucial threshold for our selection complexity metric. We first prove a new upper bound that achieves a near-optimal speed-up of $(D^2/n +D) \\cdot 2^{O(\\ell)}$ for $\u03c7({\\cal A}) \\leq 3 \\log \\log D + O(1)$. In particular, for $\\ell \\in O(1)$, the speed-up is asymptotically optimal. By comparison, the existing results for this problem [Feinerman et al.] that achieve similar speed-up require $\u03c7({\\cal A}) = \u03a9(\\log D)$. We then show that this threshold is tight by describing a lower bound showing that if $\u03c7({\\cal A}) < \\log \\log D - \u03c9(1)$, then with high probability the target is not found within $D^{2-o(1)}$ moves per agent. Hence, there is a sizable gap to the straightforward $\u03a9(D^2/n + D)$ lower bound in this setting.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We study the multi-message broadcast problem using abstract MAC layer models of wireless networks. These models capture the key guarantees of existing MAC layers while abstracting away low-level details such as signal propagation and contention. We begin by studying upper and lower bounds for this problem in a {\\em standard abstract MAC layer model}---identifying an interesting dependence between the structure of unreliable links and achievable time complexity. In more detail, given a restriction that devices connected directly by an unreliable link are not too far from each other in the reliable link topology, we can (almost) match the efficiency of the reliable case. For the related restriction, however, that two devices connected by an unreliable link are not too far from each other in geographic distance, we prove a new lower bound that shows that this efficiency is impossible. We then investigate how much extra power must be added to the model to enable a new order of magnitude of efficiency. In more detail, we consider an {\\em enhanced abstract MAC layer model} and present a new multi-message broadcast algorithm that (under certain natural assumptions) solves the problem in this model faster than any known solutions in an abstract MAC layer setting.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "Efficient communication in wireless networks is typically challenged by the possibility of interference among several transmitting nodes. Much important research has been invested in decreasing the number of collisions in order to obtain faster algorithms for communication in such networks.\n  This paper proposes a novel approach for wireless communication, which embraces collisions rather than avoiding them, over an additive channel. It introduces a coding technique called Bounded-Contention Coding (BCC) that allows collisions to be successfully decoded by the receiving nodes into the original transmissions and whose complexity depends on a bound on the contention among the transmitters.\n  BCC enables deterministic local broadcast in a network with n nodes and at most a transmitters with information of l bits each within O(a log n + al) bits of communication with full-duplex radios, and O((a log n + al)(log n)) bits, with high probability, with half-duplex radios. When combined with random linear network coding, BCC gives global broadcast within O((D + a + log n)(a log n + l)) bits, with high probability. This also holds in dynamic networks that can change arbitrarily over time by a worst-case adversary. When no bound on the contention is given, it is shown how to probabilistically estimate it and obtain global broadcast that is adaptive to the true contention in the network.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "The local broadcast problem assumes that processes in a wireless network are provided messages, one by one, that must be delivered to their neighbors. In this paper, we prove tight bounds for this problem in two well-studied wireless network models: the classical model, in which links are reliable and collisions consistent, and the more recent dual graph model, which introduces unreliable edges. Our results prove that the Decay strategy, commonly used for local broadcast in the classical setting, is optimal. They also establish a separation between the two models, proving that the dual graph setting is strictly harder than the classical setting, with respect to this primitive.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "A method of analyzing time bounds for randomized distributed algorithms is presented, in the context of a new and general framework for describing and reasoning about randomized algorithms. The method consists of proving auxiliary statements of the form U (t)->(p) U', which means that whenever the algorithm begins in a state in set U, with probability p, it will reach a state in set U' within time t. The power of the method is illustrated by its use in proving a constant upper bound on the expected time for some process to reach its critical region, in Lehmann and Rabin's Dining Philosophers algorithm.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "We consider the following scheduling problem. A system is composed of $n$ processors drawn from a pool of $N$. The processors can become faulty while in operation and faulty processors never recover. A report is issued whenever a fault occurs. This report states only the existence of a fault, but does not indicate its location. Based on this report, the scheduler can reconfigure the system and choose another set of $n$ processors. The system operates satisfactorily as long as at most $f$ of the $n$ selected processors are faulty. We exhibit a scheduling strategy allowing the system to operate satisfactorily until approximately $(N/n)f$ faults are reported in the worst case. Our precise bound is tight.\n        \u25b3 Less", "author": "Nancy Lynch"}, {"abstract": "In this paper, we present STAR, a new distributed and replicated in-memory database. By employing a single-node non-partitioned architecture for some replicas and a partitioned architecture for other replicas, STAR is able to efficiently run both highly partitionable workloads and workloads that involve cross-partition transactions. The key idea is a new phase-switching algorithm where the execution of single-partition and cross-partition transactions are separated. In the partitioned phase, single-partition transactions are run on multiple machines in parallel to exploit more concurrency. In the single-master phase, mastership for the entire database is switched to a designated coordinator node, which can execute these transactions without the use of expensive coordination protocols like two-phase commit. Because the coordinator node has a full copy of the database, this phase-switching can be done at negligible cost. Our experiments on two popular benchmarks (YCSB and TPC-C) show that high availability via replication can coexist with fast serializable transaction execution in distributed in-memory databases, with STAR outperforming systems that employ conventional concurrency control and replication algorithms by up to one order of magnitude.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Thanks to the rapid proliferation of connected devices, sensor-generated time series constitute a large and growing portion of the world's data. Often, this data is collected from distributed, resource-constrained devices and centralized at one or more servers. A key challenge in this setup is reducing the size of the transmitted data without sacrificing its quality. Lower quality reduces the data's utility, but smaller size enables both reduced network and storage costs at the servers and reduced power consumption in sensing devices. A natural solution is to compress the data at the sensing devices. Unfortunately, existing compression algorithms either violate the memory and latency constraints common for these devices or, as we show experimentally, perform poorly on sensor-generated time series.\n  We introduce a time series compression algorithm that achieves state-of-the-art compression ratios while requiring less than 1KB of memory and adding virtually no latency. This method is suitable not only for low-power devices collecting data, but also for servers storing and querying data; in the latter context, it can decompress at over 3GB/s in a single thread, even faster than many algorithms with much lower compression ratios. A key component of our method is a high-speed forecasting algorithm that can be trained online and significantly outperforms alternatives such as delta coding.\n  Extensive experiments on datasets from many domains show that these results hold not only for sensor data but also across a wide array of other time series.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "As neural networks become widely deployed in different applications and on different hardware, it has become increasingly important to optimize inference time and model size along with model accuracy. Most current techniques optimize model size, model accuracy and inference time in different stages, resulting in suboptimal results and computational inefficiency. In this work, we propose a new technique called Smallify that optimizes all three of these metrics at the same time. Specifically we present a new method to simultaneously optimize network size and model performance by neuron-level pruning during training. Neuron-level pruning not only produces much smaller networks but also produces dense weight matrices that are amenable to efficient inference. By applying our technique to convolutional as well as fully connected models, we show that Smallify can reduce network size by 35X with a 6X improvement in inference time with similar accuracy as models found by traditional training techniques.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Many database columns contain string or numerical data that conforms to a pattern, such as phone numbers, dates, addresses, product identifiers, and employee ids. These patterns are useful in a number of data processing applications, including understanding what a specific field represents when field names are ambiguous, identifying outlier values, and finding similar fields across data sets. One way to express such patterns would be to learn regular expressions for each field in the database. Unfortunately, exist- ing techniques on regular expression learning are slow, taking hundreds of seconds for columns of just a few thousand values. In contrast, we develop XSystem, an efficient method to learn patterns over database columns in significantly less time. We show that these patterns can not only be built quickly, but are expressive enough to capture a number of key applications, including detecting outliers, measuring column similarity, and assigning semantic labels to columns (based on a library of regular expressions). We evaluate these applications with datasets that range from chemical databases (based on a collaboration with a pharmaceutical company), our university data warehouse, and open data from MassData.gov.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Data integration has been a long-standing challenge in data management with many applications. A key step in data integration is entity consolidation. It takes a collection of clusters of duplicate records as input and produces a single \"golden record\" for each cluster, which contains the canonical value for each attribute. Truth discovery and data fusion methods, as well as Master Data Management (MDM) systems, can be used for entity consolidation. However, to achieve better results, the variant values (i.e., values that are logically the same with different formats) in the clusters need to be consolidated before applying these methods.\n  For this purpose, we propose a data-driven method to standardize the variant values based on two observations: (1) the variant values usually can be transformed to the same representation (e.g., \"Mary Lee\" and \"Lee, Mary\") and (2) the same transformation often appears repeatedly across different clusters (e.g., transpose the first and last name). Our approach first uses an unsupervised method to generate groups of value pairs that can be transformed in the same way (i.e., they share a transformation). Then the groups are presented to a human for verification and the approved ones are used to standardize the data. In a real-world dataset with 17,497 records, our method achieved 75% recall and 99.5% precision in standardizing variant values by asking a human 100 yes/no questions, which completely outperformed a state of the art data wrangling tool.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Data analytics applications combine multiple functions from different libraries and frameworks. Even when each function is optimized in isolation, the performance of the combined application can be an order of magnitude below hardware limits due to extensive data movement across these functions. To address this problem, we propose Weld, a new interface between data-intensive libraries that can optimize across disjoint libraries and functions. Weld exposes a lazily-evaluated API where diverse functions can submit their computations in a simple but general intermediate representation that captures their data-parallel structure. It then optimizes data movement across these functions and emits efficient code for diverse hardware. Weld can be integrated into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without changing their user-facing APIs. We demonstrate that Weld can speed up applications using these frameworks by up to 29x.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "A polystore system is a database management system (DBMS) composed of integrated heterogeneous database engines and multiple programming languages. By matching data to the storage engine best suited to its needs, complex analytics run faster and flexible storage choices helps improve data organization. BigDAWG (Big Data Working Group) is our reference implementation of a polystore system. In this paper, we describe the current BigDAWG software release which supports PostgreSQL, Accumulo and SciDB. We describe the overall architecture, API and initial results of applying BigDAWG to the MIMIC II medical dataset.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Determining if two sets are related - that is, if they have similar values or if one set contains the other - is an important problem with many applications in data cleaning, data integration, and information retrieval. A particularly popular metric that has been proposed is to measure the relatedness of two sets by treating the elements as vertices of a bipartite graph and calculating the score of the maximum matching pairing between elements. Compared to other metrics which require exact matchings between elements, this metric uses a similarity function to compare elements between the two sets, making it robust to small dissimilarities in elements and more useful for real-world, dirty data. Unfortunately, the metric suffers from expensive computational cost, taking O(n^3) time, where n is the number of elements in sets, for each set-to-set comparison. Thus for applications which try to search for all pairings of related sets in a brute-force manner, the runtime becomes unacceptably large.\n  To address this challenge, we developed SilkMoth, a system capable of rapidly discovering related set pairs in collections of sets. Internally, SilkMoth creates a signature for each set, with the property that any other set which is related must match the signature. SilkMoth then uses these signatures to prune the search space, so only sets which match the signatures are left as candidates. Finally, SilkMoth applies the maximum matching metric on remaining candidates to verify which of these candidates are truly related sets. Thus, a contribution of this paper is the characterization of the space of signatures which enable this property. We show that selecting the optimal signature in this space is NP-complete, and based on insights from the characterization of the space, we propose two novel filters which help to prune the candidates further before verification.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Big data applications have fast arriving data that must be quickly ingested. At the same time, they have specific needs to preprocess and transform the data before it could be put to use. The current practice is to do these preparatory transformations once the data is already ingested, however, this is expensive to run and cumbersome to manage. As a result, there is a need to push data preprocessing down to the ingestion itself. In this paper, we present a declarative data ingestion system, called INGESTBASE, to allow application developers to plan and specify their data ingestion logic in a more systematic manner. We introduce the notion of ingestions plans, analogous to query plans, and present a declarative ingestion language to help developers easily build sophisticated ingestion plans. INGESTBASE provides an extensible ingestion optimizer to rewrite and optimize ingestion plans by applying rules such as operator reordering and pipelining. Finally, the INGESTBASE runtime engine runs the optimized ingestion plan in a distributed and fault-tolerant manner. Later, at query processing time, INGESTBASE supports ingestion-aware data access and interfaces with upstream query processors, such as Hadoop MapReduce and Spark, to post- process the ingested data. We demonstrate through a number of experiments that INGESTBASE: (i) is flexible enough to express a variety of ingestion techniques, (ii) incurs a low ingestion overhead, (iii) provides efficient access to the ingested data, and (iv) has much better performance, up to 6 times, than preparing data as an afterthought, via a query processor.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "The Intel Science and Technology Center for Big Data is developing a reference implementation of a Polystore database. The BigDAWG (Big Data Working Group) system supports \"many sizes\" of database engines, multiple programming languages and complex analytics for a variety of workloads. Our recent efforts include application of BigDAWG to an ocean metagenomics problem and containerization of BigDAWG. We intend to release an open source BigDAWG v1.0 in the Spring of 2017. In this article, we will demonstrate a number of polystore applications developed with oceanographic researchers at MIT and describe our forthcoming open source release of the BigDAWG system.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Existing database systems are not optimized for queries with a LIMIT clause---operating instead in an all-or-nothing manner. In this paper, we propose a fast LIMIT query evaluation engine, called NeedleTail, aimed at letting analysts browse a small sample of the query results on large datasets as quickly as possible, independent of the overall size of the result set. NeedleTail introduces density maps, a lightweight in-memory indexing structure, and a set of efficient algorithms (with desirable theoretical guarantees) to quickly locate promising blocks, trading off locality and density. In settings where the samples are used to compute aggregates, we extend techniques from survey sampling to mitigate the bias in our samples. Our experimental results demonstrate that NeedleTail returns results 4x faster on HDDs and 9x faster on SSDs on average, while occupying up to 23x less memory than existing techniques.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Organizations are often faced with the challenge of providing data management solutions for large, heterogenous datasets that may have different underlying data and programming models. For example, a medical dataset may have unstructured text, relational data, time series waveforms and imagery. Trying to fit such datasets in a single data management system can have adverse performance and efficiency effects. As a part of the Intel Science and Technology Center on Big Data, we are developing a polystore system designed for such problems. BigDAWG (short for the Big Data Analytics Working Group) is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands and a middleware that provides a uniform multi--island interface. Initial results from a prototype of the BigDAWG system applied to a medical dataset validate polystore concepts. In this article, we will describe polystore databases, the current BigDAWG architecture and its application on the MIMIC II medical dataset, initial performance results and our future development plans.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "As data volumes continue to rise, manual inspection is becoming increasingly untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient, accurate, and modular analyses that highlight and aggregate important and unusual behavior, acting as a search engine for fast data. MacroBase is able to deliver order-of-magnitude speedups over alternatives by optimizing the combination of explanation and classification tasks and by leveraging a new reservoir sampler and heavy-hitters sketch specialized for fast data streams. As a result, MacroBase delivers accurate results at speeds of up to 2M events per second per query on a single core. The system has delivered meaningful results in production, including at a telematics company monitoring hundreds of thousands of vehicles.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "BigDAWG is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands of information and a middleware that provides a uniform multi-island interface. In this article, we describe the current architecture of BigDAWG, its application on the MIMIC II medical dataset, and our plans for the mechanics of cross-system queries. During the presentation, we will also deliver a brief demonstration of the current version of BigDAWG.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Organizations and teams collect and acquire data from various sources, such as social interactions, financial transactions, sensor data, and genome sequencers. Different teams in an organization as well as different data scientists within a team are interested in extracting a variety of insights which require combining and collaboratively analyzing datasets in diverse ways. DataHub is a system that aims to provide robust version control and provenance management for such a scenario. To be truly useful for collaborative data science, one also needs the ability to specify queries and analysis tasks over the versioning and the provenance information in a unified manner. In this paper, we present an initial design of our query language, called VQuel, that aims to support such unified querying over both types of information, as well as the intermediate and final results of analyses. We also discuss some of the key language design and implementation challenges moving forward.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Stream processing addresses the needs of real-time applications. Transaction processing addresses the coordination and safety of short atomic computations. Heretofore, these two modes of operation existed in separate, stove-piped systems. In this work, we attempt to fuse the two computational paradigms in a single system called S-Store. In this way, S-Store can simultaneously accommodate OLTP and streaming applications. We present a simple transaction model for streams that integrates seamlessly with a traditional OLTP system. We chose to build S-Store as an extension of H-Store, an open-source, in-memory, distributed OLTP database system. By implementing S-Store in this way, we can make use of the transaction processing facilities that H-Store already supports, and we can concentrate on the additional implementation features that are needed to support streaming. Similar implementations could be done using other main-memory OLTP platforms. We show that we can actually achieve higher throughput for streaming workloads in S-Store than an equivalent deployment in H-Store alone. We also show how this can be achieved within H-Store with the addition of a modest amount of new functionality. Furthermore, we compare S-Store to two state-of-the-art streaming systems, Spark Streaming and Storm, and show how S-Store matches and sometimes exceeds their performance while providing stronger transactional guarantees.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Graph analytics is becoming increasingly popular, with a deluge of new systems for graph analytics having been proposed in the past few years. These systems often start from the assumption that a new storage or query processing system is needed, in spite of graph data being often collected and stored in a relational database in the first place. In this paper, we study Vertica relational database as a platform for graph analytics. We show that vertex-centric graph analysis can be translated to SQL queries, typically involving table scans and joins, and that modern column-oriented databases are very well suited to running such queries. Specifically, we present an experimental evaluation of the Vertica relational database system on a variety of graph analytics, including iterative analysis, a combination of graph and relational analyses, and more complex 1- hop neighborhood graph analytics, showing that it is competitive to two popular vertex-centric graph analytics systems, namely Giraph and GraphLab.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DataHub, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "The General Anti-Particle Spectrometer (GAPS) project is being carried out to search for primary cosmic-ray antiparticles especially for antideuterons produced by cold dark matter. GAPS plans to realize the science observation by Antarctic long duration balloon flights in the late 2010s. In preparation for the Antarctic science flights, an engineering balloon flight using a prototype of the GAPS instrument, \"pGAPS\", was successfully carried out in June 2012 in Japan to verify the basic performance of each GAPS subsystem. The outline of the pGAPS flight campaign is briefly reported.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Crowd-sourcing has become a popular means of acquiring labeled data for a wide variety of tasks where humans are more accurate than computers, e.g., labeling images, matching objects, or analyzing sentiment. However, relying solely on the crowd is often impractical even for data sets with thousands of items, due to time and cost constraints of acquiring human input (which cost pennies and minutes per label). In this paper, we propose algorithms for integrating machine learning into crowd-sourced databases, with the goal of allowing crowd-sourcing applications to scale, i.e., to handle larger datasets at lower costs. The key observation is that, in many of the above tasks, humans and machine learning algorithms can be complementary, as humans are often more accurate but slow and expensive, while algorithms are usually less accurate, but faster and cheaper.\n  Based on this observation, we present two new active learning algorithms to combine humans and algorithms together in a crowd-sourced database. Our algorithms are based on the theory of non-parametric bootstrap, which makes our results applicable to a broad class of machine learning models. Our results, on three real-life datasets collected with Amazon's Mechanical Turk, and on 15 well-known UCI data sets, show that our methods on average ask humans to label one to two orders of magnitude fewer items to achieve the same accuracy as a baseline that labels random images, and two to eight times fewer questions than previous active learning schemes.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "This paper presents a new approach to select events of interest to a user in a social media setting where events are generated by the activities of the user's friends through their mobile devices. We argue that given the unique requirements of the social media setting, the problem is best viewed as an inductive learning problem, where the goal is to first generalize from the users' expressed \"likes\" and \"dislikes\" of specific events, then to produce a program that can be manipulated by the system and distributed to the collection devices to collect only data of interest. The key contribution of this paper is a new algorithm that combines existing machine learning techniques with new program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application. The approach also improves on standard machine learning techniques in that it produces clear programs that can be manipulated to optimize data collection and filtering.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Developing high-performance applications that interact with databases is a difficult task, as developers need to understand both the details of the language in which their applications are written in, and also the intricacies of the relational model. One popular solution to this problem is the use of object-relational mapping (ORM) libraries that provide transparent access to the database using the same language that the application is written in. Unfortunately, using such frameworks can easily lead to applications with poor performance because developers often end up implementing relational operations in application code, and doing so usually does not take advantage of the optimized implementations of relational operations, efficient query plans, or push down of predicates that database systems provide. In this paper we present QBS, an algorithm that automatically identifies fragments of application logic that can be pushed into SQL queries. The QBS algorithm works by automatically synthesizing invariants and postconditions for the original code fragment. The postconditions and invariants are expressed using a theory of ordered relations that allows us to reason precisely about the contents and order of the records produced even by complex code fragments that compute joins and aggregates. The theory is close in expressiveness to SQL, so the synthesized postconditions can be readily translated to SQL queries. Using 40 code fragments extracted from over 120k lines of open-source code written using the Java Hibernate ORM, we demonstrate that our approach can convert a variety of imperative constructs into relational specifications.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Database-backed applications are nearly ubiquitous in our daily lives. Applications that make many small accesses to the database create two challenges for developers: increased latency and wasted resources from numerous network round trips. A well-known technique to improve transactional database application performance is to convert part of the application into stored procedures that are executed on the database server. Unfortunately, this conversion is often difficult. In this paper we describe Pyxis, a system that takes database-backed applications and automatically partitions their code into two pieces, one of which is executed on the application server and the other on the database server. Pyxis profiles the application and server loads, statically analyzes the code's dependencies, and produces a partitioning that minimizes the number of control transfers as well as the amount of data sent during each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is able to generate partitions with up to 3x reduction in latency and 1.7x improvement in throughput when compared to a traditional non-partitioned implementation and has comparable performance to that of a custom stored procedure implementation.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Replicating data across multiple data centers not only allows moving the data closer to the user and, thus, reduces latency for applications, but also increases the availability in the event of a data center failure. Therefore, it is not surprising that companies like Google, Yahoo, and Netflix already replicate user data across geographically different regions.\n  However, replication across data centers is expensive. Inter-data center network delays are in the hundreds of milliseconds and vary significantly. Synchronous wide-area replication is therefore considered to be unfeasible with strong consistency and current solutions either settle for asynchronous replication which implies the risk of losing data in the event of failures, restrict consistency to small partitions, or give up consistency entirely. With MDCC (Multi-Data Center Consistency), we describe the first optimistic commit protocol, that does not require a master or partitioning, and is strongly consistent at a cost similar to eventually consistent protocols. MDCC can commit transactions in a single round-trip across data centers in the normal operational case. We further propose a new programming model which empowers the application developer to handle longer and unpredictable latencies caused by inter-data center communication. Our evaluation using the TPC-W benchmark with MDCC deployed across 5 geographically diverse data centers shows that MDCC is able to achieve throughput and latency similar to eventually consistent quorum protocols and that MDCC is able to sustain a data center outage without a significant impact on response times while guaranteeing strong consistency.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "In this paper, we present BlinkDB, a massively parallel, sampling-based approximate query engine for running ad-hoc, interactive SQL queries on large volumes of data. The key insight that BlinkDB builds on is that one can often make reasonable decisions in the absence of perfect answers. For example, reliably detecting a malfunctioning server using a distributed collection of system logs does not require analyzing every request processed by the system. Based on this insight, BlinkDB allows one to trade-off query accuracy for response time, enabling interactive queries over massive data by running queries on data samples and presenting results annotated with meaningful error bars. To achieve this, BlinkDB uses two key ideas that differentiate it from previous work in this area: (1) an adaptive optimization framework that builds and maintains a set of multi-dimensional, multi-resolution samples from original data over time, and (2) a dynamic sample selection strategy that selects an appropriately sized sample based on a query's accuracy and/or response time requirements. We have built an open-source version of BlinkDB and validated its effectiveness using the well-known TPC-H benchmark as well as a real-world analytic workload derived from Conviva Inc. Our experiments on a 100 node cluster show that BlinkDB can answer a wide range of queries from a real-world query trace on up to 17 TBs of data in less than 2 seconds (over 100\\times faster than Hive), within an error of 2 - 10%.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible to task people with small jobs, such as labeling images or looking up phone numbers, via a programmatic interface. MTurk tasks for processing datasets with humans are currently designed with significant reimplementation of common workflows and ad-hoc selection of parameters such as price to pay per task. We describe how we have integrated crowds into a declarative workflow engine called Qurk to reduce the burden on workflow designers. In this paper, we focus on how to use humans to compare items for sorting and joining data, two of the most common operations in DBMSs. We describe our basic query interface and the user interface of the tasks we post to MTurk. We also propose a number of optimizations, including task batching, replacing pairwise comparisons with numerical ratings, and pre-filtering tables before joining them, which dramatically reduce the overall cost of running sorts and joins on the crowd. In an experiment joining two sets of images, we reduce the overall cost from $67 in a naive implementation to about $3, without substantially affecting accuracy or latency. In an end-to-end experiment, we reduced cost by a factor of 14.5.\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "The GALEX Ultraviolet Virgo Cluster Survey (GUViCS) is a complete blind survey of the Virgo cluster covering about 40 sq. deg. in the far UV (FUV, lambda_eff=1539A, Delta-lambda=442A) and about 120 sq. deg. in the near UV (NUV, lambda_eff=2316A, Delta-lambda=1060A). The goal of the survey is to study the ultraviolet (UV) properties of galaxies in a rich cluster environment, spanning a wide luminosity range from giants to dwarfs, and regardless of prior knowledge of their star formation activity. The UV data will be combined with those in other bands (optical: NGVS; far-infrared - submm: HeViCS; HI: ALFALFA) and with our multizone chemo-spectrophotometric models of galaxy evolution to make a complete and exhaustive study of the effects of the environment on the evolution of galaxies in high density regions. We present here the scientific objectives of the survey, describing the observing strategy and briefly discussing different data reduction techniques. Using UV data already in-hand for the central 12 sq. deg. we determine the FUV and NUV luminosity functions of the Virgo cluster core for all cluster members and separately for early- and late-type galaxies and compare it to the one obtained in the field and other nearby clusters (Coma, A1367). This analysis shows that the FUV and NUV luminosity functions of the core of the Virgo clusters are flatter (alpha about -1.1) than those determined in Coma and A1367. We discuss the possible origin of this difference\n        \u25b3 Less", "author": "Samuel Madden"}, {"abstract": "We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. We propose a fine-grained analysis of state-of-the-art methods based on key aspects of this framework: gradient estimation, value prediction, optimization landscapes, and trust region enforcement. We find that from this perspective, the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict. Our analysis suggests first steps towards solidifying the foundations of these algorithms, and in particular indicates that we may need to move beyond the current benchmark-centric evaluation methodology.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "A recent line of work has uncovered a new form of data poisoning: so-called \\emph{backdoor} attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by a perturbation planted by an adversary.\n  In this paper, we identify a new property of all known backdoor attacks, which we call \\emph{spectral signatures}. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards designing ML systems secure against such backdoor attacks\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We explore the concept of co-design in the context of neural network verification. Specifically, we aim to train deep neural networks that not only are robust to adversarial perturbations but also whose robustness can be verified more easily. To this end, we identify two properties of network models - weight sparsity and so-called ReLU stability - that turn out to significantly impact the complexity of the corresponding verification task. We demonstrate that improving weight sparsity alone already enables us to turn computationally intractable verification problems into tractable ones. Then, improving ReLU stability leads to an additional 4-13x speedup in verification times. An important feature of our methodology is its \"universality,\" in the sense that it can be used with a broad range of training procedures and verification approaches.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We introduce a framework that unifies the existing work on black-box adversarial example generation. We demonstrate that the current state of the art in the field is optimal in a certain natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: ambient priors for the gradient. We identify two such priors, and give an algorithm based on bandit optimization that allows for seamless integration of these and other priors. Our framework leads to methods that are two to three times more query-efficient and two to three times smaller failure rate than the state-of-the-art approaches.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We show that there exists an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists even in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed in practice. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called \"internal covariate shift\". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high \"standard\" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of \"standard\" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "Sparsity-based methods are widely used in machine learning, statistics, and signal processing. There is now a rich class of structured sparsity approaches that expand the modeling power of the sparsity paradigm and incorporate constraints such as group sparsity, graph sparsity, or hierarchical sparsity. While these sparsity models offer improved sample complexity and better interpretability, the improvements come at a computational cost: it is often challenging to optimize over the (non-convex) constraint sets that capture various sparsity structures. In this paper, we make progress in this direction in the context of separated sparsity -- a fundamental sparsity notion that captures exclusion constraints in linearly ordered data such as time series. While prior algorithms for computing a projection onto this constraint set required quadratic time, we provide a perturbed Lagrangian relaxation approach that computes provably exact projection in only nearly-linear time. Although the sparsity constraint is non-convex, our perturbed Lagrangian approach is still guaranteed to find a globally optimal solution. In experiments, our new algorithms offer a 10$\\times$ speed-up already on moderately-size inputs.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We show that simple transformations, namely translations and rotations alone, are sufficient to fool neural network-based vision models on a significant fraction of inputs. This is in sharp contrast to previous work that relied on more complicated optimization approaches that are unlikely to appear outside of a truly adversarial setting. Moreover, fooling rotations and translations are easy to find and require only a few black-box queries to the target model. Overall, our findings emphasize the need for designing robust classifiers even in natural, benign contexts.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We present an $O((\\log k)^2)$-competitive randomized algorithm for the $k$-server problem on hierarchically separated trees (HSTs). This is the first $o(k)$-competitive randomized algorithm for which the competitive ratio is independent of the size of the underlying HST. Our algorithm is designed in the framework of online mirror descent where the mirror map is a multiscale entropy. When combined with Bartal's static HST embedding reduction, this leads to an $O((\\log k)^2 \\log n)$-competitive algorithm on any $n$-point metric space. We give a new dynamic HST embedding that yields an $O((\\log k)^3 \\log \u0394)$-competitive algorithm on any metric space where the ratio of the largest to smallest non-zero distance is at most $\u0394$.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "For over a decade now we have been witnessing the success of {\\em massive parallel computation} (MPC) frameworks, such as MapReduce, Hadoop, Dryad, or Spark. One of the reasons for their success is the fact that these frameworks are able to accurately capture the nature of large-scale computation. In particular, compared to the classic distributed algorithms or PRAM models, these frameworks allow for much more local computation. The fundamental question that arises in this context is though: can we leverage this additional power to obtain even faster parallel algorithms?\n  A prominent example here is the {\\em maximum matching} problem---one of the most classic graph problems. It is well known that in the PRAM model one can compute a 2-approximate maximum matching in $O(\\log{n})$ rounds. However, the exact complexity of this problem in the MPC framework is still far from understood. Lattanzi et al. showed that if each machine has $n^{1+\u03a9(1)}$ memory, this problem can also be solved $2$-approximately in a constant number of rounds. These techniques, as well as the approaches developed in the follow up work, seem though to get stuck in a fundamental way at roughly $O(\\log{n})$ rounds once we enter the near-linear memory regime. It is thus entirely possible that in this regime, which captures in particular the case of sparse graph computations, the best MPC round complexity matches what one can already get in the PRAM model, without the need to take advantage of the extra local computation power.\n  In this paper, we finally refute that perplexing possibility. That is, we break the above $O(\\log n)$ round complexity bound even in the case of {\\em slightly sublinear} memory per machine. In fact, our improvement here is {\\em almost exponential}: we are able to deliver a $(2+\u03b5)$-approximation to maximum matching, for any fixed constant $\u03b5>0$, in $O((\\log \\log n)^2)$ rounds.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an optimal discriminator provably converges, while first order approximations of the discriminator steps lead to unstable GAN dynamics and mode collapse. Our result suggests that using first order discriminator steps (the de-facto standard in most existing GAN setups) might be one of the factors that makes GAN training challenging in practice.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "In this paper, we study matrix scaling and balancing, which are fundamental problems in scientific computing, with a long line of work on them that dates back to the 1960s. We provide algorithms for both these problems that, ignoring logarithmic factors involving the dimension of the input matrix and the size of its entries, both run in time $\\widetilde{O}\\left(m\\log \u03ba\\log^2 (1/\u03b5)\\right)$ where $\u03b5$ is the amount of error we are willing to tolerate. Here, $\u03ba$ represents the ratio between the largest and the smallest entries of the optimal scalings. This implies that our algorithms run in nearly-linear time whenever $\u03ba$ is quasi-polynomial, which includes, in particular, the case of strictly positive matrices. We complement our results by providing a separate algorithm that uses an interior-point method and runs in time $\\widetilde{O}(m^{3/2} \\log (1/\u03b5))$.\n  In order to establish these results, we develop a new second-order optimization framework that enables us to treat both problems in a unified and principled manner. This framework identifies a certain generalization of linear system solving that we can use to efficiently minimize a broad class of functions, which we call second-order robust. We then show that in the context of the specific functions capturing matrix scaling and balancing, we can leverage and generalize the work on Laplacian system solving to make the algorithms obtained via this framework very efficient.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We present an $\\tilde{O}\\left(m^{\\frac{10}{7}}U^{\\frac{1}{7}}\\right)$-time algorithm for the maximum $s$-$t$ flow problem and the minimum $s$-$t$ cut problem in directed graphs with $m$ arcs and largest integer capacity $U$. This matches the running time of the $\\tilde{O}\\left((mU)^{\\frac{10}{7}}\\right)$-time algorithm of M\u0105dry (FOCS 2013) in the unit-capacity case, and improves over it, as well as over the $\\tilde{O}\\left(m \\sqrt{n} \\log U\\right)$-time algorithm of Lee and Sidford (FOCS 2014), whenever $U$ is moderately large and the graph is sufficiently sparse. By well-known reductions, this also gives similar running time improvements for the maximum-cardinality bipartite $b$-matching problem.\n  One of the advantages of our algorithm is that it is significantly simpler than the ones presented in Madry (FOCS 2013) and Lee and Sidford (FOCS 2014). In particular, these algorithms employ a sophisticated interior-point method framework, while our algorithm is cast directly in the classic augmenting path setting that almost all the combinatorial maximum flow algorithms use. At a high level, the presented algorithm takes a primal dual approach in which each iteration uses electrical flows computations both to find an augmenting $s$-$t$ flow in the current residual graph and to update the dual solution. We show that by maintain certain careful coupling of these primal and dual solutions we are always guaranteed to make significant progress.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "In this paper, we study a set of combinatorial optimization problems on weighted graphs: the shortest path problem with negative weights, the weighted perfect bipartite matching problem, the unit-capacity minimum-cost maximum flow problem and the weighted perfect bipartite $b$-matching problem under the assumption that $\\Vert b\\Vert_1=O(m)$. We show that each one of these four problems can be solved in $\\tilde{O}(m^{10/7}\\log W)$ time, where $W$ is the absolute maximum weight of an edge in the graph, which gives the first in over 25 years polynomial improvement in their sparse-graph time complexity.\n  At a high level, our algorithms build on the interior-point method-based framework developed by Madry (FOCS 2013) for solving unit-capacity maximum flow problem. We develop a refined way to analyze this framework, as well as provide new variants of the underlying preconditioning and perturbation techniques. Consequently, we are able to extend the whole interior-point method-based approach to make it applicable in the weighted graph regime.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We present a new algorithm for generating a uniformly random spanning tree in an undirected graph. Our algorithm samples such a tree in expected $\\tilde{O}(m^{4/3})$ time. This improves over the best previously known bound of $\\min(\\tilde{O}(m\\sqrt{n}),O(n^\u03c9))$ -- that follows from the work of Kelner and M\u0105dry [FOCS'09] and of Colbourn et al. [J. Algorithms'96] -- whenever the input graph is sufficiently sparse.\n  At a high level, our result stems from carefully exploiting the interplay of random spanning trees, random walks, and the notion of effective resistance, as well as from devising a way to algorithmically relate these concepts to the combinatorial structure of the graph. This involves, in particular, establishing a new connection between the effective resistance metric and the cut structure of the underlying graph.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We present and study the Static-Routing-Resiliency problem, motivated by routing on the Internet: Given a graph $G$, a unique destination vertex $d$, and an integer constant $c>0$, does there exist a static and destination-based routing scheme such that the correct delivery of packets from any source $s$ to the destination $d$ is guaranteed so long as (1) no more than $c$ edges fail and (2) there exists a physical path from $s$ to $d$? We embark upon a systematic exploration of this fundamental question in a variety of models (deterministic routing, randomized routing, with packet-duplication, with packet-header-rewriting) and present both positive and negative results that relate the edge-connectivity of a graph, i.e., the minimum number of edges whose deletion partitions $G$, to its resiliency.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We study the Maximum Budgeted Allocation problem, i.e., the problem of selling a set of $m$ indivisible goods to $n$ players, each with a separate budget, such that we maximize the collected revenue. Since the natural assignment LP is known to have an integrality gap of $\\frac{3}{4}$, which matches the best known approximation algorithms, our main focus is to improve our understanding of the stronger configuration LP relaxation. In this direction, we prove that the integrality gap of the configuration LP is strictly better than $\\frac{3}{4}$, and provide corresponding polynomial time roundings, in the following restrictions of the problem: (i) the Restricted Budgeted Allocation problem, in which all the players have the same budget and every item has the same value for any player it can be sold to, and (ii) the graph MBA problem, in which an item can be assigned to at most 2 players. Finally, we improve the best known upper bound on the integrality gap for the general case from $\\frac{5}{6}$ to $2\\sqrt{2}-2\\approx 0.828$ and also prove hardness of approximation results for both cases.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We present an $\\tilde{O}(m^{10/7})=\\tilde{O}(m^{1.43})$-time algorithm for the maximum s-t flow and the minimum s-t cut problems in directed graphs with unit capacities. This is the first improvement over the sparse-graph case of the long-standing $O(m \\min(\\sqrt{m},n^{2/3}))$ time bound due to Even and Tarjan [EvenT75]. By well-known reductions, this also establishes an $\\tilde{O}(m^{10/7})$-time algorithm for the maximum-cardinality bipartite matching problem. That, in turn, gives an improvement over the celebrated celebrated $O(m \\sqrt{n})$ time bound of Hopcroft and Karp [HK73] whenever the input graph is sufficiently sparse.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We give the first polylogarithmic-competitive randomized online algorithm for the $k$-server problem on an arbitrary finite metric space. In particular, our algorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any metric space on n points. Our algorithm improves upon the deterministic (2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95] whenever n is sub-exponential in k.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems. these problems are motivated by the lasso framework and have applications in machine learning and computer vision.\n  Our work shows a close connection between these problems and core questions in algorithmic graph theory. While this connection demonstrates the difficulties of obtaining runtime guarantees, it also suggests an approach of using techniques originally developed for graph algorithms.\n  We then show that most of these problems can be formulated as a grouped least squares problem, and give efficient algorithms for this formulation. Our algorithms rely on routines for solving quadratic minimization problems, which in turn are equivalent to solving linear systems. Finally we present some experimental results on applying our approximation algorithm to image processing problems.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We introduce a new approach to computing an approximately maximum s-t flow in a capacitated, undirected graph. This flow is computed by solving a sequence of electrical flow problems. Each electrical flow is given by the solution of a system of linear equations in a Laplacian matrix, and thus may be approximately computed in nearly-linear time.\n  Using this approach, we develop the fastest known algorithm for computing approximately maximum s-t flows. For a graph having n vertices and m edges, our algorithm computes a (1-\u03b5)-approximately maximum s-t flow in time \\tilde{O}(mn^{1/3} \u03b5^{-11/3}). A dual version of our approach computes a (1+\u03b5)-approximately minimum s-t cut in time \\tilde{O}(m+n^{4/3}\\eps^{-8/3}), which is the fastest known algorithm for this problem as well. Previously, the best dependence on m and n was achieved by the algorithm of Goldberg and Rao (J. ACM 1998), which can be used to compute approximately maximum s-t flows in time \\tilde{O}(m\\sqrt{n}\u03b5^{-1}), and approximately minimum s-t cuts in time \\tilde{O}(m+n^{3/2}\u03b5^{-3}).\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We present a general method of designing fast approximation algorithms for cut-based minimization problems in undirected graphs. In particular, we develop a technique that given any such problem that can be approximated quickly on trees, allows approximating it almost as quickly on general graphs while only losing a poly-logarithmic factor in the approximation guarantee.\n  To illustrate the applicability of our paradigm, we focus our attention on the undirected sparsest cut problem with general demands and the balanced separator problem. By a simple use of our framework, we obtain poly-logarithmic approximation algorithms for these problems that run in time close to linear.\n  The main tool behind our result is an efficient procedure that decomposes general graphs into simpler ones while approximately preserving the cut-flow structure. This decomposition is inspired by the cut-based graph decomposition of R\u00e4cke that was developed in the context of oblivious routing schemes, as well as, by the construction of the ultrasparsifiers due to Spielman and Teng that was employed to preconditioning symmetric diagonally-dominant matrices.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We combine the work of Garg and Konemann, and Fleischer with ideas from dynamic graph algorithms to obtain faster (1-eps)-approximation schemes for various versions of the multicommodity flow problem. In particular, if eps is moderately small and the size of every number used in the input instance is polynomially bounded, the running times of our algorithms match - up to poly-logarithmic factors and some provably optimal terms - the Omega(mn) flow-decomposition barrier for single-commodity flow.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "In this paper, we set forth a new algorithm for generating approximately uniformly random spanning trees in undirected graphs. We show how to sample from a distribution that is within a multiplicative $(1+\u03b4)$ of uniform in expected time $\\TO(m\\sqrt{n}\\log 1/\u03b4)$. This improves the sparse graph case of the best previously known worst-case bound of $O(\\min \\{mn, n^{2.376}\\})$, which has stood for twenty years.\n  To achieve this goal, we exploit the connection between random walks on graphs and electrical networks, and we use this to introduce a new approach to the problem that integrates discrete random walk-based techniques with continuous linear algebraic methods. We believe that our use of electrical networks and sparse linear system solvers in conjunction with random walks and combinatorial partitioning techniques is a useful paradigm that will find further applications in algorithmic graph theory.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We give a 7/9 - Approximation Algorithm for the Maximum Traveling Salesman Problem.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "It is well known that unconditionally secure bit commitment is impossible even in the quantum world. In this paper a weak variant of quantum bit commitment, introduced independently by Aharonov et al. [STOC, 2000] and Hardy and Kent [Phys. Rev. Lett. 92 (2004)] is investigated. In this variant, the parties require some nonzero probability of detecting a cheating, i.e. if Bob, who commits a bit b to Alice, changes his mind during the revealing phase then Alice detects the cheating with a positive probability (we call this property binding); and if Alice gains information about the committed bit before the revealing phase then Bob discovers this with positive probability (sealing). In our paper we give quantum bit commitment scheme that is simultaneously binding and sealing and we show that if a cheating gives epsilon advantage to a malicious Alice then Bob can detect the cheating with a probability Omega(epsilon^2). If Bob cheats then Alice's probability of detecting the cheating is greater than some fixed constant lambda>0. This improves the probabilities of cheating detections shown by Hardy and Kent and the scheme by Aharonov et al. who presented a protocol that is either binding or sealing, but not simultaneously both. To construct a cheat sensitive quantum bit commitment scheme we use a protocol for a weak quantum one-out-of-two oblivious transfer.\n        \u25b3 Less", "author": "Aleksander Madry"}, {"abstract": "We introduce an algorithm design technique for a class of combinatorial optimization problems with concave costs. This technique yields a strongly polynomial primal-dual algorithm for a concave cost problem whenever such an algorithm exists for the fixed-charge counterpart of the problem. For many practical concave cost problems, the fixed-charge counterpart is a well-studied combinatorial optimization problem. Our technique preserves constant factor approximation ratios, as well as ratios that depend only on certain problem parameters, and exact algorithms yield exact algorithms.\n  Using our technique, we obtain a new 1.61-approximation algorithm for the concave cost facility location problem. For inventory problems, we obtain a new exact algorithm for the economic lot-sizing problem with general concave ordering costs, and a 4-approximation algorithm for the joint replenishment problem with general concave individual ordering costs.\n        \u25b3 Less", "author": "Thomas Magnanti"}, {"abstract": "We study the problem of minimizing a nonnegative separable concave function over a compact feasible set. We approximate this problem to within a factor of 1+epsilon by a piecewise-linear minimization problem over the same feasible set. Our main result is that when the feasible set is a polyhedron, the number of resulting pieces is polynomial in the input size of the polyhedron and linear in 1/epsilon. For many practical concave cost problems, the resulting piecewise-linear cost problem can be formulated as a well-studied discrete optimization problem. As a result, a variety of polynomial-time exact algorithms, approximation algorithms, and polynomial-time heuristics for discrete optimization problems immediately yield fully polynomial-time approximation schemes, approximation algorithms, and polynomial-time heuristics for the corresponding concave cost problems.\n  We illustrate our approach on two problems. For the concave cost multicommodity flow problem, we devise a new heuristic and study its performance using computational experiments. We are able to approximately solve significantly larger test instances than previously possible, and obtain solutions on average within 4.27% of optimality. For the concave cost facility location problem, we obtain a new 1.4991+epsilon approximation algorithm.\n        \u25b3 Less", "author": "Thomas Magnanti"}, {"abstract": "The Probe of Inflation and Cosmic Origins (PICO) is a probe-class mission concept currently under study by NASA. PICO will probe the physics of the Big Bang and the energy scale of inflation, constrain the sum of neutrino masses, measure the growth of structures in the universe, and constrain its reionization history by making full sky maps of the cosmic microwave background with sensitivity 80 times higher than the Planck space mission. With bands at 21-799 GHz and arcmin resolution at the highest frequencies, PICO will make polarization maps of Galactic synchrotron and dust emission to observe the role of magnetic fields in Milky Way's evolution and star formation. We discuss PICO's optical system, focal plane, and give current best case noise estimates. The optical design is a two-reflector optimized open-Dragone design with a cold aperture stop. It gives a diffraction limited field of view (DLFOV) with throughput of 910 square cm sr at 21 GHz. The large 82 square degree DLFOV hosts 12,996 transition edge sensor bolometers distributed in 21 frequency bands and maintained at 0.1 K. We use focal plane technologies that are currently implemented on operating CMB instruments including three-color multi-chroic pixels and multiplexed readouts. To our knowledge, this is the first use of an open-Dragone design for mm-wave astrophysical observations, and the only monolithic CMB instrument to have such a broad frequency coverage. With current best case estimate polarization depth of 0.65 microK(CMB}-arcmin over the entire sky, PICO is the most sensitive CMB instrument designed to date.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The Probe of Inflation and Cosmic Origins (PICO) is a NASA-funded study of a Probe-class mission concept. The top-level science objectives are to probe the physics of the Big Bang by measuring or constraining the energy scale of inflation, probe fundamental physics by measuring the number of light particles in the Universe and the sum of neutrino masses, to measure the reionization history of the Universe, and to understand the mechanisms driving the cosmic star formation history, and the physics of the galactic magnetic field. PICO would have multiple frequency bands between 21 and 799 GHz, and would survey the entire sky, producing maps of the polarization of the cosmic microwave background radiation, of galactic dust, of synchrotron radiation, and of various populations of point sources. Several instrument configurations, optical systems, cooling architectures, and detector and readout technologies have been and continue to be considered in the development of the mission concept. We will present a snapshot of the baseline mission concept currently under development.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The rapid evolution of technology and the parallel increasing complexity of algorithmic analysis in HEP requires developers to acquire a much larger portfolio of programming skills. Young researchers graduating from universities worldwide currently do not receive adequate preparation in the very diverse fields of modern computing to respond to growing needs of the most advanced experimental challenges. There is a growing consensus in the HEP community on the need for training programmes to bring researchers up to date with new software technologies, in particular in the domains of concurrent programming and artificial intelligence. We review some of the initiatives under way for introducing new training programmes and highlight some of the issues that need to be taken into account for these to be successful.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We report results from very long baseline interferometric (VLBI) observations of the supermassive black hole in the Galactic center, Sgr A*, at 1.3 mm (230 GHz). The observations were performed in 2013 March using six VLBI stations in Hawaii, California, Arizona, and Chile. Compared to earlier observations, the addition of the APEX telescope in Chile almost doubles the longest baseline length in the array, provides additional {\\it uv} coverage in the N-S direction, and leads to a spatial resolution of $\\sim$30 $\u03bc$as ($\\sim$3 Schwarzschild radii) for Sgr A*. The source is detected even at the longest baselines with visibility amplitudes of $\\sim$4-13% of the total flux density. We argue that such flux densities cannot result from interstellar refractive scattering alone, but indicate the presence of compact intrinsic source structure on scales of $\\sim$3 Schwarzschild radii. The measured nonzero closure phases rule out point-symmetric emission. We discuss our results in the context of simple geometric models that capture the basic characteristics and brightness distributions of disk- and jet-dominated models and show that both can reproduce the observed data. Common to these models are the brightness asymmetry, the orientation, and characteristic sizes, which are comparable to the expected size of the black hole shadow. Future 1.3 mm VLBI observations with an expanded array and better sensitivity will allow a more detailed imaging of the horizon-scale structure and bear the potential for a deep insight into the physical processes at the black hole boundary.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Future remote sensing of exoplanets will be enhanced by a thorough investigation of our solar system Ice Giants (Neptune-size planets). What can the configuration of the magnetic field tell us (remotely) about the interior, and what implications does that field have for the structure of the magnetosphere; energy input into the atmosphere, and surface geophysics (for example surface weathering of satellites that might harbour sub-surface oceans). How can monitoring of auroral emission help inform future remote observations of emission from exoplanets? Our Solar System provides the only laboratory in which we can perform in-situ experiments to understand exoplanet formation, dynamos, systems and magnetospheres.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R&D activities required to prepare for this software upgrade.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We use high-resolution continuum images obtained at 870microns with the Atacama Large Millimeter Array (ALMA) to probe the surface density of star-formation in z~2 galaxies and study the different physical properties between galaxies within and above the star-formation main sequence of galaxies. This sample of eight star-forming galaxies at z~2 selected among the most massive Herschel galaxies in the GOODS-South field is supplemented with eleven galaxies from the public data of the 1.3 mm survey of the Hubble Ultra-Deep Field. ALMA reveals systematically dense concentrations of dusty star-formation close to the center of the stellar component of the galaxies. We identify two different starburst regimes: (i) the classical population of starbursts located above the SFR-M* main sequence, with enhanced gas fractions and short depletion times and (ii) a sub-population of galaxies located within the scatter of the main sequence that experience compact star formation with depletion timescales typical of starbursts of ~150 Myr. In both starburst populations, the far infrared and UV are distributed in distinct regions and dust-corrected star formation rates estimated using UV-optical-NIR data alone underestimate the total star formation rate. Starbursts hidden in the main sequence show instead the lowest gas fractions of our sample and could represent the last stage of star-formation before they become passive. Being Herschel-selected, these main sequence galaxies are located in the high-mass end of the main sequence, hence we do not know whether these \"starbursts hidden in the main sequence\" also exist below 10^11 Msun. Active galactic nuclei are found to be ubiquitous in these compact starbursts, suggesting that the triggering mechanism also feeds the central black hole or that the active nucleus triggers star formation.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We assess the science reach and technical feasibility of a satellite mission based on precision atomic sensors configured to detect gravitational radiation. Conceptual advances in the past three years indicate that a two-satellite constellation with science payloads consisting of atomic sensors based on laser cooled atomic Sr can achieve scientifically interesting gravitational wave strain sensitivities in a frequency band between the LISA and LIGO detectors, roughly 30 mHz to 10 Hz. The discovery potential of the proposed instrument ranges from from observation of new astrophysical sources (e.g. black hole and neutron star binaries) to searches for cosmological sources of stochastic gravitational radiation and searches for dark matter.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We derive water vapor column abundances and aerosol properties from Mars Science Laboratory (MSL) ChemCam passive mode observations of scattered sky light.\n  Each ChemCam passive sky observation acquires spectra at two different elevation angles. We fit these spectra with a discrete-ordinates multiple scattering radiative transfer model, using the correlated-k approximation for gas absorption bands. The retrieval proceeds by first fitting the continuum of the ratio of the two elevation angles to solve for aerosol properties, and then fitting the continuum-removed ratio to solve for gas abundances. The final step of the retrieval makes use of the observed CO2 absorptions and the known CO2 abundance to correct the retrieved water vapor abundance for the effects of the vertical distribution of scattering aerosols and to derive an aerosol scale height parameter.\n  The ChemCam-retrieved water abundances show, with only a few exceptions, the same seasonal behavior and the same timing of seasonal minima and maxima as the TES, CRISM, and REMS-H data sets that we compare them to. However ChemCam-retrieved water abundances are generally lower than zonal and regional scale from-orbit water vapor data, while at the same time being significantly larger than pre-dawn REMS-H abundances. Pending further analysis of REMS-H volume mixing ratio uncertainties, the differences between ChemCam and REMS-H pre-dawn mixing ratios appear to be much too large to be explained by large scale circulations and thus they tend to support the hypothesis of substantial diurnal interactions of water vapor with the surface. Our preliminary aerosol results, meanwhile, show the expected seasonal pattern in dust particle size but also indicate a surprising inter-annual increase in water-ice cloud opacities.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Research has shown that false alarms constitute more than 80% of the alarms triggered in the intensive care unit (ICU). The high false arrhythmia alarm rate has severe implications such as disruption of patient care, caregiver alarm fatigue, and desensitization from clinical staff to real life-threatening alarms. A method to reduce the false alarm rate would therefore greatly benefit patients as well as nurses in their ability to provide care. We here develop and describe a robust false arrhythmia alarm reduction system for use in the ICU. Building off of work previously described in the literature, we make use of signal processing and machine learning techniques to identify true and false alarms for five arrhythmia types. This baseline algorithm alone is able to perform remarkably well, with a sensitivity of 0.908, a specificity of 0.838, and a PhysioNet/CinC challenge score of 0.756. We additionally explore dynamic time warping techniques on both the entire alarm signal as well as on a beat-by-beat basis in an effort to improve performance of ventricular tachycardia, which has in the literature been one of the hardest arrhythmias to classify. Such an algorithm with strong performance and efficiency could potentially be translated for use in the ICU to promote overall patient care and recovery.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The MeerKAT telescope will be one of the most sensitive radio arrays in the pre-SKA era. Here we discuss a low-frequency SZ-selected cluster survey with MeerKAT, the MeerKAT Extended Relics, Giant Halos, and Extragalactic Radio Sources (MERGHERS) survey. The primary goal of this survey is to detect faint signatures of diffuse cluster emission, specifically radio halos and relics. SZ-selected cluster samples offer a homogeneous, mass-limited set of targets out to higher redshift than X-ray samples. MeerKAT is sensitive enough to detect diffuse radio emission at the faint levels expected in low-mass and high-redshift clusters, thereby enabling radio halo and relic formation theories to be tested with a larger statistical sample over a significantly expanded phase space. Complementary multiwavelength follow-up observations will provide a more complete picture of any clusters found to host diffuse emission, thereby enhancing the scientific return of the MERGHERS survey.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The Large Synoptic Survey Telescope (LSST) will enable revolutionary studies of galaxies, dark matter, and black holes over cosmic time. The LSST Galaxies Science Collaboration has identified a host of preparatory research tasks required to leverage fully the LSST dataset for extragalactic science beyond the study of dark energy. This Galaxies Science Roadmap provides a brief introduction to critical extragalactic science to be conducted ahead of LSST operations, and a detailed list of preparatory science tasks including the motivation, activities, and deliverables associated with each. The Galaxies Science Roadmap will serve as a guiding document for researchers interested in conducting extragalactic science in anticipation of the forthcoming LSST era.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We describe the Sloan Digital Sky Survey IV (SDSS-IV), a project encompassing three major spectroscopic programs. The Apache Point Observatory Galactic Evolution Experiment 2 (APOGEE-2) is observing hundreds of thousands of Milky Way stars at high resolution and high signal-to-noise ratio in the near-infrared. The Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey is obtaining spatially-resolved spectroscopy for thousands of nearby galaxies (median redshift of z = 0.03). The extended Baryon Oscillation Spectroscopic Survey (eBOSS) is mapping the galaxy, quasar, and neutral gas distributions between redshifts z = 0.6 and 3.5 to constrain cosmology using baryon acoustic oscillations, redshift space distortions, and the shape of the power spectrum. Within eBOSS, we are conducting two major subprograms: the SPectroscopic IDentification of eROSITA Sources (SPIDERS), investigating X-ray AGN and galaxies in X-ray clusters, and the Time Domain Spectroscopic Survey (TDSS), obtaining spectra of variable sources. All programs use the 2.5-meter Sloan Foundation Telescope at Apache Point Observatory; observations there began in Summer 2014. APOGEE-2 also operates a second near-infrared spectrograph at the 2.5-meter du Pont Telescope at Las Campanas Observatory, with observations beginning in early 2017. Observations at both facilities are scheduled to continue through 2020. In keeping with previous SDSS policy, SDSS-IV provides regularly scheduled public data releases; the first one, Data Release 13, was made available in July 2016.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Low radio frequency solar observations using the Murchison Widefield Array have recently revealed the presence of numerous weak, short-lived and narrow-band emission features, even during moderately quiet solar conditions. These non-thermal features occur at rates of many thousands per hour in the 30.72 MHz observing bandwidth, and hence, necessarily require an automated approach for their detection and characterization. Here, we employ continuous wavelet transform using a mother Ricker wavelet for feature detection from the dynamic spectrum. We establish the efficacy of this approach and present the first statistically robust characterization of the properties of these features. In particular, we examine distributions of their peak flux densities, spectral spans, temporal spans and peak frequencies. We can reliably detect features weaker than 1 SFU, making them, to the best of our knowledge, the weakest bursts reported in literature. The distribution of their peak flux densities follows a power law with an index of -2.23 in the 12-155 SFU range, implying that they can provide an energetically significant contribution to coronal and chromospheric heating. These features typically last for 1-2 seconds and possess bandwidths of about 4-5 MHz. Their occurrence rate remains fairly flat in the 140-210 MHz frequency range. At the time resolution of the data, they appear as stationary bursts, exhibiting no perceptible frequency drift. These features also appear to ride on a broadband background continuum, hinting at the likelihood of them being weak type-I bursts.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) began observations in July 2014. It pursues three core programs: APOGEE-2, MaNGA, and eBOSS. In addition, eBOSS contains two major subprograms: TDSS and SPIDERS. This paper describes the first data release from SDSS-IV, Data Release 13 (DR13), which contains new data, reanalysis of existing data sets and, like all SDSS data releases, is inclusive of previously released data. DR13 makes publicly available 1390 spatially resolved integral field unit observations of nearby galaxies from MaNGA, the first data released from this survey. It includes new observations from eBOSS, completing SEQUELS. In addition to targeting galaxies and quasars, SEQUELS also targeted variability-selected objects from TDSS and X-ray selected objects from SPIDERS. DR13 includes new reductions of the SDSS-III BOSS data, improving the spectrophotometric calibration and redshift classification. DR13 releases new reductions of the APOGEE-1 data from SDSS-III, with abundances of elements not previously included and improved stellar parameters for dwarf stars and cooler stars. For the SDSS imaging data, DR13 provides new, more robust and precise photometric calibrations. Several value-added catalogs are being released in tandem with DR13, in particular target catalogs relevant for eBOSS, TDSS, and SPIDERS, and an updated red-clump catalog for APOGEE. This paper describes the location and format of the data now publicly available, as well as providing references to the important technical papers that describe the targeting, observing, and data reduction. The SDSS website, http://www.sdss.org, provides links to the data, tutorials and examples of data access, and extensive documentation of the reduction and analysis procedures. DR13 is the first of a scheduled set that will contain new data and analyses from the planned ~6-year operations of SDSS-IV.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The combinations of particle aspect ratio and enthalpic-barrier templates that lead to translational and orientational ordering of monolayers of rectangular particles are determined using Monte Carlo simulations and density functional theory. For sufficiently high enthalpic barriers, we find that only specific combinations of particle sizes and template spacings lead to ordered arrays. The pattern multiplication factor provided by the template extends to approximately ten times the smallest dimension of the particle.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The Galactic Center black hole Sagittarius A* (Sgr A*) is a prime observing target for the Event Horizon Telescope (EHT), which can resolve the 1.3 mm emission from this source on angular scales comparable to that of the general relativistic shadow. Previous EHT observations have used visibility amplitudes to infer the morphology of the millimeter-wavelength emission. Potentially much richer source information is contained in the phases. We report on 1.3 mm phase information on Sgr A* obtained with the EHT on a total of 13 observing nights over 4 years. Closure phases, the sum of visibility phases along a closed triangle of interferometer baselines, are used because they are robust against phase corruptions introduced by instrumentation and the rapidly variable atmosphere. The median closure phase on a triangle including telescopes in California, Hawaii, and Arizona is nonzero. This result conclusively demonstrates that the millimeter emission is asymmetric on scales of a few Schwarzschild radii and can be used to break 180-degree rotational ambiguities inherent from amplitude data alone. The stability of the sign of the closure phase over most observing nights indicates persistent asymmetry in the image of Sgr A* that is not obscured by refraction due to interstellar electrons along the line of sight.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Near a black hole, differential rotation of a magnetized accretion disk is thought to produce an instability that amplifies weak magnetic fields, driving accretion and outflow. These magnetic fields would naturally give rise to the observed synchrotron emission in galaxy cores and to the formation of relativistic jets, but no observations to date have been able to resolve the expected horizon-scale magnetic-field structure. We report interferometric observations at 1.3-millimeter wavelength that spatially resolve the linearly polarized emission from the Galactic Center supermassive black hole, Sagittarius A*. We have found evidence for partially ordered fields near the event horizon, on scales of ~6 Schwarzschild radii, and we have detected and localized the intra-hour variability associated with these fields.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The Prime Focus Spectrograph (PFS) is an optical/near-infrared multifiber spectrograph with 2394 science fibers distributed across a 1.3-deg diameter field of view at the Subaru 8.2-m telescope. The wide wavelength coverage from 0.38 \u03bcm to 1.26 \u03bcm, with a resolving power of 3000, simultaneously strengthens its ability to target three main survey programs: cosmology, galactic archaeology and galaxy/AGN evolution. A medium resolution mode with a resolving power of 5000 for 0.71 \u03bcm to 0.89 \u03bcm will also be available by simply exchanging dispersers. We highlight some of the technological aspects of the design. To transform the telescope focal ratio, a broad-band coated microlens is glued to each fiber tip. A higher transmission fiber is selected for the longest part of the cable system, optimizing overall throughput; a fiber with low focal ratio degradation is selected for the fiber-positioner and fiber-slit components, minimizing the effects of fiber movements and fiber bending. Fiber positioning will be performed by a positioner consisting of two stages of piezo-electric rotary motors. The positions of these motors are measured by taking an image of artificially back-illuminated fibers with the metrology camera located in the Cassegrain container; the fibers are placed in the proper location by iteratively measuring and then adjusting the positions of the motors. Target light reaches one of the four identical fast-Schmidt spectrograph modules, each with three arms. The PFS project has passed several project-wide design reviews and is now in the construction phase.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We report on 230 GHz (1.3 mm) VLBI observations of M87 with the Event Horizon Telescope using antennas on Mauna Kea in Hawaii, Mt. Graham in Arizona and Cedar Flat in California. For the first time, we have acquired 230 GHz VLBI interferometric phase information on M87 through measurement of closure phase on the triangle of long baselines. Most of the measured closure phases are consistent with 0$^{\\circ}$ as expected by physically-motivated models for 230 GHz structure such as jet models and accretion disk models. The brightness temperature of the event-horizon-scale structure is $\\sim 1 \\times 10^{10}$ K derived from the compact flux density of $\\sim 1$ Jy and the angular size of $\\sim 40 $ $\\rm \u03bc$as $\\sim$ 5.5 $R_{\\rm s}$, which is broadly consistent with the peak brightness of the radio cores at 1-86 GHz located within $\\sim 10^2$ $R_{\\rm s}$. Our observations occurred in the middle of an enhancement in very-high-energy (VHE) $\\rm \u03b3$-ray flux, presumably originating in the vicinity of the central black hole. Our measurements, combined with results of multi-wavelength observations, favor a scenario in which the VHE region has an extended size of $\\sim$20-60 $R_{\\rm s}$.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "An FPGA-based digital-receiver has been developed for a low-frequency imaging radio interferometer, the Murchison Widefield Array (MWA). The MWA, located at the Murchison Radio-astronomy Observatory (MRO) in Western Australia, consists of 128 dual-polarized aperture-array elements (tiles) operating between 80 and 300\\,MHz, with a total processed bandwidth of 30.72 MHz for each polarization. Radio-frequency signals from the tiles are amplified and band limited using analog signal conditioning units; sampled and channelized by digital-receivers. The signals from eight tiles are processed by a single digital-receiver, thus requiring 16 digital-receivers for the MWA. The main function of the digital-receivers is to digitize the broad-band signals from each tile, channelize them to form the sky-band, and transport it through optical fibers to a centrally located correlator for further processing. The digital-receiver firmware also implements functions to measure the signal power, perform power equalization across the band, detect interference-like events, and invoke diagnostic modes. The digital-receiver is controlled by high-level programs running on a single-board-computer. This paper presents the digital-receiver design, implementation, current status, and plans for future enhancements.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "New high-resolution spectroscopy and $BVR$ photometry, together with literature data, on the Gould's Belt close binary systems GG Lup and $\u03bc^1$ Sco are presented and analysed.\n  In the case of GG Lup, light and radial velocity curve fittings confirm a near-Main-Sequence picture of a pair of close stars. Absolute parameters are found, to within a few percent, thus: $M_1$ = 4.16$\\pm$0.12, $M_2$ = 2.64$\\pm$0.12, $R_{1}$ = 2.42$\\pm$0.05, $R_2$ = 1.79$\\pm$0.04, ($\\odot$); $T_{1}$ $\\sim$13000, $T_2$ $\\sim$10600 (K); photometric distance $\\sim$ 160 (pc). The high eccentricity and relatively short period (105 y) of apsidal revolution may be related to an apparent `slow B-type pulsator' (SPB) oscillation. Disturbances of the outer envelope of at least one of the components then compromise comparisons to standard evolutionary models, at least regarding the age of the system. A rate of apsidal advance is derived, which allows a check on the mean internal structure constant $\\bar{k_2} = 0.0058 \\pm 0.0004$. This is in agreement with values recently derived for young stars of solar composition and mass $\\sim$3${\\odot}$.\n  For $\u03bc^1$ Sco, we agree with previous authors that the secondary component is considerably oversized for its mass, implying binary (interactive) stellar evolution, probably of the `Case A' type. The primary appears relatively little affected by this evolution, however. Its parameters show consistency with a star of its derived mass at age about 13 My, consistent with the star's membership of the Sco-Cen OB2 Association. The absolute parameters are as follows: $M_1$ = 8.3$\\pm$1.0, $M_2$ = 4.6$\\pm$1.0, $R_{1}$ = 3.9$\\pm$0.3, $R_2$ = 4.6$\\pm$0.4, ($\\odot$); $T_{1}$ $\\sim$24000, $T_2$ $\\sim$17000 (K); photometric distance $\\sim$ 135 (pc).\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Adding VLBI capability to the SKA arrays will greatly broaden the science of the SKA, and is feasible within the current specifications. SKA-VLBI can be initially implemented by providing phased-array outputs for SKA1-MID and SKA1-SUR and using these extremely sensitive stations with other radio telescopes, and in SKA2 by realising a distributed configuration providing baselines up to thousands of km, merging it with existing VLBI networks. The motivation for and the possible realization of SKA-VLBI is described in this paper.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Recent observations with the Murchison Widefield Array at 185~MHz have serendipitously unveiled a heretofore unknown giant and relatively nearby ($z = 0.0178$) radio galaxy associated with NGC\\,1534. The diffuse emission presented here is the first indication that NGC\\,1534 is one of a rare class of objects (along with NGC\\,5128 and NGC\\,612) in which a galaxy with a prominent dust lane hosts radio emission on scales of $\\sim$700\\,kpc. We present details of the radio emission along with a detailed comparison with other radio galaxies with disks. NGC1534 is the lowest surface brightness radio galaxy known with an estimated scaled 1.4-GHz surface brightness of just 0.2\\,mJy\\,arcmin$^{-2}$. The radio lobes have one of the steepest spectral indices yet observed: $\u03b1=-2.1\\pm0.1$, and the core to lobe luminosity ratio is $<0.1$\\%. We estimate the space density of this low brightness (dying) phase of radio galaxy evolution as $7\\times10^{-7}$\\,Mpc$^{-3}$ and argue that normal AGN cannot spend more than 6\\% of their lifetime in this phase if they all go through the same cycle.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The joint JAXA/NASA ASTRO-H mission is the sixth in a series of highly successful X-ray missions developed by the Institute of Space and Astronautical Science (ISAS), with a planned launch in 2015. The ASTRO-H mission is equipped with a suite of sensitive instruments with the highest energy resolution ever achieved at E > 3 keV and a wide energy range spanning four decades in energy from soft X-rays to gamma-rays. The simultaneous broad band pass, coupled with the high spectral resolution of Delta E < 7 eV of the micro-calorimeter, will enable a wide variety of important science themes to be pursued. ASTRO-H is expected to provide breakthrough results in scientific areas as diverse as the large-scale structure of the Universe and its evolution, the behavior of matter in the gravitational strong field regime, the physical conditions in sites of cosmic-ray acceleration, and the distribution of dark matter in galaxy clusters at different redshifts.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "This article summarizes a workshop held on March, 2014, on the potential of the James Webb Space Telescope (JWST) to revolutionize our knowledge of the physical properties of exoplanets through transit observations. JWST's unique combination of high sensitivity and broad wavelength coverage will enable the accurate measurement of transits with high signal-to-noise. Most importantly, JWST spectroscopy will investigate planetary atmospheres to determine atomic and molecular compositions, to probe vertical and horizontal structure, and to follow dynamical evolution, i.e. exoplanet weather. JWST will sample a diverse population of planets of varying masses and densities in a wide variety of environments characterized by a range of host star masses and metallicities, orbital semi-major axes and eccentricities. A broad program of exoplanet science could use a substantial fraction of the overall JWST mission.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We compute the free energy minimizing structures of particle monolayers in the presence of enthalpic barriers of a finite height \\b{eta}Vext using classical density functional theory and Monte Carlo simulations. We show that a periodic square template with dimensions up to at least ten times the particle diameter disrupts the formation of the entropically favored hexagonally close-packed 2D lattice in favor of a square lattice. The results illustrate how graphoepitaxy can successfully order nanoparticulate films into desired patterns many times smaller than those of the prepatterned template.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We present the results of an approximately 6,100 square degree 104--196MHz radio sky survey performed with the Murchison Widefield Array during instrument commissioning between 2012 September and 2012 December: the Murchison Widefield Array Commissioning Survey (MWACS). The data were taken as meridian drift scans with two different 32-antenna sub-arrays that were available during the commissioning period. The survey covers approximately 20.5 h < Right Ascension (RA) < 8.5 h, -58 deg < Declination (Dec) < -14 deg over three frequency bands centred on 119, 150 and 180 MHz, with image resolutions of 6--3 arcmin. The catalogue has 3-arcmin angular resolution and a typical noise level of 40 mJy/beam, with reduced sensitivity near the field boundaries and bright sources. We describe the data reduction strategy, based upon mosaiced snapshots, flux density calibration and source-finding method. We present a catalogue of flux density and spectral index measurements for 14,110 sources, extracted from the mosaic, 1,247 of which are sub-components of complexes of sources.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The Prime Focus Spectrograph (PFS) is an optical/near-infrared multi-fiber spectrograph with 2394 science fibers, which are distributed in 1.3 degree diameter field of view at Subaru 8.2-meter telescope. The simultaneous wide wavelength coverage from 0.38 um to 1.26 um, with the resolving power of 3000, strengthens its ability to target three main survey programs: cosmology, Galactic archaeology, and galaxy/AGN evolution. A medium resolution mode with resolving power of 5000 for 0.71 um to 0.89 um also will be available by simply exchanging dispersers. PFS takes the role for the spectroscopic part of the Subaru Measurement of Images and Redshifts project, while Hyper Suprime-Cam works on the imaging part. To transform the telescope plus WFC focal ratio, a 3-mm thick broad-band coated glass-molded microlens is glued to each fiber tip. A higher transmission fiber is selected for the longest part of cable system, while one with a better FRD performance is selected for the fiber-positioner and fiber-slit components, given the more frequent fiber movements and tightly curved structure. Each Fiber positioner consists of two stages of piezo-electric rotary motors. Its engineering model has been produced and tested. Fiber positioning will be performed iteratively by taking an image of artificially back-illuminated fibers with the Metrology camera located in the Cassegrain container. The camera is carefully designed so that fiber position measurements are unaffected by small amounts of high special-frequency inaccuracies in WFC lens surface shapes. Target light carried through the fiber system reaches one of four identical fast-Schmidt spectrograph modules, each with three arms. Prototype VPH gratings have been optically tested. CCD production is complete, with standard fully-depleted CCDs for red arms and more-challenging thinner fully-depleted CCDs with blue-optimized coating for blue arms.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We use deep panchromatic datasets in the GOODS-N field, from GALEX to the deepest Herschel far-infrared and VLA radio continuum imaging, to explore, using mass-complete samples, the evolution of the star formation activity and dust attenuation of star-forming galaxies to z~4. Our main results can be summarized as follows: i) the slope of the SFR-M correlation is consistent with being constant, and equal to ~0.8 at least up to z~1.5, while its normalization keeps increasing with redshift; ii) for the first time here we are able to explore the FIR-radio correlation for a mass-selected sample of star-forming galaxies: the correlation does not evolve up to z~4; iii) we confirm that galaxy stellar mass is a robust proxy for UV dust attenuation in star-forming galaxies, with more massive galaxies being more dust attenuated, strikingly we find that this attenuation relation evolves very weakly with redshift, the amount of dust attenuation increasing by less than 0.3 magnitudes over the redshift range [0.5-4] for a fixed stellar mass, as opposed to a tenfold increase of star formation rate; iv) the correlation between dust attenuation and the UV spectral slope evolves in redshift, with the median UV spectral slope of star-forming galaxies becoming bluer with redshift. By z~3, typical UV slopes are inconsistent, given the measured dust attenuation, with the predictions of commonly used empirical laws. Finally, building on existing results, we show that gas reddening is marginally larger (by a factor of around 1.3) than stellar reddening at all redshifts probed, and also that the amount of dust attenuation at a fixed ISM metallicity increases with redshift. We speculate that our results support evolving ISM conditions of typical star-forming galaxies such that at z~1.5 Main Sequence galaxies have ISM conditions getting closer to those of local starbursts.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The detection of the Epoch of Reionization (EoR) in the redshifted 21-cm line is a challenging task. Here we formulate the detection of the EoR signal using the drift scan strategy. This method potentially has better instrumental stability as compared to the case where a single patch of sky is tracked. We demonstrate that the correlation time between measured visibilities could extend up to 1-2 hr for an interferometer array such as the Murchison Widefield Array (MWA), which has a wide primary beam. We estimate the EoR power based on cross-correlation of visibilities across time and show that the drift scan strategy is capable of the detection of the EoR signal with comparable/better signal-to-noise as compared to the tracking case. We also estimate the visibility correlation for a set of bright point sources and argue that the statistical inhomogeneity of bright point sources might allow their separation from the EoR signal.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We present the technology and control methods developed for the pointing system of the SPIDER experiment. SPIDER is a balloon-borne polarimeter designed to detect the imprint of primordial gravitational waves in the polarization of the Cosmic Microwave Background radiation. We describe the two main components of the telescope's azimuth drive: the reaction wheel and the motorized pivot. A 13 kHz PI control loop runs on a digital signal processor, with feedback from fibre optic rate gyroscopes. This system can control azimuthal speed with < 0.02 deg/s RMS error. To control elevation, SPIDER uses stepper-motor-driven linear actuators to rotate the cryostat, which houses the optical instruments, relative to the outer frame. With the velocity in each axis controlled in this way, higher-level control loops on the onboard flight computers can implement the pointing and scanning observation modes required for the experiment. We have accomplished the non-trivial task of scanning a 5000 lb payload sinusoidally in azimuth at a peak acceleration of 0.8 deg/s$^2$, and a peak speed of 6 deg/s. We can do so while reliably achieving sub-arcminute pointing control accuracy.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The quest for Earth-like planets represents a major focus of current exoplanet research. While planets that are Earth-sized and smaller have been detected, these planets reside in orbits that are too close to their host star to allow liquid water on their surface. We present the detection of Kepler-186f, a 1.11+\\-0.14 Earth radius planet that is the outermost of five planets - all roughly Earth-sized - that transit a 0.47+\\-0.05 Rsun star. The intensity and spectrum of the star's radiation places Kepler-186f in the stellar habitable zone, implying that if Kepler-186f has an Earth-like atmosphere and H2O at its surface, then some of this H2O is likely to be in liquid form.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We present an asymptotic analysis of a mesoscale energy for bilayer membranes that has been introduced and analyzed in two space dimensions by the second and third author (Arch. Ration. Mech. Anal. 193, 2009). The energy is both non-local and non-convex. It combines a surface area and a Monge-Kantorovich-distance term, leading to a competition between preferences for maximally concentrated and maximally dispersed configurations. Here we extend key results of our previous analysis to the three dimensional case. First we prove a general lower estimate and formally identify a curvature energy in the zero-thickness limit. Secondly we construct a recovery sequence and prove a matching upper-bound estimate.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We report on the masses, sizes, and orbits of the planets orbiting 22 Kepler stars. There are 49 planet candidates around these stars, including 42 detected through transits and 7 revealed by precise Doppler measurements of the host stars. Based on an analysis of the Kepler brightness measurements, along with high-resolution imaging and spectroscopy, Doppler spectroscopy, and (for 11 stars) asteroseismology, we establish low false-positive probabilities for all of the transiting planets (41 of 42 have a false-positive probability under 1%), and we constrain their sizes and masses. Most of the transiting planets are smaller than 3X the size of Earth. For 16 planets, the Doppler signal was securely detected, providing a direct measurement of the planet's mass. For the other 26 planets we provide either marginal mass measurements or upper limits to their masses and densities; in many cases we can rule out a rocky composition. We identify 6 planets with densities above 5 g/cc, suggesting a mostly rocky interior for them. Indeed, the only planets that are compatible with a purely rocky composition are smaller than ~2 R_earth. Larger planets evidently contain a larger fraction of low-density material (H, He, and H2O).\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "In this paper we explore for the first time the relative magnitudes of three fundamental sources of uncertainty, namely, foreground contamination, thermal noise and sample variance in detecting the HI power spectrum from the Epoch of Reionization (EoR). We derive limits on the sensitivity of a Fourier synthesis telescope to detect EoR based on its array configuration and a statistical representation of images made by the instrument. We use the Murchison Widefield Array (MWA) configuration for our studies. Using a unified framework for estimating signal and noise components in the HI power spectrum, we derive an expression for and estimate the contamination from extragalactic point-like sources in three-dimensional k-space. Sensitivity for EoR HI power spectrum detection is estimated for different observing modes with MWA. With 1000 hours of observing on a single field using the 128-tile MWA, EoR detection is feasible (S/N > 1 for $k\\lesssim 0.8$ Mpc$^{-1}$). Bandpass shaping and refinements to the EoR window are found to be effective in containing foreground contamination, which makes the instrument tolerant to imaging errors. We find that for a given observing time, observing many independent fields of view does not offer an advantage over a single field observation when thermal noise dominates over other uncertainties in the derived power spectrum.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "This paper summarizes the physics potential of the CLIC high-energy e+e- linear collider. It provides input to the Snowmass 2013 process for the energy-frontier working groups on The Higgs Boson (HE1), Precision Study of Electroweak Interactions (HE2), Fully Understanding the Top Quark (HE3), as well as The Path Beyond the Standard Model -- New Particles, Forces, and Dimensions (HE4). It is accompanied by a paper describing the CLIC accelerator study, submitted to the Frontier Capabilities group of the Snowmass process.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The Qweak experiment has measured the parity-violating asymmetry in polarized e-p elastic scattering at Q^2 = 0.025(GeV/c)^2, employing 145 microamps of 89% longitudinally polarized electrons on a 34.4cm long liquid hydrogen target at Jefferson Lab. The results of the experiment's commissioning run are reported here, constituting approximately 4% of the data collected in the experiment. From these initial results the measured asymmetry is Aep = -279 +- 35 (statistics) +- 31 (systematics) ppb, which is the smallest and most precise asymmetry ever measured in polarized e-p scattering. The small Q^2 of this experiment has made possible the first determination of the weak charge of the proton, QpW, by incorporating earlier parity-violating electron scattering (PVES) data at higher Q^2 to constrain hadronic corrections. The value of QpW obtained in this way is QpW(PVES) = 0.064 +- 0.012, in good agreement with the Standard Model prediction of QpW(SM) = 0.0710 +- 0.0007. When this result is further combined with the Cs atomic parity violation (APV) measurement, significant constraints on the weak charges of the up and down quarks can also be extracted. That PVES+APV analysis reveals the neutron's weak charge to be QnW(PVES+APV) = -0.975 +- 0.010.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Since the discovery of the first exoplanet we have known that other planetary systems can look quite unlike our own. However, until recently we have only been able to probe the upper range of the planet size distribution. The high precision of the Kepler space telescope has allowed us to detect planets that are the size of Earth and somewhat smaller, but no previous planets have been found that are smaller than those we see in our own Solar System. Here we report the discovery of a planet significantly smaller than Mercury. This tiny planet is the innermost of three planets that orbit the Sun-like host star, which we have designated Kepler-37. Owing to its extremely small size, similar to that of Earth's Moon, and highly irradiated surface, Kepler-37b is probably a rocky planet with no atmosphere or water, similar to Mercury.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We report results from 5-day VLBI observations of the well-known quasar 3C 279 at 1.3 mm (230 GHz) in 2011. The measured nonzero closure phases on triangles including stations in Arizona, California and Hawaii indicate that the source structure is spatially resolved. We find an unusual inner jet direction at scales of $\\sim$1 parsec extending along the northwest-southeast direction (PA = $127^{\\circ}\\pm3^{\\circ}$), as opposed to other (previously) reported measurements on scales of a few parsecs showing inner jet direction extending to the southwest. The 1.3 mm structure corresponds closely with that observed in the central region of quasi-simultaneous super-resolution VLBA images at 7 mm. The closure phase changed significantly on the last day when compared with the rest of observations, indicating that the inner jet structure may be variable on daily timescales. The observed new direction of the inner jet shows inconsistency with the prediction of a class of jet precession models. Our observations indicate a brightness temperature of $\\sim 8\\times10^{10}$ K in the 1.3 mm core, much lower than that at centimeter wavelengths. Observations with better uv coverage and sensitivity in the coming years will allow the discrimination between different structure models and will provide direct images of the inner regions of the jet with 20--30 $\u03bc$as (5--7 light months) resolution.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We report spectroscopic and photometric observations of the Type IIb SN 2011dh obtained between 4 and 34 days after the estimated date of explosion (May 31.5 UT). The data cover a wide wavelength range from 2,000 Angstroms in the UV to 2.4 microns in the NIR. Optical spectra provide line profiles and velocity measurements of HI, HeI, CaII and FeII that trace the composition and kinematics of the SN. NIR spectra show that helium is present in the atmosphere as early as 11 days after the explosion. A UV spectrum obtained with the STIS reveals that the UV flux for SN 2011dh is low compared to other SN IIb. The HI and HeI velocities in SN 2011dh are separated by about 4,000 km/s at all phases. We estimate that the H-shell of SN 2011dh is about 8 times less massive than the shell of SN 1993J and about 3 times more massive than the shell of SN 2008ax. Light curves (LC) for twelve passbands are presented. The maximum bolometric luminosity of $1.8 \\pm 0.2 \\times 10^{42}$ erg s$^{-1}$ occurred about 22 days after the explosion. NIR emission provides more than 30% of the total bolometric flux at the beginning of our observations and increases to nearly 50% of the total by day 34. The UV produces 16% of the total flux on day 4, 5% on day 9 and 1% on day 34. We compare the bolometric light curves of SN 2011dh, SN 2008ax and SN 1993J. The LC are very different for the first twelve days after the explosions but all three SN IIb display similar peak luminosities, times of peak, decline rates and colors after maximum. This suggests that the progenitors of these SN IIb may have had similar compositions and masses but they exploded inside hydrogen shells that that have a wide range of masses. The detailed observations presented here will help evaluate theoretical models for this supernova and lead to a better understanding of SN IIb.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We report the discovery of 87 new T dwarfs uncovered with the Wide-field Infrared Survey Explorer (WISE) and three brown dwarfs with extremely red near-infrared colors that exhibit characteristics of both L and T dwarfs. Two of the new T dwarfs are likely binaries with L7+/-1 primaries and mid-type T secondaries. In addition, our follow-up program has confirmed 10 previously identified T dwarfs and four photometrically-selected L and T dwarf candidates in the literature. This sample, along with the previous WISE discoveries, triples the number of known brown dwarfs with spectral types later than T5. Using the WISE All-Sky Source Catalog we present updated color-color and color-type diagrams for all the WISE-discovered T and Y dwarfs. Near-infrared spectra of the new discoveries are presented, along with spectral classifications. To accommodate later T dwarfs we have modified the integrated flux method of determining spectral indices to instead use the median flux. Furthermore, a newly defined J-narrow index differentiates the early-type Y dwarfs from late-type T dwarfs based on the J-band continuum slope. The K/J indices for this expanded sample show that 32% of late-type T dwarfs have suppressed K-band flux and are blue relative to the spectral standards, while only 11% are redder than the standards. Comparison of the Y/J and K/J index to models suggests diverse atmospheric conditions and supports the possible re-emergence of clouds after the L/T transition. We also discuss peculiar brown dwarfs and candidates that were found not to be substellar, including two Young Stellar Objects and two Active Galactic Nuclei. The coolest WISE-discovered brown dwarfs are the closest of their type and will remain the only sample of their kind for many years to come.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Significant new opportunities for astrophysics and cosmology have been identified at low radio frequencies. The Murchison Widefield Array is the first telescope in the Southern Hemisphere designed specifically to explore the low-frequency astronomical sky between 80 and 300 MHz with arcminute angular resolution and high survey efficiency. The telescope will enable new advances along four key science themes, including searching for redshifted 21 cm emission from the epoch of reionisation in the early Universe; Galactic and extragalactic all-sky southern hemisphere surveys; time-domain astrophysics; and solar, heliospheric, and ionospheric science and space weather. The Murchison Widefield Array is located in Western Australia at the site of the planned Square Kilometre Array (SKA) low-band telescope and is the only low-frequency SKA precursor facility. In this paper, we review the performance properties of the Murchison Widefield Array and describe its primary scientific objectives.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We have compiled a large sample of 151 high redshift (z=0.5-4) galaxies selected at 24 microns (S24>100 uJy) in the GOODS-N and ECDFS fields for which we have deep Spitzer IRS spectroscopy, allowing us to decompose the mid-infrared spectrum into contributions from star formation and activity in the galactic nuclei. In addition, we have a wealth of photometric data from Spitzer IRAC/MIPS and Herschel PACS/SPIRE. We explore how effective different infrared color combinations are at separating our mid-IR spectroscopically determined active galactic nuclei from our star forming galaxies. We look in depth at existing IRAC color diagnostics, and we explore new color-color diagnostics combining mid-IR, far-IR, and near-IR photometry, since these combinations provide the most detail about the shape of a source's IR spectrum. An added benefit of using a color that combines far-IR and mid-IR photometry is that it is indicative of the power source driving the IR luminosity. For our data set, the optimal color selections are S250/S24 vs. S8.0/S3.6 and S100/S24 vs. S8.0/S3.6; both diagnostics have ~10% contamination rate in the regions occupied primarily by star forming galaxies and active galactic nuclei, respectively. Based on the low contamination rate, these two new IR color-color diagnostics are ideal for estimating both the mid-IR power source of a galaxy when spectroscopy is unavailable and the dominant power source contributing to the IR luminosity. In the absence of far-IR data, we present color diagnostics using the WISE mid-IR bands which can efficiently select out high z (z~2) star forming galaxies.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The joint JAXA/NASA ASTRO-H mission is the sixth in a series of highly successful X-ray missions initiated by the Institute of Space and Astronautical Science (ISAS). ASTRO-H will investigate the physics of the high-energy universe via a suite of four instruments, covering a very wide energy range, from 0.3 keV to 600 keV. These instruments include a high-resolution, high-throughput spectrometer sensitive over 0.3-2 keV with high spectral resolution of Delta E < 7 eV, enabled by a micro-calorimeter array located in the focal plane of thin-foil X-ray optics; hard X-ray imaging spectrometers covering 5-80 keV, located in the focal plane of multilayer-coated, focusing hard X-ray mirrors; a wide-field imaging spectrometer sensitive over 0.4-12 keV, with an X-ray CCD camera in the focal plane of a soft X-ray telescope; and a non-focusing Compton-camera type soft gamma-ray detector, sensitive in the 40-600 keV band. The simultaneous broad bandpass, coupled with high spectral resolution, will enable the pursuit of a wide variety of important science themes.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We explore the effects of active galactic nuclei (AGN) and star formation activity on the infrared (0.3-1000 microns) spectral energy distributions of luminous infrared galaxies from z = 0.5 to 4.0. We have compiled a large sample of 151 galaxies selected at 24 microns (S24 > 100 uJy) in the GOODS-N and ECDFS fields for which we have deep Spitzer IRS spectroscopy, allowing us to decompose the mid-IR spectrum into contributions from star formation and AGN activity. A significant portion (~25%) of our sample is dominated by an AGN in the mid-IR. Based on the mid-IR classification, we divide our full sample into four sub-samples: z~1 star-forming (SF) sources; z~2 SF sources; AGN with clear 9.7 micron silicate absorption; and AGN with featureless mid-IR spectra. From our large spectroscopic sample and wealth of multi-wavelength data, including deep Herschel imaging at 100, 160, 250, 350, and 500 microns, we use 95 galaxies with complete spectral coverage to create a composite spectral energy distribution (SED) for each sub-sample. We then fit a two-temperature component modified blackbody to the SEDs. We find that the IR SEDs have similar cold dust temperatures, regardless of the mid-IR power source, but display a marked difference in the warmer dust temperatures. We calculate the average effective temperature of the dust in each sub-sample and find a significant (~20 K) difference between the SF and AGN systems. We compare our composite SEDs to local templates and find that local templates do not accurately reproduce the mid-IR features and dust temperatures of our high redshift systems. High redshift IR luminous galaxies contain significantly more cool dust than their local counterparts. We find that a full suite of photometry spanning the IR peak is necessary to accurately account for the dominant dust temperature components in high redshift IR luminous galaxies.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We introduce the Fast Holographic Deconvolution method for analyzing interferometric radio data. Our new method is an extension of A-projection/software-holography/forward modeling analysis techniques and shares their precision deconvolution and widefield polarimetry, while being significantly faster than current implementations that use full direction-dependent antenna gains. Using data from the MWA 32 antenna prototype, we demonstrate the effectiveness and precision of our new algorithm. Fast Holographic Deconvolution may be particularly important for upcoming 21 cm cosmology observations of the Epoch of Reionization and Dark Energy where foreground subtraction is intimately related to the precision of the data reduction.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "The Arcminute Microkelvin Imager (AMI) is a telescope specifically designed for high sensitivity measurements of low-surface-brightness features at cm-wavelength and has unique, important capabilities. It consists of two interferometer arrays operating over 13.5-18 GHz that image structures on scales of 0.5-10 arcmin with very low systematics. The Small Array (AMI-SA; ten 3.7-m antennas) couples very well to Sunyaev-Zel'dovich features from galaxy clusters and to many Galactic features. The Large Array (AMI-LA; eight 13-m antennas) has a collecting area ten times that of the AMI-SA and longer baselines, crucially allowing the removal of the effects of confusing radio point sources from regions of low surface-brightness, extended emission. Moreover AMI provides fast, deep object surveying and allows monitoring of large numbers of objects. In this White Paper we review the new science - both Galactic and extragalactic - already achieved with AMI and outline the prospects for much more.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "We present a new and innovative near-infrared multi-band ultraprecise spectroimager (NIMBUS) for SOFIA. This design is capable of characterizing a large sample of extrasolar planet atmospheres by measuring elemental and molecular abundances during primary transit and occultation. This wide-field spectroimager would also provide new insights into Trans-Neptunian Objects (TNO), Solar System occultations, brown dwarf atmospheres, carbon chemistry in globular clusters, chemical gradients in nearby galaxies, and galaxy photometric redshifts. NIMBUS would be the premier ultraprecise spectroimager by taking advantage of the SOFIA observatory and state of the art infrared technologies.\n  This optical design splits the beam into eight separate spectral bandpasses, centered around key molecular bands from 1 to 4 microns. Each spectral channel has a wide field of view for simultaneous observations of a reference star that can decorrelate time-variable atmospheric and optical assembly effects, allowing the instrument to achieve ultraprecise calibration for imaging and photometry for a wide variety of astrophysical sources. NIMBUS produces the same data products as a low-resolution integral field spectrograph over a large spectral bandpass, but this design obviates many of the problems that preclude high-precision measurements with traditional slit and integral field spectrographs. This instrument concept is currently not funded for development.\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "This is the first in a series of papers in which we measure accurate weak-lensing masses for 51 of the most X-ray luminous galaxy clusters known at redshifts 0.15<z<0.7, in order to calibrate X-ray and other mass proxies for cosmological cluster experiments. The primary aim is to improve the absolute mass calibration of cluster observables, currently the dominant systematic uncertainty for cluster count experiments. Key elements of this work are the rigorous quantification of systematic uncertainties, high-quality data reduction and photometric calibration, and the \"blind\" nature of the analysis to avoid confirmation bias. Our target clusters are drawn from RASS X-ray catalogs, and provide a versatile calibration sample for many aspects of cluster cosmology. We have acquired wide-field, high-quality imaging using the Subaru and CFHT telescopes for all 51 clusters, in at least three bands per cluster. For a subset of 27 clusters, we have data in at least five bands, allowing accurate photo-z estimates of lensed galaxies. In this paper, we describe the cluster sample and observations, and detail the processing of the SuprimeCam data to yield high-quality images suitable for robust weak-lensing shape measurements and precision photometry. For each cluster, we present wide-field color optical images and maps of the weak-lensing mass distribution, the optical light distribution, and the X-ray emission, providing insights into the large-scale structure in which the clusters are embedded. We measure the offsets between X-ray centroids and Brightest Cluster Galaxies in the clusters, finding these to be small in general, with a median of 20kpc. For offsets <100kpc, weak-lensing mass measurements centered on the BCGs agree well with values determined relative to the X-ray centroids; miscentering is therefore not a significant source of systematic uncertainty for our mass measurements. [abridged]\n        \u25b3 Less", "author": "Roger Mark"}, {"abstract": "Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and therefore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects including contact and can be seamlessly incorporated into inference, control and co-design systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of control tasks for soft robots, including problems with nearly 3,000 decision variables.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "We present the first algorithm for designing volumetric Michell Trusses. Our method uses a parametrization approach to generate trusses made of structural elements aligned with the primary direction of an object's stress field. Such trusses exhibit high strength-to-weight ratios. We demonstrate the structural robustness of our designs via a posteriori physical simulation. We believe our algorithm serves as an important complement to existing structural optimization tools and as a novel standalone design tool itself.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "In this paper, we study the associations between human faces and voices. Audiovisual integration, specifically the integration of facial and vocal information is a well-researched area in neuroscience. It is shown that the overlapping information between the two modalities plays a significant role in perceptual tasks such as speaker identification. Through an online study on a new dataset we created, we confirm previous findings that people can associate unseen faces with corresponding voices and vice versa with greater than chance accuracy. We computationally model the overlapping information between faces and voices and show that the learned cross-modal representation contains enough information to identify matching faces and voices with performance similar to that of humans. Our representation exhibits correlations to certain demographic attributes and features obtained from either visual or aural modality alone. We release our dataset of audiovisual recordings and demographic annotations of people reading out short text used in our studies.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "Video motion magnification techniques allow us to see small motions previously invisible to the naked eyes, such as those of vibrating airplane wings, or swaying buildings under the influence of the wind. Because the motion is small, the magnification results are prone to noise or excessive blurring. The state of the art relies on hand-designed filters to extract representations that may not be optimal. In this paper, we seek to learn the filters directly from examples using deep convolutional neural networks. To make training tractable, we carefully design a synthetic dataset that captures small motion well, and use two-frame input for training. We show that the learned filters achieve high-quality results on real videos, with less ringing artifacts and better noise characteristics than previous methods. While our model is not trained with temporal filters, we found that the temporal filters can be used with our extracted representations up to a moderate magnification, enabling a frequency-based motion selection. Finally, we analyze the learned filters and show that they behave similarly to the derivative filters used in previous works. Our code, trained model, and datasets will be available online.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "In this paper we present a novel two-scale framework to optimize the structure and the material distribution of an object given its functional specifications. Our approach utilizes multi-material microstructures as low-level building blocks of the object. We start by precomputing the material property gamut -- the set of bulk material properties that can be achieved with all material microstructures of a given size. We represent the boundary of this material property gamut using a level set field. Next, we propose an efficient and general topology optimization algorithm that simultaneously computes an optimal object topology and spatially-varying material properties constrained by the precomputed gamut. Finally, we map the optimal spatially-varying material properties onto the microstructures with the corresponding properties in order to generate a high-resolution printable structure. We demonstrate the efficacy of our framework by designing, optimizing, and fabricating objects in different material property spaces on the level of a trillion voxels, i.e several orders of magnitude higher than what can be achieved with current systems.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "Single-image-based view generation (SIVG) is important for producing 3D stereoscopic content. Here, handling different spatial resolutions as input and optimizing both reconstruction accuracy and processing speed is desirable. Latest approaches are based on convolutional neural network (CNN), and they generate promising results. However, their use of fully connected layers as well as pre-trained VGG forces a compromise between reconstruction accuracy and processing speed. In addition, this approach is limited to the use of a specific spatial resolution. To remedy these problems, we propose exploiting fully convolutional networks (FCN) for SIVG. We present two FCN architectures for SIVG. The first one is based on combination of an FCN and a view-rendering network called DeepView$_{ren}$. The second one consists of decoupled networks for luminance and chrominance signals, denoted by DeepView$_{dec}$. To train our solutions we present a large dataset of 2M stereoscopic images. Results show that both of our architectures improve accuracy and speed over the state of the art. DeepView$_{ren}$ generates competitive accuracy to the state of the art, however, with the fastest processing speed of all. That is x5 times faster speed and x24 times lower memory consumption compared to the state of the art. DeepView$_{dec}$ has much higher accuracy, but with x2.5 times faster speed and x12 times lower memory consumption. We evaluated our approach with both objective and subjective studies.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "Shot boundary detection (SBD) is an important pre-processing step for video manipulation. Here, each segment of frames is classified as either sharp, gradual or no transition. Current SBD techniques analyze hand-crafted features and attempt to optimize both detection accuracy and processing speed. However, the heavy computations of optical flow prevents this. To achieve this aim, we present an SBD technique based on spatio-temporal Convolutional Neural Networks (CNN). Since current datasets are not large enough to train an accurate SBD CNN, we present a new dataset containing more than 3.5 million frames of sharp and gradual transitions. The transitions are generated synthetically using image compositing models. Our dataset contain additional 70,000 frames of important hard-negative no transitions. We perform the largest evaluation to date for one SBD algorithm, on real and synthetic data, containing more than 4.85 million frames. In comparison to the state of the art, we outperform dissolve gradual detection, generate competitive performance for sharp detections and produce significant improvement in wipes. In addition, we are up to 11 times faster than the state of the art.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.\n        \u25b3 Less", "author": "Wojciech Matusik"}, {"abstract": "We present the computational wiretap channel: Alice has some data x and wants to share some computation h(x) with Bob. To do this, she sends f(x), where f is some sufficient statistic for h. An eavesdropper, Eve, is interested in computing another function g(x). We show that, under some conditions on f and g, this channel can be approximated, from Eve's point of view, by the classic Wyner wiretap channel.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We propose a cumulative feedback-based ARQ (CF ARQ) protocol for a sliding window of size 2 over packet erasure channels with unreliable feedback. We exploit a matrix signal-flow graph approach to analyze probability-generating functions of transmission and delay times. Contrasting its performance with that of the uncoded baseline scheme for ARQ, developed by Ausavapattanakun and Nosratinia, we demonstrate that CF ARQ can provide significantly less average delay under bursty feedback, and gains up to about 20% in terms of throughput. We also outline the benefits of CF ARQ under burst errors and asymmetric channel conditions. The protocol is more predictable across statistics, hence is more stable. This can help design robust systems when feedback is unreliable. This feature may be preferable for meeting the strict end-to-end latency and reliability requirements of future use cases of ultra-reliable low-latency communications in 5G, such as mission-critical communications and industrial control for critical control messaging.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Future 5G systems will need to support ultra-reliable low-latency communications scenarios. From a latency-reliability viewpoint, it is inefficient to rely on average utility-based system design. Therefore, we introduce the notion of guaranteeable delay which is the average delay plus three standard deviations of the mean. We investigate the trade-off between guaranteeable delay and throughput for point-to-point wireless erasure links with unreliable and delayed feedback, by bringing together signal flow techniques to the area of coding. We use tiny codes, i.e. sliding window by coding with just 2 packets, and design three variations of selective-repeat ARQ protocols, by building on the baseline scheme, i.e. uncoded ARQ, developed by Ausavapattanakun and Nosratinia: (i) Hybrid ARQ with soft combining at the receiver; (ii) cumulative feedback-based ARQ without rate adaptation; and (iii) Coded ARQ with rate adaptation based on the cumulative feedback. Contrasting the performance of these protocols with uncoded ARQ, we demonstrate that HARQ performs only slightly better, cumulative feedback-based ARQ does not provide significant throughput while it has better average delay, and Coded ARQ can provide gains up to about 40% in terms of throughput. Coded ARQ also provides delay guarantees, and is robust to various challenges such as imperfect and delayed feedback, burst erasures, and round-trip time fluctuations. This feature may be preferable for meeting the strict end-to-end latency and reliability requirements of future use cases of ultra-reliable low-latency communications in 5G, such as mission-critical communications and industrial control for critical control messaging.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "In September 2017, McAffee Labs quarterly report estimated that brute force attacks represent 20% of total network attacks, making them the most prevalent type of attack ex-aequo with browser based vulnerabilities. These attacks have sometimes catastrophic consequences, and understanding their fundamental limits may play an important role in the risk assessment of password-secured systems, and in the design of better security protocols. While some solutions exist to prevent online brute-force attacks that arise from one single IP address, attacks performed by botnets are more challenging. In this paper, we analyze these distributed attacks by using a simplified model. Our aim is to understand the impact of distribution and asynchronization on the overall computational effort necessary to breach a system. Our result is based on Guesswork, a measure of the number of password queries (guesses) before the correct one is found in an optimal attack, which is a direct surrogate for the time and the computational effort. We model the lack of synchronization by a worst-case optimization in which the queries are received in the worst possible order, resulting in a min-max formulation. We show that even without synchronization and for sequences of growing length, the asymptotic optimal performance is achievable by using randomized guesses drawn from an appropriate distribution. Therefore, randomization is key for distributed asynchronous attacks. In other words, asynchronous guessers can asymptotically perform brute-force attacks as efficiently as synchronized guessers.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Atomicity or strong consistency is one of the fundamental, most intuitive, and hardest to provide primitives in distributed shared memory emulations. To ensure survivability, scalability, and availability of a storage service in the presence of failures, traditional approaches for atomic memory emulation, in message passing environments, replicate the objects across multiple servers. Compared to replication based algorithms, erasure code-based atomic memory algorithms has much lower storage and communication costs, but usually, they are harder to design. The difficulty of designing atomic memory algorithms further grows, when the set of servers may be changed to ensure survivability of the service over software and hardware upgrades, while avoiding service interruptions. Atomic memory algorithms for performing server reconfiguration, in the replicated systems, are very few, complex, and are still part of an active area of research; reconfigurations of erasure-code based algorithms are non-existent.\n  In this work, we present ARES, an algorithmic framework that allows reconfiguration of the underlying servers, and is particularly suitable for erasure-code based algorithms emulating atomic objects. ARES introduces new configurations while keeping the service available. To use with ARES we also propose a new, and to our knowledge, the first two-round erasure code based algorithm TREAS, for emulating multi-writer, multi-reader (MWMR) atomic objects in asynchronous, message-passing environments, with near-optimal communication and storage costs. Our algorithms can tolerate crash failures of any client and some fraction of servers, and yet, guarantee safety and liveness property. Moreover, by bringing together the advantages of ARES and TREAS, we propose an optimized algorithm where new configurations can be installed without the objects values passing through the reconfiguration clients.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Motivated by applications to delivery of dynamically updated, but correlated data in settings such as content distribution networks, and distributed file sharing systems, we study a single source multiple destination network coded multicast problem in a cache-aided network. We focus on models where the caches are primarily located near the destinations, and where the source has no cache. The source observes a sequence of correlated frames, and is expected to do frame-by-frame encoding with no access to prior frames. We present a novel scheme that shows how the caches can be advantageously used to decrease the overall cost of multicast, even though the source encodes without access to past data. Our cache design and update scheme works with any choice of network code designed for a corresponding cache-less network, is largely decentralized, and works for an arbitrary network. We study a convex relation of the optimization problem that results form the overall cost function. The results of the optimization problem determines the rate allocation and caching strategies. Numerous simulation results are presented to substantiate the theory developed.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We introduce a new algorithm for realizing Maximum Likelihood (ML) decoding in discrete channels with or without memory. In it, the receiver rank orders noise sequences from most likely to least likely. Subtracting noise from the received signal in that order, the first instance that results in a member of the code-book is the ML decoding. We name this algorithm GRAND for Guessing Random Additive Noise Decoding.\n  We establish that GRAND is capacity-achieving when used with random code-books. For rates below capacity we identify error exponents, and for rates beyond capacity we identify success exponents. We determine the scheme's complexity in terms of the number of computations the receiver performs. For rates beyond capacity, this reveals thresholds for the number of guesses by which if a member of the code-book is identified it is likely to be the transmitted code-word.\n  We introduce an approximate ML decoding scheme where the receiver abandons the search after a fixed number of queries, an approach we dub GRANDAB, for GRAND with ABandonment. While not an ML decoder, we establish that the algorithm GRANDAB is also capacity-achieving for an appropriate choice of abandonment threshold, and characterize its complexity, error and success exponents. Worked examples are presented for Markovian noise that indicate these decoding schemes substantially out-perform the brute force decoding approach.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We propose two schemes for selective-repeat ARQ protocols over packet erasure channels with unreliable feedback: (i) a hybrid ARQ protocol with soft combining at the receiver, and (ii) a coded ARQ protocol, by building on the uncoded baseline scheme for ARQ, developed by Ausavapattanakun and Nosratinia. Our method leverages discrete-time queuing and coding theory to analyze the performance of the proposed data transmission methods. We incorporate forward error-correction to reduce in-order delivery delay, and exploit a matrix signal-flow graph approach to analyze the throughput and delay of the protocols. We demonstrate and contrast the performance of the coded protocols with that of the uncoded scheme, illustrating the benefits of coded transmissions.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Given a collection of strings, each with an associated probability of occurrence, the guesswork of each of them is their position in a list ordered from most likely to least likely, breaking ties arbitrarily. Guesswork is central to several applications in information theory: Average guesswork provides a lower bound on the expected computational cost of a sequential decoder to decode successfully the transmitted message; the complementary cumulative distribution function of guesswork gives the error probability in list decoding; the logarithm of guesswork is the number of bits needed in optimal lossless one-to-one source coding; and guesswork is the number of trials required of an adversary to breach a password protected system in a brute-force attack. In this paper, we consider memoryless string-sources that generate strings consisting of i.i.d. characters drawn from a finite alphabet, and characterize their corresponding guesswork. Our main tool is the tilt operation. We show that the tilt operation on a memoryless string-source parametrizes an exponential family of memoryless string-sources, which we refer to as the tilted family. We provide an operational meaning to the tilted families by proving that two memoryless string-sources result in the same guesswork on all strings of all lengths if and only if their respective categorical distributions belong to the same tilted family. Establishing some general properties of the tilt operation, we generalize the notions of weakly typical set and asymptotic equipartition property to tilted weakly typical sets of different orders. We use this new definition to characterize the large deviations for all atypical strings and characterize the volume of weakly typical sets of different orders. We subsequently build on this characterization to prove large deviation bounds on guesswork and provide an accurate approximation of its PMF.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Let $T_\u03b5$ be the noise operator acting on Boolean functions $f:\\{0, 1\\}^n\\mapsto \\{0, 1\\}$, where $\u03b5\\in[0, 1/2]$ is the noise parameter. Given $\u03b1\\geq1$ and the mean $\\mathbb{E} f$, which Boolean function $f$ maximizes the $\u03b1$-th moment $\\mathbb{E}(T_\u03b5f)^\u03b1$? Our findings are: in the weak noise scenario, i.e., $\u03b5$ is small, the maximum is achieved by the lexicographic function; in the strong noise scenario, i.e., $\u03b5$ is close to 1/2, the maximum is achieved by Boolean functions with the largest degree-1 Fourier weight; and when $\u03b1$ is a large integer, among balanced Boolean functions, the maximum is achieved by any function which is 0 on all strings with fewer than $n/2$ 1's. Moreover, for any convex function $\u03a6$, we show that the maximum of $\\mathbb{E}\u03a6(T_\u03b5f)$ is achieved by some monotone function. Analogous results are established in more general contexts, such as Boolean functions defined on the discrete torus $(\\mathbb{Z}/p\\mathbb{Z})^n$, as well as noise stability in a tree model. We also discuss the relationships between this noise stability problem and the problem of non-interactive correlation distillation, as well as Courtade-Kumar's conjecture on the most informative Boolean function.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We consider an abstraction of computational security in password protected systems where a user draws a secret string of given length with i.i.d. characters from a finite alphabet, and an adversary would like to identify the secret string by querying, or guessing, the identity of the string. The concept of a \"total entropy budget\" on the chosen word by the user is natural, otherwise the chosen password would have arbitrary length and complexity. One intuitively expects that a password chosen from the uniform distribution is more secure. This is not the case, however, if we are considering only the average guesswork of the adversary when the user is subject to a total entropy budget. The optimality of the uniform distribution for the user's secret string holds when we have also a budget on the guessing adversary. We suppose that the user is subject to a \"total entropy budget\" for choosing the secret string, whereas the computational capability of the adversary is determined by his \"total guesswork budget.\" We study the regime where the adversary's chances are exponentially small in guessing the secret string chosen subject to a total entropy budget. We introduce a certain notion of uniformity and show that a more uniform source will provide better protection against the adversary in terms of his chances of success in guessing the secret string. In contrast, the average number of queries that it takes the adversary to identify the secret string is smaller for the more uniform secret string subject to the same total entropy budget.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "In this paper, we address the scheduling problem in wireless ad hoc networks by exploiting the computational advantage that comes when such scheduling problems can be represented by claw-free conflict graphs where we consider a wireless broadcast medium. It is possible to formulate a scheduling problem of network coded flows as finding maximum weighted independent set (MWIS) in the conflict graph of the network. Finding MWIS of a general graph is NP-hard leading to an NP-hard complexity of scheduling. In a claw-free conflict graph, MWIS can be found in polynomial time leading to a throughput-optimal scheduling. We show that the conflict graph of certain wireless ad hoc networks are claw-free. In order to obtain claw-free conflict graphs in general networks, we suggest introducing additional conflicts (edges) while keeping the decrease in MWIS size minimal. To this end, we introduce an iterative optimization problem to decide where to introduce edges and investigate its efficient implementation. Besides, we exemplify some physical modifications to manipulate the conflict graph of a network and also propose a mixed scheduling strategy for specific networks. We conclude that claw breaking method by adding extra edges can perform nearly optimal under the necessary assumptions.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "In this paper, we address the scheduling problem in wireless ad hoc networks by exploiting the computational advantage that comes when such scheduling problems can be represented by claw-free conflict graphs. It is possible to formulate a scheduling problem of network coded flows as finding maximum weighted independent set (MWIS) in the conflict graph of the network. We consider activation of hyperedges in a hypergraph to model a wireless broadcast medium. We show that the conflict graph of certain wireless ad hoc networks are claw-free. It is known that finding MWIS of a general graph is NP-hard, but in a claw-free conflict graph, it is possible to apply Minty's or Faenza et al.'s algorithms in polynomial time. We discuss our approach on some sample networks.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We study the central problem in data privacy: how to share data with an analyst while providing both privacy and utility guarantees to the user that owns the data. In this setting, we present an estimation-theoretic analysis of the privacy-utility trade-off (PUT). Here, an analyst is allowed to reconstruct (in a mean-squared error sense) certain functions of the data (utility), while other private functions should not be reconstructed with distortion below a certain threshold (privacy). We demonstrate how $\u03c7^2$-information captures the fundamental PUT in this case and provide bounds for the best PUT. We propose a convex program to compute privacy-assuring mappings when the functions to be disclosed and hidden are known a priori and the data distribution is known. We derive lower bounds on the minimum mean-squared error of estimating a target function from the disclosed data and evaluate the robustness of our approach when an empirical distribution is used to compute the privacy-assuring mappings instead of the true data distribution. We illustrate the proposed approach through two numerical experiments.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We study the trade-off between storage overhead and inter-cluster repair bandwidth in clustered storage systems, while recovering from multiple node failures within a cluster. A cluster is a collection of $m$ nodes, and there are $n$ clusters. For data collection, we download the entire content from any $k$ clusters. For repair of $t \\geq 2$ nodes within a cluster, we take help from $\\ell$ local nodes, as well as $d$ helper clusters. We characterize the optimal trade-off under functional repair, and also under exact repair for the minimum storage and minimum inter-cluster bandwidth (MBR) operating points. Our bounds show the following interesting facts: $1)$ When $t|(m-\\ell)$ the trade-off is the same as that under $t=1$, and thus there is no advantage in jointly repairing multiple nodes, $2)$ When $t \\nmid (m-\\ell)$, the optimal file-size at the MBR point under exact repair can be strictly less than that under functional repair. $3)$ Unlike the case of $t=1$, increasing the number of local helper nodes does not necessarily increase the system capacity under functional repair.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "As parallelism becomes critically important in the semiconductor technology, high-performance computing, and cloud applications, parallel network systems will increasingly follow suit. Today, parallelism is an essential architectural feature of 40/100/400 Gigabit Ethernet standards, whereby high speed Ethernet systems are equipped with multiple parallel network interfaces. This creates new network topology abstractions and new technology requirements: instead of a single high capacity network link, multiple Ethernet end-points and interfaces need to be considered together with multiple links in form of discrete parallel paths. This new paradigm is enabling implementations of various new features to improve overall system performance. In this paper, we analyze the performance of parallel network systems with network coding. In particular, by using random LNC (RLNC), - a code without the need for decoding, we can make use of the fact that we have codes that are both distributed (removing the need for coordination or optimization of resources) and composable (without the need to exchange code information), leading to a fully stateless operation. We propose a novel theoretical modeling framework, including derivation of the upper and lower bounds as well as an expected value of the differential delay of parallel paths, and the resulting queue size at the receiver. The results show a great promise of network system parallelism in combination with RLNC: with a proper set of design parameters, the differential delay and the buffer size at the Ethernet receiver can be reduced significantly, while the cross-layer design and routing can be greatly simplified.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "This paper considers the problem of channel coding over Gaussian intersymbol interference (ISI) channels with a given metric decoding rule. Specifically, it is assumed that the mismatched decoder has an incorrect assumption on the impulse response function. The mismatch capacity is the highest achievable rate for a given decoding rule. Existing lower bounds to the mismatch capacity for channels and decoding metrics with memory (as in our model) are presented only in the form of multi-letter expressions that have not been calculated in practice. Consequently, they provide little insight on the mismatch problem. In this paper, we derive computable single-letter lower bounds to the mismatch capacity, and discuss some implications of our results. Our achievable rates are based on two ensembles, the ensemble of codewords generated by an autoregressive process, and the ensemble of codewords drawn uniformly over a \"type class\" of real-valued sequences. Computation of our achievable rates demonstrates non-trivial behavior of the achievable rates as a function of the mismatched parameters. As a simple application of our technique, we derive also the random coding exponent associated with a mismatched decoder which assumes that there is no ISI at all. Finally, we compare our results with universal decoders which are designed outside the true class of channels that we consider in this paper.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We study a notion of guesswork, where multiple agents intend to launch a coordinated brute-force attack to find a single binary secret string, and each agent has access to side information generated through either a BEC or a BSC. The average number of trials required to find the secret string grows exponentially with the length of the string, and the rate of the growth is called the guesswork exponent. We compute the guesswork exponent for several multi-agent attacks. We show that a multi-agent attack reduces the guesswork exponent compared to a single agent, even when the agents do not exchange information to coordinate their attack, and try to individually guess the secret string using a predetermined scheme in a decentralized fashion. Further, we show that the guesswork exponent of two agents who do coordinate their attack is strictly smaller than that of any finite number of agents individually performing decentralized guesswork.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "This work introduces the particle-intensity channel (PIC) as a model for molecular communication systems and characterizes the properties of the optimal input distribution and the capacity limits for this system. In the PIC, the transmitter encodes information, in symbols of a given duration, based on the number of particles released, and the receiver detects and decodes the message based on the number of particles detected during the symbol interval. In this channel, the transmitter may be unable to control precisely the number of particles released, and the receiver may not detect all the particles that arrive. We demonstrate that the optimal input distribution for this channel always has mass points at zero and the maximum number of particles that can be released. We then consider diffusive particle transport, derive the capacity expression when the input distribution is binary, and show conditions under which the binary input is capacity-achieving. In particular, we demonstrate that when the transmitter cannot generate particles at a high rate, the optimal input distribution is binary.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We explore properties and applications of the Principal Inertia Components (PICs) between two discrete random variables $X$ and $Y$. The PICs lie in the intersection of information and estimation theory, and provide a fine-grained decomposition of the dependence between $X$ and $Y$. Moreover, the PICs describe which functions of $X$ can or cannot be reliably inferred (in terms of MMSE) given an observation of $Y$. We demonstrate that the PICs play an important role in information theory, and they can be used to characterize information-theoretic limits of certain estimation problems. In privacy settings, we prove that the PICs are related to fundamental limits of perfect privacy.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Motivated by emerging applications to the edge computing paradigm, we introduce a two-layer erasure-coded fault-tolerant distributed storage system offering atomic access for read and write operations. In edge computing, clients interact with an edge-layer of servers that is geographically near; the edge-layer in turn interacts with a back-end layer of servers. The edge-layer provides low latency access and temporary storage for client operations, and uses the back-end layer for persistent storage. Our algorithm, termed Layered Data Storage (LDS) algorithm, offers several features suitable for edge-computing systems, works under asynchronous message-passing environments, supports multiple readers and writers, and can tolerate $f_1 < n_1/2$ and $f_2 < n_2/3$ crash failures in the two layers having $n_1$ and $n_2$ servers, respectively. We use a class of erasure codes known as regenerating codes for storage of data in the back-end layer. The choice of regenerating codes, instead of popular choices like Reed-Solomon codes, not only optimizes the cost of back-end storage, but also helps in optimizing communication cost of read operations, when the value needs to be recreated all the way from the back-end. The two-layer architecture permits a modular implementation of atomicity and erasure-code protocols; the implementation of erasure-codes is mostly limited to interaction between the two layers. We prove liveness and atomicity of LDS, and also compute performance costs associated with read and write operations. Further, in a multi-object system running $N$ independent instances of LDS, where only a small fraction of the objects undergo concurrent accesses at any point during the execution, the overall storage cost is dominated by that of persistent storage in the back-end layer, and is given by $\u0398(N)$.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "The principal mission of Multi-Source Multicast (MSM) is to disseminate all messages from all sources in a network to all destinations. MSM is utilized in numerous applications. In many of them, securing the messages disseminated is critical. A common secure model is to consider a network where there is an eavesdropper which is able to observe a subset of the network links, and seek a code which keeps the eavesdropper ignorant regarding all the messages. While this is solved when all messages are located at a single source, Secure MSM (SMSM) is an open problem, and the rates required are hard to characterize in general. In this paper, we consider Individual Security, which promises that the eavesdropper has zero mutual information with each message individually. We completely characterize the rate region for SMSM under individual security, and show that such a security level is achievable at the full capacity of the network, that is, the cut-set bound is the matching converse, similar to non-secure MSM. Moreover, we show that the field size is similar to non-secure MSM and does not have to be larger due to the security constraint.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We study a generalization of the setting of regenerating codes, motivated by applications to storage systems consisting of clusters of storage nodes. There are $n$ clusters in total, with $m$ nodes per cluster. A data file is coded and stored across the $mn$ nodes, with each node storing $\u03b1$ symbols. For availability of data, we require that the file be retrievable by downloading the entire content from any subset of $k$ clusters. Nodes represent entities that can fail. We distinguish between intra-cluster and inter-cluster bandwidth (BW) costs during node repair. Node-repair in a cluster is accomplished by downloading $\u03b2$ symbols each from any set of $d$ other clusters, dubbed remote helper clusters, and also up to $\u03b1$ symbols each from any set of $\\ell$ surviving nodes, dubbed local helper nodes, in the host cluster. We first identify the optimal trade-off between storage-overhead and inter-cluster repair-bandwidth under functional repair, and also present optimal exact-repair code constructions for a class of parameters. The new trade-off is strictly better than what is achievable via space-sharing existing coding solutions, whenever $\\ell > 0$. We then obtain sharp lower bounds on the necessary intra-cluster repair BW to achieve optimal trade-off. Our bounds reveal the interesting fact that, while it is beneficial to increase the number of local helper nodes $\\ell$ in order to improve the storage-vs-inter-cluster-repair-BW trade-off, increasing $\\ell$ not only increases intra-cluster BW in the host-cluster, but also increases the intra-cluster BW in the remote helper clusters. We also analyze resilience of the clustered storage system against passive eavesdropping by providing file-size bounds and optimal code constructions.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Just recently, the concept of augmented and virtual reality (AR/VR) over wireless has taken the entire 5G ecosystem by storm spurring an unprecedented interest from both academia, industry and others. Yet, the success of an immersive VR experience hinges on solving a plethora of grand challenges cutting across multiple disciplines. This article underscores the importance of VR technology as a disruptive use case of 5G (and beyond) harnessing the latest development of storage/memory, fog/edge computing, computer vision, artificial intelligence and others. In particular, the main requirements of wireless interconnected VR are described followed by a selection of key enablers, then, research avenues and their underlying grand challenges are presented. Furthermore, we examine three VR case studies and provide numerical results under various storage, computing and network configurations. Finally, this article exposes the limitations of current networks and makes the case for more theory, and innovations to spearhead VR for the masses.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "The capability of mobile devices to use multiple interfaces to support a single session is becoming more prevalent. Prime examples include the desire to implement WiFi offloading and the introduction of 5G. Furthermore, an increasing fraction of Internet traffic is becoming delay sensitive. These two trends drive the need to investigate methods that enable communication over multiple parallel heterogeneous networks, while also ensuring that delay constraints are met. This paper approaches these challenges using a multi-path streaming code that uses forward error correction to reduce the in-order delivery delay of packets in networks with poor link quality and transient connectivity. A simple analysis is developed that provides a good approximation of the in-order delivery delay. Furthermore, numerical results help show that the delay penalty of communicating over multiple paths is insignificant when considering the potential throughput gains obtained through the fusion of multiple networks.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Several significant models have been developed that enable the study of diffusion of signals across biological, social and engineered networks. Within these established frameworks, the inverse problem of identifying the source of the propagated signal is challenging, owing to the numerous alternative possibilities for signal progression through the network. In real world networks, the challenge of determining sources is compounded as the true propagation dynamics are typically unknown, and when they have been directly measured, they rarely conform to the assumptions of any of the well-studied models. In this paper we introduce a method called Network Infusion (NI) that has been designed to circumvent these issues, making source inference practical for large, complex real world networks. The key idea is that to infer the source node in the network, full characterization of diffusion dynamics, in many cases, may not be necessary. This objective is achieved by creating a diffusion kernel that well-approximates standard diffusion models, but lends itself to inversion, by design, via likelihood maximization or error minimization. We apply NI for both single-source and multi-source diffusion, for both single-snapshot and multi-snapshot observations, and for both homogeneous and heterogeneous diffusion setups. We prove the mean-field optimality of NI for different scenarios, and demonstrate its effectiveness over several synthetic networks. Moreover, we apply NI to a real-data application, identifying news sources in the Digg social network, and demonstrate the effectiveness of NI compared to existing methods. Finally, we propose an integrative source inference framework that combines NI with a distance centrality-based method, which leads to a robust performance in cases where the underlying dynamics are unknown.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We introduce Network Maximal Correlation (NMC) as a multivariate measure of nonlinear association among random variables. NMC is defined via an optimization that infers transformations of variables by maximizing aggregate inner products between transformed variables. For finite discrete and jointly Gaussian random variables, we characterize a solution of the NMC optimization using basis expansion of functions over appropriate basis functions. For finite discrete variables, we propose an algorithm based on alternating conditional expectation to determine NMC. Moreover we propose a distributed algorithm to compute an approximation of NMC for large and dense graphs using graph partitioning. For finite discrete variables, we show that the probability of discrepancy greater than any given level between NMC and NMC computed using empirical distributions decays exponentially fast as the sample size grows. For jointly Gaussian variables, we show that under some conditions the NMC optimization is an instance of the Max-Cut problem. We then illustrate an application of NMC in inference of graphical model for bijective functions of jointly Gaussian variables. Finally, we show NMC's utility in a data application of learning nonlinear dependencies among genes in a cancer dataset.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Erasure codes offer an efficient way to decrease storage and communication costs while implementing atomic memory service in asynchronous distributed storage systems. In this paper, we provide erasure-code-based algorithms having the additional ability to perform background repair of crashed nodes. A repair operation of a node in the crashed state is triggered externally, and is carried out by the concerned node via message exchanges with other active nodes in the system. Upon completion of repair, the node re-enters active state, and resumes participation in ongoing and future read, write, and repair operations. To guarantee liveness and atomicity simultaneously, existing works assume either the presence of nodes with stable storage, or presence of nodes that never crash during the execution. We demand neither of these; instead we consider a natural, yet practical network stability condition $N1$ that only restricts the number of nodes in the crashed/repair state during broadcast of any message.\n  We present an erasure-code based algorithm $RADON_C$ that is always live, and guarantees atomicity as long as condition $N1$ holds. In situations when the number of concurrent writes is limited, $RADON_C$ has significantly improved storage and communication cost over a replication-based algorithm $RADON_R$, which also works under $N1$. We further show how a slightly stronger network stability condition $N2$ can be used to construct algorithms that never violate atomicity. The guarantee of atomicity comes at the expense of having an additional phase during the read and write operations.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Erasure codes are increasingly being studied in the context of implementing atomic memory objects in large scale asynchronous distributed storage systems. When compared with the traditional replication based schemes, erasure codes have the potential of significantly lowering storage and communication costs while simultaneously guaranteeing the desired resiliency levels. In this work, we propose the Storage-Optimized Data-Atomic (SODA) algorithm for implementing atomic memory objects in the multi-writer multi-reader setting. SODA uses Maximum Distance Separable (MDS) codes, and is specifically designed to optimize the total storage cost for a given fault-tolerance requirement. For tolerating $f$ server crashes in an $n$-server system, SODA uses an $[n, k]$ MDS code with $k=n-f$, and incurs a total storage cost of $\\frac{n}{n-f}$. SODA is designed under the assumption of reliable point-to-point communication channels. The communication cost of a write and a read operation are respectively given by $O(f^2)$ and $\\frac{n}{n-f}(\u03b4_w+1)$, where $\u03b4_w$ denotes the number of writes that are concurrent with the particular read. In comparison with the recent CASGC algorithm, which also uses MDS codes, SODA offers lower storage cost while pays more on the communication cost.\n  We also present a modification of SODA, called SODA$_{\\text{err}}$, to handle the case where some of the servers can return erroneous coded elements during a read operation. Specifically, in order to tolerate $f$ server failures and $e$ error-prone coded elements, the SODA$_{\\text{err}}$ algorithm uses an $[n, k]$ MDS code such that $k=n-2e-f$. SODA$_{\\text{err}}$ also guarantees liveness and atomicity, while maintaining an optimized total storage cost of $\\frac{n}{n-f-2e}$.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We consider a communication problem in which an update of the source message needs to be conveyed to one or more distant receivers that are interested in maintaining specific linear functions of the source message. The setting is one in which the updates are sparse in nature, and where neither the source nor the receiver(s) is aware of the exact {\\em difference vector}, but only know the amount of sparsity that is present in the difference-vector. Under this setting, we are interested in devising linear encoding and decoding schemes that minimize the communication cost involved. We show that the optimal solution to this problem is closely related to the notion of maximally recoverable codes (MRCs), which were originally introduced in the context of coding for storage systems. In the context of storage, MRCs guarantee optimal erasure protection when the system is partially constrained to have local parity relations among the storage nodes. In our problem, we show that optimal solutions exist if and only if MRCs of certain kind (identified by the desired linear functions) exist. We consider point-to-point and broadcast versions of the problem, and identify connections to MRCs under both these settings. For the point-to-point setting, we show that our linear-encoder based achievable scheme is optimal even when non-linear encoding is permitted. The theory is illustrated in the context of updating erasure coded storage nodes. We present examples based on modern storage codes such as the minimum bandwidth regenerating codes.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Consider two correlated sources $X$ and $Y$ generated from a joint distribution $p_{X,Y}$. Their G\u00e1cs-K\u00f6rner Common Information, a measure of common information that exploits the combinatorial structure of the distribution $p_{X,Y}$, leads to a source decomposition that exhibits the latent common parts in $X$ and $Y$. Using this source decomposition we construct an efficient distributed compression scheme, which can be efficiently used in the network setting as well. Then, we relax the combinatorial conditions on the source distribution, which results in an efficient scheme with a helper node, which can be thought of as a front-end cache. This relaxation leads to an inherent trade-off between the rate of the helper and the rate reduction at the sources, which we capture by a notion of optimal decomposition. We formulate this as an approximate G\u00e1cs-K\u00f6rner optimization. We then discuss properties of this optimization, and provide connections with the maximal correlation coefficient, as well as an efficient algorithm, both through the application of spectral graph theory to the induced bipartite graph of $p_{X,Y}$.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Graph alignment refers to the problem of finding a bijective mapping across vertices of two graphs such that, if two nodes are connected in the first graph, their images are connected in the second graph. This problem arises in many fields such as computational biology, social sciences, and computer vision and is often cast as a quadratic assignment problem (QAP). Most standard graph alignment methods consider an optimization that maximizes the number of matches between the two graphs, ignoring the effect of mismatches. We propose a generalized graph alignment formulation that considers both matches and mismatches in a standard QAP formulation. This modification can have a major impact in aligning graphs with different sizes and heterogenous edge densities. Moreover, we propose two methods for solving the generalized graph alignment problem based on spectral decomposition of matrices. We compare the performance of proposed methods with some existing graph alignment algorithms including Natalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite programming-based method over various synthetic and real graph models. Our proposed method based on simultaneous alignment of multiple eigenvectors leads to consistently good performance in different graph models. In particular, in the alignment of regular graph structures which is one of the most difficult graph alignment cases, our proposed method significantly outperforms other methods.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Consider a multi-source network coding problem with correlated sources. While the fundamental limits are known, achieving them, in general, involves a computational burden due to the complex decoding process. Efficient solutions, on the other hand, are by large based on source and network coding separation, thus imposing strict topological constraints on the networks which can be solved.\n  In this work, we introduce a novel notion of separation of source and network coding using G\u00e1cs-K\u00f6rner Common Information (CI). Unlike existing notions of separation, the sufficient condition for this separation to hold depends on the source structure rather than the network topology. Using the suggested separation scheme, we tackle three important multi-source problems. The first is the multi-source multicast. We construct efficient, zero error source codes, and via properties of the CI completely characterize the resulting rate region. The second is broadcast with side information. We establish a duality between this problem and the classical problem of degraded message set broadcast, and give two code constructions and their associated regions. Finally, we consider the Ahlswede-Korner problem in a network, and give an efficient solution which is tight under the CI constraints.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "The problem of storing permutations in a distributed manner arises in several common scenarios, such as efficient updates of a large, encrypted, or compressed data set. This problem may be addressed in either a combinatorial or a coding approach. The former approach boils down to presenting large sets of permutations with \\textit{locality}, that is, any symbol of the permutation can be computed from a small set of other symbols. In the latter approach, a permutation may be coded in order to achieve locality. This paper focuses on the combinatorial approach.\n  We provide upper and lower bounds for the maximal size of a set of permutations with locality, and provide several simple constructions which attain the upper bound. In cases where the upper bound is not attained, we provide alternative constructions using Reed-Solomon codes, permutation polynomials, and multi-permutations.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "A latent space model for a family of random graphs assigns real-valued vectors to nodes of the graph such that edge probabilities are determined by latent positions. Latent space models provide a natural statistical framework for graph visualizing and clustering. A latent space model of particular interest is the Random Dot Product Graph (RDPG), which can be fit using an efficient spectral method; however, this method is based on a heuristic that can fail, even in simple cases. Here, we consider a closely related latent space model, the Logistic RDPG, which uses a logistic link function to map from latent positions to edge likelihoods. Over this model, we show that asymptotically exact maximum likelihood inference of latent position vectors can be achieved using an efficient spectral method. Our method involves computing top eigenvectors of a normalized adjacency matrix and scaling eigenvectors using a regression step. The novel regression scaling step is an essential part of the proposed method. In simulations, we show that our proposed method is more accurate and more robust than common practices. We also show the effectiveness of our approach over standard real networks of the karate club and political blogs.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We consider use of FEC to reduce in-order delivery delay over packet erasure channels. We propose a class of streaming codes that is capacity achieving and provides a superior throughput-delay trade-off compared to block codes by introducing flexibility in where and when redundancy is placed. This flexibility results in significantly lower in-order delay for a given throughput for a wide range of network scenarios. Furthermore, a major contribution of this paper is the combination of queuing and coding theory to analyze the code's performance. Finally, we present simulation and experimental results illustrating the code's benefits.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Information theory is rapidly approaching its 70th birthday. What are promising future directions for research in information theory? Where will information theory be having the most impact in 10-20 years? What new and emerging areas are ripe for the most impact, of the sort that information theory has had on the telecommunications industry over the last 60 years? How should the IEEE Information Theory Society promote high-risk new research directions and broaden the reach of information theory, while continuing to be true to its ideals and insisting on the intellectual rigor that makes its breakthroughs so powerful? These are some of the questions that an ad hoc committee (composed of the present authors) explored over the past two years. We have discussed and debated these questions, and solicited detailed inputs from experts in fields including genomics, biology, economics, and neuroscience. This report is the result of these discussions.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Satellite networks provide unique challenges that can restrict users' quality of service. For example, high packet erasure rates and large latencies can cause significant disruptions to applications such as video streaming or voice-over-IP. Network coding is one promising technique that has been shown to help improve performance, especially in these environments. However, implementing any form of network code can be challenging. This paper will use an example of a generation-based network code and a sliding-window network code to help highlight the benefits and drawbacks of using one over the other. In-order packet delivery delay, as well as network efficiency, will be used as metrics to help differentiate between the two approaches. Furthermore, lessoned learned during the course of our research will be provided in an attempt to help the reader understand when and where network coding provides its benefits.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Conventional TCP performance is significantly impaired under long latency and/or constrained bandwidth. While small Pacific Island states on satellite links experience this in the extreme, small populations and remoteness often rule out submarine fibre connections and their communities struggle to reap the benefits of the Internet. Network-coded TCP (TCP/NC) can increase goodput under high latency and packet loss, but has not been used to tunnel conventional TCP and UDP across satellite links before. We report on a feasibility study aimed at determining expected goodput gain across such TCP/NC tunnels into island targets on geostationary and medium earth orbit satellite links.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We present information-theoretic definitions and results for analyzing symmetric-key encryption schemes beyond the perfect secrecy regime, i.e. when perfect secrecy is not attained. We adopt two lines of analysis, one based on lossless source coding, and another akin to rate-distortion theory. We start by presenting a new information-theoretic metric for security, called symbol secrecy, and derive associated fundamental bounds. We then introduce list-source codes (LSCs), which are a general framework for mapping a key length (entropy) to a list size that an eavesdropper has to resolve in order to recover a secret message. We provide explicit constructions of LSCs, and demonstrate that, when the source is uniformly distributed, the highest level of symbol secrecy for a fixed key length can be achieved through a construction based on minimum-distance separable (MDS) codes. Using an analysis related to rate-distortion theory, we then show how symbol secrecy can be used to determine the probability that an eavesdropper correctly reconstructs functions of the original plaintext. We illustrate how these bounds can be applied to characterize security properties of symmetric-key encryption schemes, and, in particular, extend security claims based on symbol secrecy to a functional setting.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "A client/encoder edits a file, as modeled by an insertion-deletion (InDel) process. An old copy of the file is stored remotely at a data-centre/decoder, and is also available to the client. We consider the problem of throughput- and computationally-efficient communication from the client to the data-centre, to enable the server to update its copy to the newly edited file. We study two models for the source files/edit patterns: the random pre-edit sequence left-to-right random InDel (RPES-LtRRID) process, and the arbitrary pre-edit sequence arbitrary InDel (APES-AID) process. In both models, we consider the regime in which the number of insertions/deletions is a small (but constant) fraction of the original file. For both models we prove information-theoretic lower bounds on the best possible compression rates that enable file updates. Conversely, our compression algorithms use dynamic programming (DP) and entropy coding, and achieve rates that are approximately optimal.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "For general connections, the problem of finding network codes and optimizing resources for those codes is intrinsically difficult and little is known about its complexity. Most of the existing solutions rely on very restricted classes of network codes in terms of the number of flows allowed to be coded together, and are not entirely distributed. In this paper, we consider a new method for constructing linear network codes for general connections of continuous flows to minimize the total cost of edge use based on mixing. We first formulate the minimumcost network coding design problem. To solve the optimization problem, we propose two equivalent alternative formulations with discrete mixing and continuous mixing, respectively, and develop distributed algorithms to solve them. Our approach allows fairly general coding across flows and guarantees no greater cost than any solution without network coding.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "The problem of finding network codes for general connections is inherently difficult in capacity constrained networks. Resource minimization for general connections with network coding is further complicated. Existing methods for identifying solutions mainly rely on highly restricted classes of network codes, and are almost all centralized. In this paper, we introduce linear network mixing coefficients for code constructions of general connections that generalize random linear network coding (RLNC) for multicast connections. For such code constructions, we pose the problem of cost minimization for the subgraph involved in the coding solution and relate this minimization to a path-based Constraint Satisfaction Problem (CSP) and an edge-based CSP. While CSPs are NP-complete in general, we present a path-based probabilistic distributed algorithm and an edge-based probabilistic distributed algorithm with almost sure convergence in finite time by applying Communication Free Learning (CFL). Our approach allows fairly general coding across flows, guarantees no greater cost than routing, and shows a possible distributed implementation. Numerical results illustrate the performance improvement of our approach over existing methods.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "In this paper, we study the wireline two-unicast-Z communication network over directed acyclic graphs. The two-unicast-Z network is a two-unicast network where the destination intending to decode the second message has apriori side information of the first message. We make three contributions in this paper:\n  1. We describe a new linear network coding algorithm for two-unicast-Z networks over directed acyclic graphs. Our approach includes the idea of interference alignment as one of its key ingredients. For graphs of a bounded degree, our algorithm has linear complexity in terms of the number of vertices, and polynomial complexity in terms of the number of edges.\n  2. We prove that our algorithm achieves the rate-pair (1, 1) whenever it is feasible in the network. Our proof serves as an alternative, albeit restricted to two-unicast-Z networks over directed acyclic graphs, to an earlier result of Wang et al. which studied necessary and sufficient conditions for feasibility of the rate pair (1, 1) in two-unicast networks.\n  3. We provide a new proof of the classical max-flow min-cut theorem for directed acyclic graphs.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "In non-coherent wideband fading channels where energy rather than spectrum is the limiting resource, peaky and non-peaky signaling schemes have long been considered species apart, as the first approaches asymptotically the capacity of a wideband AWGN channel with the same average SNR, whereas the second reaches a peak rate at some finite critical bandwidth and then falls to zero as bandwidth grows to infinity. In this paper it is shown that this distinction is in fact an artifact of the limited attention paid in the past to the product between the bandwidth and the fraction of time it is in use. This fundamental quantity, called bandwidth occupancy, measures average bandwidth usage over time. For all signaling schemes with the same bandwidth occupancy, achievable rates approach to the wideband AWGN capacity within the same gap as the bandwidth occupancy approaches its critical value, and decrease to zero as the occupancy goes to infinity. This unified analysis produces quantitative closed-form expressions for the ideal bandwidth occupancy, recovers the existing capacity results for (non-)peaky signaling schemes, and unveils a trade-off between the accuracy of approximating capacity with a generalized Taylor polynomial and the accuracy with which the optimal bandwidth occupancy can be bounded.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "A large number of streaming applications use reliable transport protocols such as TCP to deliver content over the Internet. However, head-of-line blocking due to packet loss recovery can often result in unwanted behavior and poor application layer performance. Transport layer coding can help mitigate this issue by helping to recover from lost packets without waiting for retransmissions. We consider the use of an on-line network code that inserts coded packets at strategic locations within the underlying packet stream. If retransmissions are necessary, additional coding packets are transmitted to ensure the receiver's ability to decode. An analysis of this scheme is provided that helps determine both the expected in-order packet delivery delay and its variance. Numerical results are then used to determine when and how many coded packets should be inserted into the packet stream, in addition to determining the trade-offs between reducing the in-order delay and the achievable rate. The analytical results are finally compared with experimental results to provide insight into how to minimize the delay of existing transport layer protocols.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "This paper considers the communication and storage costs of emulating atomic (linearizable) multi-writer multi-reader shared memory in distributed message-passing systems. The paper contains three main contributions: (1) We present a atomic shared-memory emulation algorithm that we call Coded Atomic Storage (CAS). This algorithm uses erasure coding methods. In a storage system with $N$ servers that is resilient to $f$ server failures, we show that the communication cost of CAS is $\\frac{N}{N-2f}$. The storage cost of CAS is unbounded. (2) We present a modification of the CAS algorithm known as CAS with Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer $\u03b4$ and has a bounded storage cost. We show that in every execution where the number of write operations that are concurrent with a read operation is no bigger than $\u03b4$, the CASGC algorithm with parameter $\u03b4$ satisfies atomicity and liveness. We explicitly characterize the storage cost of CASGC, and show that it has the same communication cost as CAS. (3) We describe an algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS) algorithm that achieves a smaller communication cost than CAS and CASGC. In particular, CCOAS incurs read and write communication costs of $\\frac{N}{N-f}$ measured in terms of number of object values. We also discuss drawbacks of CCOAS as compared with CAS and CASGC.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "Coding techniques may be useful for data center data survivability as well as for reducing traffic congestion. We present a queued cross-bar network (QCN) method that can be used for traffic analysis of both replication/uncoded and coded storage systems. We develop a framework for generating QCN rate regions (RRs) by analyzing their conflict graph stable set polytopes (SSPs). In doing so, we apply recent results from graph theory on the characterization of particular graph SSPs. We characterize the SSP of QCN conflict graphs under a variety of traffic patterns, allowing for their efficient RR computation. For uncoded systems, we show how to compute RRs and find rate optimal scheduling algorithms. For coded storage, we develop a RR upper bound, for which we provide an intuitive interpretation. We show that the coded storage RR upper bound is achievable in certain coded systems in which drives store sufficient coded information, as well in certain dynamic coding systems. Numerical illustrations show that coded storage can result in gains in RR volume of approximately 50%, averaged across traffic patterns.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "The Guesswork problem was originally motivated by a desire to quantify computational security for single user systems. Leveraging recent results from its analysis, we extend the remit and utility of the framework to the quantification of the computational security for multi-user systems. In particular, assume that $V$ users independently select strings stochastically from a finite, but potentially large, list. An inquisitor who does not know which strings have been selected wishes to identify $U$ of them. The inquisitor knows the selection probabilities of each user and is equipped with a method that enables the testing of each (user, string) pair, one at a time, for whether that string had been selected by that user.\n  Here we establish that, unless $U=V$, there is no general strategy that minimizes the distribution of the number of guesses, but in the asymptote as the strings become long we prove the following: by construction, there is an asymptotically optimal class of strategies; the number of guesses required in an asymptotically optimal strategy satisfies a large deviation principle with a rate function, which is not necessarily convex, that can be determined from the rate functions of optimally guessing individual users' strings; if all user's selection statistics are identical, the exponential growth rate of the average guesswork as the string-length increases is determined by the specific R\u00e9nyi entropy of the string-source with parameter $(V-U+1)/(V-U+2)$, generalizing the known $V=U=1$ case; and that the Shannon entropy of the source is a lower bound on the average guesswork growth rate for all $U$ and $V$, thus providing a bound on computational security for multi-user systems. Examples are presented to illustrate these results and their ramifications for systems design.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "We present a heuristic for designing vector non-linear network codes for non-multicast networks, which we call quasi-linear network codes. The method presented has two phases: finding an approximate linear network code over the reals, and then quantizing it to a vector non-linear network code using a fixed-point representation. Apart from describing the method, we draw some links between some network parameters and the rate of the resulting code.\n        \u25b3 Less", "author": "Muriel M\u00e9dard"}, {"abstract": "In modern power systems, the operating point, at which the demand and supply are balanced, may take different values due to changes in loads and renewable generation levels. Understanding the dynamics of stressed power systems with a range of operating points would be essential to assuring their reliable operation, and possibly allow higher integration of renewable resources. This letter introduces a non-traditional way to think about the stability assessment problem of power systems. Instead of estimating the set of initial states leading to a given operating condition, we characterize the set of operating conditions that a power grid converges to from a given initial state under changes in power injections and lines. We term this problem as \"inverse stability\", a problem which is rarely addressed in the control and systems literature, and hence, poorly understood. Exploiting quadratic approximations of the system's energy function, we introduce an estimate of the inverse stability region. Also, we briefly describe three important applications of the inverse stability notion: (i) robust stability assessment of power systems w.r.t. different renewable generation levels, (ii) stability-constrained optimal power flow (sOPF), and (iii) stability-guaranteed corrective action design.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "Model instability and poor prediction of long-term behavior are common problems when modeling dynamical systems using nonlinear \"black-box\" techniques. Direct optimization of the long-term predictions, often called simulation error minimization, leads to optimization problems that are generally non-convex in the model parameters and suffer from multiple local minima. In this work we present methods which address these problems through convex optimization, based on Lagrangian relaxation, dissipation inequalities, contraction theory, and semidefinite programming. We demonstrate the proposed methods with a model order reduction task for electronic circuit design and the identification of a pneumatic actuator from experiment.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "We consider baseband equivalent representation of transmission circuits, in the form of a nonlinear dynamical system $\\mathbf S$ in discrete time (DT) defined by a series interconnection of a phase-amplitude modulator, a nonlinear dynamical system $\\mathbf F$ in continuous time (CT), and an ideal demodulator. We show that when $\\mathbf F$ is a CT Volterra series model, the resulting $\\mathbf S$ is a series interconnection of a DT Volterra series model of same degree and memory depth, and an LTI system with special properties. The result suggests a new, non-obvious, analytically motivated structure of digital pre-compensation of analog nonlinear distortions such as those caused by power amplifiers in digital communication systems. The baseband model and the corresponding digital compensation structure readily extend to OFDM modulation. MATLAB simulation is used to verify proposed baseband equivalent model and demonstrate effectiveness of the new compensation scheme, as compared to the standard Volterra series approach.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "We consider discrete-time (DT) systems S in which a DT input is first tranformed to a continuous-time (CT) format by phase-amplitude modulation, then modified by a non-linear CT dynamical transformation F, and finally converted back to DT output using an ideal de-modulation scheme. Assuming that F belongs to a special class of CT Volterra series models with fixed degree and memory depth, we provide a complete characterization of S as a series connection of a DT Volterra series model of fixed degree and memory depth, and an LTI system with special properties. The result suggests a new, non-obvious, analytically motivated structure of digital compensation of analog nonlinear distortions (for example, those caused by power amplifiers) in digital communication systems. Results from a MATLAB simulation are used to demonstrate effectiveness of the new compensation scheme, as compared to the standard Volterra series approach.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "We construct structured H-Infinity optimal model matching problems with rational coefficients, in which the optimal solution is not rational, in the sense that the cost does not achieve its maximal lower bound on the set of rational matching models, but the same maximal lower bound can be reached by using a continuous non-rational matching model.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "This paper introduces new techniques for using convex optimization to fit input-output data to a class of stable nonlinear dynamical models. We present an algorithm that guarantees consistent estimates of models in this class when a small set of repeated experiments with suitably independent measurement noise is available. Stability of the estimated models is guaranteed without any assumptions on the input-output data. We first present a convex optimization scheme for identifying stable state-space models from empirical moments. Next, we provide a method for using repeated experiments to remove the effect of noise on these moment and model estimates. The technique is demonstrated on a simple simulated example.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "In this paper we prove optimality of a certain class of Analog to Digital Converters (ADCs), which can be viewed as generalized Delta-Sigma Modulators (DSMs), with respect to a performance measure that can be characterized as the worst-case average intensity of the signal representation error. An analytic expression for the ADC performance is given. Furthermore, our result proves separation of quantization and control for this class of ADCs subject to some technical conditions.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "This paper deals with the task of finding certified lower bounds for the performance of Analog to Digital Converters (ADCs). A general ADC is modeled as a causal, discrete-time dynamical system with outputs taking values in a finite set. We define the performance of an ADC as the worst-case average intensity of the filtered input matching error, defined as the difference between the input and output of the ADC. The passband of the shaping filter used to filter the error signal determines the frequency region of interest for minimizing the error. The problem of finding a lower bound for the performance of an ADC is formulated as a dynamic game problem in which the input signal to the ADC plays against the output of the ADC. Furthermore, the performance measure must be optimized in the presence of quantized disturbances (output of the ADC) that can exceed the control variable (input of the ADC) in magnitude. We characterize the optimal solution in terms of a Bellman-type inequality. A numerical approach is presented to compute the value function in parallel with the feedback law for generating the worst case input signal. The specific structure of the problem is used to prove certain properties of the value function that simplifies the iterative computation of a certified solution to the Bellman inequality. The solution provides a certified lower bound on the performance of any ADC with respect to the selected performance criteria.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "The paper proposes a control-theoretic framework for verification of numerical software systems, and puts forward software verification as an important application of control and systems theory. The idea is to transfer Lyapunov functions and the associated computational techniques from control systems analysis and convex optimization to verification of various software safety and performance specifications. These include but are not limited to absence of overflow, absence of division-by-zero, termination in finite time, presence of dead-code, and certain user-specified assertions. Central to this framework are Lyapunov invariants. These are properly constructed functions of the program variables, and satisfy certain properties-resembling those of Lyapunov functions-along the execution trace. The search for the invariants can be formulated as a convex optimization problem. If the associated optimization problem is feasible, the result is a certificate for the specification.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "The paper proposes a control-theoretic framework for verification of numerical software systems, and puts forward software verification as an important application of control and systems theory. The idea is to transfer Lyapunov functions and the associated computational techniques from control systems analysis and convex optimization to verification of various software safety and performance specifications. These include but are not limited to absence of overflow, absence of division-by-zero, termination in finite time, presence of dead-code, and certain user-specified assertions. Central to this framework are Lyapunov invariants. These are properly constructed functions of the program variables, and satisfy certain properties-resembling those of Lyapunov functions-along the execution trace. The search for the invariants can be formulated as a convex optimization problem. If the associated optimization problem is feasible, the result is a certificate for the specification.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "A new framework for nonlinear system identification is presented in terms of optimal fitting of stable nonlinear state space equations to input/output/state data, with a performance objective defined as a measure of robustness of the simulation error with respect to equation errors. Basic definitions and analytical results are presented. The utility of the method is illustrated on a simple simulation example as well as experimental recordings from a live neuron.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "Several variations of the classical Kalman-Yakubovich-Popov Lemma, as well the associated minimax theorem are presented.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "The paper deals with the task of optimal design of Analog to Digital Converters (ADCs). A general ADC is modeled as a causal discrete-time dynamical system with outputs taking values in a finite set, and its performance is defined as the worst-case average intensity of the filtered input matching error. The design task can be viewed as that of optimal quantized decision making in a system, with the objective being to optimize the performance measure. An algorithm is proposed for designing optimal ADCs and certifying their optimality. The algorithm is based on exploiting a special structure in the underlying dynamic program, which makes it possible to find the optimal value function, and hence the optimal quantization law exactly and analytically. Moreover, the designed ADC is shown to have the classical Delta-Sigma Modulator (DSM) structure with optimal quantization step spacing.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "We demonstrate the use of a new, control-oriented notion of finite state approximation for a particular class of hybrid systems. Specifically, we consider the problem of designing a stabilizing binary output feedback switching controller for a pair of unstable homogeneous second order systems. The constructive approach presented in this note, in addition to yielding an explicit construction of a deterministic finite state approximate model of the hybrid plant, allows us to efficiently establish a useable upper bound on the quality of approximation, and leads to a discrete optimization problem whose solution immediately provides a certifiably correct-by-design controller for the original system. The resulting controller consists of a finite state observer for the plant and a corresponding full state feedback switching control law.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "This paper asks what classes of input signals are sufficient in order to completely identify the input/output behavior of generic bilinear systems. The main results are that step inputs are not sufficient, nor are single pulses, but the family of all pulses (of a fixed amplitude but varying widths) do suffice for identification.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "We develop a sufficient condition for the least-squares measurement (LSM), or the square-root measurement, to minimize the probability of a detection error when distinguishing between a collection of mixed quantum states. Using this condition we derive the optimal measurement for state sets with a broad class of symmetries.\n  We first consider geometrically uniform (GU) state sets with a possibly nonabelian generating group, and show that if the generator satisfies a certain constraint, then the LSM is optimal. In particular, for pure-state GU ensembles the LSM is shown to be optimal. For arbitrary GU state sets we show that the optimal measurement operators are GU with generator that can be computed very efficiently in polynomial time, within any desired accuracy.\n  We then consider compound GU (CGU) state sets which consist of subsets that are GU. When the generators satisfy a certain constraint, the LSM is again optimal. For arbitrary CGU state sets the optimal measurement operators are shown to be CGU with generators that can be computed efficiently in polynomial time.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "We consider the problem of designing an optimal quantum detector to minimize the probability of a detection error when distinguishing between a collection of quantum states, represented by a set of density operators. We show that the design of the optimal detector can be formulated as a semidefinite programming problem. Based on this formulation, we derive a set of necessary and sufficient conditions for an optimal quantum measurement. We then show that the optimal measurement can be found by solving a standard (convex) semidefinite program followed by the solution of a set of linear equations or, at worst, a standard linear programming problem. By exploiting the many well-known algorithms for solving semidefinite programs, which are guaranteed to converge to the global optimum, the optimal measurement can be computed very efficiently in polynomial time.\n  Using the semidefinite programming formulation, we also show that the rank of each optimal measurement operator is no larger than the rank of the corresponding density operator. In particular, if the quantum state ensemble is a pure-state ensemble consisting of (not necessarily independent) rank-one density operators, then we show that the optimal measurement is a pure-state measurement consisting of rank-one measurement operators.\n        \u25b3 Less", "author": "Alexandre Megretski"}, {"abstract": "Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e. 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that undergone gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We report on an improved measurement of the $^8$B solar neutrino interaction rate with the Borexino detector at the Laboratori Nazionali del Gran Sasso. Neutrinos are detected via their elastic scattering on electrons in a large, radio-pure liquid scintillator target. Novel analysis approaches exploiting most of the active volume of the detector have enabled the collection of data from 1.5 kt$\\cdot$y exposure between 2008 and 2016. The measured rate of solar neutrino-induced, scattered electrons above 3 MeV of energy is $0.220\\substack{+0.015 \\\\ -0.016}\\,(stat)\\,\\substack{+0.006 \\\\ -0.006}\\,(syst)$~cpd/100~t, which corresponds to an observed solar neutrino flux assuming no neutrino flavor conversion of 2.55$\\substack{+0.17 \\\\ -0.19}(stat)\\substack{+0.07\\\\ -0.07}(syst)\\times$10$^6$~cm$^{-2}\\,$s$^{-1}$. If one assumes the $^8$B solar neutrino flux predicted by the high metallicity Standard Solar Model, the average $^8$B solar $\u03bd_e$ survival probability is 0.36$\\pm$0.08 at the mean visible energy of 7.9~MeV, in good agreement with the MSW-LMA\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "A search for the solar neutrino effective magnetic moment has been performed using data from 1291.5 days exposure during the second phase of the Borexino experiment. No significant deviations from the expected shape of the electron recoil spectrum from solar neutrinos have been found, and a new upper limit on the effective neutrino magnetic moment of $\u03bc_\u03bd^{eff}$ $<$ 2.8$\\cdot$10$^{-11}$ $\u03bc_{B}$ at 90\\% c.l. has been set using constraints on the sum of the solar neutrino fluxes implied by the radiochemical gallium experiments.Using the limit for the effective neutrino moment, new limits for the magnetic moments of the neutrino flavor states, and for the elements of the neutrino magnetic moments matrix for Dirac and Majorana neutrinos, are derived.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We present the first simultaneous measurement of the interaction rates of $pp$, $^7$Be, and $pep$ solar neutrinos performed with a global fit to the Borexino data in an extended energy range (0.19-2.93)$\\,$MeV. This result was obtained by analyzing 1291.51$\\,$days of Borexino Phase-II data, collected between December 2011 and May 2016 after an extensive scintillator purification campaign. We find: rate($pp$)$\\,$=$\\,$$134$$\\,$$\\pm$$\\,$$10$$\\,$($stat$)$\\,$$^{\\rm +6}_{\\rm -10}$$\\,$($sys$)$\\,$cpd/100$\\,$t, rate($^7$Be)$\\,$=$\\,$$48.3$$\\,$$\\pm$$\\,$$1.1$$\\,$($stat$)$\\,$$^{\\rm +0.4}_{\\rm -0.7}$$\\,$($sys$)$\\,$cpd/100$\\,$t, and rate($pep$)$\\,$=$\\,$$2.43$$\\pm$$\\,$$0.36$$\\,$($stat$)$^{+0.15}_{-0.22}$$\\,$($sys$)$\\,$cpd/100$\\,$t. These numbers are in agreement with and improve the precision of our previous measurements. In particular, the interaction rate of $^7$Be $\u03bd$'s is measured with an unprecedented precision of 2.7%, showing that discriminating between the high and low metallicity solar models is now largely dominated by theoretical uncertainties. The absence of $pep$ neutrinos is rejected for the first time at more than 5$\\,$$\u03c3$. An upper limit of $8.1$$\\,$cpd/100$\\,$t (95%$\\,$C.L.) on the CNO neutrino rate is obtained by setting an additional constraint on the ratio between the $pp$ and $pep$ neutrino rates in the fit. This limit has the same significance as that obtained by the Borexino Phase-I (currently providing the tightest bound on this component), but is obtained by applying a less stringent constraint on the $pep$ $\u03bd$ flux.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We present the results of a low-energy neutrino search using the Borexino detector in coincidence with the gravitational wave (GW) events GW150914, GW151226 and GW170104. We searched for correlated neutrino events with energies greater than 250 keV within a time window of $\\pm500$ s centered around the GW detection time. A total of five candidates were found for all three GW150914, GW151226 and GW170104. This is consistent with the number of expected solar neutrino and background events. As a result, we have obtained the best current upper limits on the GW event neutrino fluence of all flavors ($\u03bd_e, \u03bd_\u03bc, \u03bd_\u03c4$) in the energy range $(0.5 - 5.0)$ MeV.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We describe the Monte Carlo (MC) simulation package of the Borexino detector and discuss the agreement of its output with data. The Borexino MC 'ab initio' simulates the energy loss of particles in all detector components and generates the resulting scintillation photons and their propagation within the liquid scintillator volume. The simulation accounts for absorption, reemission, and scattering of the optical photons and tracks them until they either are absorbed or reach the photocathode of one of the photomultiplier tubes. Photon detection is followed by a comprehensive simulation of the readout electronics response. The algorithm proceeds with a detailed simulation of the electronics chain. The MC is tuned using data collected with radioactive calibration sources deployed inside and around the scintillator volume. The simulation reproduces the energy response of the detector, its uniformity within the fiducial scintillator volume relevant to neutrino physics, and the time distribution of detected photons to better than 1% between 100 keV and several MeV. The techniques developed to simulate the Borexino detector and their level of refinement are of possible interest to the neutrino community, especially for current and future large-volume liquid scintillator experiments such as Kamland-Zen, SNO+, and Juno.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We detected the seasonal modulation of the $^7$Be neutrino interaction rate with the Borexino detector at the Laboratori Nazionali del Gran Sasso in Italy. The period, amplitude, and phase of the observed time evolution of the signal are consistent with its solar origin, and the absence of an annual modulation is rejected at 99.99\\% C.L. The data are analyzed using three methods: the sinusoidal fit, the Lomb-Scargle and the Empirical Mode Decomposition techniques, which all yield results in excellent agreement.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We search for excess gamma-ray emission coincident with the positions of confirmed and candidate Milky Way satellite galaxies using 6 years of data from the Fermi Large Area Telescope (LAT). Our sample of 45 stellar systems includes 28 kinematically confirmed dark-matter-dominated dwarf spheroidal galaxies (dSphs) and 17 recently discovered systems that have photometric characteristics consistent with the population of known dSphs. For each of these targets, the relative predicted gamma-ray flux due to dark matter annihilation is taken from kinematic analysis if available, and estimated from a distance-based scaling relation otherwise, assuming that the stellar systems are dark-matter-dominated dSphs. LAT data coincident with four of the newly discovered targets show a slight preference (each ~$2 \u03c3$ local) for gamma-ray emission in excess of the background. However, the ensemble of derived gamma-ray flux upper limits for individual targets is consistent with the expectation from analyzing random blank-sky regions, and a combined analysis of the population of stellar systems yields no globally significant excess (global significance $<1 \u03c3$). Our analysis has increased sensitivity compared to the analysis of 15 confirmed dSphs by Ackermann et al. 2015. The observed constraints on the dark matter annihilation cross section are statistically consistent with the background expectation, improving by a factor of ~2 for large dark matter masses ($m_{{\\rm DM},b \\bar b} \\gtrsim 1$ TeV and $m_{{\\rm DM},\u03c4^{+}\u03c4^{-}} \\gtrsim 70$ GeV) and weakening by a factor of ~1.5 at lower masses relative to previously observed limits.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "List of contributions from the Cherenkov Telescope Array (CTA) Consortium presented at the 6th International Symposium on High-Energy Gamma-Ray Astronomy (Gamma 2016), July 11-15, 2016, in Heidelberg, Germany.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "A search for neutrino and antineutrino events correlated with 2,350 gamma-ray bursts (GRBs) is performed with Borexino data collected between December 2007 and November 2015. No statistically significant excess over background is observed. We look for electron antineutrinos ($\\bar\u03bd_e$) that inverse beta decay on protons with energies from 1.8\\,MeV to 15\\,MeV and set the best limit on the neutrino fluence from GRBs below 8\\,MeV. The signals from neutrinos and antineutrinos from GRBs that scatter on electrons are also searched for, a detection channel made possible by the particularly radio-pure scintillator of Borexino. We obtain currently the best limits on the neutrino fluence of all flavors and species below 7\\,MeV. Finally, time correlations between GRBs and bursts of events are investigated. Our analysis combines two semi-independent data acquisition systems for the first time: the primary Borexino readout optimized for solar neutrino physics up to a few MeV, and a fast waveform digitizer system tuned for events above 1\\,MeV.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "The research community has considered in the past the application of Artificial Intelligence (AI) techniques to control and operate networks. A notable example is the Knowledge Plane proposed by D.Clark et al. However, such techniques have not been extensively prototyped or deployed in the field yet. In this paper, we explore the reasons for the lack of adoption and posit that the rise of two recent paradigms: Software-Defined Networking (SDN) and Network Analytics (NA), will facilitate the adoption of AI techniques in the context of network operation and control. We describe a new paradigm that accommodates and exploits SDN, NA and AI, and provide use cases that illustrate its applicability and benefits. We also present simple experimental results that support its feasibility. We refer to this new paradigm as Knowledge-Defined Networking (KDN).\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "The main physical results on the registration of solar neutrinos and the search for rare processes obtained by the Borexino collaboration to date are presented.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "The nature of dark matter is a longstanding enigma of physics; it may consist of particles beyond the Standard Model that are still elusive to experiments. Among indirect search techniques, which look for stable products from the annihilation or decay of dark matter particles, or from axions coupling to high-energy photons, observations of the $\u03b3$-ray sky have come to prominence over the last few years, because of the excellent sensitivity of the Large Area Telescope (LAT) on the Fermi Gamma-ray Space Telescope mission. The LAT energy range from 20 MeV to above 300 GeV is particularly well suited for searching for products of the interactions of dark matter particles. In this report we describe methods used to search for evidence of dark matter with the LAT, and review the status of searches performed with up to six years of LAT data. We also discuss the factors that determine the sensitivities of these searches, including the magnitudes of the signals and the relevant backgrounds, considering both statistical and systematic uncertainties. We project the expected sensitivities of each search method for 10 and 15 years of LAT data taking. In particular, we find that the sensitivity of searches targeting dwarf galaxies, which provide the best limits currently, will improve faster than the square root of observing time. Current LAT limits for dwarf galaxies using six years of data reach the thermal relic level for masses up to 120 GeV for the $b\\bar{b}$ annihilation channel for reasonable dark matter density profiles. With projected discoveries of additional dwarfs, these limits could extend to about 250 GeV. With as much as 15 years of LAT data these searches would be sensitive to dark matter annihilations at the thermal relic cross section for masses to greater than 400 GeV (200 GeV) in the $b\\bar{b}$ ($\u03c4^+ \u03c4^-$) annihilation channels.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "This Supplement provides supporting material for arXiv:1602.08492 . We briefly summarize past electromagnetic (EM) follow-up efforts as well as the organization and policy of the current EM follow-up program. We compare the four probability sky maps produced for the gravitational-wave transient GW150914, and provide additional details of the EM follow-up observations that were performed in the different bands.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "A gravitational-wave (GW) transient was identified in data recorded by the Advanced Laser Interferometer Gravitational-wave Observatory (LIGO) detectors on 2015 September 14. The event, initially designated G184098 and later given the name GW150914, is described in detail elsewhere. By prior arrangement, preliminary estimates of the time, significance, and sky location of the event were shared with 63 teams of observers covering radio, optical, near-infrared, X-ray, and gamma-ray wavelengths with ground- and space-based facilities. In this Letter we describe the low-latency analysis of the GW data and present the sky localization of the first observed compact binary merger. We summarize the follow-up observations reported by 25 teams via private Gamma-ray Coordinates Network circulars, giving an overview of the participating facilities, the GW sky localization coverage, the timeline and depth of the observations. As this event turned out to be a binary black hole merger, there is little expectation of a detectable electromagnetic (EM) signature. Nevertheless, this first broadband campaign to search for a counterpart of an Advanced LIGO source represents a milestone and highlights the broad capabilities of the transient astronomy community and the observing strategies that have been developed to pursue neutron star binary merger events. Detailed investigations of the EM data and results of the EM follow-up campaign are being disseminated in papers by the individual teams.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "This paper reports the first measurement using the NOvA detectors of $\u03bd_\u03bc$ disappearance in a $\u03bd_\u03bc$ beam. The analysis uses a 14 kton-equivalent exposure of $2.74 \\times 10^{20}$ protons-on-target from the Fermilab NuMI beam. Assuming the normal neutrino mass hierarchy, we measure $\u0394m^{2}_{32}=(2.52^{+0.20}_{-0.18})\\times 10^{-3}$ eV$^{2}$ and $\\sin^2\u03b8_{23}$ in the range 0.38-0.65, both at the 68% confidence level, with two statistically-degenerate best fit points at $\\sin^2\u03b8_{23} = $ 0.43 and 0.60. Results for the inverted mass hierarchy are also presented.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We report results from the first search for $\u03bd_\u03bc\\to\u03bd_e$ transitions by the NOvA experiment. In an exposure equivalent to $2.74\\times10^{20}$ protons-on-target in the upgraded NuMI beam at Fermilab, we observe 6 events in the Far Detector, compared to a background expectation of $0.99\\pm0.11$ (syst.) events based on the Near Detector measurement. A secondary analysis observes 11 events with a background of $1.07\\pm0.14$ (syst.). The $3.3\u03c3$ excess of events observed in the primary analysis disfavors $0.1\u03c0< \u03b4_{CP} < 0.5\u03c0$ in the inverted mass hierarchy at the 90% C.L.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "Borexino is a liquid scintillation detector located deep underground at the Laboratori Nazionali del Gran Sasso (LNGS, Italy). Thanks to the unmatched radio-purity of the scintillator, and to the well understood detector response at low energy, a new limit on the stability of the electron for decay into a neutrino and a single mono-energetic photon was obtained. This new bound, tau > 6.6 10**28 yr at 90 % C.L., is two orders of magnitude better than the previous limit.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "List of contributions from the CTA Consortium presented at the 34th International Cosmic Ray Conference, 30 July - 6 August 2015, The Hague, The Netherlands.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "The Sun is fueled by a series of nuclear reactions that produce the energy that makes it shine. The primary reaction is the fusion of two protons into a deuteron, a positron and a neutrino. These neutrinos constitute the vast majority of neutrinos reaching Earth, providing us with key information about what goes on at the core of our star. Several experiments have now confirmed the observation of neutrino oscillations by detecting neutrinos from secondary nuclear processes in the Sun; this is the first direct spectral measurement of the neutrinos from the keystone proton-proton fusion. This observation is a crucial step towards the completion of the spectroscopy of pp-chain neutrinos, as well as further validation of the LMA-MSW model of neutrino oscillations.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "Neutrino produced in a chain of nuclear reactions in the Sun starting from the fusion of two protons, for the first time has been detected in a real-time detector in spectrometric mode. The unique properties of the Borexino detector provided an oppurtunity to disentangle pp-neutrino spectrum from the background components. A comparison of the total neutrino flux from the Sun with Solar luminosity in photons provides a test of the stability of the Sun on the 10$^{5}$ years time scale, and sets a strong limit on the power production in the unknown energy sources in the Sun of no more than 4\\% of the total energy production at 90\\% C.L.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We report an improved geo-neutrino measurement with Borexino from 2056 days of data taking. The present exposure is $(5.5\\pm0.3)\\times10^{31}$ proton$\\times$yr. Assuming a chondritic Th/U mass ratio of 3.9, we obtain $23.7 ^{+6.5}_{-5.7} (stat) ^{+0.9}_{-0.6} (sys)$ geo-neutrino events. The null observation of geo-neutrinos with Borexino alone has a probability of $3.6 \\times 10^{-9}$ (5.9$\u03c3$). A geo-neutrino signal from the mantle is obtained at 98\\% C.L. The radiogenic heat production for U and Th from the present best-fit result is restricted to the range 23-36 TW, taking into account the uncertainty on the distribution of heat producing elements inside the Earth.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "Borexino is a unique detector able to perform measurement of solar neutrinos fluxes in the energy region around 1 MeV or below due to its low level of radioactive background. It was constructed at the LNGS underground laboratory with a goal of solar $^{7}$Be neutrino flux measurement with 5\\% precision. The goal has been successfully achieved marking the end of the first stage of the experiment. A number of other important measurements of solar neutrino fluxes have been performed during the first stage. Recently the collaboration conducted successful liquid scintillator repurification campaign aiming to reduce main contaminants in the sub-MeV energy range. With the new levels of radiopurity Borexino can improve existing and challenge a number of new measurements including: improvement of the results on the Solar and terrestrial neutrino fluxes measurements; measurement of pp and CNO solar neutrino fluxes; search for non-standard interactions of neutrino; study of the neutrino oscillations on the short baseline with an artificial neutrino source (search for sterile neutrino) in context of SOX project.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "This work is on the Physics of the B Factories. Part A of this book contains a brief description of the SLAC and KEK B Factories as well as their detectors, BaBar and Belle, and data taking related issues. Part B discusses tools and methods used by the experiments in order to obtain results. The results themselves can be found in Part C.\n  Please note that version 3 on the archive is the auxiliary version of the Physics of the B Factories book. This uses the notation alpha, beta, gamma for the angles of the Unitarity Triangle. The nominal version uses the notation phi_1, phi_2 and phi_3. Please cite this work as Eur. Phys. J. C74 (2014) 3026.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We describe searches for B meson decays to the charmless vector-vector final states omega omega and omega phi with 471 x 10^6 B Bbar pairs produced in e+ e- annihilation at sqrt(s) = 10.58 GeV using the BABAR detector at the PEP-II collider at the SLAC National Accelerator Laboratory. We measure the branching fraction B(B0 --> omega omega) = (1.2 +- 0.3 +0.3-0.2) x 10^-6, where the first uncertainty is statistical and the second is systematic, corresponding to a significance of 4.4 standard deviations. We also determine the upper limit B(B0 --> omega phi) < 0.7 x 10^-6 at 90% confidence level. These measurements provide the first evidence for the decay B0 --> omega omega, and an improvement of the upper limit for the decay B0 --> omega phi.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We study the decay $\\bar{B}^{0}\\rightarrow\u039b_{c}^{+}\\bar{p}\u03c0^{+}\u03c0^{-}$, reconstructing the \u039b_{c}^{+} baryon in the $p K^{-}\u03c0^{+}$ mode, using a data sample of $467\\times 10^{6}$ $B\\bar{B}$ pairs collected with the BaBar detector at the PEP-2 storage rings at SLAC. We measure branching fractions for decays with intermediate $\u03a3_{c}$ baryons to be ${\\cal B}[\\bar{B}^{0}\\rightarrow\u03a3_{c}(2455)^{++}\\bar{p}\u03c0^{-}]=(21.3 \\pm 1.0 \\pm 1.0 \\pm 5.5) \\times 10^{-5}$, ${\\cal B}[\\bar{B}^{0}\\rightarrow\u03a3_{c}(2520)^{++}\\bar{p}\u03c0^{-}]=(11.5\\pm 1.0 \\pm 0.5 \\pm 3.0)\\times 10^{-5}$, ${\\cal B}[\\bar{B}^{0}\\rightarrow\u03a3_{c}(2455)^{0}\\bar{p}\u03c0^{+}]=(9.1 \\pm 0.7 \\pm 0.4 \\pm 2.4)\\times10^{-5}$, and ${\\cal B}[\\bar{B}^{0}\\rightarrow\u03a3_{c}(2520)^{0}\\bar{p}\u03c0^{+}]= (2.2 \\pm 0.7 \\pm 0.1\\pm 0.6) \\times 10^{-5}$, where the uncertainties are statistical, systematic, and due to the uncertainty on the $\u039b_{c}^{+}\\rightarrow\\proton\\Km\u03c0^{+}$ branching fraction, respectively. For decays without $\u03a3_{c}(2455)$ or $\u03a3_{c}(2520)$ resonances, we measure ${\\cal B}[\\bar{B}^{0}\\rightarrow\u039b_{c}^{+}\\bar{p}\u03c0^{+}\u03c0^{-}]_{\\mathrm{non-\u03a3_{c}}}=(79 \\pm 4 \\pm 4 \\pm 20)\\times10^{-5}$. The total branching fraction is determined to be ${\\cal B}[\\bar{B}^{0}\\rightarrow\u039b_{c}^{+}\\bar{p}\u03c0^{+}\u03c0^{-}]_{\\mathrm{total}}=(123 \\pm 5 \\pm 7 \\pm 32)\\times10^{-5}$. We examine multibody mass combinations in the resonant three-particle $\u03a3_{c}\\bar{p}\u03c0$ final states and in the four-particle $\u039b_{c}^{+}\\bar{p}\u03c0^{+}\u03c0^{-}$ final state, and observe different characteristics for the $\\bar{p}\u03c0$ combination in neutral versus doubly-charged $\u03a3_{c}$ decays.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We report on a search for direct CP asymmetry in the singly Cabibbo-suppressed decay D+- --> K+ K- pi+- using a data sample of 476 fb-1 accumulated with the BaBar detector running at and just below the Y(4S) resonance. The CP-violating decay rate asymmetry A_CP is determined to be (0.35 +- 0.30 +- 0.15)%. Model-dependent and model-independent Dalitz plot analysis techniques are used to search for CP-violating asymmetries in the various intermediate states.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We report the results of a study of the exclusive charmless semileptonic decays, B^0 --> pi^- l^+ nu, B^+ --> pi^0 l^+ nu, B^+ --> omega l^+ nu, B^+ --> eta l^+ nu and B^+ --> eta^' l^+ nu, (l = e or mu) undertaken with approximately 462x10^6 B\\bar{B} pairs collected at the Upsilon(4S) resonance with the BABAR detector. The analysis uses events in which the signal B decays are reconstructed with a loose neutrino reconstruction technique. We obtain partial branching fractions in several bins of q^2, the square of the momentum transferred to the lepton-neutrino pair, for B^0 --> pi^- l^+ nu, B^+ --> pi^0 l^+ nu, B^+ --> omega l^+ nu and B^+ --> eta l^+ nu. From these distributions, we extract the form-factor shapes f_+(q^2) and the total branching fractions BF(B^0 --> pi^- l^+ nu) = (1.45 +/- 0.04_{stat} +/- 0.06_{syst})x10^-4 (combined pi^- and pi^0 decay channels assuming isospin symmetry), BF(B^+ --> omega l^+ nu) = (1.19 +/- 0.16_{stat} +/- 0.09_{syst})x10^-4 and BF(B^+ --> eta l^+ nu) = (0.38 +/- 0.05_{stat} +/- 0.05_{syst})x10^-4. We also measure BF(B^+ --> eta^' l^+ nu) = (0.24 +/- 0.08_{stat} +/- 0.03_{syst})x10^-4. We obtain values for the magnitude of the CKM matrix element V_{ub} by direct comparison with three different QCD calculations in restricted q^2 ranges of B --> pi l^+ nu decays. From a simultaneous fit to the experimental data over the full q^2 range and the FNAL/MILC lattice QCD predictions, we obtain |V_{ub}| = (3.25 +/- 0.31)x10^-3, where the error is the combined experimental and theoretical uncertainty.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "Although CP violation in the B meson system has been well established by the B factories, there has been no direct observation of time reversal violation. The decays of entangled neutral B mesons into definite flavor states ($B^0$ or $\\bar{B}^0$), and $J/\u03c8K_S^0$ or $c\\bar{c} K_S^0$ final states (referred to as $B_+$ or $B_-$), allow comparisons between the probabilities of four pairs of T-conjugated transitions, for example, $\\bar{B}^0 \\rightarrow B_-$ and $B_- \\rightarrow \\bar{B}^0$, as a function of the time difference between the two B decays. Using 468 million $B\\bar{B}$ pairs produced in $\u03a5(4S)$ decays collected by the BABAR detector at SLAC, we measure T-violating parameters in the time evolution of neutral B mesons, yielding $\u0394S_T^+ = -1.37 \\pm 0.14 (stat.) \\pm 0.06 (syst.)$ and $\u0394S_T^- = 1.17 \\pm 0.18 (stat.) \\pm 0.11 (syst.)$. These nonzero results represent the first direct observation of T violation through the exchange of initial and final states in transitions that can only be connected by a T-symmetry transformation.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We present the results of the first search for gravitational wave bursts associated with high energy neutrinos. Together, these messengers could reveal new, hidden sources that are not observed by conventional photon astronomy, particularly at high energy. Our search uses neutrinos detected by the underwater neutrino telescope ANTARES in its 5 line configuration during the period January - September 2007, which coincided with the fifth and first science runs of LIGO and Virgo, respectively. The LIGO-Virgo data were analysed for candidate gravitational-wave signals coincident in time and direction with the neutrino events. No significant coincident events were observed. We place limits on the density of joint high energy neutrino - gravitational wave emission events in the local universe, and compare them with densities of merger and core-collapse events.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We study the process e+e- -> pi+pi-pi+pi-gamma, with a photon emitted from the initial-state electron or positron, using 454.3 fb^-1 of data collected with the BABAR detector at SLAC, corresponding to approximately 260,000 signal events. We use these data to extract the non-radiative sigma(e+e- ->pi+pi-pi+pi-) cross section in the energy range from 0.6 to 4.5 Gev. The total uncertainty of the cross section measurement in the peak region is less than 3%, higher in precision than the corresponding results obtained from energy scan data.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We report a measurement of the inclusive semileptonic branching fraction of the B_s meson using data collected with the BaBar detector in the center-of-mass (CM) energy region above the Upsilon(4S) resonance. We use the inclusive yield of phi mesons and the phi yield in association with a high-momentum lepton to perform a simultaneous measurement of the semileptonic branching fraction and the production rate of B_s mesons relative to all B mesons as a function of CM energy. The inclusive semileptonic branching fraction of the B_s meson is determined to be B(B_s to l nu X)=9.5 (+2.5/-2.0)(stat)(+1.1/-1.9)(syst)%, where l indicates the average of e and mu.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We search for hadronic decays of a light Higgs boson (A0) produced in radiative decays of an Upsilon(2S) or Upsilon(3S) meson, Upsilon --> gamma A0. The data have been recorded by the BABAR experiment at the Upsilon(3S) and Upsilon(2S) center of mass energies, and include (121.3 \\pm 1.2) x 10^6 Upsilon(3S) and (98.3 \\pm 0.9) x 10^6 Upsilon(2S) mesons. No significant signal is observed. We set 90% confidence level upper limits on the product branching fractions B(Upsilon(nS)-->gamma A0) x B(A0-->hadrons) (n=2 or 3) that range from 1 x 10^{-6} for an A0 mass of 0.3 GeV/c^2 to 8 x 10^{-5} at 7 GeV/c^2.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We report the observation of the baryonic B decay B0bar --> Lambda_c^+ anti-Lambda K- with a significance larger than 7 standard deviations based on 471x10^6$ BBbar pairs collected with the BABAR detector at the PEP-II storage ring at SLAC. We measure the branching fraction for the decay B0bar --> Lambda_c^+ anti-Lambda K- to be (3.8 \\pm 0.8_{stat} \\pm 0.2_{sys} \\pm 1.0_{Lambda_c^+})x10^{-5}. The uncertainties are statistical, systematic, and due to the uncertainty in the Lambda_c^+ branching fraction. We find that the Lambda_c^+ K^- invariant mass distribution shows an enhancement above 3.5 GeV/c^2.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We use 111+/-1 million Upsilon(3S) and 89+/-1 million Upsilon(2S) events recorded by the BaBar detector at the PEP-II B-factory at SLAC to perform a study of radiative transitions between bottomonium states using photons that have been converted to e+e- pairs by the detector material. We observe Upsilon(3S) -> gamma chi_b0,2(1P) decay, make precise measurements of the branching fractions for chi_b1,2(1P,2P) -> gamma Upsilon(1S) and chi_b1,2(2P) -> gamma Upsilon(2S) decays, and search for radiative decay to the eta_b(1S) and eta_b(2S) states.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "By Monte Carlo simulations of a variant of the bond-fluctuation model without topological constraints we examine the center-of-mass (COM) dynamics of polymer melts in $d=3$ dimensions. Our analysis focuses on the COM displacement correlation function $\\CN(t) \\approx \\partial_t^2 \\MSDcmN(t)/2$, measuring the curvature of the COM mean-square displacement $\\MSDcmN(t)$. We demonstrate that $\\CN(t) \\approx -(\\RN/\\TN)^2 (\\rhostar/\u03c1) \\ f(x=t/\\TN)$ with $N$ being the chain length ($16 \\le N \\le 8192$), $\\RN\\sim N^{1/2}$ the typical chain size, $\\TN\\sim N^2$ the longest chain relaxation time, $\u03c1$ the monomer density, $\\rhostar \\approx N/\\RN^d$ the self-density and $f(x)$ a universal function decaying asymptotically as $f(x) \\sim x^{-\u03c9}$ with $\u03c9= (d+2) \\times \u03b1$ where $\u03b1= 1/4$ for $x \\ll 1$ and $\u03b1= 1/2$ for $x \\gg 1$. We argue that the algebraic decay $N \\CN(t) \\sim - t^{-5/4}$ for $t \\ll \\TN$ results from an interplay of chain connectivity and melt incompressibility giving rise to the correlated motion of chains and subchains.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "Using a sample of 122 million Upsilon(3S) events recorded with the BaBar detector at the PEP-II asymmetric-energy e+e- collider at SLAC, we search for the $h_b(1P)$ spin-singlet partner of the P-wave chi_{bJ}(1P) states in the sequential decay Upsilon(3S) --> pi0 h_b(1P), h_b(1P) --> gamma eta_b(1S). We observe an excess of events above background in the distribution of the recoil mass against the pi0 at mass 9902 +/- 4(stat.) +/- 2(syst.) MeV/c^2. The width of the observed signal is consistent with experimental resolution, and its significance is 3.1sigma, including systematic uncertainties. We obtain the value (4.3 +/- 1.1(stat.) +/- 0.9(syst.)) x 10^{-4} for the product branching fraction BF(Upsilon(3S)-->pi0 h_b) x BF(h_b-->gamma eta_b).\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "How did the universe evolve? The fine angular scale (l>1000) temperature and polarization anisotropies in the CMB are a Rosetta stone for understanding the evolution of the universe. Through detailed measurements one may address everything from the physics of the birth of the universe to the history of star formation and the process by which galaxies formed. One may in addition track the evolution of the dark energy and discover the net neutrino mass.\n  We are at the dawn of a new era in which hundreds of square degrees of sky can be mapped with arcminute resolution and sensitivities measured in microKelvin. Acquiring these data requires the use of special purpose telescopes such as the Atacama Cosmology Telescope (ACT), located in Chile, and the South Pole Telescope (SPT). These new telescopes are outfitted with a new generation of custom mm-wave kilo-pixel arrays. Additional instruments are in the planning stages.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "A detailed study is presented of the expected performance of the ATLAS detector. The reconstruction of tracks, leptons, photons, missing energy and jets is investigated, together with the performance of b-tagging and the trigger. The physics potential for a variety of interesting physics processes, within the Standard Model and beyond, is examined. The study comprises a series of notes based on simulations of the detector and physics processes, with particular emphasis given to the data expected from the first years of operation of the LHC at CERN.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "The MAGIC Project: Contributions to ICRC 2007, Merida, Mexico. Contents pages for the Contribution on behalf of the MAGIC Collaboration to the 30th ICRC that took place in July 2007 in Merida, Mexico. The contents are in html form with clickable links to the papers that exist on the Astrophysics archive. We hope that this will make it easier to access the output of the conference in a systematic way. Comments on how useful this is/ how it could be improved should be sent to michela.demaria@iuav.it.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "All but three (M87, BL Lac and 3C 279) extragalactic sources detected so far at very high energy (VHE) gamma-rays belong to the class of high-frequency peaked BL Lac (HBL) objects. This suggested to us a systematic scan of candidate sources with the MAGIC telescope, based on the compilation of X-ray blazars by Donato et al. (2001). The observations took place from December 2004 to March 2006 and cover sources on the northern sky visible under small zenith distances zd < 30 degrees at culmination. The sensitivity of the search was planned for detecting X-ray bright F(1 keV) > 2 uJy) sources emitting at least the same energy flux at 200 GeV as at 1 keV. In order to avoid strong gamma-ray attenuation close to the energy threshold, the redshift of the sources was constrained to values z<0.3.\n  Of the fourteen sources observed, 1ES 1218+304 and 1ES 2344+514 have been detected in addition to the known bright TeV blazars Mrk 421 and Mrk 501. A marginal excess of 3.5 sigma from the position of 1ES 1011+496 was observed and has been confirmed as a source of VHE gamma-rays by a second MAGIC observation triggered by a high optical state (Albert et al. 2007). For the remaining sources, we present here the 99% confidence level upper limits on the integral flux above ~200 GeV.\n  We characterize the sample of HBLs (including all HBLs detected at VHE so far) by looking for correlations between their multi-frequency spectral indices determined from simultaneous optical, archival X-ray, and radio luminosities, finding that the VHE emitting HBLs do not seem to constitute a unique subclass. The absorption corrected gamma-ray luminosities at 200 GeV of the HBLs are generally not higher than their X-ray luminosities at 1 keV.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "We present late-time optical and mid-infrared observations of the Type-II supernova 2003gd in NGC 628. Mid-infrared excesses consistent with cooling dust in the ejecta are observed 499-678 days after outburst, and are accompanied by increasing optical extinction and growing asymmetries in the emission-line profiles. Radiative-transfer models show that up to 0.02 solar masses of dust has formed within the ejecta, beginning as early as 250 days after outburst. These observations show that dust formation in supernova ejecta can be efficient and that massive-star supernovae can be major dust producers throughout the history of the Universe.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "The MAGIC collaboration has studied the high peaked BL-Lac object 1ES1218+30.4 at a redshift z = 0.182, using the MAGIC imaging air Cherenkov telescope located on the Canary island of La Palma. A gamma-ray signal was observed with 6.4sigma significance. The differential energy spectrum for an energy threshold of 120GeV can be fitted by a simple power law yielding F_E(E) = (8.1+-2.1)*10^-7 (E/250GeV)^(-3.0+-0.4) TeV^-1 m^-2 s^-1. During the six days of observation in January 2005 no time variability on time scales of days was found within the statistical errors. The observed integral flux above 350GeV is nearly a factor two below the the upper limit reported by the Whipple Collaboration in 2003.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "The long-duration GRB050713a was observed by the MAGIC Telescope, 40 seconds after the burst onset, and followed up for 37 minutes, until twilight. The observation, triggered by a SWIFT alert, covered energies above ~175 GeV. Using standard MAGIC analysis, no evidence for a gamma signal was found. As the redshift of the GRB was not measured directly, the flux upper limit, estimated by MAGIC, is still compatible with the assumption of an unbroken power-law spectrum extending from a few hundred keV to our energy range.\n        \u25b3 Less", "author": "Albert Meyer"}, {"abstract": "A public ledger is a tamperproof sequence of data that can be read and augmented by everyone. Public ledgers have innumerable and compelling uses. They can secure, in plain sight, all kinds of transactions ---such as titles, sales, and payments--- in the exact order in which they occur. Public ledgers not only curb corruption, but also enable very sophisticated applications ---such as cryptocurrencies and smart contracts. They stand to revolutionize the way a democratic society operates. As currently implemented, however, they scale poorly and cannot achieve their potential.\n  Algorand is a truly democratic and efficient way to implement a public ledger. Unlike prior implementations based on proof of work, it requires a negligible amount of computation, and generates a transaction history that will not \"fork\" with overwhelmingly high probability.\n  Algorand is based on (a novel and super fast) message-passing Byzantine agreement.\n  For concreteness, we shall describe Algorand only as a money platform.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "Johnson-Lindenstrauss (JL) matrices implemented by sparse random synaptic connections are thought to be a prime candidate for how convergent pathways in the brain compress information. However, to date, there is no complete mathematical support for such implementations given the constraints of real neural tissue. The fact that neurons are either excitatory or inhibitory implies that every so implementable JL matrix must be sign-consistent (i.e., all entries in a single column must be either all non-negative or all non-positive), and the fact that any given neuron connects to a relatively small subset of other neurons implies that the JL matrix had better be sparse.\n  We construct sparse JL matrices that are sign-consistent, and prove that our construction is essentially optimal. Our work answers a mathematical question that was triggered by earlier work and is necessary to justify the existence of JL compression in the brain, and emphasizes that inhibition is crucial if neurons are to perform efficient, correlation-preserving compression.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "We analyze the Vickrey mechanism for auctions of multiple identical goods when the players have both Knightian uncertainty over their own valuations and incomplete preferences. In this model, the Vickrey mechanism is no longer dominant-strategy, and we prove that all dominant-strategy mechanisms are inadequate. However, we also prove that, in undominated strategies, the social welfare produced by the Vickrey mechanism in the worst case is not only very good, but also essentially optimal.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "We consider players that have very limited knowledge about their own valuations. Specifically, the only information that a Knightian player $i$ has about the profile of true valuations, $\u03b8^*$, consists of a set of distributions, from one of which $\u03b8_i^*$ has been drawn.\n  We prove a ``robustness'' theorem for Knightian players in single-parameter domains: every mechanism that is weakly dominant-strategy truthful for classical players continues to be well-behaved for Knightian players that choose undominated strategies.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "We consider auctions in which the players have very limited knowledge about their own valuations. Specifically, the only information that a Knightian player $i$ has about the profile of true valuations, $\u03b8^*$, consists of a set of distributions, from one of which $\u03b8_i^*$ has been drawn.\n  The VCG mechanism guarantees very high social welfare both in single- and multi-good auctions, so long as Knightian players do not select strategies that are dominated. With such Knightian players, however, we prove that the VCG mechanism guarantees very poor social welfare in unrestricted combinatorial auctions.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "We consider auctions in which the players have very limited knowledge about their own valuations. Specifically, the only information that a Knightian player $i$ has about the profile of true valuations, $\u03b8^*$, consists of a set of distributions, from one of which $\u03b8_i^*$ has been drawn.\n  We analyze the social-welfare performance of the VCG mechanism, for unrestricted combinatorial auctions, when Knightian players that either (a) choose a regret-minimizing strategy, or (b) resort to regret minimization only to refine further their own sets of undominated strategies, if needed. We prove that this performance is very good.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "We relate the strategy sets that a player ends up with after refining his own strategies according to two very different models of rationality: namely, utility maximization and regret minimization.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "We study single-good auctions in a setting where each player knows his own valuation only within a constant multiplicative factor \u03b4 in (0,1), and the mechanism designer knows \u03b4. The classical notions of implementation in dominant strategies and implementation in undominated strategies are naturally extended to this setting, but their power is vastly different.\n  On the negative side, we prove that no dominant-strategy mechanism can guarantee social welfare that is significantly better than that achievable by assigning the good to a random player.\n  On the positive side, we provide tight upper and lower bounds for the fraction of the maximum social welfare achievable in undominated strategies, whether deterministically or probabilistically.\n        \u25b3 Less", "author": "Silvio Micali"}, {"abstract": "We report the independent discovery of the transient source AT2018zr during commission observations of the Zwicky Transient Facility (ZTF), the first tidal disruption event (TDE) found in this survey. The ZTF light curve of the TDE samples the rise-to-peak exceptionally well, with 50 days of g and r-band detections before the time of maximum light. We also present our multi-wavelength follow-up observations that were triggered by the discovery of this flare: the detection of a weak X-ray source ($\u03bdL_\u03bd\\sim 10^{41}$ erg/s) and a stringent upper limit to the radio emission. The X-ray emission shows a thermal spectrum ($kT \\approx 100$ eV) and is two orders of magnitude fainter than the contemporaneous optical/UV blackbody luminosity. We use observations of 128 known active galactic nuclei (AGN) to assess the quality of the ZTF astrometry, finding a median host-flare distance of 0.2\" for genuine nuclear flares. Using ZTF observations of variability from known AGN and SNe we show how these sources can be separated from TDEs. A combination of light curve shape, color, and location in the host galaxy can be used to select a clean TDE sample from multi-band optical surveys such as ZTF or LSST.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "LUX-ZEPLIN (LZ) is a next generation dark matter direct detection experiment that will operate 4850 feet underground at the Sanford Underground Research Facility (SURF) in Lead, South Dakota, USA. Using a two-phase xenon detector with an active mass of 7 tonnes, LZ will search primarily for low-energy interactions with Weakly Interacting Massive Particles (WIMPs), which are hypothesized to make up the dark matter in our galactic halo. In this paper, the projected WIMP sensitivity of LZ is presented based on the latest background estimates and simulations of the detector. For a 1000 live day run using a 5.6 tonne fiducial mass, LZ is projected to exclude at 90% confidence level spin-independent WIMP-nucleon cross sections above $1.6 \\times 10^{-48}$ cm$^{2}$ for a 40 $\\mathrm{GeV}/c^{2}$ mass WIMP. Additionally, a $5\u03c3$ discovery potential is projected reaching cross sections below the existing and projected exclusion limits of similar experiments that are currently operating. For spin-dependent WIMP-neutron(-proton) scattering, a sensitivity of $2.7 \\times 10^{-43}$ cm$^{2}$ ($8.1 \\times 10^{-42}$ cm$^{2}$) for a 40 $\\mathrm{GeV}/c^{2}$ mass WIMP is expected. With construction well underway, LZ is on track for underground installation at SURF in 2019 and will start collecting data in 2020.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The \\MJ\\ Collaboration is operating an array of high purity Ge detectors to search for neutrinoless double-beta decay in $^{76}$Ge. The \\MJ\\ \\DEM\\ comprises 44.1~kg of Ge detectors (29.7 kg enriched in $^{76}$Ge) split between two modules contained in a low background shield at the Sanford Underground Research Facility in Lead, South Dakota. Here we present results from data taken during construction, commissioning, and the start of full operations. We achieve unprecedented energy resolution of 2.5 keV FWHM at \\qval\\ and a very low background with no observed candidate events in 10 kg yr of enriched Ge exposure, resulting in a lower limit on the half-life of $1.9\\times10^{25}$ yr (90\\% CL). This result constrains the effective Majorana neutrino mass to below 240 to 520 meV, depending on the matrix elements used. In our experimental configuration with the lowest background, the background is $4.0_{-2.5}^{+3.1}$ counts/(FWHM t yr).\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "We present PEC, an Event Calculus (EC) style action language for reasoning about probabilistic causal and narrative information. It has an action language style syntax similar to that of the EC variant Modular-E. Its semantics is given in terms of possible worlds which constitute possible evolutions of the domain, and builds on that of EFEC, an epistemic extension of EC. We also describe an ASP implementation of PEC and show the sense in which this is sound and complete.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "We present a pilot analysis of the influence of galaxy stellar mass and cluster environment on the probability of slow rotation in 22 central galaxies at mean redshift $z=0.07$. This includes new integral-field observations of 5 central galaxies selected from the Sloan Digital Sky Survey, observed with the SPIRAL integral-field spectrograph on the Anglo-Australian Telescope. The composite sample presented here spans a wide range of stellar masses, $10.9<$log(M$_{*}/$M$_{\\odot})<12.0$, and are embedded in halos ranging from groups to clusters, $12.9<$log(M$_{200}/$M$_{\\odot})<15.6$. We find a mean probability of slow rotation in our sample of P(SR)$=54\\pm7$percent. Our results show an increasing probability of slow rotation in central galaxies with increasing stellar mass. However, when we examine the dependence of slow rotation on host cluster halo mass we do not see a significant relationship. We also explore the influence of cluster dominance on slow rotation in central galaxies. Clusters with low dominance are associated with dynamically younger systems. We find that cluster dominance has no significant effect on the probability of slow rotation in central galaxies. These results conflict with a paradigm in which halo mass alone predetermines central galaxy properties.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The next grand challenges for society and science are in the brain sciences. A collection of 60+ scientists from around the world, together with 10+ observers from national, private, and foundations, spent two days together discussing the top challenges that we could solve as a global community in the next decade. We eventually settled on three challenges, spanning anatomy, physiology, and medicine. Addressing all three challenges requires novel computational infrastructure. The group proposed the advent of The International Brain Station (TIBS), to address these challenges, and launch brain sciences to the next level of understanding.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "Constructing a good conference schedule for a large multi-track conference needs to take into account the preferences and constraints of organizers, authors, and attendees. Creating a schedule which has fewer conflicts for authors and attendees, and thematically coherent sessions is a challenging task.\n  Cobi introduced an alternative approach to conference scheduling by engaging the community to play an active role in the planning process. The current Cobi pipeline consists of committee-sourcing and author-sourcing to plan a conference schedule. We further explore the design space of community-sourcing by introducing attendee-sourcing -- a process that collects input from conference attendees and encodes them as preferences and constraints for creating sessions and schedule. For CHI 2014, a large multi-track conference in human-computer interaction with more than 3,000 attendees and 1,000 authors, we collected attendees' preferences by making available all the accepted papers at the conference on a paper recommendation tool we built called Confer, for a period of 45 days before announcing the conference program (sessions and schedule). We compare the preferences marked on Confer with the preferences collected from Cobi's author-sourcing approach. We show that attendee-sourcing can provide insights beyond what can be discovered by author-sourcing. For CHI 2014, the results show value in the method and attendees' participation. It produces data that provides more alternatives in scheduling and complements data collected from other methods for creating coherent sessions and reducing conflicts.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The T2K off-axis near detector, ND280, is used to make the first differential cross-section measurements of electron neutrino charged current interactions at energies ~1 GeV as a function of electron momentum, electron scattering angle and four-momentum transfer of the interaction. The total flux-averaged $\u03bd_e$ charged current cross-section on carbon is measured to be $1.11\\pm0.09~(stat)\\pm0.18~(syst)\\times10^{-38} cm^2/nucleon$. The differential and total cross-section measurements agree with the predictions of two leading neutrino interaction generators, NEUT and GENIE. The NEUT prediction is $1.23\\times10^{-38} cm^2/nucleon$ and the GENIE prediction is $1.08\\times10^{-38} cm^2/nucleon$. The total $\u03bd_e$ charged current cross-section result is also in agreement with data from the Gargamelle experiment.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "We report a measurement of the $\u03bd_\u03bc$ inclusive charged current cross sections on iron and hydrocarbon in the T2K on-axis neutrino beam. The measured inclusive charged current cross sections on iron and hydrocarbon averaged over the T2K on-axis flux with a mean neutrino energy of 1.51 GeV are $(1.444\\pm0.002(stat.)_{-0.157}^{+0.189}(syst.))\\times 10^{-38}\\mathrm{cm}^2/\\mathrm{nucleon}$, and $(1.379\\pm0.009(stat.)_{-0.147}^{+0.178}(syst.))\\times 10^{-38}\\mathrm{cm}^2/\\mathrm{nucleon}$, respectively, and their cross section ratio is $1.047\\pm0.007(stat.)\\pm0.035(syst.)$. These results agree well with the predictions of the neutrino interaction model, and thus we checked the correct treatment of the nuclear effect for iron and hydrocarbon targets in the model within the measurement precisions.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "This paper develops a Reasoning about Actions and Change framework integrated with Default Reasoning, suitable as a Knowledge Representation and Reasoning framework for Story Comprehension. The proposed framework, which is guided strongly by existing knowhow from the Psychology of Reading and Comprehension, is based on the theory of argumentation from AI. It uses argumentation to capture appropriate solutions to the frame, ramification and qualification problems and generalizations of these problems required for text comprehension. In this first part of the study the work concentrates on the central problem of integration (or elaboration) of the explicit information from the narrative in the text with the implicit (in the readers mind) common sense world knowledge pertaining to the topic(s) of the story given in the text. We also report on our empirical efforts to gather background common sense world knowledge used by humans when reading a story and to evaluate, through a prototype system, the ability of our approach to capture both the majority and the variability of understanding of a story by the human readers in the experiments.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "We report the first measurement of the neutrino-oxygen neutral-current quasielastic (NCQE) cross section. It is obtained by observing nuclear deexcitation $\u03b3$-rays which follow neutrino-oxygen interactions at the Super-Kamiokande water Cherenkov detector. We use T2K data corresponding to $3.01 \\times 10^{20}$ protons on target. By selecting only events during the T2K beam window and with well-reconstructed vertices in the fiducial volume, the large background rate from natural radioactivity is dramatically reduced. We observe 43 events in the $4-30$ MeV reconstructed energy window, compared with an expectation of 51.0, which includes an estimated 16.2 background events. The background is primarily nonquasielastic neutral-current interactions and has only 1.2 events from natural radioactivity. The flux-averaged NCQE cross section we measure is $1.55 \\times 10^{-38}$ cm$^2$ with a 68\\% confidence interval of $(1.22, 2.20) \\times 10^{-38}$ cm$^2$ at a median neutrino energy of 630 MeV, compared with the theoretical prediction of $2.01 \\times 10^{-38}$ cm$^2$.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The T2K experiment has reported the first observation of the appearance of electron neutrinos in a muon neutrino beam. The main and irreducible background to the appearance signal comes from the presence in the neutrino beam of a small intrinsic component of electron neutrinos originating from muon and kaon decays. In T2K, this component is expected to represent 1.2% of the total neutrino flux. A measurement of this component using the near detector (ND280), located 280 m from the target, is presented. The charged current interactions of electron neutrinos are selected by combining the particle identification capabilities of both the time projection chambers and electromagnetic calorimeters of ND280. The measured ratio between the observed electron neutrino beam component and the prediction is 1.01+-0.10 providing a direct confirmation of the neutrino fluxes and neutrino cross section modeling used for T2K neutrino oscillation analyses. Electron neutrinos coming from muons and kaons decay are also separately measured, resulting in a ratio with respect to the prediction of 0.68+-0.30 and 1.10+-0.14, respectively.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "New data from the T2K neutrino oscillation experiment produce the most precise measurement of the neutrino mixing parameter theta_{23}. Using an off-axis neutrino beam with a peak energy of 0.6 GeV and a data set corresponding to 6.57 x 10^{20} protons on target, T2K has fit the energy-dependent nu_mu oscillation probability to determine oscillation parameters. Marginalizing over the values of other oscillation parameters yields sin^2 (theta_{23}) = 0.514 +0.055/-0.056 (0.511 +- 0.055), assuming normal (inverted) mass hierarchy. The best-fit mass-squared splitting for normal hierarchy is Delta m^2_{32} = (2.51 +- 0.10) x 10^{-3} eV^2/c^4 (inverted hierarchy: Delta m^2_{13} = (2.48 +- 0.10) x 10^{-3} eV^2/c^4). Adding a model of multinucleon interactions that affect neutrino energy reconstruction is found to produce only small biases in neutrino oscillation parameter extraction at current levels of statistical uncertainty.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The T2K experiment has observed electron neutrino appearance in a muon neutrino beam produced 295 km from the Super-Kamiokande detector with a peak energy of 0.6 GeV. A total of 28 electron neutrino events were detected with an energy distribution consistent with an appearance signal, corresponding to a significance of 7.3$\u03c3$ when compared to 4.92 $\\pm$ 0.55 expected background events. In the PMNS mixing model, the electron neutrino appearance signal depends on several parameters including three mixing angles $\u03b8_{12}$, $\u03b8_{23}$, $\u03b8_{13}$, a mass difference $\u0394m^2_{32}$ and a CP violating phase $\u03b4_{\\mathrm{CP}}$. In this neutrino oscillation scenario, assuming $|\u0394m^2_{32}| = 2.4 \\times 10^{-3}$ $\\rm eV^2$, $\\sin^2 \u03b8_{23} = 0.5$, and $\u0394m^2_{32} >0$ ($\u0394m^2_{32} <0$), a best-fit value of $\\sin^2 2 \u03b8_{13}$ = $0.140^{+0.038}_{-0.032}$ ($0.170^{+0.045}_{-0.037}$) is obtained at $\u03b4_{\\mathrm{CP}}=0$. When combining the result with the current best knowledge of oscillation parameters including the world average value of $\u03b8_{13}$ from reactor experiments, some values of $\u03b4_{\\mathrm{CP}}$ are disfavored at the 90% CL.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The T2K collaboration reports a precision measurement of muon neutrino disappearance with an off-axis neutrino beam with a peak energy of 0.6 GeV. Near detector measurements are used to constrain the neutrino flux and cross section parameters. The Super-Kamiokande far detector, which is 295 km downstream of the neutrino production target, collected data corresponding to $3.01 \\times 10^{20}$ protons on target. In the absence of neutrino oscillations, $205 \\pm 17$ (syst.) events are expected to be detected and only 58 muon neutrino event candidates are observed. A fit to the neutrino rate and energy spectrum assuming three neutrino flavors, normal mass hierarchy and $\u03b8_{23}\\leq \u03c0/4$ yields a best-fit mixing angle $\\sin^2(2\u03b8_{23})=1.000$ and mass splitting $|\u0394m^2_{32}| =2.44 \\times 10^{-3}$ eV$^2$/c$^4$. If $\u03b8_{23}\\geq \u03c0/4$ is assumed, the best-fit mixing angle changes to $\\sin^2(2\u03b8_{23})=0.999$ and the mass splitting remains unchanged.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "This White Paper, submitted to the recent ESA call for science themes to define its future large missions, advocates the need for a transformational leap in our understanding of two key questions in astrophysics: 1) How does ordinary matter assemble into the large scale structures that we see today? 2) How do black holes grow and shape the Universe? Hot gas in clusters, groups and the intergalactic medium dominates the baryonic content of the local Universe. To understand the astrophysical processes responsible for the formation and assembly of these large structures, it is necessary to measure their physical properties and evolution. This requires spatially resolved X-ray spectroscopy with a factor 10 increase in both telescope throughput and spatial resolving power compared to currently planned facilities. Feedback from supermassive black holes is an essential ingredient in this process and in most galaxy evolution models, but it is not well understood. X-ray observations can uniquely reveal the mechanisms launching winds close to black holes and determine the coupling of the energy and matter flows on larger scales. Due to the effects of feedback, a complete understanding of galaxy evolution requires knowledge of the obscured growth of supermassive black holes through cosmic time, out to the redshifts where the first galaxies form. X-ray emission is the most reliable way to reveal accreting black holes, but deep survey speed must improve by a factor ~100 over current facilities to perform a full census into the early Universe. The Advanced Telescope for High Energy Astrophysics (Athena+) mission provides the necessary performance (e.g. angular resolution, spectral resolution, survey grasp) to address these questions and revolutionize our understanding of the Hot and Energetic Universe. These capabilities will also provide a powerful observatory to be used in all areas of astrophysics.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The T2K collaboration reports evidence for electron neutrino appearance at the atmospheric mass splitting, |\u0394m_{32}^2|=2.4x10^{-3} eV^2. An excess of electron neutrino interactions over background is observed from a muon neutrino beam with a peak energy of 0.6 GeV at the Super-Kamiokande (SK) detector 295 km from the beam's origin. Signal and background predictions are constrained by data from near detectors located 280 m from the neutrino production target. We observe 11 electron neutrino candidate events at the SK detector when a background of 3.3\\pm0.4(syst.) events is expected. The background-only hypothesis is rejected with a p-value of 0.0009 (3.1\u03c3), and a fit assuming \u03bd_\u03bc->\u03bd_e oscillations with sin^2(2\u03b8_{23})=1, \u03b4_{CP}=0 and |\u0394m_{32}^2|=2.4x10^{-3} eV^2 yields sin^2(2\u03b8_{13})=0.088^{+0.049}_{-0.039}(stat.+syst.).\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "T2K has performed the first measurement of \u03bd\u03bc inclusive charged current interactions on carbon at neutrino energies of ~1 GeV where the measurement is reported as a flux-averaged double differential cross section in muon momentum and angle. The flux is predicted by the beam Monte Carlo and external data, including the results from the NA61/SHINE experiment. The data used for this measurement were taken in 2010 and 2011, with a total of 10.8 x 10^{19} protons-on-target. The analysis is performed on 4485 inclusive charged current interaction candidates selected in the most upstream fine-grained scintillator detector of the near detector. The flux-averaged total cross section is <\u03c3_CC>_\u03c6=(6.91 +/- 0.13 (stat) +/- 0.84 (syst)) x10^{-39} cm^2/nucleon for a mean neutrino energy of 0.85 GeV.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The Tokai-to-Kamioka (T2K) experiment studies neutrino oscillations using an off-axis muon neutrino beam with a peak energy of about 0.6 GeV that originates at the J-PARC accelerator facility. Interactions of the neutrinos are observed at near detectors placed at 280 m from the production target and at the far detector -- Super-Kamiokande (SK) -- located 295 km away. The flux prediction is an essential part of the successful prediction of neutrino interaction rates at the T2K detectors and is an important input to T2K neutrino oscillation and cross section measurements. A FLUKA and GEANT3 based simulation models the physical processes involved in the neutrino production, from the interaction of primary beam protons in the T2K target, to the decay of hadrons and muons that produce neutrinos. The simulation uses proton beam monitor measurements as inputs. The modeling of hadronic interactions is re-weighted using thin target hadron production data, including recent charged pion and kaon measurements from the NA61/SHINE experiment. For the first T2K analyses the uncertainties on the flux prediction are evaluated to be below 15% near the flux peak. The uncertainty on the ratio of the flux predictions at the far and near detectors is less than 2% near the flux peak.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "[abridged] The radio:X-ray correlation for hard and quiescent state black hole X-ray binaries is critically investigated in this paper. New observations of known sources, along with newly discovered ones, have resulted in an increasingly large number of outliers lying well outside the scatter about the quoted best-fit relation. Here, we employ and compare state of the art data clustering techniques in order to identify and characterize different data groupings within the radio:X-ray luminosity plane for 18 hard and quiescent state black hole X-ray binaries with nearly simultaneous multi-wavelength coverage. Linear regression is then carried out on the clustered data to infer the parameters of a relationship of the form {ell}_{r}=alpha+beta {ell}_x through a Bayesian approach (where {ell} denotes log lum). We conclude that the two cluster model, with independent linear fits, is a significant improvement over fitting all points as a single cluster. While the upper track slope (0.63\\pm0.03) is consistent, within the errors, with the fitted slope for the 2003 relation (0.7\\pm0.1), the lower track slope (0.98\\pm0.08) is not consistent with the upper track, nor it is with the widely adopted value of ~1.4 for the neutron stars. The two luminosity tracks do not reflect systematic differences in black hole spins as estimated either from reflection, or continuum fitting method. These results are insensitive to the selection of sub-samples, accuracy in the distances, and to the treatment of upper limits. Besides introducing a further level of complexity in understanding the interplay between synchrotron and Comptonised emission from black hole X-ray binaries, the existence of two tracks in the radio:X-ray domain underscores that a high level of caution must be exercised when employing black hole luminosity relations for the purpose of estimating a third parameter, such as distance or mass.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "We report a measurement of muon-neutrino disappearance in the T2K experiment. The 295-km muon-neutrino beam from Tokai to Kamioka is the first implementation of the off-axis technique in a long-baseline neutrino oscillation experiment. With data corresponding to 1.43 10**20 protons on target, we observe 31 fully-contained single muon-like ring events in Super-Kamiokande, compared with an expectation of 104 +- 14 (syst) events without neutrino oscillations. The best-fit point for two-flavor nu_mu -> nu_tau oscillations is sin**2(2 theta_23) = 0.98 and |\u0394m**2_32| = 2.65 10**-3 eV**2. The boundary of the 90 % confidence region includes the points (sin**2(2 theta_23),|\u0394m**2_32|) = (1.0, 3.1 10**-3 eV**2), (0.84, 2.65 10**-3 eV**2) and (1.0, 2.2 10**-3 eV**2).\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "Precise measurement of neutrino beam direction and intensity was achieved based on a new concept with modularized neutrino detectors. INGRID (Interactive Neutrino GRID) is an on-axis near detector for the T2K long baseline neutrino oscillation experiment. INGRID consists of 16 identical modules arranged in horizontal and vertical arrays around the beam center. The module has a sandwich structure of iron target plates and scintillator trackers. INGRID directly monitors the muon neutrino beam profile center and intensity using the number of observed neutrino events in each module. The neutrino beam direction is measured with accuracy better than 0.4 mrad from the measured profile center. The normalized event rate is measured with 4% precision.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "Neutrinoless double-beta decay experiments can potentially determine the Majorana or Dirac nature of the neutrino, and aid in understanding the neutrino absolute mass scale and hierarchy. Future 76Ge-based searches target a half-life sensitivity of >10^27 y to explore the inverted neutrino mass hierarchy. Reaching this sensitivity will require a background rate of <1 count tonne^-1 y^-1 in a 4-keV-wide spectral region of interest surrounding the Q value of the decay. We investigate the overburden required to reach this background goal in a tonne-scale experiment with a compact (copper and lead) shield based on Monte Carlo calculations of cosmic-ray background rates. We find that, in light of the presently large uncertainties in these types of calculations, a site with an underground depth >~5200 mwe is required for a tonne-scale experiment with a compact shield similar to the planned 40-kg MAJORANA DEMONSTRATOR. The required overburden is highly dependent on the chosen shielding configuration and could be relaxed significantly if, for example, a liquid cryogen and water shield, or an active neutron shield were employed. Operation of the MAJORANA DEMONSTRATOR and GERDA detectors will serve to reduce the uncertainties on cosmic-ray background rates and will impact the choice of shielding style and location for a future tonne-scale experiment.\n  4/2013: The peer review process revealed that one of the veto rejection factors (the factor-of-4 described on p12) needs to be better established. Our reevaluation of this parameter to date has not yielded strong support for the value stated in the manuscript, and we require further study to develop a solid estimate. This further study will supersede the work described in this manuscript, and may or may not lead to the same conclusion regarding the ~>5200 mwe requirement for future tonne-scale 76Ge neutrinoless double beta decay experiments.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The observation of neutrinoless double-beta decay would determine whether the neutrino is a Majorana particle and provide information on the absolute scale of neutrino mass. The MAJORANA Collaboration is constructing the DEMONSTRATOR, an array of germanium detectors, to search for neutrinoless double-beta decay of 76-Ge. The DEMONSTRATOR will contain 40 kg of germanium; up to 30 kg will be enriched to 86% in 76-Ge. The DEMONSTRATOR will be deployed deep underground in an ultra-low-background shielded environment. Operation of the DEMONSTRATOR aims to determine whether a future tonne-scale germanium experiment can achieve a background goal of one count per tonne-year in a 4-keV region of interest around the 76-Ge neutrinoless double-beta decay Q-value of 2039 keV.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The T2K experiment observes indications of $\u03bd_\u03bc\\rightarrow \u03bd_e$ appearance in data accumulated with $1.43\\times10^{20}$ protons on target. Six events pass all selection criteria at the far detector. In a three-flavor neutrino oscillation scenario with $|\u0394m_{23}^2|=2.4\\times10^{-3}$ eV$^2$, $\\sin^2 2\u03b8_{23}=1$ and $\\sin^2 2\u03b8_{13}=0$, the expected number of such events is 1.5$\\pm$0.3(syst.). Under this hypothesis, the probability to observe six or more candidate events is 7$\\times10^{-3}$, equivalent to 2.5$\u03c3$ significance. At 90% C.L., the data are consistent with 0.03(0.04)$<\\sin^2 2\u03b8_{13}<$ 0.28(0.34) for $\u03b4_{\\rm CP}=0$ and a normal (inverted) hierarchy.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "The T2K experiment is a long-baseline neutrino oscillation experiment. Its main goal is to measure the last unknown lepton sector mixing angle \u03b8_{13} by observing \u03bd_e appearance in a \u03bd_\u03bc beam. It also aims to make a precision measurement of the known oscillation parameters, \u0394m^{2}_{23} and sin^{2} 2\u03b8_{23}, via \u03bd_\u03bc disappearance studies. Other goals of the experiment include various neutrino cross section measurements and sterile neutrino searches. The experiment uses an intense proton beam generated by the J-PARC accelerator in Tokai, Japan, and is composed of a neutrino beamline, a near detector complex (ND280), and a far detector (Super-Kamiokande) located 295 km away from J-PARC. This paper provides a comprehensive review of the instrumentation aspect of the T2K experiment and a summary of the vital information for each subsystem.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "(Abridged) We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). A vast array of science will be enabled by a single wide-deep-fast sky survey, and LSST will have unique survey capability in the faint time domain. The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the Solar System, exploring the transient optical sky, and mapping the Milky Way. LSST will be a wide-field ground-based system sited at Cerro Pach\u00f3n in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg$^2$ field of view, and a 3.2 Gigapixel camera. The standard observing sequence will consist of pairs of 15-second exposures in a given field, with two such visits in each pointing in a given night. With these repeats, the LSST system is capable of imaging about 10,000 square degrees of sky in a single filter in three nights. The typical 5$\u03c3$ point-source depth in a single visit in $r$ will be $\\sim 24.5$ (AB). The project is in the construction phase and will begin regular survey operations by 2022. The survey area will be contained within 30,000 deg$^2$ with $\u03b4<+34.5^\\circ$, and will be imaged multiple times in six bands, $ugrizy$, covering the wavelength range 320--1050 nm. About 90\\% of the observing time will be devoted to a deep-wide-fast survey mode which will uniformly observe a 18,000 deg$^2$ region about 800 times (summed over all six bands) during the anticipated 10 years of operations, and yield a coadded map to $r\\sim27.5$. The remaining 10\\% of the observing time will be allocated to projects such as a Very Deep and Fast time domain survey. The goal is to make LSST data products, including a relational database of about 32 trillion observations of 40 billion objects, available to the public and scientists around the world.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "Using the Chandra X-ray Observatory, we have detected the black hole transients V4641 Sgr and XTE J1859+226 in their low luminosity, quiescent states. The 0.3-8 keV luminosities are (4.0^(+3.3)_(-2.4))E31 (d/7 kpc)^2 erg/s and (4.2^(+4.8)_(-2.2))E31 (d/11 kpc)^2 erg/s for V4641 Sgr and XTE J1859+226, respectively. With the addition of these 2 systems, 14 out of the 15 transients with confirmed black holes (via compact object mass measurements) now have measured quiescent luminosities or sensitive upper limits. The only exception is GRS 1915+105, which has not been in quiescence since its discovery in 1992. The luminosities for V4641 Sgr and XTE J1859+226 are consistent with the median luminosity of 2E31 erg/s for the systems with previous detections. Our analysis suggests that the quiescent X-ray spectrum of V4641 Sgr is harder than for the other systems in this group, but, due to the low statistical quality of the spectrum, it is not clear if V4641 Sgr is intrinsically hard or if the column density is higher than the interstellar value. Focusing on V4641 Sgr, we compare our results to theoretical models for X-ray emission from black holes in quiescence. Also, we obtain precise X-ray positions for V4641 Sgr and XTE J1859+226 via cross-correlation of the X-ray sources detected near our targets with IR sources in the 2 Micron All-Sky Survey catalog.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "We have discovered an X-ray jet due to material ejected from the black hole X-ray transient XTE J1550-564. The discovery was first reported by Corbel et al. (Science, 298, 196 and astro-ph/0210224), and here, we present an analysis of the three Chandra observations made between 2000 June and 2000 September. For these observations, a source is present that moves in an eastward direction away from the point source associated with the compact object. The separation between the new source and the compact object changes from 21.3 arcseconds in June to 23.4 arcseconds in September, implying a proper motion of 21.2 +/- 7.2 mas/day, a projected separation of 0.31-0.85 pc and an apparent jet velocity between 0.34 +/- 0.12 and 0.93 +/- 0.32 times the speed of light for a source distance range of d = 2.8-7.6 kpc. These observations represent the first time that an X-ray jet proper motion measurement has been obtained for any accretion powered Galactic or extra-galactic source. While this work deals with the jet to the east of the compact object, the western jet has also been detected in the X-ray and radio bands. The most likely scenario is that the eastern jet is the approaching jet and that the jet material was ejected from the black hole in 1998. Along with a 1998 VLBI proper motion measurement, the Chandra proper motion indicates that the eastern jet decelerated between 1998 and 2000. We present results on the morphology and energy spectrum of the jet. We cannot definitively determine the X-ray emission mechanism, but a synchrotron origin is viable and may provide the simplest explanation for the observations.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "Planning is a natural domain of application for frameworks of reasoning about actions and change. In this paper we study how one such framework, the Language E, can form the basis for planning under (possibly) incomplete information. We define two types of plans: weak and safe plans, and propose a planner, called the E-Planner, which is often able to extend an initial weak plan into a safe plan even though the (explicit) information available is incomplete, e.g. for cases where the initial state is not completely known. The E-Planner is based upon a reformulation of the Language E in argumentation terms and a natural proof theory resulting from the reformulation. It uses an extension of this proof theory by means of abduction for the generation of plans and adopts argumentation-based techniques for extending weak plans into safe plans. We provide representative examples illustrating the behaviour of the E-Planner, in particular for cases where the status of fluents is incompletely known.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "E-RES is a system that implements the Language E, a logic for reasoning about narratives of action occurrences and observations. E's semantics is model-theoretic, but this implementation is based on a sound and complete reformulation of E in terms of argumentation, and uses general computational techniques of argumentation frameworks. The system derives sceptical non-monotonic consequences of a given reformulated theory which exactly correspond to consequences entailed by E's model-theory. The computation relies on a complimentary ability of the system to derive credulous non-monotonic consequences together with a set of supporting assumptions which is sufficient for the (credulous) conclusion to hold. E-RES allows theories to contain general action laws, statements about action occurrences, observations and statements of ramifications (or universal laws). It is able to derive consequences both forward and backward in time. This paper gives a short overview of the theoretical basis of E-RES and illustrates its use on a variety of examples. Currently, E-RES is being extended so that the system can be used for planning.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "Cohesive zone models provide an illuminating and tractable way to include constitutive nonlinearity into continuum models of defects. Powerful insights have been gained by studying both dislocations and cracks using such analyses. Recent work has shown that as a result of the locality assumption present in such cohesive zone models, significant errors can be made in the treatment of defect energies. This paper aims to construct a non-local version of the Peierls-Nabarro model in which the atomic level stresses induced at the slip plane depend in a non-local way on the slip degrees of freedom. The non-local interplanar kernel is computed directly from atomistics and is used to evaluate both the structure and energetics of planar dislocations. The non-local formulation does not significantly change the dislocation core structure from that obtained with the local model, but the new formulation leads to significant improvements in the description of dislocation energetics for dislocations with planar cores.\n        \u25b3 Less", "author": "Rob Miller"}, {"abstract": "Theorems from Part 1 of this paper are generalized to \u03c8-mixing sources in this paper. Application to Markoff chains and order m Markoff chains is presented. The main result is the generalization of Theorem 1 in Part 1.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "In this paper, the problem of communication over an essentially unknown channel, which is known to be able to communicate a source to a destination to within a certain distortion level, is considered from a behavioral, interconnection view-point. Rates of reliable communication are derived and source-channel separation for communication with fidelity criteria is proved. The results are then generalized to the multi-user setting under certain assumptions. Other applications of this problem problem which follow from this perspective are discussed.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "We consider a discrete-time Linear-Quadratic-Gaussian (LQG) control problem in which Massey's directed information from the observed output of the plant to the control input is minimized while required control performance is attainable. This problem arises in several different contexts, including joint encoder and controller design for data-rate minimization in networked control systems. We show that the optimal control law is a Linear-Gaussian randomized policy. We also identify the state space realization of the optimal policy, which can be synthesized by an efficient algorithm based on semidefinite programming. Our structural result indicates that the filter-controller separation principle from the LQG control theory, and the sensor-filter separation principle from the zero-delay rate-distortion theory for Gauss-Markov sources hold simultaneously in the considered problem. A connection to the data-rate theorem for mean-square stability by Nair and Evans is also established.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "We consider a class of non-linear dynamics on a graph that contains and generalizes various models from network systems and control and study convergence to uniform agreement states using gradient methods. In particular, under the assumption of detailed balance, we provide a method to formulate the governing ODE system in gradient descent form of sum-separable energy functions, which thus represent a class of Lyapunov functions; this class coincides with Csisz\u00e1r's information divergences. Our approach bases on a transformation of the original problem to a mass-preserving transport problem and it reflects a little-noticed general structure result for passive network synthesis obtained by B.D.O. Anderson and P.J. Moylan in 1975. The proposed gradient formulation extends known gradient results in dynamical systems obtained recently by M. Erbar and J. Maas in the context of porous medium equations. Furthermore, we exhibit a novel relationship between inhomogeneous Markov chains and passive non-linear circuits through gradient systems, and show that passivity of resistor elements is equivalent to strict convexity of sum-separable stored energy. Eventually, we discuss our results at the intersection of Markov chains and network systems under sinusoidal coupling.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "A randomized covering-packing duality between source and channel coding will be discussed by considering the source coding problem of coding a source with a certain distortion level and by considering a channel which communicates the source within a certain distortion level. An operational view of source-channel separation for communication with a fidelity criterion will be discussed in brief.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "Sequential rate-distortion (SRD) theory provides a framework for studying the fundamental trade-off between data-rate and data-quality in real-time communication systems. In this paper, we consider the SRD problem for multi-dimensional time-varying Gauss-Markov processes under mean-square distortion criteria. We first revisit the sensor-estimator separation principle, which asserts that considered SRD problem is equivalent to a joint sensor and estimator design problem in which data-rate of the sensor output is minimized while the estimator's performance satisfies the distortion criteria. We then show that the optimal joint design can be performed by semidefinite programming. A semidefinite representation of the corresponding SRD function is obtained. Implications of the obtained result in the context of zero-delay source coding theory and applications to networked control theory are also discussed.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "In this theoretical study, we determine the maximum amount of work extractable in finite time by a demon performing continuous measurements on a quadratic Hamiltonian system subjected to thermal fluctuations, in terms of the information extracted from the system. This is in contrast to many recent studies that focus on demons' maximizing the extracted work over received information, and operate close to equilibrium. The maximum work demon is found to apply a high-gain continuous feedback using a Kalman-Bucy estimate of the system state. A simple and concrete electrical implementation of the feedback protocol is proposed, which allows for analytic expressions of the flows of energy and entropy inside the demon. This let us show that any implementation of the demon must necessarily include an external power source, which we prove both from classical thermodynamics arguments and from a version of Landauer's memory erasure argument extended to non-equilibrium linear systems.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "The hypothesis that high dimensional data tend to lie in the vicinity of a low dimensional manifold is the basis of manifold learning. The goal of this paper is to develop an algorithm (with accompanying complexity guarantees) for fitting a manifold to an unknown probability distribution supported in a separable Hilbert space, only using i.i.d samples from that distribution. More precisely, our setting is the following. Suppose that data are drawn independently at random from a probability distribution $P$ supported on the unit ball of a separable Hilbert space $H$. Let $G(d, V, \u03c4)$ be the set of submanifolds of the unit ball of $H$ whose volume is at most $V$ and reach (which is the supremum of all $r$ such that any point at a distance less than $r$ has a unique nearest point on the manifold) is at least $\u03c4$. Let $L(M, P)$ denote mean-squared distance of a random point from the probability distribution $P$ to $M$.\n  We obtain an algorithm that tests the manifold hypothesis in the following sense.\n  The algorithm takes i.i.d random samples from $P$ as input, and determines which of the following two is true (at least one must be):\n  (a) There exists $M \\in G(d, CV, \\frac\u03c4{C})$ such that $L(M, P) \\leq C \u03b5.$\n  (b) There exists no $M \\in G(d, V/C, C\u03c4)$ such that $L(M, P) \\leq \\frac\u03b5{C}.$\n  The answer is correct with probability at least $1-\u03b4$.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "This is a three part paper.\n  Optimality of source-channel separation for communication with a fidelity criterion when the channel is compound as defined by Csiszar and Korner in their book and general as defined by Verdu and Han, is proved in Part I. It is assumed that random codes are permitted. The word \"universal\" in the title of this paper refers to the fact that the channel model is compound. The proof uses a layered black-box or a layered input-output view-point. In particular, only the end-to-end description of the channel as being capable of communicating a source to within a certain distortion level is used when proving separation. This implies that the channel model does not play any role for separation to hold as long as there is a source model. Further implications of the layered black-box view-point are discussed.\n  Optimality of source-medium separation for multi-user communication with fidelity criteria over a general, compound medium in the unicast setting is proved in Part II, thus generalizing Part I to the unicast, multi-user setting.\n  Part III gets to an understanding of the question, \"Why is a channel which is capable of communicating a source to within a certain distortion level, also capable of communicating bits at any rate less than the infimum of the rates needed to code the source to within the distortion level\": this lies at the heart of why optimality of separation for communication with a fidelity criterion holds. The perspective taken to get to this understanding is a randomized covering-packing perspective, and the proof is operational.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "In this paper, we study distributed estimation and control problems over graphs under partially nested information patterns. We show a duality result that is very similar to the classical duality result between state estimation and state feedback control with a classical information pattern, under the condition that the disturbances entering different systems on the graph are uncorrelated. The distributed estimation problem decomposes into $N$ separate estimation problems, where $N$ is the number of interconnected subsystems over the graph, and the solution to each subproblem is simply the optimal Kalman filter. This also gives the solution to the distributed control problem due to the duality of distributed estimation and control under partially nested information pattern. We then consider a weighted distributed estimation problem, where we get coupling between the estimators, and separation between the estimators is not possible. We propose a solution based on linear quadratic team decision theory, which provides a generalized Riccati equation for teams. We show that the weighted estimation problem is the dual to a distributed state feedback problem, where the disturbances entering the interconnected systems are correlated.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "Given the possibility of communication systems failing catastrophically, we investigate limits to communicating over channels that fail at random times. These channels are finite-state semi-Markov channels. We show that communication with arbitrarily small probability of error is not possible. Making use of results in finite blocklength channel coding, we determine sequences of blocklengths that optimize transmission volume communicated at fixed maximum message error probabilities. We provide a partial ordering of communication channels. A dynamic programming formulation is used to show the structural result that channel state feedback does not improve performance.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "The paper proposes a framework for modeling and analysis of the dynamics of supply, demand, and clearing prices in power system with real-time retail pricing and information asymmetry. Real-time retail pricing is characterized by passing on the real-time wholesale electricity prices to the end consumers, and is shown to create a closed-loop feedback system between the physical layer and the market layer of the power system. In the absence of a carefully designed control law, such direct feedback between the two layers could increase volatility and lower the system's robustness to uncertainty in demand and generation. A new notion of generalized price-elasticity is introduced, and it is shown that price volatility can be characterized in terms of the system's maximal relative price elasticity, defined as the maximal ratio of the generalized price-elasticity of consumers to that of the producers. As this ratio increases, the system becomes more volatile, and eventually, unstable. As new demand response technologies and distributed storage increase the price-elasticity of demand, the architecture under examination is likely to lead to increased volatility and possibly instability. This highlights the need for assessing architecture systematically and in advance, in order to optimally strike the trade-offs between volatility, economic efficiency, and system reliability.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "We prove that in order to communicate independent sources (this is the unicast problem) between various users over an unknown medium to within various distortion levels, it is sufficient to consider source-channel separation based architectures: architectures which first compress the sources to within the corresponding distortion levels followed by reliable communication over the unknown medium. We are reducing the problem of universal rate-distortion communication of independent sources over a network to the universal reliable communication problem over networks. This is a reductionist view. We are not solving the reliable communication problem in networks.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "An operational perspective is used to understand the relationship between source and channel coding. This is based on a direct reduction of one problem to another that uses random coding (and hence common randomness) but unlike all prior work, does not involve any functional computations, in particular, no mutual-information computations. This result is then used to prove a universal source-channel separation theorem in the rate-distortion context where universality is in the sense of a compound ``general channel.''\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "In part I, we reviewed how Shannon's classical notion of capacity is not sufficient to characterize a noisy communication channel if the channel is intended to be used as part of a feedback loop to stabilize an unstable scalar linear system. While classical capacity is not enough, a sense of capacity (parametrized by reliability) called \"anytime capacity\" is both necessary and sufficient for channel evaluation in this context. The rate required is the log of the open-loop system gain and the required reliability comes from the desired sense of stability. Sufficiency is maintained even in cases with noisy observations and without any explicit feedback between the observer and the controller. This established the asymptotic equivalence between scalar stabilization problems and delay-universal communication problems with feedback.\n  Here in part II, the vector-state generalizations are established and it is the magnitudes of the unstable eigenvalues that play an essential role. To deal with such systems, the concept of the anytime rate-region is introduced. This is the region of rates that the channel can support while still meeting potentially different anytime reliability targets for parallel message streams. All the scalar results generalize on an eigenvalue by eigenvalue basis. When there is no explicit feedback of the noisy channel outputs, the intrinsic delay of the unstable system tells us what the feedback delay needs to be while evaluating the anytime-rate-region for the channel. An example involving a binary erasure channel is used to illustrate how differentiated service is required in any separation-based control architecture.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "Our understanding of information in systems has been based on the foundation of memoryless processes. Extensions to stable Markov and auto-regressive processes are classical. Berger proved a source coding theorem for the marginally unstable Wiener process, but the infinite-horizon exponentially unstable case has been open since Gray's 1970 paper. There were also no theorems showing what is needed to communicate such processes across noisy channels.\n  In this work, we give a fixed-rate source-coding theorem for the infinite-horizon problem of coding an exponentially unstable Markov process. The encoding naturally results in two distinct bitstreams that have qualitatively different QoS requirements for communicating over a noisy medium. The first stream captures the information that is accumulating within the nonstationary process and requires sufficient anytime reliability from the channel used to communicate the process. The second stream captures the historical information that dissipates within the process and is essentially classical. This historical information can also be identified with a natural stable counterpart to the unstable process. A converse demonstrating the fundamentally layered nature of unstable sources is given by means of information-embedding ideas.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "Shannon proved that if we can transmit bits reliably at rates larger than the rate distortion function $R(D)$, then we can transmit this source to within a distortion $D$. We answer the converse question ``If we can transmit a source to within a distortion $D$, can we transmit bits reliably at rates less than the rate distortion function?'' in the affirmative. This can be viewed as a direct converse of the rate distortion theorem.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "We introduce a general framework for treating channels with memory and feedback. First, we generalize Massey's concept of directed information and use it to characterize the feedback capacity of general channels. Second, we present coding results for Markov channels. This requires determining appropriate sufficient statistics at the encoder and decoder. Third, a dynamic programming framework for computing the capacity of Markov channels is presented. Fourth, it is shown that the average cost optimality equation (ACOE) can be viewed as an implicit single-letter characterization of the capacity. Fifth, scenarios with simple sufficient statistics are described.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "We review how Shannon's classical notion of capacity is not enough to characterize a noisy communication channel if the channel is intended to be used as part of a feedback loop to stabilize an unstable scalar linear system. While classical capacity is not enough, another sense of capacity (parametrized by reliability) called ``anytime capacity'' is shown to be necessary for the stabilization of an unstable process. The required rate is given by the log of the unstable system gain and the required reliability comes from the sense of stability desired. A consequence of this necessity result is a sequential generalization of the Schalkwijk/Kailath scheme for communication over the AWGN channel with feedback.\n  In cases of sufficiently rich information patterns between the encoder and decoder, adequate anytime capacity is also shown to be sufficient for there to exist a stabilizing controller. These sufficiency results are then generalized to cases with noisy observations, delayed control actions, and without any explicit feedback between the observer and the controller. Both necessary and sufficient conditions are extended to continuous time systems as well. We close with comments discussing a hierarchy of difficulty for communication problems and how these results establish where stabilization problems sit in that hierarchy.\n        \u25b3 Less", "author": "Sanjoy Mitter"}, {"abstract": "A search for $CP$ violation in the Cabibbo-suppressed $D^0 \\rightarrow K^+ K^- \u03c0^+ \u03c0^-$ decay mode is performed using an amplitude analysis. The measurement uses a sample of $pp$ collisions recorded by the LHCb experiment during 2011 and 2012, corresponding to an integrated luminosity of 3.0 fb$^{-1}$. The $D^0$ mesons are reconstructed from semileptonic $b$-hadron decays into $D^0\u03bc^- X$ final states. The selected sample contains more than 160000 signal decays, allowing the most precise amplitude modelling of this $D^0$ decay to date. The obtained amplitude model is used to perform the search for $CP$ violation. The result is compatible with $CP$ symmetry, with a sensitivity ranging from 1% to 15% depending on the amplitude considered.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "Since their introduction by Erd\u0151s in 1950, covering systems (that is, finite collections of arithmetic progressions that cover the integers) have been extensively studied, and numerous questions and conjectures have been posed regarding the existence of covering systems with various properties. In particular, Erd\u0151s asked if the moduli can be distinct and all arbitrarily large, Erd\u0151s and Selfridge asked if the moduli can be distinct and all odd, and Schinzel conjectured that in any covering system there exists a pair of moduli, one of which divides the other.\n  Another beautiful conjecture, proposed by Erd\u0151s and Graham in 1980, states that if the moduli are distinct elements of the interval $[n,Cn]$, and $n$ is sufficiently large, then the density of integers uncovered by the union is bounded below by a constant (depending only on $C$). This conjecture was confirmed (in a strong form) by Filaseta, Ford, Konyagin, Pomerance and Yu in 2007, who moreover asked whether the same conclusion holds if the moduli are distinct and sufficiently large, and $\\sum_{i=1}^k \\frac{1}{d_i} < C$. Although this condition turns out not to be sufficiently strong to imply the desired conclusion, as the main result of this paper we will give an essentially best possible condition which is sufficient.\n  Our method has a number of further applications. Most importantly, we prove the conjecture of Schinzel stated above, which was made in 1967. We moreover give an alternative (somewhat simpler) proof of a breakthrough result of Hough, who resolved Erd\u0151s' minimum modulus problem, with an improved bound on the smallest difference. Finally, we make further progress on the problem of Erd\u0151s and Selfridge.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "We count orientations of $G(n,p)$ avoiding certain classes of oriented graphs. In particular, we study $T_r(n,p)$, the number of orientations of the binomial random graph $G(n,p)$ in which every copy of $K_r$ is transitive, and $S_r(n,p)$, the number of orientations of $G(n,p)$ containing no strongly connected copy of $K_r$. We give the correct order of growth of $\\log T_r(n,p)$ and $\\log S_r(n,p)$ up to polylogarithmic factors; for orientations with no cyclic triangle, this significantly improves a result of Allen, Kohayakawa, Mota and Parente. We also discuss the problem for a single forbidden oriented graph, and state a number of open problems and conjectures.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The first measurement of heavy-flavour production by the LHCb experiment in its fixed-target mode is presented. The production of $J/\u03c8$ and $D^0$ mesons is studied with beams of protons of different energies colliding with gaseous targets of helium and argon with nucleon-nucleon centre-of-mass energies of $\\sqrt{s_{NN}} = 86.6 $ and $ 110.4$ ${\\rm GeV}$, respectively. The $J/\u03c8$ and $D^0$ production cross-sections in $p{\\rm He}$ collisions in the rapidity range $[2,4.6]$ are found to be $\u03c3_{J/\u03c8} = 652 \\pm 33$ (stat) $\\pm 42$ (syst) nb$/$nucleon and $\u03c3_{D^0} = 80.8 \\pm 2.4$ (stat) $\\pm 6.3$ (syst) $\u03bc$b$/$nucleon, where the first uncertainty is statistical and the second is systematic. No evidence for a substantial intrinsic charm content of the nucleon is observed in the large Bjorken-$x$ region.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "We report measurements on the high temperature ionic and low temperature electronic properties of the 3D topological insulator Bi$_2$Te$_2$Se using $^8$Li $\u03b2$-detected nuclear magnetic relaxation and resonance. At temperatures above ~150 K, spin-lattice relaxation measurements reveal isolated $^8$Li$^{+}$ diffusion with an activation energy $E_{A} = 0.185(8)$ eV and attempt frequency $\u03c4_{0}^{-1} = 8(3) \\times 10^{11}$ s$^{-1}$ for atomic site-to-site hopping. At lower temperature, we find a linear Korringa-like relaxation mechanism with a field dependent slope and intercept, which is accompanied by an anomalous field dependence to the resonance shift. We suggest that these may be related to a strong contribution from orbital currents or the magnetic freezeout of charge carriers in this heavily compensated semiconductor, but that conventional theories are unable to account for the extent of the field dependence. Conventional NMR of the stable host nuclei may help elucidate their origin.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The production of $\u03a5(nS)$ mesons ($n=1,2,3$) in $p$Pb and Pb$p$ collisions at a centre-of-mass energy per nucleon pair $\\sqrt{s_{NN}}=8.16$ TeV is measured by the LHCb experiment, using a data sample corresponding to an integrated luminosity of 31.8 nb$^{-1}$. The $\u03a5(nS)$ mesons are reconstructed through their decays into two opposite-sign muons. The measurements comprise the differential production cross-sections of the $\u03a5(1S)$ and $\u03a5(2S)$ states, their forward-to-backward ratios and nuclear modification factors, performed as a function of the transverse momentum \\pt and rapidity in the nucleon-nucleon centre-of-mass frame $y^*$ of the $\u03a5(nS)$ states, in the kinematic range $p_{\\rm{T}}<25$ GeV/$c$ and $1.5<y^*<4.0$ ($-5.0<y^*<-2.5$) for $p$Pb (Pb$p$) collisions. In addition, production cross-sections for $\u03a5(3S)$ are measured integrated over phase space and the production ratios between all three $\u03a5(nS)$ states are determined. The measurements are compared to theoretical predictions and suppressions for quarkonium in $p$Pb collisions are observed.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A measurement of the charm-mixing parameter $y_{CP}$ using $D^0 \\to K^+ K^-$, $D^0 \\to \u03c0^+ \u03c0^-$, and $D^0 \\to K^- \u03c0^+$ decays is reported. The $D^0$ mesons are required to originate from semimuonic decays of $B^-$ and $\\overline{B}^0$ mesons. These decays are partially reconstructed in a data set of proton-proton collisions at center-of-mass energies of 7 and 8 TeV collected with the LHCb experiment and corresponding to an integrated luminosity of 3 fb$^{-1}$. The $y_{CP}$ parameter is measured to be $(0.57 \\pm 0.13(\\rm{stat.}) \\pm 0.09(\\rm{syst.}))\\%$, in agreement with, and as precise as, the current world-average value.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The branching fractions of the doubly Cabibbo-suppressed decays $D^+\\rightarrow K^-K^+K^+$, $D^+\\rightarrow \u03c0^-\u03c0^+K^+$ and $D^+_s\\rightarrow\u03c0^-K^+K^+$ are measured using the decays $D^+\\rightarrow K^-\u03c0^+\u03c0^+$ and $D^+_s\\rightarrow K^-K^+\u03c0^+$ as normalisation channels. The measurements are performed using proton-proton collision data collected with the LHCb detector at a centre-of-mass energy of 8 TeV, corresponding to an integrated luminosity of 2.0 fb$^{-1}$. The results are\n  \\begin{align}\n  \\frac {\\mathcal{B}(D^+\\rightarrow K^-K^+K^+)} {\\mathcal{B}(D^+\\rightarrow K^-\u03c0^+\u03c0^+)}& = (6.541 \\pm 0.025 \\pm 0.042) \\times 10^{-4},\\nonumber\n  \\frac {\\mathcal{B}(D^+\\rightarrow \u03c0^-\u03c0^+K^+)} {\\mathcal{B}(D^+\\rightarrow K^-\u03c0^+\u03c0^+)}& = (5.231 \\pm 0.009 \\pm 0.023) \\times 10^{-3}, \\nonumber\n  \\frac {\\mathcal{B}(D^+_s\\rightarrow\u03c0^-K^+K^+)} {\\mathcal{B}(D^+_s\\rightarrow K^-K^+\u03c0^+)}& = (2.372 \\pm 0.024 \\pm 0.025) \\times 10^{-3},\\nonumber\n  \\end{align} where the uncertainties are statistical and systematic, respectively. These are the most precise measurements up to date.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "We report the first confirmation of a hot Jupiter discovered by the Transiting Exoplanet Survey Satellite (TESS) mission: HD 202772A b. The transit signal was detected in the data from TESS Sector 1, and was confirmed to be of planetary origin through radial-velocity measurements. HD 202772A b is orbiting a mildly evolved star with a period of 3.3 days. With an apparent magnitude of V = 8.3, the star is among the brightest known to host a hot Jupiter. Based on the 27days of TESS photometry, and radial velocity data from the CHIRON and HARPS spectrographs, the planet has a mass of 1.008+/-0.074 M_J and radius of 1.562+/-0.053 R_J , making it an inflated gas giant. HD 202772A b is a rare example of a transiting hot Jupiter around a quickly evolving star. It is also one of the most strongly irradiated hot Jupiters currently known.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "We demonstrate the use of an optimized 5 core photonic lantern (PL) to simultaneously measure tip/tilt errors at the telescope focal plane, while also providing the input to an instrument. By replacing a single mode (SM) fiber with the PL we show that it is possible to stabilize the input PSF to an instrument due to non-common path tip/tilt aberrations in an adaptive optics system. We show the PL in two different regimes, (i) using only the outer cores for tip/tilt measurements while feeding an instrument with the central core and, (ii) using all cores to measure tip/tilt when used in an instrument such as a spectrograph. In simulations our PL displays the ability to retrieve tip/tilt measurements in a linear range of +/- 55 milliarcseconds. At the designed central wavelength of 1.55 microns, configuration (i) matches the throughput of an on-axis SM fiber but declines as we move away from this wavelength. In configuration (ii) we make use of the whole multimode input of the PL resulting in a potential increase of overall throughput compared to a SM fiber, while eliminating modal noise.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The first observation of two structures consistent with resonances in the final states $\u039b_b^0 \u03c0^-$ and $\u039b_b^0 \u03c0^+$ is reported using samples of $pp$ collision data collected by the LHCb experiment at $\\sqrt{s} = 7$ and $8$ TeV, corresponding to an integrated luminosity of 3 $\\mathrm{fb}^{-1}$. The ground states $\u03a3_b^\\pm$ and $\u03a3_b^{*\\pm}$ are also confirmed and their masses and widths are precisely measured.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A Dalitz plot analysis of $B^0 \\to \u03b7_c(1S) K^+\u03c0^-$ decays is performed using data samples of $pp$ collisions collected with the LHCb detector at centre-of-mass energies of $\\sqrt{s}=7,~8$ and $13$ TeV, corresponding to a total integrated luminosity of $4.7~\\text{fb}^{-1}$. A satisfactory description of the data is obtained when including a contribution representing an exotic $\u03b7_c(1S) \u03c0^-$ resonant state. The significance of this exotic resonance is more than three standard deviations, while its mass and width are $4096 \\pm 20~^{+18}_{-22}$ MeV and $152 \\pm 58~^{+60}_{-35}$ MeV, respectively. The spin-parity assignments $J^P=0^+$ and $J^{P}=1^-$ are both consistent with the data. In addition, the first measurement of the $B^0 \\to \u03b7_c(1S) K^+\u03c0^-$ branching fraction is performed and gives $\\displaystyle \\mathcal{B}(B^0 \\to \u03b7_c(1S) K^+\u03c0^-) = (5.73 \\pm 0.24 \\pm 0.13 \\pm 0.66) \\times 10^{-4}$, where the first uncertainty is statistical, the second systematic, and the third is due to limited knowledge of external branching fractions.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The production of $\u039b^+_c$ baryons produced directly at the interacting point is studied in proton-lead collisions collected with the LHCb detector at the LHC. The data sample corresponds to an integrated luminosity of $1.58\\mathrm{nb}^{-1}$ recorded at a nucleon-nucleon centre-of-mass energy of $\\sqrt{s_{NN}}=5.02$ TeV. Measurements of the differential cross-section and the forward-backward production ratio are reported for $\u039b^+_c$ baryons with transverse momenta in the range $2<p_{T}<10$GeV/$c$ and rapidities in the ranges $1.5<y^*<4.0$ and $-4.5<y^*<-2.5$ in the nucleon-nucleon centre-of-mass system. The ratio of cross-sections of $\u039b^+_c$ baryons and $D^0$ mesons is also reported. The results are compared with next-to-leading order calculations that use nuclear parton distribution functions.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The LHCb Upgrade II will fully exploit the flavour-physics opportunities of the HL-LHC, and study additional physics topics that take advantage of the forward acceptance of the LHCb spectrometer. The LHCb Upgrade I will begin operation in 2020. Consolidation will occur, and modest enhancements of the Upgrade I detector will be installed, in Long Shutdown 3 of the LHC (2025) and these are discussed here. The main Upgrade II detector will be installed in long shutdown 4 of the LHC (2030) and will build on the strengths of the current LHCb experiment and the Upgrade I. It will operate at a luminosity up to $ 2 \\times 10^{34} \\rm cm^{-2}s^{-1}$, ten times that of the Upgrade I detector. New detector components will improve the intrinsic performance of the experiment in certain key areas. An Expression Of Interest proposing Upgrade II was submitted in February 2017. The physics case for the Upgrade II is presented here in more depth. $CP$-violating phases will be measured with precisions unattainable at any other envisaged facility. The experiment will probe $b\\to s \\ell^+\\ell^-$ and $b\\to d \\ell^+\\ell^-$ transitions in both muon and electron decays in modes not accessible at Upgrade I. Minimal flavour violation will be tested with a precision measurement of the ratio of $B(B^0\\to\u03bc^+\u03bc^-)/B(B_s^0\\to \u03bc^+\u03bc^-)$. Probing charm $CP$ violation at the $10^{-5}$ level may result in its long sought discovery. Major advances in hadron spectroscopy will be possible, which will be powerful probes of low energy QCD. Upgrade II potentially will have the highest sensitivity of all the LHC experiments on the Higgs to charm-quark couplings. Generically, the new physics mass scale probed, for fixed couplings, will almost double compared with the pre-HL-LHC era; this extended reach for flavour physics is similar to that which would be achieved by the HE-LHC proposal for the energy frontier.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A search is presented for a Higgs-like boson with mass in the range 45 to 195 GeV/$c^2$ decaying into a muon and a tau lepton. The dataset consists of proton-proton interactions at a centre-of-mass energy of 8 TeV, collected by the LHCb experiment, corresponding to an integrated luminosity of 2 fb$^{-1}$. The tau leptons are reconstructed in both leptonic and hadronic decay channels. An upper limit on the production cross-section multiplied by the branching fraction at 95% confidence level is set and ranges from 22 pb for a boson mass of 45 GeV/$c^2$ to 4 pb for a mass of 195 GeV/$c^2$.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The cross-section for prompt antiproton production in collisions of protons with an energy of $6.5$ TeV incident on helium nuclei at rest is measured with the LHCb experiment from a data set corresponding to an integrated luminosity of $0.5\\,nb^{-1}$. The target is provided by injecting helium gas into the LHC beam line at the LHCb interaction point. The reported results, covering antiproton momenta between $12$ and $110\\,\\mathrm{GeV/}c$, represent the first direct determination of the antiproton production cross-section in ${\\rm p He}$ collisions, and impact the interpretation of recent results on antiproton cosmic rays from space-borne experiments.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "An analysis of the angular distribution of the decay $\u039b_b^0 \\rightarrow \u039b\u03bc^{+} \u03bc^{-}$ is presented, using data collected with the LHCb detector between 2011 and 2016 and corresponding to an integrated luminosity of approximately $5\\,fb^{-1}$. Angular observables are determined using a moment analysis of the angular distribution at low hadronic recoil, corresponding to the dimuon invariant mass squared range $15 < q^{2} < 20\\, GeV^2/c^4$. The full basis of observables is measured for the first time. The lepton-side, hadron-side and combined forward-backward asymmetries of the decay are determined to be \\begin{align} A_{FB}^{l} & = -0.39 \\pm 0.04\\,\\rm{stat} \\pm 0.01\\, \\rm{syst}, \\nonumber\\\\ A_{FB}^{h} & = -0.30 \\pm 0.05\\,\\rm{stat} \\pm 0.02\\, \\rm{syst}, \\nonumber\\\\ A_{FB}^{lh} & = +0.25 \\pm 0.04\\,\\rm{stat} \\pm 0.01\\, \\rm{syst}. \\nonumber \\end{align} The measurements are consistent with Standard Model predictions.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The decay of the narrow resonance $\\overline{B}{}_{s2}^{*0}\\!\\rightarrow B^- K^+$ can be used to determine the $B^-$ momentum in partially reconstructed decays without any assumptions on the decay products of the $B^-$ meson. This technique is employed for the first time to distinguish contributions from $D^0$, $D^{*0}$, and higher-mass charmed states ($D^{**0}$) in semileptonic $B^-$ decays by using the missing-mass distribution. The measurement is performed using a data sample corresponding to an integrated luminosity of 3.0 fb${}^{-1}$ collected with the LHCb detector in $pp$ collisions at center-of-mass energies of 7 and 8 TeV. The resulting branching fractions relative to the inclusive $B^- \\!\\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc$ are $f_{D^0} = \\mathcal{B}( B^- \\rightarrow D^0\u03bc^-\\overline\u03bd_\u03bc)/\\mathcal{B}( B^- \\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc) = 0.25 \\pm 0.06$, $f_{D^{**0}} = \\mathcal{B}( B^- \\rightarrow ( D^{**0} \\rightarrow D^0 X)\u03bc^-\\overline\u03bd_\u03bc)/\\mathcal{B}( B^- \\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc) = 0.21 \\pm 0.07$, with $f_{D^{*0}} = 1 - f_{D^0} - f_{D^{**0}}$ making up the remainder.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A search for $C\\!P$ violation in $\u039b^0_b \\to p K^-$ and $\u039b^0_b \\to p \u03c0^-$ decays is presented using a sample of $pp$ collisions collected with the LHCb detector and corresponding to an integrated luminosity of 3.0 fb$^{-1}$. The $C\\!P$-violating asymmetries are measured to be $A_{\\mathrm{CP}}^{pK^-} = -0.020 \\pm 0.013\\pm 0.019$ and $A_{\\mathrm{CP}}^{p\u03c0^-} = -0.035 \\pm 0.017 \\pm 0.020 $, and their difference $A_{\\mathrm{CP}}^{pK^-}-A_{\\mathrm{CP}}^{p\u03c0^-} = 0.014 \\pm 0.022 \\pm 0.010$, where the first uncertainties are statistical and the second systematic. These are the most precise measurements of such asymmetries to date.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "One of the most useful techniques in astronomical instrumentation is image slicing. It enables a spectrograph to have a more compact angular slit, whilst retaining throughput and increasing resolving power. Astrophotonic components like the photonic lanterns and photonic reformatters can be used to replace bulk optics used so far. This study investigates the performance of such devices using end-to-end simulations to approximate realistic on-sky conditions. It investigates existing components, tries to optimize their performance and aims to understand better how best to design instruments to maximize their performance. This work complements the recent work in the field and provides an estimation for the performance of the new components.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "We report a measurement of the lifetime of the $\u03a9_c^0$ baryon using proton-proton collision data at center-of-mass energies of 7 and 8~TeV, corresponding to an integrated luminosity of 3.0 fb$^{-1}$ collected by the LHCb experiment. The sample consists of about 1000 $\u03a9_b^-\\to\u03a9_c^0\u03bc^-\\bar\u03bd_\u03bc X$ signal decays, where the $\u03a9_c^0$ baryon is detected in the $pK^-K^-\u03c0^+$ final state and $X$ represents possible additional undetected particles in the decay. The $\u03a9_c^0$ lifetime is measured to be $\u03c4_{\u03a9_c^0} = 268\\pm24\\pm10\\pm2$ fs, where the uncertainties are statistical, systematic, and from the uncertainty in the $D^+$ lifetime, respectively. This value is nearly four times larger than, and inconsistent with, the current world-average value.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The doubly charmed baryon decay $\u039e_{cc}^{++} \\rightarrow \u039e_{c}^{+} \u03c0^{+}$ is observed for the first time, with a statistical significance of $5.9\u03c3$, confirming a recent observation of the baryon in the $\u039b_c^{+} K^{-} \u03c0^{+} \u03c0^{+}$ final state. The data sample used corresponds to an integrated luminosity of $1.7\\,\\mathrm{fb}^{-1}$, collected by the LHCb experiment in $pp$ collisions at a center-of-mass energy of $13\\mathrm{\\,Te\\kern -0.1em V}$. The $\u039e_{cc}^{++}$ mass is measured to be\n  \\begin{equation}\\nonumber\n  3620.6\\pm 1.5~(\\text{stat})\\pm 0.4~(\\text{syst}) \\pm 0.3~(\u039e_{c}^{+})~\\text{MeV}/\\it{c}^{2},\n  \\end{equation}\n  and is consistent with the previous result. The ratio of branching fractions between the decay modes is measured to be\n  \\begin{equation}\\nonumber\n  \\frac{\\mathcal{B} (\u039e_{cc}^{++} \\rightarrow \u039e_{c}^{+} \u03c0^{+}) \\times \\mathcal{B}(\u039e_{c}^{+} \\rightarrow pK^{-}\u03c0^{+})}\n  {\\mathcal{B} (\u039e_{cc}^{++} \\rightarrow \u039b_c^{+} K^{-} \u03c0^{+} \u03c0^{+}) \\times \\mathcal{B}(\u039b_c^{+} \\rightarrow pK^{-}\u03c0^{+})}\n  = 0.035\\pm 0.009~(\\text{stat}) \\pm 0.003~(\\text{syst}).\n  \\end{equation}\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The first observation of the $B_s^0 \\to \\overline{D}^{*0} \u03c6$ decay is reported, with a significance of more than seven standard deviations, from an analysis of $pp$ collision data corresponding to an integrated luminosity of 3 fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of $7$ and $8$ TeV. The branching fraction is measured relative to that of the topologically similar decay $B^0 \\to \\overline{D}^0 \u03c0^+\u03c0^-$ and is found to be $\\mathcal{B}(B_s^0 \\to \\overline{D}^{*0} \u03c6) = (3.7 \\pm 0.5 \\pm 0.3 \\pm 0.2) \\times 10^{-5}$, where the first uncertainty is statistical, the second systematic, and the third from the branching fraction of the $B^0 \\to \\overline{D}^0 \u03c0^+\u03c0^-$ decay. The fraction of longitudinal polarisation in this decay is measured to be ${f_{\\rm L} =(73 \\pm 15 \\pm 3)\\%}$. The most precise determination of the branching fraction for the $B_s^0 \\to \\overline{D}^{0} \u03c6$ decay is also obtained, $\\mathcal{B}(B_s^0 \\to \\overline{D}^{0} \u03c6) = (3.0 \\pm 0.3 \\pm 0.2 \\pm 0.2) \\times 10^{-5}$. An upper limit, $\\mathcal{B}(B^0 \\to \\overline{D}^{0} \u03c6) < 2.0 \\ (2.2) \\times 10^{-6}$ at $90\\%$ (95\\%) confidence level is set. A constraint on the $\u03c9-\u03c6$ mixing angle $\u03b4$ is set at $|\u03b4| < 5.2^\\circ~ (5.5^\\circ)$ at $90\\%$ ($95\\%$) confidence level.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The first observation of the $B_s^0 \\to \\overline{D}^0 K^+ K^-$ decay is reported, together with the most precise branching fraction measurement of the mode $B^0 \\to \\overline{D}^0 K^+ K^-$. The results are obtained from an analysis of $pp$ collision data corresponding to an integrated luminosity of $3.0~\\textrm{fb}^{-1}$. The data were collected with the LHCb detector at centre-of-mass energies of $7$ and $8$ TeV. The branching fraction of the $B^0 \\to \\overline{D}^0 K^+ K^-$ decay is measured relative to that of the decay $B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-$ to be $$\\frac{\\mathcal{B}(B^0 \\to \\overline{D}^0 K^+ K^-)}{\\mathcal{B}(B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-)} = (6.9 \\pm 0.4 \\pm 0.3)\\%,$$ where the first uncertainty is statistical and the second is systematic. The measured branching fraction of the $B_s^0 \\to \\overline{D}^0 K^+ K^-$ decay mode relative to that of the corresponding $B^0$ decay is $$\\frac{\\mathcal{B}(B_s^0 \\to \\overline{D}^0 K^+ K^-)}{\\mathcal{B}(B^0 \\to \\overline{D}^0 K^+ K^-)} = (93.0 \\pm 8.9 \\pm 6.9)\\%.$$ Using the known branching fraction of ${B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-}$, the values of ${{\\mathcal B}(B^0 \\to \\overline{D}^0 K^+ K^- )=(6.1 \\pm 0.4 \\pm 0.3 \\pm 0.3) \\times 10^{-5}}$, and ${{\\cal B}(B_s^0 \\to \\overline{D}^0 K^+ K^-)=}$ $(5.7 \\pm 0.5 \\pm 0.4 \\pm 0.5) \\times 10^{-5}$ are obtained, where the third uncertainties arise from the branching fraction of the decay modes ${B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-}$ and $B^0 \\to \\overline{D}^0 K^+ K^-$, respectively.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The first measurements of the forward-backward asymmetry of the dimuon pair ($A_{FB}$), the triple-product asymmetry ($A_{2\u03c6}$), and the charge-parity-conjugation asymmetry ($A_{CP}$), in $D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-$ and $D^0\\to K^+K^-\u03bc^+\u03bc^-$ decays are reported. They are performed using data from proton-proton collisions collected with the LHCb experiment from 2011 to 2016, corresponding to a total integrated luminosity of 5 fb$^{-1}$. The asymmetries are measured to be \\begin{align*} A_{FB}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-) &= (\\phantom{-}3.3\\pm3.7\\pm0.6)\\%,\\\\ A_{2\u03c6}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-)&= (-0.6\\pm3.7\\pm0.6)\\%,\\\\ A_{CP}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-) &= (\\phantom{-}4.9\\pm3.8\\pm0.7)\\%,\\\\ A_{FB}(D^0\\to K^+K^-\u03bc^+\u03bc^-) &= (0\\pm11\\pm2)\\%,\\\\ A_{2\u03c6}(D^0\\to K^+K^-\u03bc^+\u03bc^-)&= (9\\pm11\\pm1)\\%,\\\\ A_{CP}(D^0\\to K^+K^-\u03bc^+\u03bc^-) &= (0\\pm11\\pm2)\\%, \\end{align*} where the first uncertainty is statistical and the second systematic. The asymmetries are also measured as a function of the dimuon invariant mass. The results are consistent with the Standard Model predictions.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The $\\overline{B_s^0} \\rightarrow \u03c7_{c2} K^+ K^- $ decay mode is observed and its branching fraction relative to the corresponding $\u03c7_{c1}$ decay mode, in a $\\pm 15 \\textrm{MeV}/c^2$ window around the $\u03c6$ mass, is found to be $\\frac{\\mathcal{B}(\\overline{B_s^0} \\rightarrow \u03c7_{c2} K^+ K^-) }{ \\mathcal{B}(\\overline{B_s^0} \\rightarrow \u03c7_{c1} K^+ K^-)} = (17.1 \\pm 3.1 \\pm 0.4 \\pm 0.9)\\%,$ where the first uncertainty is statistical, the second systematic and the third due to the knowledge of the branching fractions of radiative $\u03c7_c$ decays. The decay mode $\\overline{B_s^0} \\rightarrow \u03c7_{c1} K^+ K^- $ allows the $ B_s^0$ mass to be measured as $m(B_s^0) = 5366.83 \\pm 0.25 \\pm 0.27 \\, \\textrm{MeV}/c^2,$ where the first uncertainty is statistical and the second systematic. A combination of this result with other LHCb determinations of the $B_s^0$ mass is made.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The $\u03a5(1S)\u03bc^+\u03bc^-$ invariant-mass distribution is investigated for a possible exotic meson state composed of two $b$ quarks and two $\\overline{b}$ quarks, $X_{b\\overline{b}b\\overline{b}}$. The analysis is based on a data sample of $pp$ collisions recorded with the LHCb detector at centre-of-mass energies $\\sqrt{s} =$ 7, 8 and 13 TeV, corresponding to an integrated luminosity of 6.3 fb$^{-1}$. No significant excess is found, and upper limits are set on the product of the production cross-section and the branching fraction as functions of the mass of the $X_{b\\overline{b}b\\overline{b}}$ state. The limits are set in the fiducial volume where all muons have pseudorapidity in the range $[2.0,5.0]$, and the $X_{b\\overline{b}b\\overline{b}}$ state has rapidity in the range $[2.0,4.5]$ and transverse momentum less than 15 GeV/$c$.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "In the $r$-neighbour bootstrap process on a graph $G$, vertices are infected (in each time step) if they have at least $r$ already-infected neighbours. Motivated by its close connections to models from statistical physics, such as the Ising model of ferromagnetism, and kinetically constrained spin models of the liquid-glass transition, the most extensively-studied case is the two-neighbour bootstrap process on the two-dimensional grid $[n]^2$. Around 15 years ago, in a major breakthrough, Holroyd determined the sharp threshold for percolation in this model, and his bounds were subsequently sharpened further by Gravner and Holroyd, and by Gravner, Holroyd and Morris.\n  In this paper we strengthen the lower bound of Gravner, Holroyd and Morris by proving that the critical probability $p_c\\big( [n]^2,2 \\big)$ for percolation in the two-neighbour model on $[n]^2$ satisfies \\[p_c\\big( [n]^2,2 \\big) = \\frac{\u03c0^2}{18\\log n} - \\frac{\u0398(1)}{(\\log n)^{3/2}}\\,.\\] The proof of this result requires a very precise understanding of the typical growth of a critical droplet, and involves a number of technical innovations. We expect these to have other applications, for example, to the study of more general two-dimensional cellular automata, and to the $r$-neighbour process in higher dimensions.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The Cabibbo-suppressed decay $\u039b^0_b\\rightarrow\u03c8(2S)p\u03c0^-$ is observed for the first time using a data sample collected by the LHCb experiment in proton-proton collisions corresponding to 1.0, 2.0 and 1.9fb$^{-1}$ of integrated luminosity at centre-of-mass energies of 7, 8 and 13TeV, respectively. The $\u03c8(2S)$ mesons are reconstructed in the $\u03bc^+\u03bc^-$ final state. The~branching fraction with respect to that of the $\u039b^0_b\\rightarrow\u03c8(2S)pK^-$ decay mode is measured to be $$\\frac{\\mathcal{B}\\left(\u039b^0_b\\rightarrow\u03c8(2S)p\u03c0^- \\right)} {\\mathcal{B}\\left(\u039b^0_b\\rightarrow\u03c8(2S)pK^-\\right)}=\\left(11.4 \\pm 1.3 \\pm 0.2\\right)\\!\\%\\,,$$ where the first uncertainty is statistical and the second is systematic. The $\u03c8(2S)p$ and $\u03c8(2S)\u03c0^-$ mass spectra are investigated and no evidence for exotic resonances is found.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A measurement of $Z\\rightarrow\u03c4^+\u03c4^-$ production cross-section is presented using data, corresponding to an integrated luminosity of 2 fb$^{-1}$, from $pp$ collisions at $\\sqrt{s}=8$ TeV collected by the LHCb experiment. The $\u03c4^+\u03c4^-$ candidates are reconstructed in final states with the first tau lepton decaying leptonically, and the second decaying either leptonically or to one or three charged hadrons. The production cross-section is measured for $Z$ bosons with invariant mass between 60 and 120 GeV/$c^2$, which decay to tau leptons with transverse momenta greater than 20 GeV/$c$ and pseudorapidities between 2.0 and 4.5. The cross-section is determined to be $\u03c3_{pp\\rightarrow{}Z\\rightarrow{}\u03c4^+\u03c4^-} = 95.8 \\pm 2.1 \\pm 4.6 \\pm 0.2 \\pm 1.1 \\mathrm{pb}$, where the first uncertainty is statistical, the second is systematic, the third is due to the LHC beam energy uncertainty, and the fourth to the integrated luminosity uncertainty. This result is compatible with NNLO Standard model predictions. The ratio of the cross-sections for $Z\\rightarrow\u03c4^+\u03c4^-$ to $Z\\rightarrow\u03bc^+\u03bc^-$ ($Z\\rightarrow{}e^+e^-$), determined to be $1.01 \\pm 0.05$ ($1.02 \\pm 0.06$), is consistent with the lepton-universality hypothesis in $Z$ decays.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "Measurements are reported of the central exclusive production of \\jpsi and \\psitwos mesons in $pp$ collisions at a centre-of-mass energy of 13 TeV. Backgrounds are significantly reduced compared to previous measurements made at lower energies through the use of new forward shower counters. The products of the cross-sections and the branching fractions for the decays to dimuons, where both muons are within the pseudorapidity range $2.0<\u03b7<4.5$, are measured to be $$\n  \\begin{array}{rcl} \u03c3_{J/\u03c8\\rightarrow\u03bc^+\u03bc^-}&=&435 \\pm 18 \\pm 17 \\pm 16 {\\rm \\ pb},\\\\ \u03c3_{\u03c8(2S)\\rightarrow\u03bc^+\u03bc^-}&=&11.1 \\pm 1.1 \\pm 0.3 \\pm 0.4 {\\rm \\ pb}.\\\\ \\end{array} $$ The first uncertainties are statistical, the second are systematic, and the third are due to the luminosity determination. The cross-sections are also measured differentially for meson rapidities between 2.0 and 4.5. Good agreement is observed with theoretical predictions. Photoproduction cross-sections are derived and compared to previous experiments, and a deviation from a pure power-law extrapolation of lower energy data is observed.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The method of hypergraph containers, introduced recently by Balogh, Morris, and Samotij, and independently by Saxton and Thomason, has proved to be an extremely useful tool in the study of various monotone graph properties. In particular, a fairly straightforward application of this technique allows one to locate, for each non-bipartite graph $H$, the threshold at which the distribution of edges in a typical $H$-free graph with a given number of edges undergoes a transition from 'random-like' to 'structured'. On the other hand, for non-monotone hereditary graph properties the standard version of this method does not allow one to establish even the existence of such a threshold.\n  In this paper we introduce a refinement of the container method that takes into account the asymmetry between edges and non-edges in a sparse member of a hereditary graph property. As an application, we determine the approximate structure of a typical graph with $n$ vertices, $m$ edges, and no induced copy of the $4$-cycle, for each function $m = m(n)$ satisfying $n^{4/3} (\\log n)^4 \\leqslant m \\ll n^2$. We show that almost all such graphs $G$ have the following property: the vertex set of $G$ can be partitioned into an 'almost-independent' set (a set with $o(m)$ edges) and an 'almost-clique' (a set inducing a subgraph with density $1-o(1)$). The lower bound on $m$ is optimal up to a polylogarithmic factor, as standard arguments show that if $n \\ll m \\ll n^{4/3}$, then almost all such graphs are 'random-like'. As a further consequence, we deduce that the random graph $G(n,p)$ conditioned to contain no induced $4$-cycles undergoes phase transitions at $p = n^{-2/3 + o(1)}$ and $p = n^{-1/3 + o(1)}$.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The first measurement of the lifetime of the doubly charmed baryon $\u039e_{cc}^{++}$ is presented, with the signal reconstructed in the final state $\u039b_c^+ K^- \u03c0^+ \u03c0^+$. The data sample used corresponds to an integrated luminosity of $1.7\\,\\mathrm{fb}^{-1}$, collected by the LHCb experiment in proton-proton collisions at a centre-of-mass energy of $13\\mathrm{\\,Te\\kern -0.1em V}$. The $\u039e_{cc}^{++}$ lifetime is measured to be $0.256\\,^{+0.024}_{-0.022}{\\,\\rm (stat)\\,} \\pm 0.014 {\\,\\rm(syst)}\\mathrm{\\,ps}$.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A measurement of the time-integrated $CP$ asymmetry in $D^0\\rightarrow K^0_S K^0_S$ decays is reported. The data correspond to an integrated luminosity of about $2$ fb$^{-1}$ collected in 2015-2016 by the LHCb collaboration in $pp$ collisions at a centre-of-mass energy of $13$ TeV. The $D^0$ candidate is required to originate from a $D^{\\ast +} \\rightarrow D^0 \u03c0^+$ decay, allowing the determination of the flavour of the $D^0$ meson using the pion charge. The $D^0 \\rightarrow K^{+}K^{-}$ decay, which has a well measured $CP$ asymmetry, is used as a calibration channel. The $CP$ asymmetry for $D^0\\rightarrow K^0_S K^0_S$ is measured to be \\begin{equation*} \\mathcal{A}^{CP}(D^0\\rightarrow K^0_S K^0_S) = (4.3\\pm 3.4\\pm 1.0)\\%, \\end{equation*} where the first uncertainty is statistical and the second is systematic. This result is combined with the previous LHCb measurement at lower centre-of-mass energies to obtain \\begin{equation*} \\mathcal{A}^{CP}(D^0\\rightarrow K^0_S K^0_S) = (2.3\\pm 2.8\\pm 0.9)\\%. \\end{equation*}\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A binned Dalitz plot analysis of $B^\\pm \\to D K^\\pm$ decays, with $D\\to K_\\text{S}^0\u03c0^+\u03c0^-$ and $D\\to K_\\text{S}^0K^+K^-$, is used to perform a measurement of the CP-violating observables $x_{\\pm}$ and $y_{\\pm}$, which are sensitive to the Cabibbo-Kobayashi-Maskawa angle $\u03b3$. The analysis is performed without assuming any $D$ decay model, through the use of information on the strong-phase variation over the Dalitz plot from the CLEO collaboration. Using a sample of proton-proton collision data collected with the LHCb experiment in 2015 and 2016, and corresponding to an integrated luminosity of 2.0$\\,\\text{fb}^{-1}$, the values of the CP violation parameters are found to be $x_- = ( 9.0 \\pm 1.7 \\pm 0.7 \\pm 0.4) \\times 10^{-2}$, $y_- = ( 2.1 \\pm 2.2 \\pm 0.5 \\pm 1.1) \\times 10^{-2}$, $x_+ = (- 7.7 \\pm 1.9 \\pm 0.7 \\pm 0.4) \\times 10^{-2}$, and $y_+ = (- 1.0 \\pm 1.9 \\pm 0.4 \\pm 0.9) \\times 10^{-2}$. The first uncertainty is statistical, the second is systematic, and the third is due to the uncertainty on the strong-phase measurements. These values are used to obtain $\u03b3= \\left(87\\,^{+11}_{-12}\\right)^\\circ$, $r_B = 0.086^{+ 0.013}_{-0.014}$, and $\u03b4_B = (101 \\pm 11)^\\circ$, where $r_B$ is the ratio between the suppressed and favoured $B$-decay amplitudes and $\u03b4_B$ is the corresponding strong-interaction phase difference. This measurement is combined with the result obtained using 2011 and 2012 data collected with the \\lhcb experiment, to give $\u03b3= \\left(80\\,^{+10}_{\\,-9}\\right)^\\circ$, $r_B = 0.080 \\pm 0.011$, and $\u03b4_B = (110 \\pm 10)^\\circ$.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The inclusive $D_s^{\\pm}$ production asymmetry is measured in $pp$ collisions collected by the LHCb experiment at centre-of-mass energies of $\\sqrt{s} =7$ and 8 TeV. Promptly produced $D_s^{\\pm}$ mesons are used, which decay as $D_s^{\\pm}\\to\u03c6\u03c0^{\\pm}$, with $\u03c6\\to K^+K^-$. The measurement is performed in bins of transverse momentum, $p_{\\rm T}$, and rapidity, $y$, covering the range $2.5<p_{\\rm T}<25.0$ GeV$/c$ and $2.0<y<4.5$. No kinematic dependence is observed. Evidence of nonzero $D_s^{\\pm}$ production asymmetry is found with a significance of 3.3 standard deviations.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A search is performed for a spin-0 boson, $\u03c6$, produced in proton-proton collisions at centre-of-mass energies of 7 and 8 TeV, using prompt $\u03c6\\rightarrow\u03bc^+\u03bc^-$ decays and a data sample corresponding to an integrated luminosity of approximately 3.0 ${\\rm fb}^{-1}$ collected with the LHCb detector. No evidence is found for a signal in the mass range from 5.5 to 15 GeV. Upper limits are placed on the product of the production cross-section and the branching fraction into the dimuon final state. The limits are comparable to the best existing over most of the mass region considered and are the first to be set near the $\u03a5$ resonances.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "From samples of $pp$ collision data collected by the LHCb experiment at $\\sqrt{s}=7$, $8$ and $13$ TeV corresponding to integrated luminosities of 1.0, 2.0 and 1.5 fb$^{-1}$, respectively, a peak in both the $\u039b_b^0K^-$ and $\u039e_b^0\u03c0^-$ invariant mass spectra is observed. In the quark model, radially and orbitally excited $\u039e_b^-$ resonances with quark content $bds$ are expected. Referring to this peak as $\u039e_b(6227)^-$, the mass and natural width are measured to be $m_{\u039e_{b}(6227)^-}=6226.9\\pm2.0\\pm0.3\\pm0.2$ MeV/$c^2$ and $\u0393_{\u039e_b(6227)^-}=18.1\\pm5.4\\pm1.8$ MeV/$c^2$, where the first uncertainty is statistical, the second is systematic, and the third, on $m_{\u039e_b(6227)^-}$, is due to the knowledge of the $\u039b_b^0$ baryon mass. Relative production rates of the ${\u039e_b(6227)^-\\to\u039b_b^0K^-}$ and ${\u039e_b(6227)^-\\to\u039e_b^0\u03c0^-}$ decays are also reported.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The time-dependent $C\\!P$ asymmetries in $B^0\\to\u03c0^+\u03c0^-$ and $B_s^0\\to K^+\\!K^-$ decays are measured using a data sample of $pp$ collisions corresponding to an integrated luminosity of 3.0 fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of 7 and 8 TeV. The same data sample is used to measure the time-integrated $C\\!P$ asymmetries in $B^0\\to K^+\u03c0^-$ and $B_s^0\\to\u03c0^+ K^-$ decays. The results are $C_{\u03c0^+\u03c0^-} = -0.34 \\pm 0.06 \\pm 0.01$, $S_{\u03c0^+\u03c0^-} = -0.63 \\pm 0.05 \\pm 0.01$, $C_{K^+\\!K^-} = 0.20 \\pm 0.06 \\pm 0.02$, $S_{K^+\\!K^-} = 0.18 \\pm 0.06 \\pm 0.02$, $C_{K^+\\!K^-}^{\u0394\u0393} = -0.79 \\pm 0.07 \\pm 0.10$, $A_{C\\!P}^{B^0} = -0.084 \\pm 0.004 \\pm 0.003$, and $A_{C\\!P}^{B_s^0} = 0.213 \\pm 0.015 \\pm 0.007$, where the first uncertainties are statistical and the second systematic. Evidence for $C\\!P$ violation is found in the $B_s^0\\to K^+\\!K^-$ decay for the first time.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A search for $C$P and $P$ violation using triple-product asymmetries is performed with $\u039b^{0}_{b}\\to pK^{-}\u03c0^{+}\u03c0^{-}$, $\u039b^{0}_{b}\\to pK^{-}K^{+}K^{-}$ and $\u039e^{0}_{b}\\to pK^{-}K^{-}\u03c0^{+}$ decays. The data sample corresponds to integrated luminosities of 1.0fb$^{-1}$ and 2.0fb$^{-1}$, recorded with the LHCb detector at centre-of-mass energies of 7TeV and 8TeV, respectively. The $CP$- and $P$-violating asymmetries are measured both integrating over all phase space and in specific phase-space regions. No significant deviation from $CP$ or $P$ symmetry is found. The first observation of $\u039b^{0}_{b}\\to pK^{-}\u03c7_{c0}(1P)(\\to\u03c0^{+}\u03c0^{-}, K^{+}K^{-})$ decay is also reported.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A measurement of the $CP$ asymmetries $S_{f}$ and $S_{\\bar{f}}$ in $B^0\\to D^{\\mp}\u03c0^{\\pm}$ decays is reported. The decays are reconstructed in a dataset collected with the LHCb experiment in proton-proton collisions at centre-of-mass energies of 7 and 8 TeV and corresponding to an integrated luminosity of $3.0 \\rm{ fb}^{-1}$. The $CP$ asymmetries are measured to be $S_{f} = 0.058 \\pm 0.020 (\\rm{stat}) \\pm 0.011(\\rm{syst})$ and $S_{\\bar{f}} = 0.038\\pm 0.020 (\\text{stat})\\pm 0.007 (\\text{syst})$. These results are in agreement with, and more precise than, previous determinations. They are used to constrain $|\\sin\\left(2\u03b2+\u03b3\\right)|$ and $\u03b3$ to intervals that are consistent with the current world-average values.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The decay $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ is observed using $pp$ collision data collected with the LHCb detector at centre-of-mass energies of $\\sqrt{s}=$ 7 and 8 TeV, corresponding to an integrated luminosity of 3 $fb^{-1}$. The ratio of branching fractions between $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ and $\u039b_b^0 \\to \u039b_c^+ \u03c0^-$ decays is measured to be \\begin{equation*}\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ \u03c0^-)} = 0.0540 \\pm 0.0023 \\pm 0.0032. \\end{equation*} Two resonant structures are observed in the $ \u039b_c^+ \u03c0^-$ mass spectrum of the ${\u039b_b^0 \\to \u039b_c^+ p\\overline{p} \u03c0^-}$ decays, corresponding to the $\u03a3_c(2455)^0$ and $\u03a3_c^{*}(2520)^0$ states. The ratios of branching fractions with respect to the decay $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ are \\begin{align*}\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u03a3_c^0 p\\overline{p})\\times\\mathcal{B}(\u03a3_c^0\\to \u039b_c^+ \u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)} = 0.089\\pm0.015\\pm0.006,\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u03a3_c^{*0} p\\overline{p})\\times\\mathcal{B}(\u03a3_c^{*0}\\to \u039b_c^+ \u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)} = 0.119\\pm0.020\\pm0.014. \\end{align*} In all of the above results, the first uncertainty is statistical and the second is systematic. The phase space is also examined for the presence of dibaryon resonances. No evidence for such resonances is found.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The production cross-sections of $\u03a5(1S)$, $\u03a5(2S)$ and $\u03a5(3S)$ mesons in proton-proton collisions at $\\sqrt{s}$= 13 TeV are measured with a data sample corresponding to an integrated luminosity of $277 \\pm 11$ $\\rm pb^{-1}$ recorded by the LHCb experiment in 2015. The $\u03a5$ mesons are reconstructed in the decay mode $\u03a5\\to\u03bc^{+}\u03bc^{-}$. The differential production cross-sections times the dimuon branching fractions are measured as a function of the $\u03a5$ transverse momentum, $p_{\\rm T}$, and rapidity, $y$, over the range $0 < p_{\\rm T}< 30$ GeV/c and $2.0 < y < 4.5$. The ratios of the cross-sections with respect to the LHCb measurement at $\\sqrt{s}$= 8 TeV are also determined. The measurements are compared with theoretical predictions based on NRQCD.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "A search for the decay $B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-$ is presented using data sets corresponding to 1.0, 2.0 and 1.6 $\\text{fb}^{-1}$ of integrated luminosity collected during $pp$ collisions with the LHCb experiment at centre-of-mass energies of 7, 8 and 13 TeV, respectively. An excess is found over the background-only hypothesis with a significance of 3.4 standard deviations. The branching fraction of the $B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-$ decay is determined to be $\\mathcal{B}(B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-) = [2.9 \\pm 1.0~(\\text{stat}) \\pm 0.2~(\\text{syst}) \\pm 0.3~(\\text{norm})] \\times 10^{-8}$, where the first and second uncertainties are statistical and systematic, respectively. The third uncertainty is due to limited knowledge of external parameters used to normalise the branching fraction measurement.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The $CP$ asymmetry in $B^-\\to D_s^-D^0$ and $B^-\\to D^-D^0$ decays is measured using LHCb data corresponding to an integrated luminosity of 3.0 fb$^{-1}$, collected in $pp$ collisions at centre-of-mass energies of 7 and 8 TeV. The results are $A^{CP}(B^-\\to D_s^-D^0)=(-0.4\\pm 0.5\\pm 0.5)\\%$ and $A^{CP}(B^-\\to D^-D^0)=( 2.3\\pm 2.7\\pm 0.4)\\%$, where the first uncertainties are statistical and the second systematic. This is the first measurement of $A^{CP}(B^-\\to D_s^-D^0)$ and the most precise determination of $A^{CP}(B^-\\to D^-D^0)$. Neither result shows evidence of $CP$ violation.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "The cross-section for inelastic proton-proton collisions at a centre-of-mass energy of 13\\,TeV is measured with the LHCb detector. The fiducial cross-section for inelastic interactions producing at least one prompt long-lived charged particle with momentum $p>2$\\,GeV/$c$ in the pseudorapidity range $2<\u03b7<5$ is determined to be $\u03c3_{\\rm acc}= 62.2 \\pm 0.2 \\pm 2.5$\\,mb. The first uncertainty is the intrinsic systematic uncertainty of the measurement, the second is due to the uncertainty on the integrated luminosity. The statistical uncertainty is negligible. Extrapolation to full phase space yields the total inelastic proton-proton cross-section $\u03c3_{\\rm inel}= 75.4 \\pm 3.0 \\pm 4.5$\\,mb, where the first uncertainty is experimental and the second due to the extrapolation. An updated value of the inelastic cross-section at a centre-of-mass energy of 7\\,TeV is also reported.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "Forward top quark pair production is studied in $pp$ collisions in the $\u03bceb$ final state using a data sample corresponding to an integrated luminosity of 1.93 fb$^{-1}$ collected with the LHCb experiment at a centre-of-mass energy of 13 TeV. The cross-section is measured in a fiducial region where both leptons have a transverse momentum greater than 20 GeV and a pseudorapidity between 2.0 and 4.5. The quadrature sum of the azimuthal separation and the difference in pseudorapidities, denoted $\u0394R$, between the two leptons must be larger than 0.1. The $b$-jet axis is required to be separated from both leptons by a $\u0394R$ of 0.5, and to have a transverse momentum in excess of 20 GeV and a pseudorapidity between 2.2 and 4.2. The cross-section is measured to be $$\u03c3_{t\\bar{t}}= 126\\pm19\\,(\\mathrm{stat})\\pm16\\,(\\mathrm{syst})\\pm5\\,(\\mathrm{lumi})\\,\\,\\mathrm{ fb}$$ where the first uncertainty is statistical, the second is systematic, and the third is due to the luminosity determination. The measurement is compatible with the Standard Model prediction.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "In this survey we describe a recently-developed technique for bounding the number (and controlling the typical structure) of finite objects with forbidden substructures. This technique exploits a subtle clustering phenomenon exhibited by the independent sets of uniform hypergraphs whose edges are sufficiently evenly distributed; more precisely, it provides a relatively small family of 'containers' for the independent sets, each of which contains few edges. We attempt to convey to the reader a general high-level overview of the method, focusing on a small number of illustrative applications in areas such as extremal graph theory, Ramsey theory, additive combinatorics, and discrete geometry, and avoiding technical details as much as possible.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "Kinetically constrained models (KCM) are reversible interacting particle systems on $\\mathbb Z^d$ with continuous time Markov dynamics of Glauber type, which represent a natural stochastic (and non-monotone) counterpart of the family of cellular automata known as $\\mathcal U$-bootstrap percolation. KCM also display some of the peculiar features of the so-called \"glassy dynamics\", and as such they are extensively used in the physics literature to model the liquid-glass transition, a major and longstanding open problem in condensed matter physics.\n  We consider two-dimensional KCM with update rule $\\mathcal U$, and focus on proving universality results for the mean infection time of the origin, in the same spirit as those recently established in the setting of $\\mathcal U$-bootstrap percolation. We first identify what we believe are the correct universality classes, which turn out to be different from those of $\\mathcal U$-bootstrap percolation. We then prove universal upper bounds on the mean infection time within each class, which we conjecture to be sharp up to logarithmic corrections. In certain cases, including all supercritical models, and the well-known Duarte model, our conjecture has recently been confirmed in [MMT]. In fact, in these cases our upper bound is sharp up to a constant factor in the exponent. For certain classes of update rules, it turns out that the infection time of the KCM diverges much faster than for the corresponding $\\mathcal U$-bootstrap process when the equilibrium density of infected sites goes to zero. This is due to the occurrence of energy barriers which determine the dominant behaviour for KCM, but which do not matter for the monotone bootstrap dynamics.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "We present a photometric detection of the first brightness dips of the unique variable star KIC 8462852 since the end of the Kepler space mission in 2013 May. Our regular photometric surveillance started in October 2015, and a sequence of dipping began in 2017 May continuing on through the end of 2017, when the star was no longer visible from Earth. We distinguish four main 1-2.5% dips, named \"Elsie,\" \"Celeste,\" \"Skara Brae,\" and \"Angkor\", which persist on timescales from several days to weeks. Our main results so far are: (i) there are no apparent changes of the stellar spectrum or polarization during the dips; (ii) the multiband photometry of the dips shows differential reddening favoring non-grey extinction. Therefore, our data are inconsistent with dip models that invoke optically thick material, but rather they are in-line with predictions for an occulter consisting primarily of ordinary dust, where much of the material must be optically thin with a size scale <<1um, and may also be consistent with models invoking variations intrinsic to the stellar photosphere. Notably, our data do not place constraints on the color of the longer-term \"secular\" dimming, which may be caused by independent processes, or probe different regimes of a single process.\n        \u25b3 Less", "author": "Robert Morris"}, {"abstract": "We present the Data Release 10 Quasar (DR10Q) catalog from the Baryon Oscillation Spectroscopic Survey (BOSS) of the Sloan Digital Sky Survey III. The catalog includes all BOSS objects that were targeted as quasar candidates during the first 2.5 years of the survey and that are confirmed as quasars via visual inspection of the spectra. The catalog also includes known quasars (mostly from SDSS-I and II) that were reobserved by BOSS. The catalog contains 166,583 quasars (74,454 are new discoveries since SDSS-DR9) detected over 6,373 deg$^{2}$ with robust identification and redshift measured by a combination of principal component eigenspectra. The number of quasars with $z>2.15$ (117,668) is $\\sim$5 times greater than the number of $z>2.15$ quasars known prior to BOSS. Redshifts and FWHMs are provided for the strongest emission lines (CIV, CIII, MgII). The catalog identifies 16,461 broad absorption line quasars and gives their characteristics. For each object, the catalog presents five-band (u, g, r, i, z) CCD-based photometry with typical accuracy of 0.03 mag and information on the optical morphology and selection method. The catalog also contains X-ray, ultraviolet, near-infrared, and radio emission properties of the quasars, when available, from other large-area surveys. The calibrated digital spectra cover the wavelength region 3,600-10,500\u00c5 at a spectral resolution in the range 1,300$<$R$<$2,500; the spectra can be retrieved from the SDSS Catalog Archive Server. We also provide a supplemental list of an additional 2,376 quasars that have been identified among the galaxy targets of the SDSS-III/BOSS.\n        \u25b3 Less", "author": "Joel Moses"}, {"abstract": "We measure the large-scale cross-correlation of quasars with the Lyman alpha forest absorption, using over 164,000 quasars from Data Release 11 of the SDSS-III Baryon Oscillation Spectroscopic Survey. We extend the previous study of roughly 60,000 quasars from Data Release 9 to larger separations, allowing a measurement of the Baryonic Acoustic Oscillation (BAO) scale along the line of sight $c/(H(z=2.36) ~ r_s) = 9.0 \\pm 0.3$ and across the line of sight $D_A(z=2.36) / ~ r_s = 10.8 \\pm 0.4$, consistent with CMB and other BAO data. Using the best fit value of the sound horizon from Planck data ($r_s=147.49 Mpc$), we can translate these results to a measurement of the Hubble parameter of $H(z=2.36) = 226 \\pm 8 km/s / Mpc$ and of the angular diameter distance of $D_A(z=2.36) = 1590 \\pm 60 Mpc$. The measured cross-correlation function and an update of the code to fit the BAO scale (baofit) are made publicly available.\n        \u25b3 Less", "author": "Joel Moses"}, {"abstract": "We present a system that allows users to visualize complex human motion via 3D motion sculptures---a representation that conveys the 3D structure swept by a human body as it moves through space. Given an input video, our system computes the motion sculptures and provides a user interface for rendering it in different styles, including the options to insert the sculpture back into the original video, render it in a synthetic scene or physically print it.\n  To provide this end-to-end workflow, we introduce an algorithm that estimates that human's 3D geometry over time from a set of 2D images and develop a 3D-aware image-based rendering approach that embeds the sculpture back into the scene. By automating the process, our system takes motion sculpture creation out of the realm of professional artists, and makes it applicable to a wide range of existing video material.\n  By providing viewers with 3D information, motion sculptures reveal space-time motion information that is difficult to perceive with the naked eye, and allow viewers to interpret how different parts of the object interact over time. We validate the effectiveness of this approach with user studies, finding that our motion sculpture visualizations are significantly more informative about motion than existing stroboscopic and space-time visualization methods.\n        \u25b3 Less", "author": "Stefanie Mueller"}, {"abstract": "International challenges have become the standard for validation of biomedical image analysis methods. Given their scientific impact, it is surprising that a critical analysis of common practices related to the organization of challenges has not yet been performed. In this paper, we present a comprehensive analysis of biomedical image analysis challenges conducted up to now. We demonstrate the importance of challenges and show that the lack of quality control has critical consequences. First, reproducibility and interpretation of the results is often hampered as only a fraction of relevant information is typically provided. Second, the rank of an algorithm is generally not robust to a number of variables such as the test data used for validation, the ranking scheme applied and the observers that make the reference annotations. To overcome these problems, we recommend best practice guidelines and define open research questions to be addressed in the future.\n        \u25b3 Less", "author": "Stefanie Mueller"}, {"abstract": "Altering the geometric structure of a polyatomic molecule by populating an excited electronic state is one possible mechanism that can influence chemical reactivity and is often utilized in photochemical reactions. Although excited state structures can be prepared and subsequently measured using pump-probe techniques and high-resolution rotational spectroscopy measurements, they can only indirectly determine the geometric structure of a molecule in an excited electronic state. Here we show that we can simultaneously pump a polyatomic molecule to an excited electronic state and, for the first time, directly image the geometric structure of the excited state molecule with sub-${\\rm \u00c5}$ngstrom and sub-femtosecond resolution using a single laser-induced electron diffraction pulse. We visualize the ultrafast bending and symmetric stretching of CS$_2$ through the well-examined but difficult to interpret linear-to-bent B$^1$B$_2 \\leftarrow$X$^1\u03a3^{+}_{\\rm g}$ transition in CS$_2$. Our results shed light on the vibronic excitations of neutral polyatomic molecules in an intense laser field with unprecedented spatio-temporal resolution, paving the way for recording the ultimate \"molecular movie\" of photo-induced reactions in polyatomic molecules.\n        \u25b3 Less", "author": "Stefanie Mueller"}, {"abstract": "Surgical data science is a new research field that aims to observe all aspects of the patient treatment process in order to provide the right assistance at the right time. Due to the breakthrough successes of deep learning-based solutions for automatic image annotation, the availability of reference annotations for algorithm training is becoming a major bottleneck in the field. The purpose of this paper was to investigate the concept of self-supervised learning to address this issue.\n  Our approach is guided by the hypothesis that unlabeled video data can be used to learn a representation of the target domain that boosts the performance of state-of-the-art machine learning algorithms when used for pre-training. Core of the method is an auxiliary task based on raw endoscopic video data of the target domain that is used to initialize the convolutional neural network (CNN) for the target task. In this paper, we propose the re-colorization of medical images with a generative adversarial network (GAN)-based architecture as auxiliary task. A variant of the method involves a second pre-training step based on labeled data for the target task from a related domain. We validate both variants using medical instrument segmentation as target task.\n  The proposed approach can be used to radically reduce the manual annotation effort involved in training CNNs. Compared to the baseline approach of generating annotated data from scratch, our method decreases exploratively the number of labeled images by up to 75% without sacrificing performance. Our method also outperforms alternative methods for CNN pre-training, such as pre-training on publicly available non-medical or medical data using the target task (in this instance: segmentation).\n  As it makes efficient use of available (non-)public and (un-)labeled data, the approach has the potential to become a valuable tool for CNN (pre-)training.\n        \u25b3 Less", "author": "Stefanie Mueller"}, {"abstract": "Studies of the physical properties of Trans-Neptunian Objects (TNOs) are a powerful probe into the processes of planetesimal formation and solar system evolution. JWST will provide unique new capabilities for such studies. Here we outline where the capabilities of JWST open new avenues of investigation, potential valuable observations and surveys, and conclude with a discussion of community actions that may serve to enhance the eventual science return of JWSTs TNO observations.\n        \u25b3 Less", "author": "Stefanie Mueller"}, {"abstract": "We present a generalization of the Simultaneous Long-Short (SLS) trading strategy described in recent control literature wherein we allow for different parameters across the short and long sides of the controller; we refer to this new strategy as Generalized SLS (GSLS). Furthermore, we investigate the conditions under which positive gain can be assured within the GSLS setup for both deterministic stock price evolution and geometric Brownian motion. In contrast to existing literature in this area (which places little emphasis on the practical application of SLS strategies), we suggest optimization procedures for selecting the control parameters based on historical data, and we extensively test these procedures across a large number of real stock price trajectories (495 in total). We find that the implementation of such optimization procedures greatly improves the performance compared with fixing control parameters, and, indeed, the GSLS strategy outperforms the simpler SLS strategy in general.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Report of the first workshop to identify approaches and techniques in the domain of quantum sensing that can be utilized by future High Energy Physics applications to further the scientific goals of High Energy Physics.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "We present a multi-frequency study of the intermediate spiral SAB(r)bc type galaxy NGC 6744, using available data from the Chandra X-Ray telescope, radio continuum data from the Australia Telescope Compact Array and Murchison Widefield Array, and Wide-field Infrared Survey Explorer infrared observations. We identify 117 X-ray sources and 280 radio sources. Of these, we find nine sources in common between the X-ray and radio catalogues, one of which is a faint central black hole with a bolometric radio luminosity similar to the Milky Way's central black hole. We classify 5 objects as supernova remnant candidates, 2 objects as likely supernova remnants, 17 as HII regions, 1 source as an AGN; the remaining 255 radio sources are categorised as background objects and one X-ray source is classified as a foreground star. We find the star-formation rate (SFR) of NGC 6744 to be in the range 2.8 - 4.7 $\\rm{M_{\\odot}~yr^{-1}}$ signifying the galaxy is still actively forming stars. The specific SFR of NGC 6744 is greater than that of late-type spirals such as the Milky Way, but considerably less that that of a typical starburst galaxy.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Wildland fire dynamics is a complex turbulent dimensional process. Cellular automata (CA) is an efficient tool to predict fire dynamics, but the main parameters of the method are challenging to estimate. To overcome this challenge, we compute statistical distributions of the key parameters of a CA model using infrared images from controlled burns. Moreover, we apply this analysis to different spatial scales and compare the experimental results to a simple statistical model. By performing this analysis and making this comparison, several capabilities and limitations of CA are revealed.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "OpenMP is a shared memory programming model which supports the offloading of target regions to accelerators such as NVIDIA GPUs. The implementation in Clang/LLVM aims to deliver a generic GPU compilation toolchain that supports both the native CUDA C/C++ and the OpenMP device offloading models. There are situations where the semantics of OpenMP and those of CUDA diverge. One such example is the policy for implicitly handling local variables. In CUDA, local variables are implicitly mapped to thread local memory and thus become private to a CUDA thread. In OpenMP, due to semantics that allow the nesting of regions executed by different numbers of threads, variables need to be implicitly \\emph{shared} among the threads of a contention group. In this paper we introduce a re-design of the OpenMP device data sharing infrastructure that is responsible for the implicit sharing of local variables in the Clang/LLVM toolchain. We introduce a new data sharing infrastructure that lowers implicitly shared variables to the shared memory of the GPU. We measure the amount of shared memory used by our scheme in cases that involve scalar variables and statically allocated arrays. The evaluation is carried out by offloading to K40 and P100 NVIDIA GPUs. For scalar variables the pressure on shared memory is relatively low, under 26\\% of shared memory utilization for the K40, and does not negatively impact occupancy. The limiting occupancy factor in that case is register pressure. The data sharing scheme offers the users a simple memory model for controlling the implicit allocation of device shared memory.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Quantum simulation of chemistry and materials is predicted to be an important application for both near-term and fault-tolerant quantum devices. However, at present, developing and studying algorithms for these problems can be difficult due to the prohibitive amount of domain knowledge required in both the area of chemistry and quantum algorithms. To help bridge this gap and open the field to more researchers, we have developed the OpenFermion software package (www.openfermion.org). OpenFermion is an open-source software library written largely in Python under an Apache 2.0 license, aimed at enabling the simulation of fermionic models and quantum chemistry problems on quantum hardware. Beginning with an interface to common electronic structure packages, it simplifies the translation between a molecular specification and a quantum circuit for solving or studying the electronic structure problem on a quantum computer, minimizing the amount of domain expertise required to enter the field. The package is designed to be extensible and robust, maintaining high software standards in documentation and testing. This release paper outlines the key motivations behind design choices in OpenFermion and discusses some basic OpenFermion functionality which we believe will aid the community in the development of better quantum algorithms and tools for this exciting area of research.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "We provide new observations of the LMC X-1 O star and its extended nebula structure using spectroscopic data from VLT/UVES as well as H$\u03b1$ imaging from the Wide Field Imager on the Max Planck Gesellschaft / European Southern Observatory 2.2m telescope and ATCA imaging of the 2.1 GHz radio continuum. This nebula is one of the few known to be energized by an X-ray binary. We use a new spectrum extraction technique that is superior to other methods to obtain both radial velocities and fluxes. This provides an updated spatial velocity of $\\simeq 21.0~\\pm~4.8$ km s$^{-1}$ for the O star. The slit encompasses both the photo-ionized and shock-ionized regions of the nebula. The imaging shows a clear arc-like structure reminiscent of a wind bow shock in between the ionization cone and shock-ionized nebula. The observed structure can be fit well by the parabolic shape of a wind bow shock. If an interpretation of a wind bow shock system is valid, we investigate the N159-O1 star cluster as a potential parent of the system, suggesting a progenitor mass of $\\sim 60$ M$_{\\odot}$ for the black hole. We further note that the radio emission could be non-thermal emission from the wind bow shock, or synchrotron emission associated with the jet inflated nebula. For both wind and jet-powered origins, this would represent one of the first radio detections of such a structure.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "DESI (Dark Energy Spectropic Instrument) is a Stage IV ground-based dark energy experiment that will study baryon acoustic oscillations and the growth of structure through redshift-space distortions with a wide-area galaxy and quasar redshift survey. The DESI instrument is a robotically-actuated, fiber-fed spectrograph capable of taking up to 5,000 simultaneous spectra over a wavelength range from 360 nm to 980 nm. The fibers feed ten three-arm spectrographs with resolution $R= \u03bb/\u0394\u03bb$ between 2000 and 5500, depending on wavelength. The DESI instrument will be used to conduct a five-year survey designed to cover 14,000 deg$^2$. This powerful instrument will be installed at prime focus on the 4-m Mayall telescope in Kitt Peak, Arizona, along with a new optical corrector, which will provide a three-degree diameter field of view. The DESI collaboration will also deliver a spectroscopic pipeline and data management system to reduce and archive all data for eventual public use.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "DESI (Dark Energy Spectroscopic Instrument) is a Stage IV ground-based dark energy experiment that will study baryon acoustic oscillations (BAO) and the growth of structure through redshift-space distortions with a wide-area galaxy and quasar redshift survey. To trace the underlying dark matter distribution, spectroscopic targets will be selected in four classes from imaging data. We will measure luminous red galaxies up to $z=1.0$. To probe the Universe out to even higher redshift, DESI will target bright [O II] emission line galaxies up to $z=1.7$. Quasars will be targeted both as direct tracers of the underlying dark matter distribution and, at higher redshifts ($ 2.1 < z < 3.5$), for the Ly-$\u03b1$ forest absorption features in their spectra, which will be used to trace the distribution of neutral hydrogen. When moonlight prevents efficient observations of the faint targets of the baseline survey, DESI will conduct a magnitude-limited Bright Galaxy Survey comprising approximately 10 million galaxies with a median $z\\approx 0.2$. In total, more than 30 million galaxy and quasar redshifts will be obtained to measure the BAO feature and determine the matter power spectrum, including redshift space distortions.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "In standard quantum mechanics, complex numbers are used to describe the wavefunction. Although complex numbers have proven sufficient to predict the results of existing experiments, there is no apparent theoretical reason to choose them over real numbers or generalizations of complex numbers, i.e. hyper-complex numbers. Experiments performed to date have proven that real numbers are insufficient, but whether or not hyper-complex numbers are required remains an open question. Quantum theories based on hyper-complex numbers are one example of a post-quantum theory, which must be put on a firm experimental foundation. Here we experimentally probe hyper-complex quantum theories, by studying one of their deviations from complex quantum theory: the non-commutativity of phases. We do so by passing single photons through a Sagnac interferometer containing two physically different phases, having refractive indices of opposite sign. By showing that the phases commute with high precision, we place limits on a particular prediction of hyper-complex quantum theories.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Strongly anisotropic media where the principal components of electric permittivity or magnetic permeability tensors have opposite signs are termed as hyperbolic media. Such media support propagating electromagnetic waves with extremely large wavevectors exhibiting unique optical properties. However in all artificial and natural optical materials studied to date, the hyperbolic dispersion originates solely from the electric response. This restricts material functionality to one polarization of light and inhibits free-space impedance matching. Such restrictions can be overcome in media having components of opposite signs for both electric and magnetic tensors. Here we present the experimental demonstration of the magnetic hyperbolic dispersion in three-dimensional metamaterials. We measure metamaterial isofrequecy contours and reveal the topological phase transition between the elliptic and hyperbolic dispersion. In the hyperbolic regime, we demonstrate the strong enhancement of thermal emission, which becomes directional, coherent and polarized. Our findings show the possibilities for realizing efficient impedance-matched hyperbolic media for unpolarized light.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Frustrated quantum magnets can harbor unconventional spin liquid ground states in which the elementary magnetic moments fractionalize into new emergent degrees of freedom. While the fractionalization of quantum numbers is one of the recurring themes in modern condensed matter physics, it often remains a challenge to devise a controlled analytical framework tracking this phenomenon. A notable exception is the exactly solvable Kitaev model, in which spin degrees of freedom fractionalize into Majorana fermions and a Z2 gauge field. Here we discuss the physics of fractionalization in three-dimensional Kitaev models and demonstrate that the itinerant Majorana fermions generically form a (semi)metal which, depending on the underlying lattice structure, exhibits Majorana Fermi surfaces, nodal lines or topologically protected Weyl nodes. We show that the nature of these Majorana metals can be deduced from an elementary symmetry analysis of the projective time-reversal and inversion symmetries for a given lattice. This allows us to comprehensively classify the gapless spin liquids of Kitaev models for the most elementary tricoordinated lattices in three dimensions. We further expand this classification by addressing the effects of time-reversal symmetry breaking and additional interactions.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "The Keck science community is entering an era of unprecedented change. Powerful new instrument like ZTF, JWST, LSST, and the ELTs will catalyze this change, and we must be ready to take full advantage to maintain our position of scientific leadership. The best way to do this is to continue the UC and Caltech tradition of technical excellence in instrumentation. In this whitepaper we describe a new instrument called KRAKENS to help meet these challenges. KRAKENS uses a unique detector technology (MKIDs) to enable groundbreaking science across a wide range of astrophysical research topics. This document will lay out the detailed expected science return of KRAKENS.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "According to the generally accepted scenario, the last giant impact on the Earth formed the Moon and initiated the final phase of core formation by melting the Earth's mantle. A key goal of geochemistry is to date this event, but different ages have been proposed. Some argue for an early Moon-forming event, approximately 30 million years (Myr) after the condensation of the first solids in the Solar System, whereas others claim a date later than 50 Myr (and possibly as late as around 100 My) after condensation. Here we show that a Moon-forming event at 40 Myr after condensation, or earlier, is ruled out at a 99.9 per cent confidence level. We use a large number of N-body simulations to demonstrate a relationship between the time of the last giant impact on an Earth-like planet and the amount of mass subsequently added during the era known as Late Accretion. As the last giant impact is delayed, the late-accreted mass decreases in a predictable fashion. This relationship exists within both the classical scenario and the Grand Tack scenario of terrestrial planet formation, and it holds across a wide range of disk conditions. The concentration of highly siderophile elements (HSEs) in Earth's mantle constrains the mass of chondritic material added to Earth during Late Accretion. Using HSE abundance measurements, we determine a Moon-formation age of 95 +/- 32 Myr since condensation. The possibility exists that some late projectiles were differentiated and left an incomplete HSE record in Earth's mantle. Even in this case, various isotopic constraints strongly suggest that the late-accreted mass did not exceed 1 per cent of Earth's mass, and so the HSE clock still robustly limits the timing of the Moon-forming event to significantly later than 40 My after condensation.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "The asteroid belt is the leftover of the original planetesimal population in the inner solar system. However, currently the asteroids have orbits with all possible values of eccentricities and inclinations compatible with long-term dynamical stability, whereas the initial planetesimal orbits should have been quasi-circular and almost co-planar. The total mass in the asteroid population is a small fraction of that existing primordially. Also, asteroids with different chemical/mineralogical properties are not ranked in an orderly manner with mean heliocentric distance as one could expect from the existence of a radial gradient of the temperature in the proto-planetary disk, but they are partially mixed. These properties show that the asteroid belt has been severely sculpted by one or a series of processes during its lifetime. This paper reviews the processes that have been proposed so far, discussing the properties that they explain and the problems that they are confronted with. Emphasis is paid to the interplay between the dynamical and the collisional evolution of the asteroid population, which allows the use of the size distribution to constrain the dynamical models. We divide the asteroid belt evolution into three phases. The first phase started during the lifetime of the gaseous proto-planetary disk, when the giant planets formed and presumably experienced large-scale migrations, and continued after the removal of the gas, during the build-up of the terrestrial planets. The second phase occurred after the removal of the gaseous proto-planetary disk and it became particularly lively for the asteroid belt when the giant planets suddenly changed their orbits, as a result of a mutual dynamical instability and the interaction with the trans-Neptunian planetesimal disk. The third phase covers the aftermath of the giant planet instability, until today.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "The fractionalization of quantum numbers in interacting quantum-many body systems is a central motif in condensed matter physics with prominent examples including the fractionalization of the electron in quantum Hall liquids or the emergence of magnetic monopoles in spin-ice materials. Here we discuss the fractionalization of magnetic moments in three-dimensional Kitaev models into Majorana fermions (and a $\\mathbb Z_2$ gauge field) and their emergent collective behavior. We analytically demonstrate that the Majorana fermions form a Weyl superconductor for the Kitaev model on the recently synthesized hyperhoneycomb structure of $\u03b2$-Li$_2$IrO$_3$ when applying a magnetic field. We characterize the topologically protected bulk and surface features of this state, which we dub a Weyl spin liquid, including thermodynamic and transport signatures.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "A new model for terrestrial planet formation (Hansen 2009, Walsh et al. 2011) has explored accretion in a truncated protoplanetary disk, and found that such a configuration is able to reproduce the distribution of mass among the planets in the Solar System, especially the Earth/Mars mass ratio, which earlier simulations have generally not been able to match. Walsh et al. tested a possible mechanism to truncate the disk--a two-stage, inward-then-outward migration of Jupiter and Saturn, as found in numerous hydrodynamical simulations of giant planet formation. In addition to truncating the disk and producing a more realistic Earth/Mars mass ratio, the migration of the giant planets also populates the asteroid belt with two distinct populations of bodies--the inner belt is filled by bodies originating inside of 3 AU, and the outer belt is filled with bodies originating from between and beyond the giant planets (which are hereafter referred to as `primitive' bodies).\n  We find here that the planets will accrete on order 1-2% of their total mass from primitive planetesimals scattered onto planet-crossing orbits during the formation of the planets. For an assumed value of 10% for the water mass fraction of the primitive planetesimals, this model delivers a total amount of water comparable to that estimated to be on the Earth today. While the radial distribution of the planetary masses and the dynamical excitation of their orbits are a good match to the observed system, we find that the last giant impact is typically earlier than 20 Myr, and a substantial amount of mass is accreted after that event. However, 5 of the 27 planets larger than half an Earth mass formed in all simulations do experience large late impacts and subsequent accretion consistent with the dating of the Moon-forming impact and the estimated amount of mass accreted by Earth following that event.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "We develop a technique to overcome phase-mismatch in Josephson-junction traveling wave parametric amplifiers in order to achieve high gain over a broad bandwidth. Using \"resonant phase matching,\" we design a compact superconducting device consisting of a transmission line with subwavelength resonant inclusions that simultaneously achieves a gain of 20 dB, an instantaneous bandwidth of 3 GHz, and a saturation power of -98 dBm. Such an amplifier is well-suited to cryogenic broadband microwave measurements such as the multiplexed readout of quantum coherent circuits based on superconducting, semiconducting, or nano-mechanical elements as well as traditional astronomical detectors.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "The first gravitational-wave (GW) observations will greatly benefit from the detection of coincident electromagnetic counterparts. Electromagnetic follow-ups will nevertheless be challenging for GWs with poorly reconstructed directions. GW source localization can be inefficient (i) if only two GW observatories are in operation; (ii) if the detectors' sensitivities are highly non-uniform; (iii) for events near the detectors' horizon distance. For these events, follow-up observations will need to cover 100-1000 square degrees of the sky over a limited period of time, reducing the list of suitable telescopes. We demonstrate that the Cherenkov Telescope Array will be capable of following up GW event candidates over the required large sky area with sufficient sensitivity to detect short gamma-ray bursts, which are thought to originate from compact binary mergers, out to the horizon distance of advanced LIGO/Virgo. CTA can therefore be invaluable starting with the first multimessenger detections, even with poorly reconstructed GW source directions. This scenario also provides a further scientific incentive for GW observatories to further decrease the delay of their event reconstruction.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Transition metal dichalcogenide (TMDC) monolayer has recently emerged as an important two-dimensional semiconductor with promising potentials for electronic and optoelectronic devices. Unlike semi-metallic graphene, layered TMDC has a sizable band gap. More interestingly, when thinned down to a monolayer, TMDC transforms from an indirect bandgap to a direct bandgap semiconductor, exhibiting a number of intriguing optical phenomena such as valley selective circular dichroism, doping dependent charged excitons, and strong photocurrent responses. However, the fundamental mechanism underlying such a strong light-matter interaction is still under intensive investigation. The observed optical resonance was initially considered to be band-to-band transitions. In contrast, first-principle calculations predicted a much larger quasiparticle band gap size and an optical response that is dominated by excitonic effects. Here, we report experimental evidence of the exciton dominance mechanism by discovering a series of excitonic dark states in single-layer WS2 using two-photon excitation spectroscopy. In combination with GW-BSE theory, we find the excitons are Wannier excitons in nature but possess extraordinarily large binding energy (~0.7 eV), leading to a quasiparticle band gap of 2.7 eV. These strongly bound exciton states are observed stable even at room temperature. We reveal an exciton series in significant deviation from hydrogen models, with a novel inverse energy dependence on the orbital angular momentum. These excitonic energy levels are experimentally found robust against environmental perturbations. The discovery of excitonic dark states and exceptionally large binding energy not only sheds light on the importance of many-electron effects in this two-dimensional gapped system, but also holds exciting potentials for the device application of TMDC monolayers and their heterostructures.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "This White Paper, submitted to the recent ESA call for science themes to define its future large missions, advocates the need for a transformational leap in our understanding of two key questions in astrophysics: 1) How does ordinary matter assemble into the large scale structures that we see today? 2) How do black holes grow and shape the Universe? Hot gas in clusters, groups and the intergalactic medium dominates the baryonic content of the local Universe. To understand the astrophysical processes responsible for the formation and assembly of these large structures, it is necessary to measure their physical properties and evolution. This requires spatially resolved X-ray spectroscopy with a factor 10 increase in both telescope throughput and spatial resolving power compared to currently planned facilities. Feedback from supermassive black holes is an essential ingredient in this process and in most galaxy evolution models, but it is not well understood. X-ray observations can uniquely reveal the mechanisms launching winds close to black holes and determine the coupling of the energy and matter flows on larger scales. Due to the effects of feedback, a complete understanding of galaxy evolution requires knowledge of the obscured growth of supermassive black holes through cosmic time, out to the redshifts where the first galaxies form. X-ray emission is the most reliable way to reveal accreting black holes, but deep survey speed must improve by a factor ~100 over current facilities to perform a full census into the early Universe. The Advanced Telescope for High Energy Astrophysics (Athena+) mission provides the necessary performance (e.g. angular resolution, spectral resolution, survey grasp) to address these questions and revolutionize our understanding of the Hot and Energetic Universe. These capabilities will also provide a powerful observatory to be used in all areas of astrophysics.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Jupiter and Saturn formed in a few million years (Haisch et al. 2001) from a gas-dominated protoplanetary disk, and were susceptible to gas-driven migration of their orbits on timescales of only ~100,000 years (Armitage 2007). Hydrodynamic simulations show that these giant planets can undergo a two-stage, inward-then-outward, migration (Masset & Snellgrove 2001, Morbidelli & Crida 2007, Pierens & Nelson 2008). The terrestrial planets finished accreting much later (Klein et al. 2009), and their characteristics, including Mars' small mass, are best reproduced by starting from a planetesimal disk with an outer edge at about one astronomical unit from the Sun (Wetherill 1978, Hansen 2009) (1 AU is the Earth-Sun distance). Here we report simulations of the early Solar System that show how the inward migration of Jupiter to 1.5 AU, and its subsequent outward migration, lead to a planetesimal disk truncated at 1 AU; the terrestrial planets then form from this disk over the next 30-50 million years, with an Earth/Mars mass ratio consistent with observations. Scattering by Jupiter initially empties but then repopulates the asteroid belt, with inner-belt bodies originating between 1 and 3 AU and outer-belt bodies originating between and beyond the giant planets. This explains the significant compositional differences across the asteroid belt. The key aspect missing from previous models of terrestrial planet formation is the substantial radial migration of the giant planets, which suggests that their behaviour is more similar to that inferred for extrasolar planets than previously thought.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "The evolution of electronic structure of graphene nanoribbons (GNRs) as a function of the number of layers stacked together is investigated using \\textit{ab initio} density functional theory (DFT) including interlayer van der Waals interactions. Multilayer armchair GNRs (AGNRs), similar to single-layer AGNRs, exhibit three classes of band gaps depending on their width. In zigzag GNRs (ZGNRs), the geometry relaxation resulting from interlayer interactions plays a crucial role in determining the magnetic polarization and the band structure. The antiferromagnetic (AF) interlayer coupling is more stable compared to the ferromagnetic (FM) interlayer coupling. ZGNRs with the AF in-layer and AF interlayer coupling have a finite band gap while ZGNRs with the FM in-layer and AF interlayer coupling do not have a band gap. The ground state of the bi-layer ZGNR is non-magnetic with a small but finite band gap. The magnetic ordering is less stable in multilayer ZGNRs compared to single-layer ZGNRs. The quasipartcle GW corrections are smaller for bilayer GNRs compared to single-layer GNRs because of the reduced Coulomb effects in bilayer GNRs compared to single-layer GNRs.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "GRB 051103 is considered to be a candidate soft gamma repeater (SGR) extragalactic giant magnetar flare by virtue of its proximity on the sky to M81/M82, as well as its time history, localization, and energy spectrum. We have derived a refined interplanetary network localization for this burst which reduces the size of the error box by over a factor of two. We examine its time history for evidence of a periodic component, which would be one signature of an SGR giant flare, and conclude that this component is neither detected nor detectable under reasonable assumptions. We analyze the time-resolved energy spectra of this event with improved time- and energy resolution, and conclude that although the spectrum is very hard, its temporal evolution at late times cannot be determined, which further complicates the giant flare association. We also present new optical observations reaching limiting magnitudes of R > 24.5, about 4 magnitudes deeper than previously reported. In tandem with serendipitous observations of M81 taken immediately before and one month after the burst, these place strong constraints on any rapidly variable sources in the region of the refined error ellipse proximate to M81. We do not find any convincing afterglow candidates from either background galaxies or sources in M81, although within the refined error region we do locate two UV bright star forming regions which may host SGRs. A supernova remnant (SNR) within the error ellipse could provide further support for an SGR giant flare association, but we were unable to identify any SNR within the error ellipse. These data still do not allow strong constraints on the nature of the GRB 051103 progenitor, and suggest that candidate extragalactic SGR giant flares will be difficult, although not impossible, to confirm.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "Fault-tolerant quantum computing requires gates which function correctly despite the presence of errors, and are scalable if the error probability-per-gate is below a threshold value. To date, no method has been described for calculating this probability from measurements on a gate. Here we introduce a technique enabling quantitative benchmarking of quantum-logic gates against fault-tolerance thresholds for any architecture. We demonstrate our technique experimentally using a photonic entangling-gate. The relationship between experimental errors and their quantum logic effect is non-trivial: revealing this relationship requires a comprehensive theoretical model of the quantum-logic gate. We show the first such model for any architecture, and find multi-photon emission--a small effect previously regarded as secondary to mode-mismatch--to be the dominant source of logic error. We show that reducing this will move photonic quantum computing to within striking distance of fault-tolerance.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "We present optical and ultraviolet spectra, lightcurves, and Doppler tomograms of the low-mass X-ray binary EXO 0748-676. Using an extensive set of 15 emission line tomograms, we show that, along with the usual emission from the stream and ``hot spot'', there is extended non-axisymmetric emission from the disk rim. Some of the emission and Halpha and beta absorption features lend weight to the hypothesis that part of the stream overflows the disk rim and forms a two phase medium. The data are consistent with a 1.35M_sun neutron star with a main sequence companion and hence a mass ratio q~0.34.\n        \u25b3 Less", "author": "Kevin O'Brien"}, {"abstract": "The theoretical basis for conventional acquisition of bandlimited signals typically relies on uniform time sampling and assumes infinite-precision amplitude values. In this paper, we explore signal representation and recovery based on uniform amplitude sampling with assumed infinite precision timing information. The approach is based on the delta-ramp encoder which consists of applying a one-level level-crossing detector to the result of adding an appropriate sawtooth-like waveform to the input signal. The output samples are the time instants of these level crossings, thus representing a time-encoded version of the input signal. For theoretical purposes, this system can be equivalently analyzed by reversibly transforming through ramp addition a nonmonotonic input signal into a monotonic one which is then uniformly sampled in amplitude. The monotonic function is then represented by the times at which the signal crosses a predefined and equally-spaced set of amplitude values. We refer to this technique as amplitude sampling. The time sequence generated can be interpreted alternatively as nonuniform time sampling of the original source signal. We derive duality and frequency-domain properties for the functions involved in the transformation. Iterative algorithms are proposed and implemented for recovery of the original source signal. As indicated in the simulations, the proposed iterative amplitude-sampling algorithm achieves a faster convergence rate than frame-based reconstruction for nonuniform sampling. The performance can also be improved by appropriate choice of the parameters while maintaining the same sampling density.\n        \u25b3 Less", "author": "Alan Oppenheim"}, {"abstract": "Analog-to-digital (A/D) converters are the common interface between analog signals and the domain of digital discrete-time signal processing. In essence, this domain simultaneously incorporates quantization both in amplitude and time, i.e. amplitude quantization and uniform time sampling. Thus, we view A/D conversion as a sampling process in both the time and amplitude domains based on the observation that the underlying continuous-time signals representing digital sequences can be sampled in a lattice---i.e. at points restricted to lie in a uniform grid both in time and amplitude. We refer to them as lattice functions. This is in contrast with the traditional approach based on the classical sampling theorem and quantization error analysis. The latter has been mainly addressed with the help of probabilistic models, or deterministic ones either confined to very particular scenarios or considering worst-case assumptions. In this paper, we provide a deterministic theoretical analysis and framework for the functions involved in digital discrete-time processing. We show that lattice functions possess a rich analytic structure in the context of integral-valued entire functions of exponential type. We derive set and spectral properties of this class of functions. This allows us to prove in a deterministic way and for general bandlimited functions a fundamental bound on the spectrum of the quantization error that is independent of the resolution of the quantizer.\n        \u25b3 Less", "author": "Alan Oppenheim"}, {"abstract": "Designing and implementing systems as an interconnection of smaller subsystems is a common practice for modularity and standardization of components and design algorithms. Although not typically cast in this framework, many of these approaches can be viewed within the mathematical context of functional composition. This paper re-interprets and generalizes within the functional composition framework one such approach known as filter sharpening, i.e. interconnecting filter modules which have significant approximation error in order to obtain improved filter characteristics. More specifically, filter sharpening is approached by determining the composing polynomial to minimize the infinity-norm of the approximation error, utilizing the First Algorithm of Remez. This is applied both to sharpening for FIR, even-symmetric filters and for the more general case of subfilters that have complex-valued frequency responses including causal IIR filters and for continuous-time filters. Within the framework of functional composition, this paper also explores the use of functional decomposition to approximate a desired system as a composition of simpler functions based on a two-norm on the approximation error. Among the potential advantages of this decomposition is the ability for modular implementation in which the inner component of the functional decomposition represents the subfilters and the outer the interconnection.\n        \u25b3 Less", "author": "Alan Oppenheim"}, {"abstract": "This paper presents two novel regularization methods motivated in part by the geometric significance of biorthogonal bases in signal processing applications. These methods, in particular, draw upon the structural relevance of orthogonality and biorthogonality principles and are presented from the perspectives of signal processing, convex programming, continuation methods and nonlinear projection operators. Each method is specifically endowed with either a homotopy or tuning parameter to facilitate tradeoff analysis between accuracy and numerical stability. An example involving a basis comprised of real exponential signals illustrates the utility of the proposed methods on an ill-conditioned inverse problem and the results are compared to standard regularization techniques from the signal processing literature.\n        \u25b3 Less", "author": "Alan Oppenheim"}, {"abstract": "This paper presents a general framework for generating greedy algorithms for solving convex constraint satisfaction problems for sparse solutions by mapping the satisfaction problem into one of graph traversal on a rooted tree of unknown topology. For every pre-walk of the tree an initial set of generally dense feasible solutions is processed in such a way that the sparsity of each solution increases with each generation unveiled. The specific computation performed at any particular child node is shown to correspond to an embedding of a polytope into the polytope received from that nodes parent. Several issues related to pre-walk order selection, computational complexity and tractability, and the use of heuristic and/or side information is discussed. An example of a single-path, depth-first algorithm on a tree with randomized vertex reduction and a run-time path selection algorithm is presented in the context of sparse lowpass filter design.\n        \u25b3 Less", "author": "Alan Oppenheim"}, {"abstract": "Temporal quantum coherence and control is foundational to the science and engineering of quantum systems. In van der Waals (vdW) materials, the aggregate coherent behavior of carriers has been probed successfully by transport measurements. However, temporal coherence and control, as exemplified by manipulating a single quantum degree of freedom, remains to be verified. Here we demonstrate such coherence and control of a superconducting circuit incorporating graphene-based Josephson junctions. Furthermore, we show that this device can be operated as a voltage-tunable transmon qubit, whose spectrum reflects the electronic properties of massless Dirac fermions traveling ballistically. In addition to the potential for advancing extensible quantum computing technology, our results represent a new approach to studying vdW materials using microwave photons in coherent quantum circuits.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "The prospect of computational hardware with quantum advantage relies critically on the quality of quantum gate operations. Imperfect two-qubit gates is a major bottleneck for achieving scalable quantum information processors. Here, we propose a generalizable and extensible scheme for a two-qubit coupler switch that controls the qubit-qubit coupling by modulating the coupler frequency. Two-qubit gate operations can be implemented by operating the coupler in the dispersive regime, which is non-invasive to the qubit states. We investigate the performance of the scheme by simulating a universal two-qubit gate on a superconducting quantum circuit, and find that errors from known parasitic effects are strongly suppressed. The scheme is compatible with existing high-coherence hardware, thereby promising a higher gate fidelity with current technologies.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "In the cavity-QED architecture, photon number fluctuations from residual cavity photons cause qubit dephasing due to the AC Stark effect. These unwanted photons originate from a variety of sources, such as thermal radiation, leftover measurement photons, and crosstalk. Using a capacitively-shunted flux qubit coupled to a transmission line cavity, we demonstrate a method that identifies and distinguishes coherent and thermal photons based on noise-spectral reconstruction from time-domain spin-locking relaxometry. Using these measurements, we attribute the limiting dephasing source in our system to thermal photons, rather than coherent photons. By improving the cryogenic attenuation on lines leading to the cavity, we successfully suppress residual thermal photons and achieve $T_1$-limited spin-echo decay time. The spin-locking noise spectroscopy technique can readily be applied to other qubit modalities for identifying general asymmetric non-classical noise spectra.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "Dynamical error suppression techniques are commonly used to improve coherence in quantum systems. They reduce dephasing errors by applying control pulses designed to reverse erroneous coherent evolution driven by environmental noise. However, such methods cannot correct for irreversible processes such as energy relaxation. In this work, we investigate a complementary, stochastic approach to reducing errors: instead of deterministically reversing the unwanted qubit evolution, we use control pulses to shape the noise environment dynamically. In the context of superconducting qubits, we implement a pumping sequence to reduce the number of unpaired electrons (quasiparticles) in close proximity to the device. We report a 70% reduction in the quasiparticle density, resulting in a threefold enhancement in qubit relaxation times, and a comparable reduction in coherence variability.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We assess independently the impact of high-temperature substrate annealing and metal deposition conditions on the coherence of transmon qubits in the standard 2D circuit-QED architecture. We restrict our study to devices made with aluminum interdigital capacitors on sapphire substrates. We record more than an order-of-magnitude improvement in the relaxation time of devices made with an annealed substrate, independent of whether a conventional evaporator or molecular beam epitaxy chamber was used to deposit the aluminum. We also infer similar levels of flux noise and photon shot noise through dephasing measurements on these devices. Our results indicate that substrate annealing plays a primary role in fabricating low-loss qubits, consistent with the hypothesis that substrate-air and substrate-metal interfaces are essential factors limiting the qubit lifetimes in superconducting circuits.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We present measurements of coherence and successive decay dynamics of higher energy levels of a superconducting transmon qubit. By applying consecutive $\u03c0$-pulses for each sequential transition frequency, we excite the qubit from the ground state up to its fourth excited level and characterize the decay and coherence of each state. We find the decay to proceed mainly sequentially, with relaxation times in excess of 20 $\u03bc$s for all transitions. We also provide a direct measurement of the charge dispersion of these levels by analyzing beating patterns in Ramsey fringes. The results demonstrate the feasibility of using higher levels in transmon qubits for encoding quantum information.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We present a new method for determining pulse imperfections and improving the single-gate fidelity in a superconducting qubit. By applying consecutive positive and negative $\u03c0$ pulses, we amplify the qubit evolution due to microwave pulse distortion, which causes the qubit state to rotate around an axis perpendicular to the intended rotation axis. Measuring these rotations as a function of pulse period allows us to reconstruct the shape of the microwave pulse arriving at the sample. Using the extracted response to predistort the input signal, we are able to improve the pulse shapes and to reach an average single-qubit gate fidelity higher than 99.8%.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We implement dynamical decoupling techniques to mitigate noise and enhance the lifetime of an entangled state that is formed in a superconducting flux qubit coupled to a microscopic two-level system. By rapidly changing the qubit's transition frequency relative to the two-level system, we realize a refocusing pulse that reduces dephasing due to fluctuations in the transition frequencies, thereby improving the coherence time of the entangled state. The coupling coherence is further enhanced when applying multiple refocusing pulses, in agreement with our $1/f$ noise model. The results are applicable to any two-qubit system with transverse coupling, and they highlight the potential of decoupling techniques for improving two-qubit gate fidelities, an essential prerequisite for implementing fault-tolerant quantum computing.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We have investigated the driven dynamics of a superconducting flux qubit that is tunably coupled to a microwave resonator. We find that the qubit experiences an oscillating field mediated by off-resonant driving of the resonator, leading to strong modifications of the qubit Rabi frequency. This opens an additional noise channel, and we find that low-frequency noise in the coupling parameter causes a reduction of the coherence time during driven evolution. The noise can be mitigated with the rotary-echo pulse sequence, which, for driven systems, is analogous to the Hahn-echo sequence.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We report a direct measurement of the low-frequency noise spectrum in a superconducting flux qubit. Our method uses the noise sensitivity of a free-induction Ramsey interference experiment, comprising free evolution in the presence of noise for a fixed period of time followed by single-shot qubit-state measurement. Repeating this procedure enables Fourier-transform noise spectroscopy with access to frequencies up to the achievable repetition rate, a regime relevant to dephasing in ensemble-averaged time-domain measurements such as Ramsey interferometry. Rotating the qubit's quantization axis allows us to measure two types of noise: effective flux noise and effective critical-current or charge noise. For both noise sources, we observe that the very same 1/f-type power laws measured at considerably higher frequencies (0.2-20 MHz) are consistent with the noise in the 0.01-100-Hz range measured here. We find no evidence of temperature dependence of the noises over 65-200 mK, and also no evidence of time-domain correlations between the two noises. These methods and results are pertinent to the dephasing of all superconducting qubits.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "The energy-level structure of a quantum system plays a fundamental role in determining its behavior and manifests itself in a discrete absorption and emission spectrum. Conventionally, spectra are probed via frequency spectroscopy whereby the frequency \u03bdof a harmonic driving field is varied to fulfill the conditions \u0394E = h \u03bd, where the driving field is resonant with the level separation \u0394E (h is Planck's constant). Although this technique has been successfully employed in a variety of physical systems, including natural and artificial atoms and molecules, its application is not universally straightforward, and becomes extremely challenging for frequencies in the range of 10's and 100's of gigahertz. Here we demonstrate an alternative approach, whereby a harmonic driving field sweeps the atom through its energy-level avoided crossings at a fixed frequency, surmounting many of the limitations of the conventional approach. Spectroscopic information is obtained from the amplitude dependence of the system response. The resulting ``spectroscopy diamonds'' contain interference patterns and population inversion that serve as a fingerprint of the atom's spectrum. By analyzing these features, we determine the energy spectrum of a manifold of states with energies from 0.01 to 120 GHz \\times h in a superconducting artificial atom, using a driving frequency near 0.1 GHz. This approach provides a means to manipulate and characterize systems over a broad bandwidth, using only a single driving frequency that may be orders of magnitude smaller than the energy scales being probed.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We demonstrate Mach-Zehnder-type interferometry in a superconducting flux qubit. The qubit is a tunable artificial atom, whose ground and excited states exhibit an avoided crossing. Strongly driving the qubit with harmonic excitation sweeps it through the avoided crossing two times per period. As the induced Landau-Zener transitions act as coherent beamsplitters, the accumulated phase between transitions, which varies with microwave amplitude, results in quantum interference fringes for n=1...20 photon transitions. The generalization of optical Mach-Zehnder interferometry, performed in qubit phase space, provides an alternative means to manipulate and characterize the qubit in the strongly-driven regime.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We have implemented a resonant circuit that uses a SQUID as a flux-sensitive Josephson inductor for qubit readout. In contrast to the conventional switching current measurement that generates undesired quasi-particles when the SQUID switches to the voltage state, our approach keeps the readout SQUID biased along the supercurrent branch during the measurement. By incorporating the SQUID inductor in a high-Q resonant circuit, we can distinguish the two flux states of a niobium persistent-current (PC) qubit by observing a shift in the resonant frequency of both the magnitude and the phase spectra. The readout circuit was also characterized in the nonlinear regime to investigate its potential use as a nonlinear amplifier.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "A scalable superconducting architecture for adiabatic quantum computers is proposed. The architecture is based on time-independent, nearest-neighbor interqubit couplings: it can handle any problem in the class NP even in the presence of measurement errors, noise, and decoherence. The implementation of this architecture with superconducting persistent-current qubits and the natural robustness of such an implementation to manufacturing imprecision and decoherence are discussed.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "We present analytical and numerical studies of pinned superconducting states of open-ended Josephson ladder arrays, neglecting inductances but taking edge effects into account. Treating the edge effects perturbatively, we find analytical approximations for three of these superconducting states -- the no-vortex, fully-frustrated and single-vortex states -- as functions of the dc bias current $I$ and the frustration $f$. Bifurcation theory is used to derive formulas for the depinning currents and critical frustrations at which the superconducting states disappear or lose dynamical stability as $I$ and $f$ are varied. These results are combined to yield a zero-temperature stability diagram of the system with respect to $I$ and $f$. To highlight the effects of the edges, we compare this dynamical stability diagram to the thermodynamic phase diagram for the infinite system where edges have been neglected. We briefly indicate how to extend our methods to include self-inductances.\n        \u25b3 Less", "author": "Terry Orlando"}, {"abstract": "In this paper, we study the problem of escaping from saddle points in smooth nonconvex optimization problems subject to a convex set $\\mathcal{C}$. We propose a generic framework that yields convergence to a second-order stationary point of the problem, if the convex set $\\mathcal{C}$ is simple for a quadratic objective function. Specifically, our results hold if one can find a $\u03c1$-approximate solution of a quadratic program subject to $\\mathcal{C}$ in polynomial time, where $\u03c1<1$ is a positive constant that depends on the structure of the set $\\mathcal{C}$. Under this condition, we show that the sequence of iterates generated by the proposed framework reaches an $(\u03b5,\u03b3)$-second order stationary point (SOSP) in at most $\\mathcal{O}(\\max\\{\u03b5^{-2},\u03c1^{-3}\u03b3^{-3}\\})$ iterations. We further characterize the overall complexity of reaching an SOSP when the convex set $\\mathcal{C}$ can be written as a set of quadratic constraints and the objective function Hessian has a specific structure over the convex set $\\mathcal{C}$. Finally, we extend our results to the stochastic setting and characterize the number of stochastic gradient and Hessian evaluations to reach an $(\u03b5,\u03b3)$-SOSP.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "This paper considers a novel framework to detect communities in a graph from the observation of signals at its nodes. We model the observed signals as noisy outputs of an unknown network process -- represented as a graph filter -- that is excited by a set of low-rank inputs. Rather than learning the precise parameters of the graph itself, the proposed method retrieves the community structure directly; Furthermore, as in blind system identification methods, it does not require knowledge of the system excitation. The paper shows that communities can be detected by applying spectral clustering to the low-rank output covariance matrix obtained from the graph signals. The performance analysis indicates that the community detection accuracy depends on the spectral properties of the graph filter considered. Furthermore, we show that the accuracy can be improved via a low-rank matrix decomposition method when the excitation signals are known. Numerical experiments demonstrate that our approach is effective for analyzing network data from diffusion, consumers, and social dynamics.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study a routing game in an environment with multiple heterogeneous information systems and an uncertain state that affects edge costs of a congested network. Each information system sends a noisy signal about the state to its subscribed traveler population. Travelers make route choices based on their private beliefs about the state and other populations' signals. The question then arises, \"How does the presence of asymmetric and incomplete information affect the travelers' equilibrium route choices and costs?\" We develop a systematic approach to characterize the equilibrium structure, and determine the effect of population sizes on the relative value of information (i.e. difference in expected traveler costs) between any two populations. This effect can be evaluated using a population-specific size threshold. One population enjoys a strictly positive value of information in comparison to the other if and only if its size is below the corresponding threshold. We also consider the situation when travelers may choose an information system based on its value, and characterize the set of equilibrium adoption rates delineating the sizes of subscribed traveler populations. The resulting routing strategies are such that all travelers face an identical expected cost, and no traveler has the incentive to change her subscription.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "Semidefinite programming (SDP) with equality constraints arise in many optimization and machine learning problems, such as Max-Cut, community detection and robust PCA. Although SDPs can be solved to arbitrary precision in polynomial time, generic convex solvers do not scale well with the dimension of the problem. In order to address this issue, Burer and Monteiro \\cite{burer2003nonlinear} proposed to reduce the dimension of the problem by appealing to a low-rank factorization, and solve the subsequent non-convex problem instead. It is well-understood that the resulting non-convex problem acts as a reliable surrogate to the original SDP, and can be efficiently solved using the block-coordinate maximization method. Despite its simplicity, remarkable success, and wide use in practice, the theoretical understanding of the convergence of this method is limited. We prove that the block-coordinate maximization algorithm applied to the non-convex Burer-Monteiro approach enjoys a global sublinear rate without any assumptions on the problem, and a local linear convergence rate despite no local maxima is locally strongly concave. We illustrate our results through examples and numerical experiments.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study the trade-offs between convergence rate and robustness to gradient errors in designing a first-order algorithm. We focus on gradient descent (GD) and accelerated gradient (AG) methods for minimizing strongly convex functions when the gradient has random errors in the form of additive white noise. With gradient errors, the function values of the iterates need not converge to the optimal value; hence, we define the robustness of an algorithm to noise as the asymptotic expected suboptimality of the iterate sequence to input noise power. For this robustness measure, we provide exact expressions for the quadratic case using tools from robust control theory and tight upper bounds for the smooth strongly convex case using Lyapunov functions certified through matrix inequalities. We use these characterizations within an optimization problem which selects parameters of each of the algorithms to achieve a particular trade-off between rate and robustness. Our results show that AG can achieve acceleration while being more robust to random gradient errors. This behavior is quite different than previously reported in the deterministic gradient noise setting. Our framework also leads to practical algorithms that can perform better than other state-of-the-art methods in the presence of random gradient noise.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider coordinate descent (CD) methods with exact line search on convex quadratic problems. Our main focus is to study the performance of the CD method that use random permutations in each epoch and compare it to the performance of the CD methods that use deterministic orders and random sampling with replacement. We focus on a class of convex quadratic problems with a diagonally dominant Hessian matrix, for which we show that using random permutations instead of random with-replacement sampling improves the performance of the CD method in the worst-case. Furthermore, we prove that as the Hessian matrix becomes more diagonally dominant, the performance improvement attained by using random permutations increases. We also show that for this problem class, using any fixed deterministic order yields a superior performance than using random permutations. We present detailed theoretical analyses with respect to three different convergence criteria that are used in the literature and support our theoretical results with numerical experiments.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "The study of strategic behavior in large scale networks via standard game theoretical methods is a challenging, if not intractable, task. In this paper, we propose a way to approximate games played over networks of increasing size, by using the graph limiting concept of graphon. To this end, we introduce the new class of graphon games for populations of infinite size. As a first contribution, we investigate properties of the Nash equilibrium of this newly defined class of games, including existence, uniqueness and comparative statics. As a second contribution, we illustrate how graphon games can be used to approximate strategic behavior in large but finite network games by assuming that the network is randomly drawn according to the graphon and we derive precise bounds for the distance between graphon and sampled network game equilibria in terms of the population size. Finally, we derive a closed form expression for the Nash equilibrium of linear quadratic graphon games and we illustrate its relation to Bonacich centrality.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We provide a unified variational inequality framework for the study of fundamental properties of the Nash equilibrium in network games. We identify several conditions on the underlying network (in terms of spectral norm, infinity norm and minimum eigenvalue of its adjacency matrix) that guarantee existence, uniqueness, convergence and continuity of equilibrium in general network games with multidimensional and possibly constrained strategy sets. We delineate the relations between these conditions and characterize classes of networks that satisfy each of these conditions.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We investigate the sensitivity of the Nash equilibrium of constrained network aggregative games to changes in exogenous parameters affecting the cost function of the players. This setting is motivated by two applications. The first is the analysis of interventions by a social planner with a networked objective function while the second is network routing games with atomic players and information constraints. By exploiting a primal reformulation of a sensitivity analysis result for variational inequalities, we provide a characterization of the sensitivity of the Nash equilibrium that depends on primal variables only. To derive this result we assume strong monotonicity of the mapping associated with the game. As the second main result, we derive sufficient conditions that guarantee this strong monotonicity property in network aggregative games. These two characterizations allows us to systematically study changes in the Nash equilibrium due to perturbations or parameter variations in the two applications mentioned above.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study the optimal pricing strategy of a monopolist selling homogeneous goods to customers over multiple periods. The customers choose their time of purchase to maximize their payoff that depends on their valuation of the product, the purchase price, and the utility they derive from past purchases of others, termed the network effect. We first show that the optimal price sequence is non-decreasing. Therefore, by postponing purchase to future rounds, customers trade-off a higher utility from the network effects with a higher price. We then show that a customer's equilibrium strategy can be characterized by a threshold rule in which at each round a customer purchases the product if and only if her valuation exceeds a certain threshold. This implies that customers face an inference problem regarding the valuations of others, i.e., observing that a customer has not yet purchased the product, signals that her valuation is below a threshold. We consider a block model of network interactions, where there are blocks of buyers subject to the same network effect. A natural benchmark, this model allows us to provide an explicit characterization of the optimal price sequence asymptotically as the number of agents goes to infinity, which notably is linearly increasing in time with a slope that depends on the network effect through a scalar given by the sum of entries of the inverse of the network weight matrix. Our characterization shows that increasing the \"imbalance\" in the network defined as the difference between the in and out degree of the nodes increases the revenue of the monopolist. We further study the effects of price discrimination and show that in earlier periods monopolist offers lower prices to blocks with higher Bonacich centrality to encourage them to purchase, which in turn further incentivizes other customers to buy in subsequent periods.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider a multi agent optimization problem where a set of agents collectively solves a global optimization problem with the objective function given by the sum of locally known convex functions. We focus on the case when information exchange among agents takes place over a directed network and propose a distributed subgradient algorithm in which each agent performs local processing based on information obtained from his incoming neighbors. Our algorithm uses weight balancing to overcome the asymmetries caused by the directed communication network, i.e., agents scale their outgoing information with dynamically updated weights that converge to balancing weights of the graph. We show that both the objective function values and the consensus violation, at the ergodic average of the estimates generated by the algorithm, converge with rate $O(\\frac{\\log T}{\\sqrt{T}})$, where $T$ is the number of iterations. A special case of our algorithm provides a new distributed method to compute average consensus over directed graphs.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "To systematically study the implications of additional information about routes provided to certain users (e.g., via GPS-based route guidance systems), we introduce a new class of congestion games in which users have differing information sets about the available edges and can only use routes consisting of edges in their information set. After defining the notion of Information Constrained Wardrop Equilibrium (ICWE) for this class of congestion games and studying its basic properties, we turn to our main focus: whether additional information can be harmful (in the sense of generating greater equilibrium costs/delays). We formulate this question in the form of Informational Braes' Paradox (IBP), which extends the classic Braess' Paradox in traffic equilibria, and asks whether users receiving additional information can become worse off. We provide a comprehensive answer to this question showing that in any network in the series of linearly independent (SLI) class, which is a strict subset of series-parallel networks, IBP cannot occur, and in any network that is not in the SLI class, there exists a configuration of edge-specific cost functions for which IBP will occur. In the process, we establish several properties of the SLI class of networks, which include the characterization of the complement of the SLI class in terms of embedding a specific set of networks, and also an algorithm which determines whether a graph is SLI in linear time. We further prove that the worst-case inefficiency performance of ICWE is no worse than the standard Wardrop equilibrium.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We propose a distributed algorithm based on Alternating Direction Method of Multipliers (ADMM) to minimize the sum of locally known convex functions using communication over a network. This optimization problem emerges in many applications in distributed machine learning and statistical estimation. We show that when functions are convex, both the objective function values and the feasibility violation converge with rate $O(\\frac{1}{T})$, where $T$ is the number of iterations. We then show that if the functions are strongly convex and have Lipschitz continuous gradients, the sequence generated by our algorithm converges linearly to the optimal solution. In particular, an $\u03b5$-optimal solution can be computed with $O(\\sqrt{\u03ba_f} \\log (1/\u03b5))$ iterations, where $\u03ba_f$ is the condition number of the problem. Our analysis also highlights the effect of network structure on the convergence rate through maximum and minimum degree of nodes as well as the algebraic connectivity of the network.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider an SIS-type epidemic process that evolves on a known graph. We assume that a fixed curing budget can be allocated at each instant to the nodes of the graph, towards the objective of minimizing the expected extinction time of the epidemic. We provide a lower bound on the optimal expected extinction time as a function of the available budget, the epidemic parameters, the maximum degree, and the CutWidth of the graph. For graphs with large CutWidth (close to the largest possible), and under a budget which is sublinear in the number of nodes, our lower bound scales exponentially with the size of the graph.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider the propagation of a contagion process (epidemic) on a network and study the problem of dynamically allocating a fixed curing budget to the nodes of the graph, at each time instant. For bounded degree graphs, we provide a lower bound on the expected time to extinction under any such dynamic allocation policy, in terms of a combinatorial quantity that we call the resistance of the set of initially infected nodes, the available budget, and the number of nodes n. Specifically, we consider the case of bounded degree graphs, with the resistance growing linearly in n. We show that if the curing budget is less than a certain multiple of the resistance, then the expected time to extinction grows exponentially with n. As a corollary, if all nodes are initially infected and the CutWidth of the graph grows linearly, while the curing budget is less than a certain multiple of the CutWidth, then the expected time to extinction grows exponentially in n. The combination of the latter with our prior work establishes a fairly sharp phase transition on the expected time to extinction (sub-linear versus exponential) based on the relation between the CutWidth and the curing budget.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We introduce a new class of (dynamical) systems that inherently capture cascading effects (viewed as consequential effects) and are naturally amenable to combinations. We develop an axiomatic general theory around those systems, and guide the endeavor towards an understanding of cascading failure. The theory evolves as an interplay of lattices and fixed points, and its results may be instantiated to commonly studied models of cascade effects.\n  We characterize the systems through their fixed points, and equip them with two operators. We uncover properties of the operators, and express global systems through combinations of local systems. We enhance the theory with a notion of failure, and understand the class of shocks inducing a system to failure. We develop a notion of mu-rank to capture the energy of a system, and understand the minimal amount of effort required to fail a system, termed resilience. We deduce a dual notion of fragility and show that the combination of systems sets a limit on the amount of fragility inherited.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "Motivated by applications to distributed optimization over networks and large-scale data processing in machine learning, we analyze the deterministic incremental aggregated gradient method for minimizing a finite sum of smooth functions where the sum is strongly convex. This method processes the functions one at a time in a deterministic order and incorporates a memory of previous gradient values to accelerate convergence. Empirically it performs well in practice; however, no theoretical analysis with explicit rate results was previously given in the literature to our knowledge, in particular most of the recent efforts concentrated on the randomized versions. In this paper, we show that this deterministic algorithm has global linear convergence and characterize the convergence rate. We also consider an aggregated method with momentum and demonstrate its linear convergence. Our proofs rely on a careful choice of a Lyapunov function that offers insight into the algorithm's behavior and simplifies the proofs considerably.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We present a distributed asynchronous algorithm for approximating a single component of the solution to a system of linear equations $Ax = b$, where $A$ is a positive definite real matrix, and $b \\in \\mathbb{R}^n$. This is equivalent to solving for $x_i$ in $x = Gx + z$ for some $G$ and $z$ such that the spectral radius of $G$ is less than 1. Our algorithm relies on the Neumann series characterization of the component $x_i$, and is based on residual updates. We analyze our algorithm within the context of a cloud computation model, in which the computation is split into small update tasks performed by small processors with shared access to a distributed file system. We prove a robust asymptotic convergence result when $\u03c1(\\tilde{G}) < 1$, regardless of the precise order and frequency in which the update tasks are performed, where $\\tilde{G}_{ij} = |G_{ij}|$. We provide convergence rate bounds which depend on the order of update tasks performed, analyzing both deterministic update rules via counting weighted random walks, as well as probabilistic update rules via concentration bounds. The probabilistic analysis requires analyzing the product of random matrices which are drawn from distributions that are time and path dependent. We specifically consider the setting where $n$ is large, yet $G$ is sparse, e.g., each row has at most $d$ nonzero entries. This is motivated by applications in which $G$ is derived from the edge structure of an underlying graph. Our results prove that if the local neighborhood of the graph does not grow too quickly as a function of $n$, our algorithm can provide significant reduction in computation cost as opposed to any algorithm which computes the global solution vector $x$. Our algorithm obtains an $\u03b5\\|x\\|_2$ additive approximation for $x_i$ in constant time with respect to the size of the matrix when $d = O(1)$ and $1/(1-\\|G\\|_2) = O(1)$.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "Motivated by machine learning problems over large data sets and distributed optimization over networks, we develop and analyze a new method called incremental Newton method for minimizing the sum of a large number of strongly convex functions. We show that our method is globally convergent for a variable stepsize rule. We further show that under a gradient growth condition, convergence rate is linear for both variable and constant stepsize rules. By means of an example, we show that without the gradient growth condition, incremental Newton method cannot achieve linear convergence. Our analysis can be extended to study other incremental methods: in particular, we obtain a linear convergence rate result for the incremental Gauss-Newton algorithm under a variable stepsize rule.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We provide a dynamic policy for the rapid containment of a contagion process modeled as an SIS epidemic on a bounded degree undirected graph with n nodes. We show that if the budget $r$ of curing resources available at each time is $\u03a9(W)$, where $W$ is the CutWidth of the graph, and also of order $\u03a9(\\log n)$, then the expected time until the extinction of the epidemic is of order $O(n/r)$, which is within a constant factor from optimal, as well as sublinear in the number of nodes. Furthermore, if the CutWidth increases only sublinearly with n, a sublinear expected time to extinction is possible with a sublinearly increasing budget $r$.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "In this paper, we present a novel iterative Monte Carlo method for approximating the stationary probability of a single state of a positive recurrent Markov chain. We utilize the characterization that the stationary probability of a state $i$ is inversely proportional to the expected return time of a random walk beginning at $i$. Our method obtains an $\u03b5$-multiplicative close estimate with probability greater than $1 - \u03b1$ using at most $\\tilde{O}\\left(t_{\\text{mix}} \\ln(1/\u03b1) / \u03c0_i \u03b5^2 \\right)$ simulated random walk steps on the Markov chain across all iterations, where $t_{\\text{mix}}$ is the standard mixing time and $\u03c0_i$ is the stationary probability. In addition, the estimate at each iteration is guaranteed to be an upper bound with high probability, and is decreasing in expectation with the iteration count, allowing us to monitor the progress of the algorithm and design effective termination criteria. We propose a termination criteria which guarantees a $\u03b5(1 + 4 \\ln(2) t_{\\text{mix}})$ multiplicative error performance for states with stationary probability larger than $\u0394$, while providing an additive error for states with stationary probability less than $\u0394\\in (0,1)$. The algorithm along with this termination criteria uses at most $\\tilde{O}\\left(\\frac{\\ln(1/\u03b1)}{\u03b5^2} \\min\\left(\\frac{t_{\\text{mix}}}{\u03c0_i}, \\frac{1}{\u03b5\u0394}\\right)\\right)$ simulated random walk steps, which is bounded by a constant with respect to the Markov Chain. We provide a tight analysis of our algorithm based on a locally weighted variant of the mixing time. Our results naturally extend for countably infinite state space Markov chains via Lyapunov function analysis.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "This paper proposes a new measure of node centrality in social networks, the Harmonic Influence Centrality, which emerges naturally in the study of social influence over networks. Using an intuitive analogy between social and electrical networks, we introduce a distributed message passing algorithm to compute the Harmonic Influence Centrality of each node. Although its design is based on theoretical results which assume the network to have no cycle, the algorithm can also be successfully applied on general graphs.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider a network of agents that are cooperatively solving a global optimization problem, where the objective function is the sum of privately known local objective functions of the agents and the decision variables are coupled via linear constraints. Recent literature focused on special cases of this formulation and studied their distributed solution through either subgradient based methods with O(1/sqrt(k)) rate of convergence (where k is the iteration number) or Alternating Direction Method of Multipliers (ADMM) based methods, which require a synchronous implementation and a globally known order on the agents. In this paper, we present a novel asynchronous ADMM based distributed method for the general formulation and show that it converges at the rate O(1/k).\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We introduce the notion of exchangeable equilibria of a symmetric bimatrix game, defined as those correlated equilibria in which players' strategy choices are conditionally independently and identically distributed given some hidden variable. We give several game-theoretic interpretations and a version of the \"revelation principle\". Geometrically, the set of exchangeable equilibria is convex and lies between the symmetric Nash equilibria and the symmetric correlated equilibria. Exchangeable equilibria can achieve higher expected utility than symmetric Nash equilibria.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study a model for cascade effects over finite networks based on a deterministic binary linear threshold model. Our starting point is a networked coordination game where each agent's payoff is the sum of the payoffs coming from pairwise interactions with each of the neighbors. We first establish that the best response dynamics in this networked game is equivalent to the linear threshold dynamics with heterogeneous thresholds over the agents. While the previous literature has studied such linear threshold models under the assumption that each agent may change actions at most once, a study of best response dynamics in such networked games necessitates an analysis that allows for multiple switches in actions. In this paper, we develop such an analysis and construct a combinatorial framework to understand the behavior of the model. To this end, we establish that the agents behavior cycles among different actions in the limit and provide three sets of results.\n  We first characterize the limiting behavioral properties of the dynamics. We determine the length of the limit cycles and reveal bounds on the time steps required to reach such cycles for different network structures. We then study the complexity of decision/counting problems that arise within the context. Specifically, we consider the tractability of counting the number of limit cycles and fixed-points, and deciding the reachability of action profiles. We finally propose a measure of network resilience that captures the nature of the involved dynamics. We prove bounds and investigate the resilience of different network structures under this measure.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We present a distributed proximal-gradient method for optimizing the average of convex functions, each of which is the private local objective of an agent in a network with time-varying topology. The local objectives have distinct differentiable components, but they share a common nondifferentiable component, which has a favorable structure suitable for effective computation of the proximal operator. In our method, each agent iteratively updates its estimate of the global minimum by optimizing its local objective function, and exchanging estimates with others via communication in the network. Using Nesterov-type acceleration techniques and multiple communication steps per iteration, we show that this method converges at the rate 1/k (where k is the number of communication rounds between the agents), which is faster than the convergence rate of the existing distributed methods for solving this problem. The superior convergence rate of our method is also verified by numerical experiments.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider an infinite collection of agents who make decisions, sequentially, about an unknown underlying binary state of the world. Each agent, prior to making a decision, receives an independent private signal whose distribution depends on the state of the world. Moreover, each agent also observes the decisions of its last K immediate predecessors. We study conditions under which the agent decisions converge to the correct value of the underlying state. We focus on the case where the private signals have bounded information content and investigate whether learning is possible, that is, whether there exist decision rules for the different agents that result in the convergence of their sequence of individual decisions to the correct state of the world. We first consider learning in the almost sure sense and show that it is impossible, for any value of K. We then explore the possibility of convergence in probability of the decisions to the correct state. Here, a distinction arises: if K equals 1, learning in probability is impossible under any decision rule, while for K greater or equal to 2, we design a decision rule that achieves it. We finally consider a new model, involving forward looking strategic agents, each of which maximizes the discounted sum (over all agents) of the probabilities of a correct decision. (The case, studied in previous literature, of myopic agents who maximize the probability of their own decision being correct is an extreme special case.) We show that for any value of K, for any equilibrium of the associated Bayesian game, and under the assumption that each private signal has bounded information content, learning in probability fails to obtain.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We present a framework for studying the problem of media streaming in technology and cost heterogeneous environments. We first address the problem of efficient streaming in a technology-heterogeneous setting. We employ random linear network coding to simplify the packet selection strategies and alleviate issues such as duplicate packet reception. Then, we study the problem of media streaming from multiple cost-heterogeneous access networks. Our objective is to characterize analytically the trade-off between access cost and user experience. We model the Quality of user Experience (QoE) as the probability of interruption in playback as well as the initial waiting time. We design and characterize various control policies, and formulate the optimal control problem using a Markov Decision Process (MDP) with a probabilistic constraint. We present a characterization of the optimal policy using the Hamilton-Jacobi-Bellman (HJB) equation. For a fluid approximation model, we provide an exact and explicit characterization of a threshold policy and prove its optimality using the HJB equation.\n  Our simulation results show that under properly designed control policy, the existence of alternative access technology as a complement for a primary access network can significantly improve the user experience without any bandwidth over-provisioning.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "This paper examines the value of storage in securing reliability of a system with uncertain supply and demand, and supply friction. The storage is frictionless as a supply source, but once used, it cannot be filled up instantaneously. The focus application is a power supply network in which the base supply and demand are assumed to match perfectly, while deviations from the base are modeled as random shocks with stochastic arrivals. Due to friction, the random surge shocks cannot be tracked by the main supply sources. Storage, when available, can be used to compensate, fully or partially, for the surge in demand or loss of supply. The problem of optimal utilization of storage with the objective of maximizing system reliability is formulated as minimization of the expected discounted cost of blackouts over an infinite horizon. It is shown that when the stage cost is linear in the size of the blackout, the optimal policy is myopic in the sense that all shocks are compensated by storage up to the available level of storage. However, when the stage cost is strictly convex, it may be optimal to curtail some of the demand and allow a small current blackout in the interest of maintaining a higher level of reserve to avoid a large blackout in the future. The value of storage capacity in improving system's reliability, as well as the effects of the associated optimal policies under different stage costs on the probability distribution of blackouts are examined.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "Except for special classes of games, there is no systematic framework for analyzing the dynamical properties of multi-agent strategic interactions. Potential games are one such special but restrictive class of games that allow for tractable dynamic analysis. Intuitively, games that are \"close\" to a potential game should share similar properties. In this paper, we formalize and develop this idea by quantifying to what extent the dynamic features of potential games extend to \"near-potential\" games. We study convergence of three commonly studied classes of adaptive dynamics: discrete-time better/best response, logit response, and discrete-time fictitious play dynamics. For better/best response dynamics, we focus on the evolution of the sequence of pure strategy profiles and show that this sequence converges to a (pure) approximate equilibrium set, whose size is a function of the \"distance\" from a close potential game. We then study logit response dynamics and provide a characterization of the stationary distribution of this update rule in terms of the distance of the game from a close potential game and the corresponding potential function. We further show that the stochastically stable strategy profiles are pure approximate equilibria. Finally, we turn attention to fictitious play, and establish that the sequence of empirical frequencies of player actions converges to a neighborhood of (mixed) equilibria of the game, where the size of the neighborhood increases with distance of the game to a potential game. Thus, our results suggest that games that are close to a potential game inherit the dynamical properties of potential games. Since a close potential game to a given game can be found by solving a convex optimization problem, our approach also provides a systematic framework for studying convergence behavior of adaptive learning dynamics in arbitrary finite strategic form games.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study the optimal pricing strategies of a monopolist selling a divisible good (service) to consumers that are embedded in a social network. A key feature of our model is that consumers experience a (positive) local network effect. In particular, each consumer's usage level depends directly on the usage of her neighbors in the social network structure. Thus, the monopolist's optimal pricing strategy may involve offering discounts to certain agents, who have a central position in the underlying network.\n  First, we consider a setting where the monopolist can offer individualized prices and derive an explicit characterization of the optimal price for each consumer as a function of her network position. In particular, we show that it is optimal for the monopolist to charge each agent a price that is proportional to her Bonacich centrality in the social network. In the second part of the paper, we discuss the optimal strategy of a monopolist that can only choose a single uniform price for the good and derive an algorithm polynomial in the number of agents to compute such a price. Thirdly, we assume that the monopolist can offer the good in two prices, full and discounted, and study the problem of determining which set of consumers should be given the discount. We show that the problem is NP-hard, however we provide an explicit characterization of the set of agents that should be offered the discounted price. Next, we describe an approximation algorithm for finding the optimal set of agents. We show that if the profit is nonnegative under any feasible price allocation, the algorithm guarantees at least 88% of the optimal profit. Finally, we highlight the value of network information by comparing the profits of a monopolist that does not take into account the network effects when choosing her pricing policy to those of a monopolist that uses this information optimally.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "In this paper we study the generalized version of weighted matching in bipartite networks. Consider a weighted matching in a bipartite network in which the nodes derive value from the split of the matching edge assigned to them if they are matched. The value a node derives from the split depends both on the split as well as the partner the node is matched to. We assume that the value of a split to the node is continuous and strictly increasing in the part of the split assigned to the node. A stable weighted matching is a matching and splits on the edges in the matching such that no two adjacent nodes in the network can split the edge between them so that both of them can derive a higher value than in the matching. We extend the weighted matching problem to this general case and study the existence of a stable weighted matching. We also present an algorithm that converges to a stable weighted matching. The algorithm generalizes the Hungarian algorithm for bipartite matching. Faster algorithms can be made when there is more structure on the value functions.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "In this paper we show that when individuals in a bipartite network exclusively choose partners and exchange valued goods with their partners, then there exists a set of exchanges that are pair-wise stable. Pair-wise stability implies that no individual breaks her partnership and no two neighbors in the network can form a new partnership while breaking other partnerships if any so that at least one of them improves her payoff and the other one does at least as good. We consider a general class of continuous, strictly convex and strongly monotone preferences over bundles of goods for individuals. Thus, this work extends the general equilibrium framework from markets to networks with exclusive exchanges. We present the complete existence proof using the existence of a generalized stable matching in \\cite{Generalized-Stable-Matching}. The existence proof can be extended to problems in social games as in \\cite{Matching-Equilibrium} and \\cite{Social-Games}.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "The authors have decided to withdraw this submission. Clarifications/corrections, if any, may follow at a later date.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study a tractable opinion dynamics model that generates long-run disagreements and persistent opinion fluctuations. Our model involves an inhomogeneous stochastic gossip process of continuous opinion dynamics in a society consisting of two types of agents: regular agents, who update their beliefs according to information that they receive from their social neighbors; and stubborn agents, who never update their opinions. When the society contains stubborn agents with different opinions, the belief dynamics never lead to a consensus (among the regular agents). Instead, beliefs in the society fail to converge almost surely, the belief profile keeps on fluctuating in an ergodic fashion, and it converges in law to a non-degenerate random vector. The structure of the network and the location of the stubborn agents within it shape the opinion dynamics. The expected belief vector evolves according to an ordinary differential equation coinciding with the Kolmogorov backward equation of a continuous-time Markov chain with absorbing states corresponding to the stubborn agents and converges to a harmonic vector, with every regular agent's value being the weighted average of its neighbors' values, and boundary conditions corresponding to the stubborn agents'. Expected cross-products of the agents' beliefs allow for a similar characterization in terms of coupled Markov chains on the network. We prove that, in large-scale societies which are highly fluid, meaning that the product of the mixing time of the Markov chain on the graph describing the social network and the relative size of the linkages to stubborn agents vanishes as the population size grows large, a condition of \\emph{homogeneous influence} emerges, whereby the stationary beliefs' marginal distributions of most of the regular agents have approximately equal first and second moments.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "This document consists of two parts: the second part was submitted earlier as a new proof of Nash's theorem, and the first part is a note explaining a problem found in that proof. We are indebted to Sergiu Hart and Eran Shmaya for their careful study which led to their simultaneous discovery of this error. So far the error has not been fixed, but many of the results and techniques of the paper remain valid, so we will continue to make it available online.\n  Abstract for the original paper:\n  We give a novel proof of the existence of Nash equilibria in all finite games without using fixed point theorems or path following arguments. Our approach relies on a new notion intermediate between Nash and correlated equilibria called exchangeable equilibria, which are correlated equilibria with certain symmetry and factorization properties. We prove these exist by a duality argument, using Hart and Schmeidler's proof of correlated equilibrium existence as a first step.\n  In an appropriate limit exchangeable equilibria converge to the convex hull of Nash equilibria, proving that these exist as well. Exchangeable equilibria are defined in terms of symmetries of the game, so this method automatically proves the stronger statement that a symmetric game has a symmetric Nash equilibrium. The case without symmetries follows by a symmetrization argument.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "Most existing work uses dual decomposition and subgradient methods to solve Network Utility Maximization (NUM) problems in a distributed manner, which suffer from slow rate of convergence properties. This work develops an alternative distributed Newton-type fast converging algorithm for solving network utility maximization problems with self-concordant utility functions. By using novel matrix splitting techniques, both primal and dual updates for the Newton step can be computed using iterative schemes in a decentralized manner with limited information exchange. Similarly, the stepsize can be obtained via an iterative consensus-based averaging scheme. We show that even when the Newton direction and the stepsize in our method are computed within some error (due to finite truncation of the iterative schemes), the resulting objective function value still converges superlinearly to an explicitly characterized error neighborhood. Simulation results demonstrate significant convergence rate improvement of our algorithm relative to the existing subgradient methods based on dual decomposition.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "In this paper we introduce a novel flow representation for finite games in strategic form. This representation allows us to develop a canonical direct sum decomposition of an arbitrary game into three components, which we refer to as the potential, harmonic and nonstrategic components. We analyze natural classes of games that are induced by this decomposition, and in particular, focus on games with no harmonic component and games with no potential component. We show that the first class corresponds to the well-known potential games. We refer to the second class of games as harmonic games, and study the structural and equilibrium properties of this new class of games. Intuitively, the potential component of a game captures interactions that can equivalently be represented as a common interest game, while the harmonic part represents the conflicts between the interests of the players. We make this intuition precise, by studying the properties of these two classes, and show that indeed they have quite distinct and remarkable characteristics. For instance, while finite potential games always have pure Nash equilibria, harmonic games generically never do. Moreover, we show that the nonstrategic component does not affect the equilibria of a game, but plays a fundamental role in their efficiency properties, thus decoupling the location of equilibria and their payoff-related properties. Exploiting the properties of the decomposition framework, we obtain explicit expressions for the projections of games onto the subspaces of potential and harmonic games. This enables an extension of the properties of potential and harmonic games to \"nearby\" games. We exemplify this point by showing that the set of approximate equilibria of an arbitrary game can be characterized through the equilibria of its projection onto the set of potential games.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study the design of media streaming applications in the presence of multiple heterogeneous wireless access methods with different throughputs and costs.  Our objective is to analytically characterize the trade-off between the usage cost and the Quality of user Experience (QoE), which is represented by the probability of interruption in media playback and the initial waiting time. We model each access network as a server that provides packets to the user according to a Poisson process with a certain rate and cost.  Blocks are coded using random linear codes to alleviate the duplicate packet reception problem. Users must take decisions on how many packets to buffer before playout, and which networks to access during playout. We design, analyze and compare several control policies with a threshold structure. We formulate the problem of finding the optimal control policy as an MDP with a probabilistic constraint. We present the HJB equation for this problem by expanding the state space, and exploit it as a verification method for optimality of the proposed control law.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We study distributed algorithms for solving global optimization problems in which the objective function is the sum of local objective functions of agents and the constraint set is given by the intersection of local constraint sets of agents. We assume that each agent knows only his own local objective function and constraint set, and exchanges information with the other agents over a randomly varying network topology to update his information state. We assume a state-dependent communication model over this topology: communication is Markovian with respect to the states of the agents and the probability with which the links are available depends on the states of the agents. In this paper, we study a projected multi-agent subgradient algorithm under state-dependent communication. The algorithm involves each agent performing a local averaging to combine his estimate with the other agents' estimates, taking a subgradient step along his local objective function, and projecting the estimates on his local constraint set. The state-dependence of the communication introduces significant challenges and couples the study of information exchange with the analysis of subgradient steps and projection errors. We first show that the multi-agent subgradient algorithm when used with a constant stepsize may result in the agent estimates to diverge with probability one. Under some assumptions on the stepsize sequence, we provide convergence rate bounds on a \"disagreement metric\" between the agent estimates. Our bounds are time-nonhomogeneous in the sense that they depend on the initial starting time. Despite this, we show that agent estimates reach an almost sure consensus and converge to the same optimal solution of the global optimization problem with probability one under different assumptions on the local constraint sets and the stepsize sequence.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We exhibit the rich structure of the set of correlated equilibria by analyzing the simplest of polynomial games: the mixed extension of matching pennies. We show that while the correlated equilibrium set is convex and compact, the structure of its extreme points can be quite complicated. In finite games the ratio of extreme correlated to extreme Nash equilibria can be greater than exponential in the size of the strategy spaces. In polynomial games there can exist extreme correlated equilibria which are not finitely supported; we construct a large family of examples using techniques from ergodic theory. We show that in general the set of correlated equilibrium distributions of a polynomial game cannot be described by conditions on finitely many moments (means, covariances, etc.), in marked contrast to the set of Nash equilibria which is always expressible in terms of finitely many moments.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We focus on a particular form of network coding, reverse carpooling, in a wireless network where the potentially coded transmitted messages are to be decoded immediately upon reception. The network is fixed and known, and the system performance is measured in terms of the number of wireless broadcasts required to meet multiple unicast demands. Motivated by the structure of the coding scheme, we formulate the problem as a linear program by introducing a flow variable for each triple of connected nodes. This allows us to have a formulation polynomial in the number of nodes. Using dual decomposition and projected subgradient method, we present a decentralized algorithm to obtain optimal routing schemes in presence of coding opportunities. We show that the primal sub-problem can be expressed as a shortest path problem on an \\emph{edge-graph}, and the proposed algorithm requires each node to exchange information only with its neighbors.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We provide a model to investigate the tension between information aggregation and spread of misinformation in large societies (conceptualized as networks of agents communicating with each other). Each individual holds a belief represented by a scalar. Individuals meet pairwise and exchange information, which is modeled as both individuals adopting the average of their pre-meeting beliefs. When all individuals engage in this type of information exchange, the society will be able to effectively aggregate the initial information held by all individuals. There is also the possibility of misinformation, however, because some of the individuals are \"forceful,\" meaning that they influence the beliefs of (some) of the other individuals they meet, but do not change their own opinion. The paper characterizes how the presence of forceful agents interferes with information aggregation. Under the assumption that even forceful agents obtain some information (however infrequent) from some others (and additional weak regularity conditions), we first show that beliefs in this class of societies converge to a consensus among all individuals. This consensus value is a random variable, however, and we characterize its behavior. Our main results quantify the extent of misinformation in the society by either providing bounds or exact results (in some special cases) on how far the consensus value can be from the benchmark without forceful agents (where there is efficient information aggregation). The worst outcomes obtain when there are several forceful agents and forceful agents themselves update their beliefs only on the basis of information they obtain from individuals most likely to have received their own information previously.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We present several new characterizations of correlated equilibria in games with continuous utility functions.  These have the advantage of being more computationally and analytically tractable than the standard definition in terms of departure functions.  We use these characterizations to construct effective algorithms for approximating a single correlated equilibrium or the entire set of correlated equilibria of a game with polynomial utility functions.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "A wireless packet network is considered in which each user transmits a stream of packets to its destination. The transmit power of each user interferes with the transmission of all other users. A convex cost function of the completion times of the user packets is minimized by optimally allocating the users' transmission power subject to their respective power constraints. At all ranges of SINR, completion time minimization can be formulated as a convex optimization problem and hence can be efficiently solved. In particular, although the feasible rate region of the wireless network is non-convex, its corresponding completion time region is shown to be convex. When channel knowledge is imperfect, robust power control is considered based on the channel fading distribution subject to outage probability constraints. The problem is shown to be convex when the fading distribution is log-concave in exponentiated channel power gains; e.g., when each user is under independent Rayleigh, Nakagami, or log-normal fading. Applying the optimization frameworks in a wireless cellular network, the average completion time is significantly reduced as compared to full power transmission.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider the problem of rate allocation in a fading Gaussian multiple-access channel with fixed transmission powers. The goal is to maximize a general concave utility function of the expected achieved rates of the users. There are different approaches to this problem in the literature. From an information theoretic point of view, rates are allocated only by using the channel state information. The queueing theory approach utilizes the global queue-length information for rate allocation to guarantee throughput optimality as well as maximizing a utility function of the rates. In this work, we make a connection between these two approaches by showing that the information theoretic capacity region of a multiple-access channel and its stability region are equivalent. Moreover, our numerical results show that a simple greedy policy which does not use the queue-length information can outperform queue-length based policies in terms of convergence rate and fairness.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider the problem of rate and power allocation in a fading multiple-access channel. Our objective is to obtain rate and power allocation policies that maximize a utility function defined over average transmission rates. In contrast with the literature, which focuses on the linear case, we present results for general concave utility functions. We consider two cases. In the first case, we assume that power control is possible and channel statistics are known. In this case, we show that the optimal policies can be obtained greedily by maximizing a linear utility function at each channel state. In the second case, we assume that power control is not possible and channel statistics are not available. In this case, we define a greedy rate allocation policy and provide upper bounds on the performance difference between the optimal and the greedy policy. Our bounds highlight the dependence of the performance difference on the channel variations and the structure of the utility function.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider the problem of rate allocation in a fading Gaussian multiple-access channel (MAC) with fixed transmission powers. Our goal is to maximize a general concave utility function of transmission rates over the throughput capacity region. In contrast to earlier works in this context that propose solutions where a potentially complex optimization problem must be solved in every decision instant, we propose a low-complexity approximate rate allocation policy and analyze the effect of temporal channel variations on its utility performance. To the best of our knowledge, this is the first work that studies the tracking capabilities of an approximate rate allocation scheme under fading channel conditions. We build on an earlier work to present a new rate allocation policy for a fading MAC that implements a low-complexity approximate gradient projection iteration for each channel measurement, and explicitly characterize the effect of the speed of temporal channel variations on the tracking neighborhood of our policy. We further improve our results by proposing an alternative rate allocation policy for which tighter bounds on the size of the tracking neighborhood are derived. These proposed rate allocation policies are computationally efficient in our setting since they implement a single gradient projection iteration per channel measurement and each such iteration relies on approximate projections which has polynomial-complexity in the number of users.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider the problem of rate allocation in a Gaussian multiple-access channel, with the goal of maximizing a utility function over transmission rates. In contrast to the literature which focuses on linear utility functions, we study general concave utility functions. We present a gradient projection algorithm for this problem. Since the constraint set of the problem is described by exponentially many constraints, methods that use exact projections are computationally intractable. Therefore, we develop a new method that uses approximate projections. We use the polymatroid structure of the capacity region to show that the approximate projection can be implemented by a recursive algorithm in time polynomial in the number of users. We further propose another algorithm for implementing the approximate projections using rate-splitting and show improved bounds on its convergence time.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "We consider the problem of rate and power allocation in a multiple-access channel. Our objective is to obtain rate and power allocation policies that maximize a general concave utility function of average transmission rates on the information theoretic capacity region of the multiple-access channel. Our policies does not require queue-length information. We consider several different scenarios. First, we address the utility maximization problem in a nonfading channel to obtain the optimal operating rates, and present an iterative gradient projection algorithm that uses approximate projection. By exploiting the polymatroid structure of the capacity region, we show that the approximate projection can be implemented in time polynomial in the number of users. Second, we consider resource allocation in a fading channel. Optimal rate and power allocation policies are presented for the case that power control is possible and channel statistics are available. For the case that transmission power is fixed and channel statistics are unknown, we propose a greedy rate allocation policy and provide bounds on the performance difference of this policy and the optimal policy in terms of channel variations and structure of the utility function. We present numerical results that demonstrate superior convergence rate performance for the greedy policy compared to queue-length based policies. In order to reduce the computational complexity of the greedy policy, we present approximate rate allocation policies which track the greedy policy within a certain neighborhood that is characterized in terms of the speed of fading.\n        \u25b3 Less", "author": "Asuman Ozdaglar"}, {"abstract": "The effect of a two dimensional (2D) graphene layer (GL) on top of the silicon nitride (SiN) passivation layer of AlGaN/GaN metal-insulator-semiconductor high-electron-mobility transistors (MIS-HEMTs) has been systematically analyzed. Results showed that in the devices without the GL, the maximum drain current density (I_D,max) and the maximum transconductance (g_m,max) decreased gradually as the mist exposure time increased, up to 23% and 10%, respectively. Moreover, the gate lag ratio (GLR) increased around 10% during mist exposure. In contrast, devices with a GL showed a robust behavior and not significant changes in the electrical characteristics in both DC and pulsed conditions. The origin of these behaviors has been discussed and the results pointed to the GL as the key factor for improving the moisture resistance of the SiN passivation layer.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "A Weyl semimetal (WSM) is a novel topological phase of matter, in which Weyl fermions (WFs) arise as pseudo-magnetic monopoles in its momentum space. The chirality of the WFs, given by the sign of the monopole charge, is central to the Weyl physics, since it directly serves as the sign of the topological number and gives rise to exotic properties such as Fermi arcs and the chiral anomaly. Despite being the defining property of a WSM, the chirality of the WFs has never been experimentally measured. Here, we directly detect the chirality of the WFs by measuring the photocurrent in response to circularly polarized mid-infrared light. The resulting photocurrent is determined by both the chirality of WFs and that of the photons. Our results pave the way for realizing a wide range of theoretical proposals for studying and controlling the WFs and their associated quantum anomalies by optical and electrical means. More broadly, the two chiralities, analogous to the two valleys in 2D materials, lead to a new degree of freedom in a 3D crystal with potential novel pathways to store and carry information.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "Single photon emitters play a central role in many photonic quantum technologies. A promising class of single photon emitters consists of atomic color centers in wide-bandgap crystals, such as diamond silicon carbide and hexagonal boron nitride. However, it is currently not possible to grow these materials as sub-micron thick films on low-refractive index substrates, which is necessary for mature photonic integrated circuit technologies. Hence, there is great interest in identifying quantum emitters in technologically mature semiconductors that are compatible with suitable heteroepitaxies. Here, we demonstrate robust single photon emitters based on defects in gallium nitride (GaN), the most established and well understood semiconductor that can emit light over the entire visible spectrum. We show that the emitters have excellent photophysical properties including a brightness in excess of 500x10^3 counts/s. We further show that the emitters can be found in a variety of GaN wafers, thus offering reliable and scalable platform for further technological development. We propose a theoretical model to explain the origin of these emitters based on cubic inclusions in hexagonal gallium nitride. Our results constitute a feasible path to scalable, integrated on-chip quantum technologies based on GaN.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "Diverse parallel stitched two-dimensional heterostructures are synthesized, including metal-semiconductor (graphene-MoS2), semiconductor-semiconductor (WS2-MoS2), and insulator-semiconductor (hBN-MoS2), directly through selective sowing of aromatic molecules as the seeds in chemical vapor deposition (CVD) method. Our methodology enables the large-scale fabrication of lateral heterostructures with arbitrary patterns, and clean and precisely aligned interfaces, which offers tremendous potential for its application in integrated circuits.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "Graphene, owing to its ability to support plasmon polariton waves in the terahertz frequency range, enables the miniaturization of antennas to allow wireless communications among nanosystems. One of the main challenges in the demonstration of graphene antennas is finding suitable terahertz sources to feed the antenna. This paper estimates the performance of a graphene RF plasmonic micro-antenna fed with a photoconductive source. The terahertz source is modeled and, by means of a full-wave EM solver, the radiated power of the device is estimated with respect to material, laser illumination and antenna geometry parameters. The results show that the proposed device radiates terahertz pulses with an average power up to 1$\u03bc$W, proving the feasibility of feeding miniaturized graphene antennas with photoconductive materials.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "We demonstrate second order optical nonlinearity in a silicon architecture through heterogeneous integration of single-crystalline gallium nitride (GaN) on silicon (100) substrates. By engineering GaN microrings for dual resonance around 1560 nm and 780 nm, we achieve efficient, tunable second harmonic generation at 780 nm. The \\{chi}(2) nonlinear susceptibility is measured to be as high as 16 plus minus 7 pm/V. Because GaN has a wideband transparency window covering ultraviolet, visible and infrared wavelengths, our platform provides a viable route for the on-chip generation of optical wavelengths in both the far infrared and near-UV through a combination of \\{chi}(2) enabled sum-/difference-frequency processes.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "2D nanoelectronics based on single-layer MoS2 offers great advantages for both conventional and ubiquitous applications. This paper discusses the large-scale CVD growth of single-layer MoS2 and fabrication of devices and circuits for the first time. Both digital and analog circuits are fabricated to demonstrate its capability for mixed-signal applications.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "Two-dimensional (2D) materials, such as molybdenum disulfide (MoS2), have been shown to exhibit excellent electrical and optical properties. The semiconducting nature of MoS2 allows it to overcome the shortcomings of zero-bandgap graphene, while still sharing many of graphene's advantages for electronic and optoelectronic applications. Discrete electronic and optoelectronic components, such as field-effect transistors, sensors and photodetectors made from few-layer MoS2 show promising performance as potential substitute of Si in conventional electronics and of organic and amorphous Si semiconductors in ubiquitous systems and display applications. An important next step is the fabrication of fully integrated multi-stage circuits and logic building blocks on MoS2 to demonstrate its capability for complex digital logic and high-frequency ac applications. This paper demonstrates an inverter, a NAND gate, a static random access memory, and a five-stage ring oscillator based on a direct-coupled transistor logic technology. The circuits comprise between two to twelve transistors seamlessly integrated side-by-side on a single sheet of bilayer MoS2. Both enhancement-mode and depletion-mode transistors were fabricated thanks to the use of gate metals with different work functions.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "In this letter, we analyze the carrier transit delay in graphene field-effect transistors (GFETs). GFETs are fabricated at the wafer-scale on sapphire substrate. For a device with a gate length of 210 nm, a current gain cut-off frequency fT of 18 GHz and 22 GHz is obtained before and after de-embedding. The extraction of the internal (Cgs,i, Cgd,i) and external capacitances (Cgs,ex and Cgd,ex) from the scaling behavior of the gate capacitances Cgs and Cgd allows the intrinsic (\u03c4_int), extrinsic (\u03c4_ext) and parasitic delays (\u03c4_par) to be obtained. In addition, the extraction of the intrinsic delay provides a new way to directly estimate carrier velocity from the experimental data while the breakdown of the total delay into intrinsic, extrinsic, and parasitic components can offer valuable information for optimizing RF GFETs structures.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "In this letter, we demonstrate the first BN/Graphene/BN field effect transistor for RF applications. The BN/Graphene/BN structure can preserve the high mobility of graphene, even when it is sandwiched between a substrate and a gate dielectric. Field effect transistors (FETs) using a bilayer graphene channel have been fabricated with a gate length LG=450 nm. A current density in excess of 1 A/mm and DC transconductance close to 250 mS/mm are achieved for both electron and hole conductions. RF characterization is performed for the first time on this device structure, giving a current-gain cut-off frequency fT=33 GHz and an fT.LG product of 15 GHz.um. The improved performance obtained by the BN/Graphene/BN structure is very promising to enable the next generation of high frequency graphene RF electronics.\n        \u25b3 Less", "author": "Tom\u00e1s Palacios"}, {"abstract": "Semidefinite programming (SDP) with equality constraints arise in many optimization and machine learning problems, such as Max-Cut, community detection and robust PCA. Although SDPs can be solved to arbitrary precision in polynomial time, generic convex solvers do not scale well with the dimension of the problem. In order to address this issue, Burer and Monteiro \\cite{burer2003nonlinear} proposed to reduce the dimension of the problem by appealing to a low-rank factorization, and solve the subsequent non-convex problem instead. It is well-understood that the resulting non-convex problem acts as a reliable surrogate to the original SDP, and can be efficiently solved using the block-coordinate maximization method. Despite its simplicity, remarkable success, and wide use in practice, the theoretical understanding of the convergence of this method is limited. We prove that the block-coordinate maximization algorithm applied to the non-convex Burer-Monteiro approach enjoys a global sublinear rate without any assumptions on the problem, and a local linear convergence rate despite no local maxima is locally strongly concave. We illustrate our results through examples and numerical experiments.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We show that existence of a global polynomial Lyapunov function for a homogeneous polynomial vector field or a planar polynomial vector field (under a mild condition) implies existence of a polynomial Lyapunov function that is a sum of squares (sos) and that the negative of its derivative is also a sum of squares. This result is extended to show that such sos-based certificates of stability are guaranteed to exist for all stable switched linear systems. For this class of systems, we further show that if the derivative inequality of the Lyapunov function has an sos certificate, then the Lyapunov function itself is automatically a sum of squares. These converse results establish cases where semidefinite programming is guaranteed to succeed in finding proofs of Lyapunov inequalities. Finally, we demonstrate some merits of replacing the sos requirement on a polynomial Lyapunov function with an sos requirement on its top homogeneous component. In particular, we show that this is a weaker algebraic requirement in addition to being cheaper to impose computationally.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "In this paper we consider a parametric family of polynomial optimization problems over algebraic sets. Although these problems are typically nonconvex, tractable convex relaxations via semidefinite programming (SDP) have been proposed. Often times in applications there is a natural value of the parameters for which the relaxation will solve the problem exactly. We study conditions (and quantitative bounds) under which the relaxation will continue to be exact as the parameter moves in a neighborhood of the original one. It suffices to restrict to quadratically constrained quadratic programs. Our framework captures several estimation problems such as low rank approximation, camera triangulation, rotation synchronization, approximate matrix completion and approximate GCD. In these applications, a solution is easy under noiseless observations, and our results guarantee that the SDP relaxation will continue to solve the problem in the low noise regime.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "The matrix logarithm, when applied to Hermitian positive definite matrices, is concave with respect to the positive semidefinite order. This operator concavity property leads to numerous concavity and convexity results for other matrix functions, many of which are of importance in quantum information theory. In this paper we show how to approximate the matrix logarithm with functions that preserve operator concavity and can be described using the feasible regions of semidefinite optimization problems of fairly small size. Such approximations allow us to use off-the-shelf semidefinite optimization solvers for convex optimization problems involving the matrix logarithm and related functions, such as the quantum relative entropy. The basic ingredients of our approach apply, beyond the matrix logarithm, to functions that are operator concave and operator monotone. As such, we introduce strategies for constructing semidefinite approximations that we expect will be useful, more generally, for studying the approximation power of functions with small semidefinite representations.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We study stability criteria for discrete-time switched systems and provide a meta-theorem that characterizes all Lyapunov theorems of a certain canonical type. For this purpose, we investigate the structure of sets of LMIs that provide a sufficient condition for stability. Various such conditions have been proposed in the literature in the past fifteen years. We prove in this note that a family of languagetheoretic conditions recently provided by the authors encapsulates all the possible LMI conditions, thus putting a conclusion to this research effort. As a corollary, we show that it is PSPACE-complete to recognize whether a particular set of LMIs implies stability of a switched system. Finally, we provide a geometric interpretation of these conditions, in terms of existence of an invariant set.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We propose a new method for simplifying semidefinite programs (SDP) inspired by symmetry reduction. Specifically, we show if an orthogonal projection satisfies certain invariance conditions, restricting to its range yields an equivalent primal-dual pair over a lower-dimensional symmetric cone---namely, the cone-of-squares of a Jordan subalgebra of symmetric matrices. We then give a simple algorithm for minimizing the rank of this projection and hence the dimension of this cone. Through the theory of Jordan algebras, the proposed method easily extends to linear programming, second-order cone programming, and, more generally, symmetric cone optimization.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "This is an experimental case study in real algebraic geometry, aimed at computing the image of a semialgebraic subset of 3-space under a polynomial map into the plane. For general instances, the boundary of the image is given by two highly singular curves. We determine these curves and show how they demarcate the \"flattened soccer ball\". We explore cylindrical algebraic decompositions, by working through concrete examples. Maps onto convex polygons and connections to convex optimization are also discussed.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We introduce a novel representation of structured polynomial ideals, which we refer to as chordal networks. The sparsity structure of a polynomial system is often described by a graph that captures the interactions among the variables. Chordal networks provide a computationally convenient decomposition into simpler (triangular) polynomial sets, while preserving the underlying graphical structure. We show that many interesting families of polynomial ideals admit compact chordal network representations (of size linear in the number of variables), even though the number of components is exponentially large. Chordal networks can be computed for arbitrary polynomial systems using a refinement of the chordal elimination algorithm from [Cifuentes-Parrilo-2016]. Furthermore, they can be effectively used to obtain several properties of the variety, such as its dimension, cardinality, and equidimensional components, as well as an efficient probabilistic test for radical ideal membership. We apply our methods to examples from algebraic statistics and vector addition systems; for these instances, algorithms based on chordal networks outperform existing techniques by orders of magnitude.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We study sum of squares (SOS) relaxations to optimize polynomial functions over a set $V\\cap R^n$, where $V$ is a complex algebraic variety. We propose a new methodology that, rather than relying on some algebraic description, represents $V$ with a generic set of complex samples. This approach depends only on the geometry of $V$, avoiding representation issues such as multiplicity and choice of generators. It also takes advantage of the coordinate ring structure to reduce the size of the corresponding semidefinite program (SDP). In addition, the input can be given as a straight-line program. Our methods are particularly appealing for varieties that are easy to sample from but for which the defining equations are complicated, such as $SO(n)$, Grassmannians or rank $k$ tensors. For arbitrary varieties we can obtain the required samples by using the tools of numerical algebraic geometry. In this way we connect the areas of SOS optimization and numerical algebraic geometry.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "The incremental gradient method is a prominent algorithm for minimizing a finite sum of smooth convex functions, used in many contexts including large-scale data processing applications and distributed optimization over networks. It is a first-order method that processes the functions one at a time based on their gradient information. The incremental Newton method, on the other hand, is a second-order variant which exploits additionally the curvature information of the underlying functions and can therefore be faster. In this paper, we focus on the case when the objective function is strongly convex and present fast convergence results for the incremental gradient and incremental Newton methods under the constant and diminishing stepsizes. For a decaying stepsize rule $\u03b1_k = \u0398(1/k^s)$ with $s \\in (0,1]$, we show that the distance of the IG iterates to the optimal solution converges at rate ${\\cal O}(1/k^{s})$ (which translates into ${\\cal O}(1/k^{2s})$ rate in the suboptimality of the objective value). For $s>1/2$, this improves the previous ${\\cal O}(1/\\sqrt{k})$ results in distances obtained for the case when functions are non-smooth. We show that to achieve the fastest ${\\cal O}(1/k)$ rate, incremental gradient needs a stepsize that requires tuning to the strong convexity parameter whereas the incremental Newton method does not. The results are based on viewing the incremental gradient method as a gradient descent method with gradient errors, devising efficient upper bounds for the gradient error to derive inequalities that relate distances of the consecutive iterates to the optimal solution and finally applying Chung's lemmas from the stochastic approximation literature to these inequalities to determine their asymptotic behavior. In addition, we construct examples to show tightness of our rate results.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We analyze the convergence rate of the random reshuffling (RR) method, which is a randomized first-order incremental algorithm for minimizing a finite sum of convex component functions. RR proceeds in cycles, picking a uniformly random order (permutation) and processing the component functions one at a time according to this order, i.e., at each cycle, each component function is sampled without replacement from the collection. Though RR has been numerically observed to outperform its with-replacement counterpart stochastic gradient descent (SGD), characterization of its convergence rate has been a long standing open question. In this paper, we answer this question by showing that when the component functions are quadratics or smooth and the sum function is strongly convex, RR with iterate averaging and a diminishing stepsize $\u03b1_k=\u0398(1/k^s)$ for $s\\in (1/2,1)$ converges at rate $\u0398(1/k^{2s})$ with probability one in the suboptimality of the objective value, thus improving upon the $\u03a9(1/k)$ rate of SGD. Our analysis draws on the theory of Polyak-Ruppert averaging and relies on decoupling the dependent cycle gradient error into an independent term over cycles and another term dominated by $\u03b1_k^2$. This allows us to apply law of large numbers to an appropriately weighted version of the cycle gradient errors, where the weights depend on the stepsize. We also provide high probability convergence rate estimates that shows decay rate of different terms and allows us to propose a modification of RR with convergence rate ${\\cal O}(\\frac{1}{k^2})$.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We present an efficient algorithm to compute permanents, mixed discriminants and hyperdeterminants of structured matrices and multidimensional arrays (tensors). We describe the sparsity structure of an array in terms of a graph, and we assume that its treewidth, denoted as $\u03c9$, is small. Our algorithm requires $O(n 2^\u03c9)$ arithmetic operations to compute permanents, and $O(n^2 + n 3^\u03c9)$ for mixed discriminants and hyperdeterminants. We finally show that mixed volume computation continues to be hard under bounded treewidth assumptions.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Motivated by applications to distributed optimization over networks and large-scale data processing in machine learning, we analyze the deterministic incremental aggregated gradient method for minimizing a finite sum of smooth functions where the sum is strongly convex. This method processes the functions one at a time in a deterministic order and incorporates a memory of previous gradient values to accelerate convergence. Empirically it performs well in practice; however, no theoretical analysis with explicit rate results was previously given in the literature to our knowledge, in particular most of the recent efforts concentrated on the randomized versions. In this paper, we show that this deterministic algorithm has global linear convergence and characterize the convergence rate. We also consider an aggregated method with momentum and demonstrate its linear convergence. Our proofs rely on a careful choice of a Lyapunov function that offers insight into the algorithm's behavior and simplifies the proofs considerably.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Let G be a finite abelian group. This paper is concerned with nonnegative functions on G that are sparse with respect to the Fourier basis. We establish combinatorial conditions on subsets S and T of Fourier basis elements under which nonnegative functions with Fourier support S are sums of squares of functions with Fourier support T. Our combinatorial condition involves constructing a chordal cover of a graph related to G and S (the Cayley graph Cay($\\hat{G}$,S)) with maximal cliques related to T. Our result relies on two main ingredients: the decomposition of sparse positive semidefinite matrices with a chordal sparsity pattern, as well as a simple but key observation exploiting the structure of the Fourier basis elements of G.\n  We apply our general result to two examples. First, in the case where $G = \\mathbb{Z}_2^n$, by constructing a particular chordal cover of the half-cube graph, we prove that any nonnegative quadratic form in n binary variables is a sum of squares of functions of degree at most $\\lceil n/2 \\rceil$, establishing a conjecture of Laurent. Second, we consider nonnegative functions of degree d on $\\mathbb{Z}_N$ (when d divides N). By constructing a particular chordal cover of the d'th power of the N-cycle, we prove that any such function is a sum of squares of functions with at most $3d\\log(N/d)$ nonzero Fourier coefficients. Dually this shows that a certain cyclic polytope in $\\mathbb{R}^{2d}$ with N vertices can be expressed as a projection of a section of the cone of psd matrices of size $3d\\log(N/d)$. Putting $N=d^2$ gives a family of polytopes $P_d \\subset \\mathbb{R}^{2d}$ with LP extension complexity $\\text{xc}_{LP}(P_d) = \u03a9(d^2)$ and SDP extension complexity $\\text{xc}_{PSD}(P_d) = O(d\\log(d))$. To the best of our knowledge, this is the first explicit family of polytopes in increasing dimensions where $\\text{xc}_{PSD}(P_d) = o(\\text{xc}_{LP}(P_d))$.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Sequential rate-distortion (SRD) theory provides a framework for studying the fundamental trade-off between data-rate and data-quality in real-time communication systems. In this paper, we consider the SRD problem for multi-dimensional time-varying Gauss-Markov processes under mean-square distortion criteria. We first revisit the sensor-estimator separation principle, which asserts that considered SRD problem is equivalent to a joint sensor and estimator design problem in which data-rate of the sensor output is minimized while the estimator's performance satisfies the distortion criteria. We then show that the optimal joint design can be performed by semidefinite programming. A semidefinite representation of the corresponding SRD function is obtained. Implications of the obtained result in the context of zero-delay source coding theory and applications to networked control theory are also discussed.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Chordal structure and bounded treewidth allow for efficient computation in numerical linear algebra, graphical models, constraint satisfaction and many other areas. In this paper, we begin the study of how to exploit chordal structure in computational algebraic geometry, and in particular, for solving polynomial systems. The structure of a system of polynomial equations can be described in terms of a graph. By carefully exploiting the properties of this graph (in particular, its chordal completions), more efficient algorithms can be developed. To this end, we develop a new technique, which we refer to as chordal elimination, that relies on elimination theory and Gr\u00f6bner bases. By maintaining graph structure throughout the process, chordal elimination can outperform standard Gr\u00f6bner basis algorithms in many cases. The reason is that all computations are done on \"smaller\" rings, of size equal to the treewidth of the graph. In particular, for a restricted class of ideals, the computational complexity is linear in the number of variables. Chordal structure arises in many relevant applications. We demonstrate the suitability of our methods in examples from graph colorings, cryptography, sensor localization and differential equations.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Motivated by machine learning problems over large data sets and distributed optimization over networks, we develop and analyze a new method called incremental Newton method for minimizing the sum of a large number of strongly convex functions. We show that our method is globally convergent for a variable stepsize rule. We further show that under a gradient growth condition, convergence rate is linear for both variable and constant stepsize rules. By means of an example, we show that without the gradient growth condition, incremental Newton method cannot achieve linear convergence. Our analysis can be extended to study other incremental methods: in particular, we obtain a linear convergence rate result for the incremental Gauss-Newton algorithm under a variable stepsize rule.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We consider the problem of jointly estimating the attitude and spin-rate of a spinning spacecraft. Psiaki (J. Astronautical Sci., 57(1-2):73--92, 2009) has formulated a family of optimization problems that generalize the classical least-squares attitude estimation problem, known as Wahba's problem, to the case of a spinning spacecraft. If the rotation axis is fixed and known, but the spin-rate is unknown (such as for nutation-damped spin-stabilized spacecraft) we show that Psiaki's problem can be reformulated exactly as a type of tractable convex optimization problem called a semidefinite optimization problem. This reformulation allows us to globally solve the problem using standard numerical routines for semidefinite optimization. It also provides a natural semidefinite relaxation-based approach to more complicated variations on the problem.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Given a polytope P in $\\mathbb{R}^n$, we say that P has a positive semidefinite lift (psd lift) of size d if one can express P as the linear projection of an affine slice of the positive semidefinite cone $\\mathbf{S}^d_+$. If a polytope P has symmetry, we can consider equivariant psd lifts, i.e. those psd lifts that respect the symmetry of P. One of the simplest families of polytopes with interesting symmetries are regular polygons in the plane, which have played an important role in the study of linear programming lifts (or extended formulations). In this paper we study equivariant psd lifts of regular polygons. We first show that the standard Lasserre/sum-of-squares hierarchy for the regular N-gon requires exactly ceil(N/4) iterations and thus yields an equivariant psd lift of size linear in N. In contrast we show that one can construct an equivariant psd lift of the regular 2^n-gon of size 2n-1, which is exponentially smaller than the psd lift of the sum-of-squares hierarchy. Our construction relies on finding a sparse sum-of-squares certificate for the facet-defining inequalities of the regular 2^n-gon, i.e., one that only uses a small (logarithmic) number of monomials. Since any equivariant LP lift of the regular 2^n-gon must have size 2^n, this gives the first example of a polytope with an exponential gap between sizes of equivariant LP lifts and equivariant psd lifts. Finally we prove that our construction is essentially optimal by showing that any equivariant psd lift of the regular N-gon must have size at least logarithmic in N.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We develop a practical semidefinite programming (SDP) facial reduction procedure that utilizes computationally efficient approximations of the positive semidefinite cone. The proposed method simplifies SDPs with no strictly feasible solution (a frequent output of parsers) by solving a sequence of easier optimization problems and could be a useful pre-processing technique for SDP solvers. We demonstrate effectiveness of the method on SDPs arising in practice, and describe our publicly-available software implementation. We also show how to find maximum rank matrices in our PSD cone approximations (which helps us find maximal simplifications), and we give a post-processing procedure for dual solution recovery that generally applies to facial-reduction-based pre-processing techniques. Finally, we show how approximations can be chosen to preserve problem sparsity.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Let M be a p-by-q matrix with nonnegative entries. The positive semidefinite rank (psd rank) of M is the smallest integer k for which there exist positive semidefinite matrices $A_i, B_j$ of size $k \\times k$ such that $M_{ij} = \\text{trace}(A_i B_j)$. The psd rank has many appealing geometric interpretations, including semidefinite representations of polyhedra and information-theoretic applications. In this paper we develop and survey the main mathematical properties of psd rank, including its geometry, relationships with other rank notions, and computational and algorithmic aspects.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "The nonnegative rank of a matrix A is the smallest integer r such that A can be written as the sum of r rank-one nonnegative matrices. The nonnegative rank has received a lot of attention recently due to its application in optimization, probability and communication complexity. In this paper we study a class of atomic rank functions defined on a convex cone which generalize several notions of \"positive\" ranks such as nonnegative rank or cp-rank (for completely positive matrices). The main contribution of the paper is a new method to obtain lower bounds for such ranks which improve on previously known bounds. Additionally the bounds we propose can be computed by semidefinite programming. The idea of the lower bound relies on an atomic norm approach where the atoms are self-scaled according to the vector (or matrix, in the case of nonnegative rank) of interest. This results in a lower bound that is invariant under scaling and that is at least as good as other existing norm-based bounds.\n  We mainly focus our attention on the two important cases of nonnegative rank and cp-rank where our bounds satisfying interesting properties: For the nonnegative rank we show that our lower bound can be interpreted as a non-combinatorial version of the fractional rectangle cover number, while the sum-of-squares relaxation is closely related to the Lov\u00e1sz theta number of the rectangle graph of the matrix. We also prove that the lower bound inherits many of the structural properties satisfied by the nonnegative rank such as invariance under diagonal scaling, subadditivity, etc. We also apply our method to obtain lower bounds on the cp-rank for completely positive matrices. In this case we prove that our lower bound is always greater than or equal the plain rank lower bound, and we show that it has interesting connections with combinatorial lower bounds based on edge-clique cover number.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We study the convex hull of $SO(n)$, thought of as the set of $n\\times n$ orthogonal matrices with unit determinant, from the point of view of semidefinite programming. We show that the convex hull of $SO(n)$ is doubly spectrahedral, i.e. both it and its polar have a description as the intersection of a cone of positive semidefinite matrices with an affine subspace. Our spectrahedral representations are explicit, and are of minimum size, in the sense that there are no smaller spectrahedral representations of these convex bodies.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Distributed control problems under some specific information constraints can be formulated as (possibly infinite dimensional) convex optimization problems. The underlying motivation of this work is to develop an understanding of the optimal decision making architecture for such problems. In this paper, we particularly focus on the N-player triangular LQG problems and show that the optimal output feedback controllers have attractive state space realizations. The optimal controller can be synthesized using a set of stabilizing solutions to 2N linearly coupled algebraic Riccati equations, which turn out to be easily solvable under reasonable assumptions.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "A central question in optimization is to maximize (or minimize) a linear function over a given polytope P. To solve such a problem in practice one needs a concise description of the polytope P. In this paper we are interested in representations of P using the positive semidefinite cone: a positive semidefinite lift (psd lift) of a polytope P is a representation of P as the projection of an affine slice of the positive semidefinite cone $\\mathbf{S}^d_+$. Such a representation allows linear optimization problems over P to be written as semidefinite programs of size d. Such representations can be beneficial in practice when d is much smaller than the number of facets of the polytope P. In this paper we are concerned with so-called equivariant psd lifts (also known as symmetric psd lifts) which respect the symmetries of the polytope P. We present a representation-theoretic framework to study equivariant psd lifts of a certain class of symmetric polytopes known as orbitopes. Our main result is a structure theorem where we show that any equivariant psd lift of size d of an orbitope is of sum-of-squares type where the functions in the sum-of-squares decomposition come from an invariant subspace of dimension smaller than d^3. We use this framework to study two well-known families of polytopes, namely the parity polytope and the cut polytope, and we prove exponential lower bounds for equivariant psd lifts of these polytopes.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "There has been a lot of interest recently in proving lower bounds on the size of linear programs needed to represent a given polytope P. In a breakthrough paper Fiorini et al. [Proceedings of 44th ACM Symposium on Theory of Computing 2012, pages 95-106] showed that any linear programming formulation of maximum-cut must have exponential size. A natural question to ask is whether one can prove such strong lower bounds for semidefinite programming formulations. In this paper we take a step towards this goal and we prove strong lower bounds for a certain class of SDP formulations, namely SDPs over the Cartesian product of fixed-size positive semidefinite cones. In practice this corresponds to semidefinite programs with a block-diagonal structure and where blocks have constant size d. We show that any such extended formulation of the cut polytope must have exponential size (when d is fixed). The result of Fiorini et al. for LP formulations is obtained as a special case when d=1. For blocks of size d=2 the result rules out any small formulations using second-order cone programming. Our study of SDP lifts over Cartesian product of fixed-size positive semidefinite cones is motivated mainly from practical considerations where it is well known that such SDPs can be solved more efficiently than general SDPs. The proof of our lower bound relies on new results about the sparsity pattern of certain matrices with small psd rank, combined with an induction argument inspired from the recent paper by Kaibel and Weltge [arXiv:1307.3543] on the LP extension complexity of the correlation polytope.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "SOSTOOLS v3.00 is the latest release of the freely available MATLAB toolbox for formulating and solving sum of squares (SOS) optimization problems. Such problems arise naturally in the analysis and control of nonlinear dynamical systems, but also in other areas such as combinatorial optimization. Highlights of the new release include the ability to create polynomial matrices and formulate polynomial matrix inequalities, compatibility with MuPAD, the new MATLAB symbolic engine, as well as the multipoly toolbox v2.01. SOSTOOLS v3.00 can interface with five semidefinite programming solvers, and includes ten demonstration examples.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We consider polynomial differential equations and make a number of contributions to the questions of (i) complexity of deciding stability, (ii) existence of polynomial Lyapunov functions, and (iii) existence of sum of squares (sos) Lyapunov functions.\n  (i) We show that deciding local or global asymptotic stability of cubic vector fields is strongly NP-hard. Simple variations of our proof are shown to imply strong NP-hardness of several other decision problems: testing local attractivity of an equilibrium point, stability of an equilibrium point in the sense of Lyapunov, invariance of the unit ball, boundedness of trajectories, convergence of all trajectories in a ball to a given equilibrium point, existence of a quadratic Lyapunov function, local collision avoidance, and existence of a stabilizing control law.\n  (ii) We present a simple, explicit example of a globally asymptotically stable quadratic vector field on the plane which does not admit a polynomial Lyapunov function (joint work with M. Krstic). For the subclass of homogeneous vector fields, we conjecture that asymptotic stability implies existence of a polynomial Lyapunov function, but show that the minimum degree of such a Lyapunov function can be arbitrarily large even for vector fields in fixed dimension and degree. For the same class of vector fields, we further establish that there is no monotonicity in the degree of polynomial Lyapunov functions.\n  (iii) We show via an explicit counterexample that if the degree of the polynomial Lyapunov function is fixed, then sos programming may fail to find a valid Lyapunov function even though one exists. On the other hand, if the degree is allowed to increase, we prove that existence of a polynomial Lyapunov function for a planar or a homogeneous vector field implies existence of a polynomial Lyapunov function that is sos and that the negative of its derivative is also sos.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "In this paper we show how to construct inner and outer convex approximations of a polytope from an approximate cone factorization of its slack matrix. This provides a robust generalization of the famous result of Yannakakis that polyhedral lifts of a polytope are controlled by (exact) nonnegative factorizations of its slack matrix. Our approximations behave well under polarity and have efficient representations using second order cones. We establish a direct relationship between the quality of the factorization and the quality of the approximations, and our results extend to generalized slack matrices that arise from a polytope contained in a polyhedron.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We introduce the notion of exchangeable equilibria of a symmetric bimatrix game, defined as those correlated equilibria in which players' strategy choices are conditionally independently and identically distributed given some hidden variable. We give several game-theoretic interpretations and a version of the \"revelation principle\". Geometrically, the set of exchangeable equilibria is convex and lies between the symmetric Nash equilibria and the symmetric correlated equilibria. Exchangeable equilibria can achieve higher expected utility than symmetric Nash equilibria.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Model-based compressed sensing refers to compressed sensing with extra structure about the underlying sparse signal known a priori. Recent work has demonstrated that both for deterministic and probabilistic models imposed on the signal, this extra information can be successfully exploited to enhance recovery performance. In particular, weighted $\\ell_1$-minimization with suitable choice of weights has been shown to improve performance in the so called non-uniform sparse model of signals. In this paper, we consider a full generalization of the non-uniform sparse model with very mild assumptions. We prove that when the measurements are obtained using a matrix with i.i.d Gaussian entries, weighted $\\ell_1$-minimization successfully recovers the sparse signal from its measurements with overwhelming probability. We also provide a method to choose these weights for any general signal model from the non-uniform sparse class of signal models.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Rejoinder to \"Latent variable graphical model selection via convex optimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "The nonnegative rank of an entrywise nonnegative matrix A of size mxn is the smallest integer r such that A can be written as A=UV where U is mxr and V is rxn and U and V are both nonnegative. The nonnegative rank arises in different areas such as combinatorial optimization and communication complexity. Computing this quantity is NP-hard in general and it is thus important to find efficient bounding techniques especially in the context of the aforementioned applications. In this paper we propose a new lower bound on the nonnegative rank which, unlike most existing lower bounds, does not explicitly rely on the matrix sparsity pattern and applies to nonnegative matrices with arbitrary support. The idea involves computing a certain nuclear norm with nonnegativity constraints which allows to lower bound the nonnegative rank, in the same way the standard nuclear norm gives lower bounds on the standard rank. Our lower bound is expressed as the solution of a copositive programming problem and can be relaxed to obtain polynomial-time computable lower bounds using semidefinite programming. We compare our lower bound with existing ones, and we show examples of matrices where our lower bound performs better than currently known ones.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We give explicit polynomial-sized (in $n$ and $k$) semidefinite representations of the hyperbolicity cones associated with the elementary symmetric polynomials of degree $k$ in $n$ variables. These convex cones form a family of non-polyhedral outer approximations of the non-negative orthant that preserve low-dimensional faces while successively discarding high-dimensional faces. More generally we construct explicit semidefinite representations (polynomial-sized in $k,m$, and $n$) of the hyperbolicity cones associated with $k$th directional derivatives of polynomials of the form $p(x) = \\det(\\sum_{i=1}^{n}A_i x_i)$ where the $A_i$ are $m\\times m$ symmetric matrices. These convex cones form an analogous family of outer approximations to any spectrahedral cone. Our representations allow us to use semidefinite programming to solve the linear cone programs associated with these convex cones as well as their (less well understood) dual cones.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "In this paper we establish links between, and new results for, three problems that are not usually considered together. The first is a matrix decomposition problem that arises in areas such as statistical modeling and signal processing: given a matrix $X$ formed as the sum of an unknown diagonal matrix and an unknown low rank positive semidefinite matrix, decompose $X$ into these constituents. The second problem we consider is to determine the facial structure of the set of correlation matrices, a convex set also known as the elliptope. This convex body, and particularly its facial structure, plays a role in applications from combinatorial optimization to mathematical finance. The third problem is a basic geometric question: given points $v_1,v_2,...,v_n\\in \\R^k$ (where $n > k$) determine whether there is a centered ellipsoid passing \\emph{exactly} through all of the points.\n  We show that in a precise sense these three problems are equivalent. Furthermore we establish a simple sufficient condition on a subspace $U$ that ensures any positive semidefinite matrix $L$ with column space $U$ can be recovered from $D+L$ for any diagonal matrix $D$ using a convex optimization-based heuristic known as minimum trace factor analysis. This result leads to a new understanding of the structure of rank-deficient correlation matrices and a simple condition on a set of points that ensures there is a centered ellipsoid passing through them.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We study stability criteria for discrete time switching systems. We investigate the structure of sets of LMIs that are a sufficient condition for stability (i.e., such that any switching system which satisfies these LMIs is stable). We provide an exact characterization of these sets. As a corollary, we show that it is PSPACE-complete to recognize whether a particular set of LMIs implies the stability of a switching system.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We propose a novel and natural architecture for decentralized control that is applicable whenever the underlying system has the structure of a partially ordered set (poset). This controller architecture is based on the concept of Moebius inversion for posets, and enjoys simple and appealing separation properties, since the closed-loop dynamics can be analyzed in terms of decoupled subsystems. The controller structure provides rich and interesting connections between concepts from order theory such as Moebius inversion and control-theoretic concepts such as state prediction, correction, and separability. In addition, using our earlier results on H_2-optimal decentralized control for arbitrary posets, we prove that the H_2-optimal controller in fact possesses the proposed structure, thereby establishing the optimality of the new controller architecture.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Our first contribution in this paper is to prove that three natural sum of squares (sos) based sufficient conditions for convexity of polynomials, via the definition of convexity, its first order characterization, and its second order characterization, are equivalent. These three equivalent algebraic conditions, henceforth referred to as sos-convexity, can be checked by semidefinite programming whereas deciding convexity is NP-hard. If we denote the set of convex and sos-convex polynomials in $n$ variables of degree $d$ with $\\tilde{C}_{n,d}$ and $\\tilde{\u03a3C}_{n,d}$ respectively, then our main contribution is to prove that $\\tilde{C}_{n,d}=\\tilde{\u03a3C}_{n,d}$ if and only if $n=1$ or $d=2$ or $(n,d)=(2,4)$. We also present a complete characterization for forms (homogeneous polynomials) except for the case $(n,d)=(3,4)$ which is joint work with G. Blekherman and is to be published elsewhere. Our result states that the set $C_{n,d}$ of convex forms in $n$ variables of degree $d$ equals the set $\u03a3C_{n,d}$ of sos-convex forms if and only if $n=2$ or $d=2$ or $(n,d)=(3,4)$. To prove these results, we present in particular explicit examples of polynomials in $\\tilde{C}_{2,6}\\setminus\\tilde{\u03a3C}_{2,6}$ and $\\tilde{C}_{3,4}\\setminus\\tilde{\u03a3C}_{3,4}$ and forms in $C_{3,6}\\setminus\u03a3C_{3,6}$ and $C_{4,4}\\setminus\u03a3C_{4,4}$, and a general procedure for constructing forms in $C_{n,d+2}\\setminus\u03a3C_{n,d+2}$ from nonnegative but not sos forms in $n$ variables and degree $d$. Although for disparate reasons, the remarkable outcome is that convex polynomials (resp. forms) are sos-convex exactly in cases where nonnegative polynomials (resp. forms) are sums of squares, as characterized by Hilbert.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We introduce the framework of path-complete graph Lyapunov functions for approximation of the joint spectral radius. The approach is based on the analysis of the underlying switched system via inequalities imposed among multiple Lyapunov functions associated to a labeled directed graph. Inspired by concepts in automata theory and symbolic dynamics, we define a class of graphs called path-complete graphs, and show that any such graph gives rise to a method for proving stability of the switched system. This enables us to derive several asymptotically tight hierarchies of semidefinite programming relaxations that unify and generalize many existing techniques such as common quadratic, common sum of squares, and maximum/minimum-of-quadratics Lyapunov functions. We compare the quality of approximation obtained by certain classes of path-complete graphs including a family of dual graphs and all path-complete graphs with two nodes on an alphabet of two matrices. We provide approximation guarantees for several families of path-complete graphs, such as the De Bruijn graphs, establishing as a byproduct a constructive converse Lyapunov theorem for maximum/minimum-of-quadratics Lyapunov functions.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "In this paper we address the basic geometric question of when a given convex set is the image under a linear map of an affine slice of a given closed convex cone. Such a representation or 'lift' of the convex set is especially useful if the cone admits an efficient algorithm for linear optimization over its affine slices. We show that the existence of a lift of a convex set to a cone is equivalent to the existence of a factorization of an operator associated to the set and its polar via elements in the cone and its dual. This generalizes a theorem of Yannakakis that established a connection between polyhedral lifts of a polytope and nonnegative factorizations of its slack matrix. Symmetric lifts of convex sets can also be characterized similarly. When the cones live in a family, our results lead to the definition of the rank of a convex set with respect to this family. We present results about this rank in the context of cones of positive semidefinite matrices. Our methods provide new tools for understanding cone lifts of convex sets.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We develop a complete state-space solution to H_2-optimal decentralized control of poset-causal systems with state-feedback. Our solution is based on the exploitation of a key separability property of the problem, that enables an efficient computation of the optimal controller by solving a small number of uncoupled standard Riccati equations. Our approach gives important insight into the structure of optimal controllers, such as controller degree bounds that depend on the structure of the poset. A novel element in our state-space characterization of the controller is a remarkable pair of transfer functions, that belong to the incidence algebra of the poset, are inverses of each other, and are intimately related to prediction of the state along the different paths on the poset. The results are illustrated by a numerical example.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Except for special classes of games, there is no systematic framework for analyzing the dynamical properties of multi-agent strategic interactions. Potential games are one such special but restrictive class of games that allow for tractable dynamic analysis. Intuitively, games that are \"close\" to a potential game should share similar properties. In this paper, we formalize and develop this idea by quantifying to what extent the dynamic features of potential games extend to \"near-potential\" games. We study convergence of three commonly studied classes of adaptive dynamics: discrete-time better/best response, logit response, and discrete-time fictitious play dynamics. For better/best response dynamics, we focus on the evolution of the sequence of pure strategy profiles and show that this sequence converges to a (pure) approximate equilibrium set, whose size is a function of the \"distance\" from a close potential game. We then study logit response dynamics and provide a characterization of the stationary distribution of this update rule in terms of the distance of the game from a close potential game and the corresponding potential function. We further show that the stochastically stable strategy profiles are pure approximate equilibria. Finally, we turn attention to fictitious play, and establish that the sequence of empirical frequencies of player actions converges to a neighborhood of (mixed) equilibria of the game, where the size of the neighborhood increases with distance of the game to a potential game. Thus, our results suggest that games that are close to a potential game inherit the dynamical properties of potential games. Since a close potential game to a given game can be found by solving a convex optimization problem, our approach also provides a systematic framework for studying convergence behavior of adaptive learning dynamics in arbitrary finite strategic form games.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We show that unless P=NP, there exists no polynomial time (or even pseudo-polynomial time) algorithm that can decide whether a multivariate polynomial of degree four (or higher even degree) is globally convex. This solves a problem that has been open since 1992 when N. Z. Shor asked for the complexity of deciding convexity for quartic polynomials. We also prove that deciding strict convexity, strong convexity, quasiconvexity, and pseudoconvexity of polynomials of even degree four or higher is strongly NP-hard. By contrast, we show that quasiconvexity and pseudoconvexity of odd degree polynomials can be decided in polynomial time.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "The structural properties of graphs are usually characterized in terms of invariants, which are functions of graphs that do not depend on the labeling of the nodes. In this paper we study convex graph invariants, which are graph invariants that are convex functions of the adjacency matrix of a graph. Some examples include functions of a graph such as the maximum degree, the MAXCUT value (and its semidefinite relaxation), and spectral invariants such as the sum of the $k$ largest eigenvalues. Such functions can be used to construct convex sets that impose various structural constraints on graphs, and thus provide a unified framework for solving a number of interesting graph problems via convex optimization. We give a representation of all convex graph invariants in terms of certain elementary invariants, and describe methods to compute or approximate convex graph invariants tractably. We also compare convex and non-convex invariants, and discuss connections to robust optimization. Finally we use convex graph invariants to provide efficient convex programming solutions to graph problems such as the deconvolution of the composition of two graphs into the individual components, hypothesis testing between graph families, and the generation of graphs with certain desired structural properties.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "In applications throughout science and engineering one is often faced with the challenge of solving an ill-posed inverse problem, where the number of available measurements is smaller than the dimension of the model to be estimated. However in many practical situations of interest, models are constrained structurally so that they only have a few degrees of freedom relative to their ambient dimension. This paper provides a general framework to convert notions of simplicity into convex penalty functions, resulting in convex optimization solutions to linear, underdetermined inverse problems. The class of simple models considered are those formed as the sum of a few atoms from some (possibly infinite) elementary atomic set; examples include well-studied cases such as sparse vectors and low-rank matrices, as well as several others including sums of a few permutations matrices, low-rank tensors, orthogonal matrices, and atomic measures. The convex programming formulation is based on minimizing the norm induced by the convex hull of the atomic set; this norm is referred to as the atomic norm. The facial structure of the atomic norm ball carries a number of favorable properties that are useful for recovering simple models, and an analysis of the underlying convex geometry provides sharp estimates of the number of generic measurements required for exact and robust recovery of models from partial information. These estimates are based on computing the Gaussian widths of tangent cones to the atomic norm ball. When the atomic set has algebraic structure the resulting optimization problems can be solved or approximated via semidefinite programming. The quality of these approximations affects the number of measurements required for recovery. Thus this work extends the catalog of simple models that can be recovered from limited linear information via tractable convex programming.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "The authors have decided to withdraw this submission. Clarifications/corrections, if any, may follow at a later date.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Suppose we observe samples of a subset of a collection of random variables. No additional information is provided about the number of latent variables, nor of the relationship between the latent and observed variables. Is it possible to discover the number of latent components, and to learn a statistical model over the entire collection of variables? We address this question in the setting in which the latent and observed variables are jointly Gaussian, with the conditional statistics of the observed variables conditioned on the latent variables being specified by a graphical model. As a first step we give natural conditions under which such latent-variable Gaussian graphical models are identifiable given marginal statistics of only the observed variables. Essentially these conditions require that the conditional graphical model among the observed variables is sparse, while the effect of the latent variables is \"spread out\" over most of the observed variables. Next we propose a tractable convex program based on regularized maximum-likelihood for model selection in this latent-variable setting; the regularizer uses both the $\\ell_1$ norm and the nuclear norm. Our modeling framework can be viewed as a combination of dimensionality reduction (to identify latent variables) and graphical modeling (to capture remaining statistical structure not attributable to the latent variables), and it consistently estimates both the number of latent components and the conditional graphical model structure among the observed variables. These results are applicable in the high-dimensional setting in which the number of latent/observed variables grows with the number of samples of the observed variables. The geometric properties of the algebraic varieties of sparse matrices and of low-rank matrices play an important role in our analysis.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "This document consists of two parts: the second part was submitted earlier as a new proof of Nash's theorem, and the first part is a note explaining a problem found in that proof. We are indebted to Sergiu Hart and Eran Shmaya for their careful study which led to their simultaneous discovery of this error. So far the error has not been fixed, but many of the results and techniques of the paper remain valid, so we will continue to make it available online.\n  Abstract for the original paper:\n  We give a novel proof of the existence of Nash equilibria in all finite games without using fixed point theorems or path following arguments. Our approach relies on a new notion intermediate between Nash and correlated equilibria called exchangeable equilibria, which are correlated equilibria with certain symmetry and factorization properties. We prove these exist by a duality argument, using Hart and Schmeidler's proof of correlated equilibrium existence as a first step.\n  In an appropriate limit exchangeable equilibria converge to the convex hull of Nash equilibria, proving that these exist as well. Exchangeable equilibria are defined in terms of symmetries of the game, so this method automatically proves the stronger statement that a symmetric game has a symmetric Nash equilibrium. The case without symmetries follows by a symmetrization argument.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "In this paper we introduce a novel flow representation for finite games in strategic form. This representation allows us to develop a canonical direct sum decomposition of an arbitrary game into three components, which we refer to as the potential, harmonic and nonstrategic components. We analyze natural classes of games that are induced by this decomposition, and in particular, focus on games with no harmonic component and games with no potential component. We show that the first class corresponds to the well-known potential games. We refer to the second class of games as harmonic games, and study the structural and equilibrium properties of this new class of games. Intuitively, the potential component of a game captures interactions that can equivalently be represented as a common interest game, while the harmonic part represents the conflicts between the interests of the players. We make this intuition precise, by studying the properties of these two classes, and show that indeed they have quite distinct and remarkable characteristics. For instance, while finite potential games always have pure Nash equilibria, harmonic games generically never do. Moreover, we show that the nonstrategic component does not affect the equilibria of a game, but plays a fundamental role in their efficiency properties, thus decoupling the location of equilibria and their payoff-related properties. Exploiting the properties of the decomposition framework, we obtain explicit expressions for the projections of games onto the subspaces of potential and harmonic games. This enables an extension of the properties of potential and harmonic games to \"nearby\" games. We exemplify this point by showing that the set of approximate equilibria of an arbitrary game can be characterized through the equilibria of its projection onto the set of potential games.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "We exhibit the rich structure of the set of correlated equilibria by analyzing the simplest of polynomial games: the mixed extension of matching pennies. We show that while the correlated equilibrium set is convex and compact, the structure of its extreme points can be quite complicated. In finite games the ratio of extreme correlated to extreme Nash equilibria can be greater than exponential in the size of the strategy spaces. In polynomial games there can exist extreme correlated equilibria which are not finitely supported; we construct a large family of examples using techniques from ergodic theory. We show that in general the set of correlated equilibrium distributions of a polynomial game cannot be described by conditions on finitely many moments (means, covariances, etc.), in marked contrast to the set of Nash equilibria which is always expressible in terms of finitely many moments.\n        \u25b3 Less", "author": "Pablo Parrilo"}, {"abstract": "Ad hoc electrical networks are formed by connecting power sources and loads without pre-determining the network topology. These systems are well-suited to addressing the lack of electricity in rural areas because they can be assembled and modified by non-expert users without central oversight. There are two core aspects to ad hoc system design: 1) designing source and load units such that the microgrid formed from the arbitrary interconnection of many units is always stable and 2) developing control strategies to autonomously manage the microgrid (i.e., perform power dispatch and voltage regulation) in a decentralized manner and under large uncertainty. To address these challenges we apply a number of nonlinear control techniques---including Brayton-Moser potential theory and primal-dual dynamics---to obtain conditions under which an ad hoc dc microgrid will have a suitable and asymptotically stable equilibrium point. Further, we propose a new decentralized control scheme that coordinates many sources to achieve a specified power dispatch from each. A simulated comparison to previous research is included.\n        \u25b3 Less", "author": "David Perreault"}, {"abstract": "We study a generalization of the well-known model of broadcasting on trees to the case of directed acyclic graphs (DAGs). At time $0$, a source vertex $X$ sends out a uniform bit along binary symmetric channels to a set of vertices called layer $1$. Each vertex except $X$ is assumed to have indegree $d$. At time $k\\geq1$, vertices at layer $k$ apply $d$-input Boolean processing functions to their received bits and send out the results to vertices at layer $k+1$. We say that broadcasting is possible if we can reconstruct $X$ with probability of error bounded away from $1/2$ using knowledge of all vertices at an arbitrarily deep layer $k$. This question is also related to models of reliable computation and storage, and information flow in biological networks.\n  In this paper, we study randomly constructed DAGs, for which we show that broadcasting is only possible if the noise level is below a certain (degree and function dependent) critical threshold. For $d\\geq3$, and random DAGs with layer sizes $\u03a9(\\log k)$ and majority processing functions, we identify the critical threshold. For $d=2$, we establish a similar result for NAND processing functions. We also prove a partial converse for odd $d\\geq3$ illustrating that the identified thresholds are impossible to improve by selecting different processing functions if the decoder is restricted to using a single vertex.\n  Finally, for any noise level, we construct explicit DAGs (using expander graphs) with bounded degree and layer sizes $\u0398(\\log k)$ admitting reconstruction. In particular, we show that such DAGs can be generated in deterministic quasi-polynomial time or randomized polylogarithmic time in the depth. These results portray a doubly-exponential advantage for storing a bit in bounded degree DAGs compared to trees, where $d=1$ but layer sizes need to grow exponentially with depth in order for broadcasting to be possible.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper studies the problem of estimating the differential entropy $h(S+Z)$, where $S$ and $Z$ are independent $d$-dimensional random variables with $Z\\sim\\mathcal{N}(0,\u03c3^2 \\mathrm{I}_d)$. The distribution of $S$ is unknown, but $n$ independently and identically distributed (i.i.d) samples from it are available. The question is whether having access to samples of $S$ as opposed to samples of $S+Z$ can improve estimation performance. We show that the answer is positive.\n  More concretely, we first show that despite the regularizing effect of noise, the number of required samples still needs to scale exponentially in $d$. This result is proven via a random-coding argument that reduces the question to estimating the Shannon entropy on a $2^{O(d)}$-sized alphabet. Next, for a fixed $d$ and $n$ large enough, it is shown that a simple plugin estimator, given by the differential entropy of the empirical distribution from $S$ convolved with the Gaussian density, achieves the loss of $O\\left((\\log n)^{d/4}/\\sqrt{n}\\right)$. Note that the plugin estimator amounts here to the differential entropy of a $d$-dimensional Gaussian mixture, for which we propose an efficient Monte Carlo computation algorithm. At the same time, estimating $h(S+Z)$ via popular differential entropy estimators (based on kernel density estimation (KDE) or k nearest neighbors (kNN) techniques) applied to samples from $S+Z$ would only attain much slower rates of order $O(n^{-1/d})$, despite the smoothness of $P_{S+Z}$.\n  As an application, which was in fact our original motivation for the problem, we estimate information flows in deep neural networks and discuss Tishby's Information Bottleneck and the compression conjecture, among others.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "We study the flow of information and the evolution of internal representations during deep neural network (DNN) training, aiming to demystify the compression aspect of the information bottleneck theory. The theory suggests that DNN training comprises a rapid fitting phase followed by a slower compression phase, in which the mutual information $I(X;T)$ between the input $X$ and internal representations $T$ decreases. Several papers observe compression of estimated mutual information on different DNN models, but the true $I(X;T)$ over these networks is provably either constant (discrete $X$) or infinite (continuous $X$). This work explains the discrepancy between theory and experiments, and clarifies what was actually measured by these past works. To this end, we introduce an auxiliary (noisy) DNN framework for which $I(X;T)$ is a meaningful quantity that depends on the network's parameters. This noisy framework is shown to be a good proxy for the original (deterministic) DNN both in terms of performance and the learned representations. We then develop a rigorous estimator for $I(X;T)$ in noisy DNNs and observe compression in various models. By relating $I(X;T)$ in the noisy DNN to an information-theoretic communication problem, we show that compression is driven by the progressive clustering of hidden representations of inputs from the same class. Several methods to directly monitor clustering of hidden representations, both in noisy and deterministic DNNs, are used to show that meaningful clusters form in the $T$ space. Finally, we return to the estimator of $I(X;T)$ employed in past works, and demonstrate that while it fails to capture the true (vacuous) mutual information, it does serve as a measure for clustering. This clarifies the past observations of compression and isolates the geometric clustering of hidden representations as the true phenomenon of interest.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "In this paper we propose a method of proving impossibility results based on applying strong data-processing inequalities to estimate mutual information between sets of variables forming certain Markov random fields. The end result is that mutual information between two `far away' (as measured by the graph distance) variables is bounded by the probability of existence of open path in a bond-percolation problem on the same graph. Furthermore, stronger bounds can be obtained by establishing mutual comparison results with an erasure model on the same graph, with erasure probabilities given by the contraction coefficients.\n  As application, we show that our method gives sharp threshold for partially recovering a rank-one perturbation of a random Gaussian matrix (spiked Wigner model), recovers (and generalizes) the best known upper bound on noise-level for group synchronization due to Abbe and Boix, and establishes new impossibility result for a $k$-community detection (stochastic block model).\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Most information systems store data by modifying the local state of matter, in the hope that atomic (or sub-atomic) local interactions would stabilize the state for a sufficiently long time, thereby allowing later recovery. In this work we initiate the study of information retention in locally-interacting systems. The evolution in time of the interacting particles is modeled via the stochastic Ising model (SIM). The initial spin configuration $X_0$ serves as the user-controlled input. The output configuration $X_t$ is produced by running $t$ steps of the Glauber chain. Our main goal is to evaluate the information capacity $I_n(t)\\triangleq\\max_{p_{X_0}}I(X_0;X_t)$ when the time $t$ scales with the size of the system $n$. For the zero-temperature SIM on the two-dimensional $\\sqrt{n}\\times\\sqrt{n}$ grid and free boundary conditions, it is easy to show that $I_n(t) = \u0398(n)$ for $t=O(n)$. In addition, we show that on the order of $\\sqrt{n}$ bits can be stored for infinite time in striped configurations. The $\\sqrt{n}$ achievability is optimal when $t\\to\\infty$ and $n$ is fixed.\n  One of the main results of this work is an achievability scheme that stores more than $\\sqrt{n}$ bits (in orders of magnitude) for superlinear (in $n$) times. The analysis of the scheme decomposes the system into $\u03a9(\\sqrt{n})$ independent Z-channels whose crossover probability is found via the (recently rigorously established) Lifshitz law of phase boundary movement. We also provide results for the positive but small temperature regime. We show that an initial configuration drawn according to the Gibbs measure cannot retain more than a single bit for $t\\geq e^{cn^{\\frac{1}{4}+\u03b5}}$. On the other hand, when scaling time with $\u03b2$, the stripe-based coding scheme (that stores for infinite time at zero temperature) is shown to retain its bits for time that is exponential in $\u03b2$.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "We study the following generalization of the well-known model of broadcasting on trees. Consider an infinite directed acyclic graph (DAG) with a unique source node $X$. Let the collection of nodes at distance $k$ from $X$ be called the $k$th layer. At time zero, the source node is given a bit. At time $k\\geq 1$, each node in the $(k-1)$th layer inspects its inputs and sends a bit to its descendants in the $k$th layer. Each bit is flipped with a probability of error $\u03b4\\in \\left(0,\\frac{1}{2}\\right)$ in the process of transmission. The goal is to be able to recover the original bit with probability of error better than $\\frac{1}{2}$ from the values of all nodes at an arbitrarily deep layer $k$.\n  Besides its natural broadcast interpretation, the DAG broadcast is a natural model of noisy computation. Some special cases of the model represent information flow in biological networks, and other cases represent noisy finite automata models.\n  We show that there exist DAGs with bounded degree and layers of size $\u03c9(\\log(k))$ that permit recovery provided $\u03b4$ is sufficiently small and find the critical $\u03b4$ for the DAGs constructed. Our result demonstrates a doubly-exponential advantage for storing a bit in bounded degree DAGs compared to trees. On the negative side, we show that if the DAG is a two-dimensional regular grid, then recovery is impossible for any $\u03b4\\in \\left(0,\\frac{1}{2}\\right)$ provided all nodes use either AND or XOR for their processing functions.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Consider a binary linear code of length $N$, minimum distance $d_{\\text{min}}$, transmission over the binary erasure channel with parameter $0 < \u03b5< 1$ or the binary symmetric channel with parameter $0 < \u03b5< \\frac12$, and block-MAP decoding. It was shown by Tillich and Zemor that in this case the error probability of the block-MAP decoder transitions \"quickly\" from $\u03b4$ to $1-\u03b4$ for any $\u03b4>0$ if the minimum distance is large. In particular the width of the transition is of order $O(1/\\sqrt{d_{\\text{min}}})$. We strengthen this result by showing that under suitable conditions on the weight distribution of the code, the transition width can be as small as $\u0398(1/N^{\\frac12-\u03ba})$, for any $\u03ba>0$, even if the minimum distance of the code is not linear. This condition applies e.g., to Reed-Mueller codes. Since $\u0398(1/N^{\\frac12})$ is the smallest transition possible for any code, we speak of \"almost\" optimal scaling. We emphasize that the width of the transition says nothing about the location of the transition. Therefore this result has no bearing on whether a code is capacity-achieving or not. As a second contribution, we present a new estimate on the derivative of the EXIT function, the proof of which is based on the Blowing-Up Lemma.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "We consider list-decoding in the zero-rate regime for two cases: the binary alphabet and the spherical codes in Euclidean space. Specifically, we study the maximal $\u03c4\\in [0,1]$ for which there exists an arrangement of $M$ balls of relative Hamming radius $\u03c4$ in the binary hypercube (of arbitrary dimension) with the property that no point of the latter is covered by $L$ or more of them. As $M\\to \\infty$ the maximal $\u03c4$ decreases to a well-known critical value $\u03c4_L$. In this work, we prove several results on the rate of this convergence.\n  For the binary case, we show that the rate is $\u0398(M^{-1})$ when $L$ is even, thus extending the classical results of Plotkin and Levenshtein for $L=2$. For $L=3$ the rate is shown to be $\u0398(M^{-\\tfrac{2}{3}})$.\n  For the similar question about spherical codes, we prove the rate is $\u03a9(M^{-1})$ and $O(M^{-\\tfrac{2L}{L^2-L+2}})$.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "In this paper, we consider a problem of sampling a Wiener process, with samples forwarded to a remote estimator over a channel that is modeled as a queue. The estimator reconstructs an estimate of the real-time signal value from causally received samples. We study the optimal online sampling strategy that minimizes the mean square estimation error subject to a sampling rate constraint. We prove that the optimal sampling strategy is a threshold policy, and find the optimal threshold. This threshold is determined by how much the Wiener process varies during the random service time and the maximum allowed sampling rate. Further, if the sampling times are independent of the observed Wiener process, the optimal sampling problem reduces to an age of information optimization problem that has been recently solved. Our comparisons show that the estimation error of the optimal sampling policy can be much smaller than those of age-optimal sampling, zero-wait sampling, and classic periodic sampling.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "It is well known that the mutual information between two random variables can be expressed as the difference of two relative entropies that depend on an auxiliary distribution, a relation sometimes referred to as the golden formula. This paper is concerned with a finite-blocklength extension of this relation. This extension consists of two elements: 1) a finite-blocklength channel-coding converse bound by Polyanskiy and Verd\u00fa (2014), which involves the ratio of two Neyman-Pearson $\u03b2$ functions (beta-beta converse bound); and 2) a novel beta-beta channel-coding achievability bound, expressed again as the ratio of two Neyman-Pearson $\u03b2$ functions.\n  To demonstrate the usefulness of this finite-blocklength extension of the golden formula, the beta-beta achievability and converse bounds are used to obtain a finite-blocklength extension of Verd\u00fa's (2002) wideband-slope approximation. The proof parallels the derivation of the latter, with the beta-beta bounds used in place of the golden formula.\n  The beta-beta (achievability) bound is also shown to be useful in cases where the capacity-achieving output distribution is not a product distribution due to, e.g., a cost constraint or structural constraints on the codebook, such as orthogonality or constant composition. As an example, the bound is used to characterize the channel dispersion of the additive exponential-noise channel and to obtain a finite-blocklength achievability bound (the tightest to date) for multiple-input multiple-output Rayleigh-fading channels with perfect channel state information at the receiver.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "In this paper we consider a channel model that is often used to describe the mobile wireless scenario: multiple-antenna additive white Gaussian noise channels subject to random (fading) gain with full channel state information at the receiver. Dynamics of the fading process are approximated by a piecewise-constant process (frequency non-selective isotropic block fading). This work addresses the finite blocklength fundamental limits of this channel model. Specifically, we give a formula for the channel dispersion -- a quantity governing the delay required to achieve capacity. Multiplicative nature of the fading disturbance leads to a number of interesting technical difficulties that required us to enhance traditional methods for finding channel dispersion. Alas, one difficulty remains: the converse (impossibility) part of our result holds under an extra constraint on the growth of the peak-power with blocklength.\n  Our results demonstrate, for example, that while capacities of $n_t\\times n_r$ and $n_r \\times n_t$ antenna configurations coincide (under fixed received power), the coding delay can be quite sensitive to this switch. For example, at the received SNR of $20$ dB the $16\\times 100$ system achieves capacity with codes of length (delay) which is only $60\\%$ of the length required for the $100\\times 16$ system. Another interesting implication is that for the MISO channel, the dispersion-optimal coding schemes require employing orthogonal designs such as Alamouti's scheme -- a surprising observation considering the fact that Alamouti's scheme was designed for reducing demodulation errors, not improving coding rate. Finding these dispersion-optimal coding schemes naturally gives a criteria for producing orthogonal design-like inputs in dimensions where orthogonal designs do not exist.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "New lower and upper bounds on the reliability function of typewriter channels are given. Our lower bounds improve upon the (multiletter) expurgated bound of Gallager, furnishing a new and simple counterexample to a conjecture made in 1967 by Shannon, Gallager and Berlekamp on its tightness. The only other known counterexample is due to Katsman, Tsfasman and Vl\u0103du\u0163 who used algebraic-geometric codes on a $q$-ary symmetric channels, $q\\geq 49$. Here we prove, by introducing dependence between codewords of a random ensemble, that the conjecture is false even for a typewriter channel with $q=4$ inputs. In the process, we also demonstrate that Lov\u00e1sz's proof of the capacity of the pentagon was implicitly contained (but unnoticed!) in the works of Jelinek and Gallager on the expurgated bound done at least ten years before Lov\u00e1sz. In the opposite direction, new upper bounds on the reliability function are derived for channels with an odd number of inputs by using an adaptation of Delsarte's linear programming bound. First we derive a bound based on the minimum distance, which combines Lov\u00e1sz's construction for bounding the graph capacity with the McEliece-Rodemich-Rumsey-Welch construction for bounding the minimum distance of codes in the Hamming space. Then, for the particular case of cross-over probability $1/2$, we derive an improved bound by also using the method of Kalai and Linial to study the spectrum distribution of codes.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "The problem of population recovery refers to estimating a distribution based on incomplete or corrupted samples. Consider a random poll of sample size $n$ conducted on a population of individuals, where each pollee is asked to answer $d$ binary questions. We consider one of the two polling impediments: (a) in lossy population recovery, a pollee may skip each question with probability $\u03b5$, (b) in noisy population recovery, a pollee may lie on each question with probability $\u03b5$. Given $n$ lossy or noisy samples, the goal is to estimate the probabilities of all $2^d$ binary vectors simultaneously within accuracy $\u03b4$ with high probability.\n  This paper settles the sample complexity of population recovery. For lossy model, the optimal sample complexity is $\\tilde\u0398(\u03b4^{-2\\max\\{\\frac\u03b5{1-\u03b5},1\\}})$, improving the state of the art by Moitra and Saks in several ways: a lower bound is established, the upper bound is improved and the result depends at most on the logarithm of the dimension. Surprisingly, the sample complexity undergoes a phase transition from parametric to nonparametric rate when $\u03b5$ exceeds $1/2$. For noisy population recovery, the sharp sample complexity turns out to be more sensitive to dimension and scales as $\\exp(\u0398(d^{1/3} \\log^{2/3}(1/\u03b4)))$ except for the trivial cases of $\u03b5=0,1/2$ or $1$.\n  For both models, our estimators simply compute the empirical mean of a certain function, which is found by pre-solving a linear program (LP). Curiously, the dual LP can be understood as Le Cam's method for lower-bounding the minimax risk, thus establishing the statistical optimality of the proposed estimators. The value of the LP is determined by complex-analytic methods.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "In this paper, we consider a problem of sampling a Wiener process, with samples forwarded to a remote estimator via a channel that consists of a queue with random delay. The estimator reconstructs a real-time estimate of the signal from causally received samples. Motivated by recent research on age-of-information, we study the optimal sampling strategy that minimizes the mean square estimation error subject to a sampling frequency constraint. We prove that the optimal sampling strategy is a threshold policy, and find the optimal threshold. This threshold is determined by the sampling frequency constraint and how much the Wiener process varies during the channel delay. An interesting consequence is that even in the absence of the sampling frequency constraint, the optimal strategy is not zero-wait sampling in which a new sample is taken once the previous sample is delivered; rather, it is optimal to wait for a non-zero amount of time after the previous sample is delivered, and then take the next sample. Further, if the sampling times are independent of the observed Wiener process, the optimal sampling problem reduces to an age-of-information optimization problem that has been recently solved. Our comparisons show that the estimation error of the optimal sampling policy is much smaller than those of age-optimal sampling, zero-wait sampling, and classic uniform sampling.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper addresses the problem of adding redundancy to a collection of physical objects so that the overall system is more robust to failures. In contrast to its information counterpart, which can exploit parity to protect multiple information symbols from a single erasure, physical redundancy can only be realized through duplication and substitution of objects. We propose a bipartite graph model for designing defect-tolerant systems in which defective objects are replaced by judiciously connected redundant objects. The fundamental limits of this model are characterized under various asymptotic settings and both asymptotic and finite-size systems that approach these limits are constructed. Among other results, we show that simple modular redundancy is in general suboptimal. As we develop, this combinatorial problem of defect tolerant system design has a natural interpretation as one of graph coloring, and the analysis is significantly different from that traditionally used in information redundancy for error-control codes.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper studies the basic question of whether a given channel $V$ can be dominated (in the precise sense of being more noisy) by a $q$-ary symmetric channel. The concept of \"less noisy\" relation between channels originated in network information theory (broadcast channels) and is defined in terms of mutual information or Kullback-Leibler divergence. We provide an equivalent characterization in terms of $\u03c7^2$-divergence. Furthermore, we develop a simple criterion for domination by a $q$-ary symmetric channel in terms of the minimum entry of the stochastic matrix defining the channel $V$. The criterion is strengthened for the special case of additive noise channels over finite Abelian groups. Finally, it is shown that domination by a symmetric channel implies (via comparison of Dirichlet forms) a logarithmic Sobolev inequality for the original channel.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "The capacity of a graph is defined as the rate of exponential growth of independent sets in the strong powers of the graph. In the strong power an edge connects two sequences if at each position their letters are equal or adjacent. We consider a variation of the problem where edges in the power graphs are removed between sequences which differ in more than a fraction $\u03b4$ of coordinates. The proposed generalization can be interpreted as the problem of determining the highest rate of zero undetected-error communication over a link with adversarial noise, where only a fraction $\u03b4$ of symbols can be perturbed and only some substitutions are allowed.\n  We derive lower bounds on achievable rates by combining graph homomorphisms with a graph-theoretic generalization of the Gilbert-Varshamov bound. We then give an upper bound, based on Delsarte's linear programming approach, which combines Lov\u00e1sz' theta function with the construction used by McEliece et al. for bounding the minimum distance of codes in Hamming spaces.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "We give new bounds on the reliability function of a typewriter channel with 5 inputs and crossover probability $1/2$. The lower bound is more of theoretical than practical importance; it improves very marginally the expurgated bound, providing a counterexample to a conjecture on its tightness by Shannon, Gallager and Berlekamp which does not need the construction of algebraic-geometric codes previously used by Katsman, Tsfasman and Vl\u0103du\u0163. The upper bound is derived by using an adaptation of the linear programming bound and it is essentially useful as a low-rate anchor for the straight line bound.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Log-Sobolev inequalities (LSIs) upper-bound entropy via a multiple of the Dirichlet form (i.e. norm of a gradient). In this paper we prove a family of entropy-energy inequalities for the binary hypercube which provide a non-linear comparison between the entropy and the Dirichlet form and improve on the usual LSIs for functions with small support. These non-linear LSIs, in turn, imply a new version of the hypercontractivity for such functions. As another consequence, we derive a sharp form of the uncertainty principle for the hypercube: a function whose energy is concentrated on a set of small size, and whose Fourier energy is concentrated on a small Hamming ball must be zero. The tradeoff between the sizes that we derive is asymptotically optimal. This new uncertainty principle implies a new estimate on the size of Fourier coefficients of sparse Boolean functions. We observe that an analogous (asymptotically optimal) uncertainty principle in the Euclidean space follows from the sharp form of Young's inequality due to Beckner. This hints that non-linear LSIs augment Young's inequality (which is sharp for finite groups).\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "A channel coding achievability bound expressed in terms of the ratio between two Neyman-Pearson $\u03b2$ functions is proposed. This bound is the dual of a converse bound established earlier by Polyanskiy and Verd\u00fa (2014). The new bound turns out to simplify considerably the analysis in situations where the channel output distribution is not a product distribution, for example due to a cost constraint or a structural constraint (such as orthogonality or constant composition) on the channel inputs. Connections to existing bounds in the literature are discussed. The bound is then used to derive 1) an achievability bound on the channel dispersion of additive non-Gaussian noise channels with random Gaussian codebooks, 2) the channel dispersion of the exponential-noise channel, 3) a second-order expansion for the minimum energy per bit of an AWGN channel, and 4) a lower bound on the maximum coding rate of a multiple-input multiple-output Rayleigh-fading channel with perfect channel state information at the receiver, which is the tightest known achievability result.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper quantifies the intuitive observation that adding noise reduces available information by means of non-linear strong data processing inequalities. Consider the random variables $W\\to X\\to Y$ forming a Markov chain, where $Y=X+Z$ with $X$ and $Z$ real-valued, independent and $X$ bounded in $L_p$-norm. It is shown that $I(W;Y) \\le F_I(I(W;X))$ with $F_I(t)<t$ whenever $t>0$, if and only if $Z$ has a density whose support is not disjoint from any translate of itself. A related question is to characterize for what couplings $(W,X)$ the mutual information $I(W;Y)$ is close to maximum possible. To that end we show that in order to saturate the channel, i.e. for $I(W;Y)$ to approach capacity, it is mandatory that $I(W;X)\\to\\infty$ (under suitable conditions on the channel). A key ingredient for this result is a deconvolution lemma which shows that post-convolution total variation distance bounds the pre-convolution Kolmogorov-Smirnov distance. Explicit bounds are provided for the special case of the additive Gaussian noise channel with quadratic cost constraint. These bounds are shown to be order-optimal. For this case simplified proofs are provided leveraging Gaussian-specific tools such as the connection between information and estimation (I-MMSE) and Talagrand's information-transportation inequality.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "The data-processing inequality, that is, $I(U;Y) \\le I(U;X)$ for a Markov chain $U \\to X \\to Y$, has been the method of choice for proving impossibility (converse) results in information theory and many other disciplines. Various channel-dependent improvements (called strong data-processing inequalities, or SDPIs) of this inequality have been proposed both classically and more recently. In this note we first survey known results relating various notions of contraction for a single channel. Then we consider the basic extension: given SDPI for each constituent channel in a Bayesian network, how to produce an end-to-end SDPI?\n  Our approach is based on the (extract of the) Evans-Schulman method, which is demonstrated for three different kinds of SDPIs, namely, the usual Ahslwede-G\u00e1cs type contraction coefficients (mutual information), Dobrushin's contraction coefficients (total variation), and finally the $F_I$-curve (the best possible non-linear SDPI for a given channel). Resulting bounds on the contraction coefficients are interpreted as probability of site percolation. As an example, we demonstrate how to obtain SDPI for an $n$-letter memoryless channel with feedback given an SDPI for $n=1$.\n  Finally, we discuss a simple observation on the equivalence of a linear SDPI and comparison to an erasure channel (in the sense of \"less noisy\" order). This leads to a simple proof of a curious inequality of Samorodnitsky (2015), and sheds light on how information spreads in the subsets of inputs of a memoryless channel.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "The capacity of a graph is defined as the rate of exponential grow of independent sets in the strong powers of the graph. In strong power, an edge connects two sequences if at each position letters are equal or adjacent. We consider a variation of the problem where edges in the power graphs are removed among sequences which differ in more than a fraction $\u03b4$ of coordinates. For odd cycles, we derive an upper bound on the corresponding rate which combines Lov\u00e1sz' bound on the capacity with Delsarte's linear programming bounds on the minimum distance of codes in Hamming spaces. For the pentagon, this shows that for $\u03b4\\ge {1-{1\\over\\sqrt{5}}}$ the Lov\u00e1sz rate is the best possible, while we prove by a Gilbert-Varshamov-type bound that a higher rate is achievable for $\u03b4< {2\\over 5}$.\n  Communication interpretation of this question is the problem of sending quinary symbols subject to $\\pm 1\\mod 5$ disturbance. The maximal communication rate subject to the zero undetected-error equals capacity of a pentagon. The question addressed here is how much this rate can be increased if only a fraction $\u03b4$ of symbols is allowed to be disturbed\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper investigates the minimum energy required to transmit $k$ information bits with a given reliability over a multiple-antenna Rayleigh block-fading channel, with and without channel state information (CSI) at the receiver. No feedback is assumed. It is well known that the ratio between the minimum energy per bit and the noise level converges to $-1.59$ dB as $k$ goes to infinity, regardless of whether CSI is available at the receiver or not. This paper shows that lack of CSI at the receiver causes a slowdown in the speed of convergence to $-1.59$ dB as $k\\to\\infty$ compared to the case of perfect receiver CSI. Specifically, we show that, in the no-CSI case, the gap to $-1.59$ dB is proportional to $((\\log k) /k)^{1/3}$, whereas when perfect CSI is available at the receiver, this gap is proportional to $1/\\sqrt{k}$. In both cases, the gap to $-1.59$ dB is independent of the number of transmit antennas and of the channel's coherence time. Numerically, we observe that, when the receiver is equipped with a single antenna, to achieve an energy per bit of $ - 1.5$ dB in the no-CSI case, one needs to transmit at least $7\\times 10^7$ information bits, whereas $6\\times 10^4$ bits suffice for the case of perfect CSI at the receiver.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "It is shown that under suitable regularity conditions, differential entropy is a Lipschitz functional on the space of distributions on $n$-dimensional Euclidean space with respect to the quadratic Wasserstein distance. Under similar conditions, (discrete) Shannon entropy is shown to be Lipschitz continuous in distributions over the product space with respect to Ornstein's $\\bar d$-distance (Wasserstein distance corresponding to the Hamming distance). These results together with Talagrand's and Marton's transportation-information inequalities allow one to replace the unknown multi-user interference with its i.i.d. approximations. As an application, a new outer bound for the two-user Gaussian interference channel is proved, which, in particular, settles the \"missing corner point\" problem of Costa (1985).\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "A mapping of $k$-bit strings into $n$-bit strings is called an $(\u03b1,\u03b2)$-map if $k$-bit strings which are more than $\u03b1k$ apart are mapped to $n$-bit strings that are more than $\u03b2n$ apart. This is a relaxation of the classical problem of constructing error-correcting codes, which corresponds to $\u03b1=0$. Existence of an $(\u03b1,\u03b2)$-map is equivalent to existence of a graph homomorphism $\\bar H(k,\u03b1k)\\to \\bar H(n,\u03b2n)$, where $H(n,d)$ is a Hamming graph with vertex set $\\{0,1\\}^n$ and edges connecting vertices differing in $d$ or fewer entries.\n  This paper proves impossibility results on achievable parameters $(\u03b1,\u03b2)$ in the regime of $n,k\\to\\infty$ with a fixed ratio ${n\\over k}= \u03c1$. This is done by developing a general criterion for existence of graph-homomorphism based on the semi-definite relaxation of the independence number of a graph (known as the Schrijver's $\u03b8$-function). The criterion is then evaluated using some known and some new results from coding theory concerning the $\u03b8$-function of Hamming graphs. As an example, it is shown that if $\u03b2>1/2$ and $n\\over k$ -- integer, the ${n\\over k}$-fold repetition map achieving $\u03b1=\u03b2$ is asymptotically optimal.\n  Finally, constraints on configurations of points and hyperplanes in projective spaces over $\\mathbb{F}_2$ are derived.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper quantifies the fundamental limits of variable-length transmission of a general (possibly analog) source over a memoryless channel with noiseless feedback, under a distortion constraint. We consider excess distortion, average distortion and guaranteed distortion ($d$-semifaithful codes). In contrast to the asymptotic fundamental limit, a general conclusion is that allowing variable-length codes and feedback leads to a sizable improvement in the fundamental delay-distortion tradeoff. In addition, we investigate the minimum energy required to reproduce $k$ source samples with a given fidelity after transmission over a memoryless Gaussian channel, and we show that the required minimum energy is reduced with feedback and an average (rather than maximal) power constraint.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Motivated by the current interest in ultra-reliable, low-latency, machine-type communication systems, we investigate the tradeoff between reliability, throughput, and latency in the transmission of information over multiple-antenna Rayleigh block-fading channels. Specifically, we obtain finite-blocklength, finite-SNR upper and lower bounds on the maximum coding rate achievable over such channels for a given constraint on the packet error probability. Numerical evidence suggests that our bounds delimit tightly the maximum coding rate already for short blocklengths (packets of about 100 symbols). Furthermore, our bounds reveal the existence of a tradeoff between the rate gain obtainable by spreading each codeword over all available time-frequency-spatial degrees of freedom, and the rate loss caused by the need of estimating the fading coefficients over these degrees of freedom. In particular, our bounds allow us to determine the optimal number of transmit antennas and the optimal number of time-frequency diversity branches that maximize the rate. Finally, we show that infinite-blocklength performance metrics such as the ergodic capacity and the outage capacity yield inaccurate throughput estimates.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Consider the problem of packing Hamming balls of a given relative radius subject to the constraint that they cover any point of the ambient Hamming space with multiplicity at most $L$. For odd $L\\ge 3$ an asymptotic upper bound on the rate of any such packing is proven. Resulting bound improves the best known bound (due to Blinovsky'1986) for rates below a certain threshold. Method is a superposition of the linear-programming idea of Ashikhmin, Barg and Litsyn (that was used previously to improve the estimates of Blinovsky for $L=2$) and a Ramsey-theoretic technique of Blinovsky. As an application it is shown that for all odd $L$ the slope of the rate-radius tradeoff is zero at zero rate.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper investigates the maximal channel coding rate achievable at a given blocklength $n$ and error probability $\u03b5$, when the codewords are subject to a long-term (i.e., averaged-over-all-codeword) power constraint. The second-order term in the large-$n$ expansion of the maximal channel coding rate is characterized both for additive white Gaussian noise (AWGN) channels and for quasi-static fading channels with perfect channel state information available at both the transmitter and the receiver. It is shown that in both cases the second-order term is proportional to $\\sqrt{n^{-1}\\ln n}$. For the quasi-static fading case, this second-order term is achieved by truncated channel inversion, namely, by concatenating a dispersion-optimal code for an AWGN channel subject to a short-term power constraint, with a power controller that inverts the channel whenever the fading gain is above a certain threshold. Easy-to-evaluate approximations of the maximal channel coding rate are developed for both the AWGN and the quasi-static fading case.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "One of the basic tenets in information theory, the data processing inequality states that output divergence does not exceed the input divergence for any channel. For channels without input constraints, various estimates on the amount of such contraction are known, Dobrushin's coefficient for the total variation being perhaps the most well-known. This work investigates channels with average input cost constraint. It is found that while the contraction coefficient typically equals one (no contraction), the information nevertheless dissipates. A certain non-linear function, the \\emph{Dobrushin curve} of the channel, is proposed to quantify the amount of dissipation. Tools for evaluating the Dobrushin curve of additive-noise channels are developed based on coupling arguments. Some basic applications in stochastic control, uniqueness of Gibbs measures and fundamental limits of noisy circuits are discussed.\n  As an application, it shown that in the chain of $n$ power-constrained relays and Gaussian channels the end-to-end mutual information and maximal squared correlation decay as $\u0398(\\frac{\\log\\log n}{\\log n})$, which is in stark contrast with the exponential decay in chains of discrete channels. Similarly, the behavior of noisy circuits (composed of gates with bounded fan-in) and broadcasting of information on trees (of bounded degree) does not experience threshold behavior in the signal-to-noise ratio (SNR). Namely, unlike the case of discrete channels, the probability of bit error stays bounded away from $1\\over 2$ regardless of the SNR.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper studies the fundamental limits of the minimum average length of lossless and lossy variable-length compression, allowing a nonzero error probability $\u03b5$, for lossless compression. We give non-asymptotic bounds on the minimum average length in terms of Erokhin's rate-distortion function and we use those bounds to obtain a Gaussian approximation on the speed of approach to the limit which is quite accurate for all but small blocklengths: $$(1 - \u03b5) k H(\\mathsf S) - \\sqrt{\\frac{k V(\\mathsf S)}{2 \u03c0} } e^{- \\frac {(Q^{-1}(\u03b5))^2} 2 }$$ where $Q^{-1}(\\cdot)$ is the functional inverse of the standard Gaussian complementary cdf, and $V(\\mathsf S)$ is the source dispersion. A nonzero error probability thus not only reduces the asymptotically achievable rate by a factor of $1 - \u03b5$, but this asymptotic limit is approached from below, i.e. larger source dispersions and shorter blocklengths are beneficial. Variable-length lossy compression under an excess distortion constraint is shown to exhibit similar properties.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Directed acyclic graphical models (DAGs) are often used to describe common structural properties in a family of probability distributions. This paper addresses the question of classifying DAGs up to an isomorphism. By considering Gaussian densities, the question reduces to verifying equality of certain algebraic varieties. A question of computing equations for these varieties has been previously raised in the literature. Here it is shown that the most natural method adds spurious components with singular principal minors, proving a conjecture of Sullivant. This characterization is used to establish an algebraic criterion for isomorphism, and to provide a randomized algorithm for checking that criterion. Results are applied to produce a list of the isomorphism classes of tree models on 4,5, and 6 nodes. Finally, some evidence is provided to show that projectivized DAG varieties contain useful information in the sense that their relative embedding is closely related to efficient inference.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper investigates the maximal achievable rate for a given blocklength and error probability over quasi-static multiple-input multiple-output (MIMO) fading channels, with and without channel state information (CSI) at the transmitter and/or the receiver. The principal finding is that outage capacity, despite being an asymptotic quantity, is a sharp proxy for the finite-blocklength fundamental limits of slow-fading channels. Specifically, the channel dispersion is shown to be zero regardless of whether the fading realizations are available at both transmitter and receiver, at only one of them, or at neither of them. These results follow from analytically tractable converse and achievability bounds. Numerical evaluation of these bounds verifies that zero dispersion may indeed imply fast convergence to the outage capacity as the blocklength increases. In the example of a particular $1 \\times 2$ single-input multiple-output (SIMO) Rician fading channel, the blocklength required to achieve $90\\%$ of capacity is about an order of magnitude smaller compared to the blocklength required for an AWGN channel with the same capacity. For this specific scenario, the coding/decoding schemes adopted in the LTE-Advanced standard are benchmarked against the finite-blocklength achievability and converse bounds.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Consider the linear space of functions on the binary hypercube and the linear operator $S_\u03b4$ acting by averaging a function over a Hamming sphere of radius $\u03b4n$ around every point. It is shown that this operator has a dimension-independent bound on the norm $L_p \\to L_2$ with $p = 1+(1-2\u03b4)^2$. This result evidently parallels a classical estimate of Bonami and Gross for $L_p \\to L_q$ norms for the operator of convolution with a Bernoulli noise. The estimate for $S_\u03b4$ is harder to obtain since the latter is neither a part of a semigroup, nor a tensor power. The result is shown by a detailed study of the eigenvalues of $S_\u03b4$ and $L_p\\to L_2$ norms of the Fourier multiplier operators $\u03a0_a$ with symbol equal to a characteristic function of the Hamming sphere of radius $a$ (in the notation common in boolean analysis $\u03a0_a f=f^{=a}$, where $f^{=a}$ is a degree-$a$ component of function $f$). A sample application of the result is given: Any set $A\\subset \\FF_2^n$ with the property that $A+A$ contains a large portion of some Hamming sphere (counted with multiplicity) must have cardinality a constant multiple of $2^n$.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "This paper studies several properties of channel codes that approach the fundamental limits of a given (discrete or Gaussian) memoryless channel with a non-vanishing probability of error. The output distribution induced by an $\u03b5$-capacity-achieving code is shown to be close in a strong sense to the capacity achieving output distribution. Relying on the concentration of measure (isoperimetry) property enjoyed by the latter, it is shown that regular (Lipschitz) functions of channel outputs can be precisely estimated and turn out to be essentially non-random and independent of the actual code. It is also shown that the output distribution of a good code and the capacity achieving one cannot be distinguished with exponential reliability. The random process produced at the output of the channel is shown to satisfy the asymptotic equipartition property. Using related methods it is shown that quadratic forms and sums of $q$-th powers when evaluated at codewords of good AWGN codes approach the values obtained from a randomly generated Gaussian codeword.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Locally decodable channel codes form a special class of error-correcting codes with the property that the decoder is able to reconstruct any bit of the input message from querying only a few bits of a noisy codeword. It is well known that such codes require significantly more redundancy (in particular have vanishing rate) compared to their non-local counterparts. In this paper, we define a dual problem, i.e. locally decodable source codes (LDSC). We consider both almost lossless (block error) and lossy (bit error) cases. In almost lossless case, we show that optimal compression (to entropy) is possible with O(log n) queries to compressed string by the decompressor. We also show the following converse bounds: 1) linear LDSC cannot achieve any rate below one, with a bounded number of queries, 2) rate of any source coding with linear decoder (not necessarily local) in one, 3) for 2 queries, any code construction cannot have a rate below one. In lossy case, we show that any rate above rate distortion is achievable with a bounded number of queries. We also show that, rate distortion is achievable with any scaling number of queries. We provide an achievability bound in the finite block-length regime and compare it with the existing bounds in succinct data structures literature.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "The problem of estimating frequency moments of a data stream has attracted a lot of attention since the onset of streaming algorithms [AMS99]. While the space complexity for approximately computing the $p^{\\rm th}$ moment, for $p\\in(0,2]$ has been settled [KNW10], for $p>2$ the exact complexity remains open. For $p>2$ the current best algorithm uses $O(n^{1-2/p}\\log n)$ words of space [AKO11,BO10], whereas the lower bound is of $\u03a9(n^{1-2/p})$ [BJKS04].\n  In this paper, we show a tight lower bound of $\u03a9(n^{1-2/p}\\log n)$ words for the class of algorithms based on linear sketches, which store only a sketch $Ax$ of input vector $x$ and some (possibly randomized) matrix $A$. We note that all known algorithms for this problem are linear sketches.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "We investigate the maximal achievable rate for a given blocklength and error probability over quasi-static single-input multiple-output (SIMO) fading channels. Under mild conditions on the channel gains, it is shown that the channel dispersion is zero regardless of whether the fading realizations are available at the transmitter and/or the receiver. The result follows from computationally and analytically tractable converse and achievability bounds. Through numerical evaluation, we verify that, in some scenarios, zero dispersion indeed entails fast convergence to outage capacity as the blocklength increases. In the example of a particular 1*2 SIMO Rician channel, the blocklength required to achieve 90% of capacity is about an order of magnitude smaller compared to the blocklength required for an AWGN channel with the same capacity.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "Consider a problem of forward error-correction for the additive white Gaussian noise (AWGN) channel. For finite blocklength codes the backoff from the channel capacity is inversely proportional to the square root of the blocklength. In this paper it is shown that codes achieving this tradeoff must necessarily have peak-to-average power ratio (PAPR) proportional to logarithm of the blocklength. This is extended to codes approaching capacity slower, and to PAPR measured at the output of an OFDM modulator. As a by-product the convergence of (Smith's) amplitude-constrained AWGN capacity to Shannon's classical formula is characterized in the regime of large amplitudes. This converse-type result builds upon recent contributions in the study of empirical output distributions of good channel codes.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "We study the maximal achievable rate R*(n, \u03b5) for a given block-length n and block error probability \u03b5over Rayleigh block-fading channels in the noncoherent setting and in the finite block-length regime. Our results show that for a given block-length and error probability, R*(n, \u03b5) is not monotonic in the channel's coherence time, but there exists a rate maximizing coherence time that optimally trades between diversity and cost of estimating the channel.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "The momentum spectra of K+ produced at small angles in proton-proton and proton-deuteron collisions have been measured at four beam energies, 1.826, 1.920, 2.020, and 2.650 GeV, using the ANKE spectrometer at COSY-Juelich. After making corrections for Fermi motion and shadowing, the data indicate that K+ production near threshold is stronger in pp- than in pn-induced reactions. However, most of this difference could be made up by the unobserved K0 production in the pn case.\n        \u25b3 Less", "author": "Yury Polyanskiy"}, {"abstract": "We present a low-loss integrated photonics platform in the visible and near ultraviolet regime. Fully-etched waveguides based on atomic layer deposition (ALD) of aluminum oxide operate in a single transverse mode with $<$3 dB/cm propagation loss at a wavelength of 371 nm. Ring resonators with intrinsic quality factors exceeding 470,000 are demonstrated at 405 nm, and the thermo optic coefficient of ALD aluminum oxide is estimated to be $2.75\\times10^{-5}$ [RIU/$^\\circ$C]. Absorption loss is sufficiently low to allow on-resonance operation with intra-cavity powers up to at least 12.5 mW, limited by available laser power. Experimental and simulated data indicates the propagation loss is dominated by sidewall roughness, suggesting lower loss in the blue and UV is achievable.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "In this work, we propose a two-stage algorithm based on Bayesian modeling and computation aiming at quantifying analyte concentrations or quantities in complex mixtures with Raman spectroscopy. A hierarchical Bayesian model is built for spectral signal analysis, and reversible-jump Markov chain Monte Carlo (RJMCMC) computation is carried out for model selection and spectral variable estimation. Processing is done in two stages. In the first stage, the peak representations for a target analyte spectrum are learned. In the second, the peak variables learned from the first stage are used to estimate the concentration or quantity of the target analyte in a mixture. Numerical experiments validated its quantification performance over a wide range of simulation conditions and established its advantages for analyte quantification tasks under the small training sample size regime over conventional multivariate regression algorithms. We also used our algorithm to analyze experimental spontaneous Raman spectroscopy data collected for glucose concentration estimation in biopharmaceutical process monitoring applications. Our work shows that this algorithm can be a promising complementary tool alongside conventional multivariate regression algorithms in Raman spectroscopy-based mixture quantification studies, especially when collecting a large training dataset with high quality is challenging or resource-intensive.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "We present the design and characterization of waveguide grating devices that couple visible-wavelength light at $\u03bb=674$ nm from single-mode, high index-contrast dielectric waveguides to free-space beams forming micron-scale diffraction-limited spots a designed distance and angle from the grating. With a view to application in spatially-selective optical addressing, and in contrast to previous work on similar devices, deviations from the main Gaussian lobe up to $25$ microns from the focus and down to the $5\\times10^{-6}$ level in relative intensity are characterized as well; we show that along one dimension the intensity of these weak sidelobes approaches the limit imposed by diffraction from the finite field extent in the grating region. Additionally, we characterize the polarization purity in the focal region, observing at the center of the focus a low impurity $< 3 \\times 10^{-4}$ in relative intensity. Our approach allows quick, intuitive design of devices with such performance, which may be applied in trapped-ion quantum information processing and generally in any systems requiring optical routing to or from objects 10s--100s of microns from a chip surface, but benefitting from the parallelism and density of planar-fabricated dielectric integrated optics.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "We report on vertically-illuminated photodiodes fabricated in the GlobalFoundries 45nm 12SOI node and on a packaging concept for optically-interconnected chips. The photodiodes are responsive at 1180 nm, a wavelength currently used in chip-to-chip communications. They have further a wide field-of-view which enables chip-to-board positional feedback in chip-board assemblies. Monolithic integration enables on-chip processing of the positional data.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "A microring depletion modulator is demonstrated with T-shaped lateral p-n junctions used to realize efficient modulation while maximizing the RC limited bandwidth. The device having a 3 dB bandwidth of 13 GHz has been fabricated in a standard 45 nm microelectronics CMOS process. The cavity has a linewidth of 17 GHz and an average wavelength-shift of 9 pm/V in reverse-bias conditions.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "A photodiode with 0.55$\\pm$0.1 A/W responsivity at a wavelength of 1176.9 nm has been fabricated in a 45 nm microelectronics silicon-on-insulator foundry process. The resonant waveguide photodetector exploits carrier generation in silicon-germanium (SiGe) within a microring which is compatible with high-performance electronics. A 3 dB bandwidth of 5 GHz at -4 V bias is obtained with a dark current of less than 20 pA.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The long coherence times and strong Coulomb interactions afforded by trapped ion qubits have enabled realizations of the necessary primitives for quantum information processing (QIP), and indeed the highest-fidelity quantum operations in any qubit to date. But while light delivery to each individual ion in a system is essential for general quantum manipulations and readout, experiments so far have employed optical systems cumbersome to scale to even a few tens of qubits. Here we demonstrate lithographically defined nanophotonic waveguide devices for light routing and ion addressing fully integrated within a surface-electrode ion trap chip. Ion qubits are addressed at multiple locations via focusing grating couplers emitting through openings in the trap electrodes to ions trapped 50 $\u03bc$m above the chip; using this light we perform quantum coherent operations on the optical qubit transition in individual $^{88}$Sr$^+$ ions. The grating focuses the beam to a diffraction-limited spot near the ion position with a 2 $\u03bc$m 1/$e^2$-radius along the trap axis, and we measure crosstalk errors between $10^{-2}$ and $4\\times10^{-4}$ at distances 7.5-15 $\u03bc$m from the beam center. Owing to the scalability of the planar fabrication employed, together with the tight focusing and stable alignment afforded by optics integration within the trap chip, this approach presents a path to creating the optical systems required for large-scale trapped-ion QIP.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "Recently, the authors have demonstrated large-scale integrated systems with several million transistors and hundreds of photonic elements. Yielding such large-scale integrated systems requires a design-for-manufacture rigour that is embodied in the 10 000 to 50 000 design rules that these designs must comply within advanced complementary metal-oxide semiconductor manufacturing. Here, the authors present a photonic design automation tool which allows automatic generation of layouts without design-rule violations. This tool is written in SKILL, the native language of the mainstream electric design automation software, Cadence. This allows seamless integration of photonic and electronic design in a single environment. The tool leverages intuitive photonic layer definitions, allowing the designer to focus on the physical properties rather than on technology-dependent details. For the first time the authors present an algorithm for removal of design-rule violations from photonic layouts based on Manhattan discretisation, Boolean and sizing operations. This algorithm is not limited to the implementation in SKILL, and can in principle be implemented in any scripting language. Connectivity is achieved with software-defined waveguide ports and low-level procedures that enable auto-routing of waveguide connections.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "We calculate the amount of primordial matter density contrast and the size of the very early universe in the recent Quantum Big Bang theory [arXiv:0705.4549 [gr-qc](2007)] of the cosmological constant. We obtain $(\u03b4\u03c1/\u03c1)_M = 1.75 \\times 10^{-5}$, {\\it without} the introduction of an adjustable free parameter. Harrison-Zel'dovich $k$-dependence with $A = 64/9\u03c0^2 = 0.72$ and $n = 1$ in $|\u03b4_k|^2 = Ak^n$ arises inherently. The size of the universe with which it enters the classical Friedmann-Robertson-Walker (FRW) phase comes out to be 0.2 cm. We conclude that the hypothesis of classical inflation at an early stage of cosmic evolution is {\\bf not} needed.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The present paper seeks to construct a quantum theory of the cosmological constant in which its presently observed very small value emerges naturally.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "In this paper we investigate the quantum nature of a 2+1 dimensional black hole using the method [arXiv: gr-qc/0504030] which earlier revealed the quantum nature of a black hole in 3+1 dimensions.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The quantum nature of a black hole is revealed using the simplest terms that one learns in undergraduate and beginning graduate courses. The exposition demonstrates -- vividly -- the importance and power of the quantum oscillator in contemporary research in theoretical physics.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "In the present note we elucidate how physical considerations based on Planck's oscillator led to the construction of Heisenberg's mechanics.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The paper reports on efforts taken to create lexical resources pertaining to Indian languages, using the collaborative model. The lexical resources being developed are: (1) Transfer lexicon and grammar from English to several Indian languages. (2) Dependencey tree bank of annotated corpora for several Indian languages. The dependency trees are based on the Paninian model. (3) Bilingual dictionary of 'core meanings'.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The anusaaraka system (a kind of machine translation system) makes text in one Indian language accessible through another Indian language. The machine presents an image of the source text in a language close to the target language. In the image, some constructions of the source language (which do not have equivalents in the target language) spill over to the output. Some special notation is also devised.\n  Anusaarakas have been built from five pairs of languages: Telugu,Kannada, Marathi, Bengali and Punjabi to Hindi. They are available for use through Email servers.\n  Anusaarkas follows the principle of substitutibility and reversibility of strings produced. This implies preservation of information while going from a source language to a target language.\n  For narrow subject areas, specialized modules can be built by putting subject domain knowledge into the system, which produce good quality grammatical output. However, it should be remembered, that such modules will work only in narrow areas, and will sometimes go wrong. In such a situation, anusaaraka output will still remain useful.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The anusaaraka system makes text in one Indian language accessible in another Indian language. In the anusaaraka approach, the load is so divided between man and computer that the language load is taken by the machine, and the interpretation of the text is left to the man. The machine presents an image of the source text in a language close to the target language.In the image, some constructions of the source language (which do not have equivalents) spill over to the output. Some special notation is also devised. The user after some training learns to read and understand the output. Because the Indian languages are close, the learning time of the output language is short, and is expected to be around 2 weeks.\n  The output can also be post-edited by a trained user to make it grammatically correct in the target language. Style can also be changed, if necessary. Thus, in this scenario, it can function as a human assisted translation system.\n  Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali and Punjabi to Hindi. They can be built for all Indian languages in the near future. Everybody must pitch in to build such systems connecting all Indian languages, using the free software model.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The world is passing through a major revolution called the information revolution, in which information and knowledge is becoming available to people in unprecedented amounts wherever and whenever they need it. Those societies which fail to take advantage of the new technology will be left behind, just like in the industrial revolution.\n  The information revolution is based on two major technologies: computers and communication. These technologies have to be delivered in a COST EFFECTIVE manner, and in LANGUAGES accessible to people.\n  One way to deliver them in cost effective manner is to make suitable technology choices (discussed later), and to allow people to access through shared resources. This could be done throuch street corner shops (for computer usage, e-mail etc.), schools, community centers and local library centres.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "The world is passing through a major revolution called the information revolution, in which information and knowledge is becoming available to people in unprecedented amounts wherever and whenever they need it. Those societies which fail to take advantage of the new technology will be left behind, just like in the industrial revolution.\n  The information revolution is based on two major technologies: computers and communication. These technologies have to be delivered in a COST EFFECTIVE manner, and in LANGUAGES accessible to people.\n  One way to deliver them in cost effective manner is to make suitable technology choices, and to allow people to access through shared resources. This could be done throuch street corner shops (for computer usage, e-mail etc.), schools, community centres and local library centres.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "Fully-automatic general-purpose high-quality machine translation systems (FGH-MT) are extremely difficult to build. In fact, there is no system in the world for any pair of languages which qualifies to be called FGH-MT. The reasons are not far to seek. Translation is a creative process which involves interpretation of the given text by the translator. Translation would also vary depending on the audience and the purpose for which it is meant. This would explain the difficulty of building a machine translation system. Since, the machine is not capable of interpreting a general text with sufficient accuracy automatically at present - let alone re-expressing it for a given audience, it fails to perform as FGH-MT. FOOTNOTE{The major difficulty that the machine faces in interpreting a given text is the lack of general world knowledge or common sense knowledge.}\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "A comprehensive model for the photon number fluctuations and the current noise in quantum cascade lasers is presented. It is shown that the photon intensity noise in quantum cascade lasers exhibits little amplitude squeezing even when noise in the drive current is suppressed below the shot noise value. This is in contrast to interband semiconductor diode lasers in which the laser intensity noise can be squeezed well below the shot noise limit by high impedance suppression of fluctuations in the drive current. The theoretical model presented in this paper self-consistently accounts for the suppression of current noise in electron transport in multiple quantum well structures due to various electronic correlations. The nature of these electronic correlations is discussed. Mechanisms responsible for the reduced photon number squeezing in intersubband lasers are elucidated. Scaling of the laser intensity noise and the current noise with the number of cascaded gain stages is also described. Direct current modulation response of quantum cascade lasers is also studied, and it is shown that contrary to the predictions in the literature of terahertz modulation bandwidth for these lasers, bandwidth of almost all quantum cascade lasers that have been reported in the literature is limited by the inverse photon lifetime inside the laser cavity to tens of gigahertz.\n        \u25b3 Less", "author": "Rajeev Ram"}, {"abstract": "Modern out-of-order processors have increased capacity to exploit instruction level parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide superscalar pipelines and vector execution units, as well as deep buffers for in-flight memory requests. These resources, however, often exhibit poor utilization rates on workloads with large working sets, e.g., in-memory databases, key-value stores, and graph analytics, as compilers and hardware struggle to expose ILP and MLP from the instruction stream automatically.\n  In this paper, we introduce the IMLP (Instruction and Memory Level Parallelism) task programming model. IMLP tasks execute as coroutines that yield execution at annotated long-latency operations, e.g., memory accesses, divisions, or unpredictable branches. IMLP tasks are interleaved on a single thread, and integrate well with thread parallelism and vectorization. Our DSL embedded in C++, Cimple, allows exploration of task scheduling and transformations, such as buffering, vectorization, pipelining, and prefetching.\n  We demonstrate state-of-the-art performance on core algorithms used in in-memory databases that operate on arrays, hash tables, trees, and skip lists. Cimple applications reach 2.5x throughput gains over hardware multithreading on a multi-core, and 6.4x single thread speedup.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We present Warp, a hardware platform to support research in approximate computing, sensor energy optimization, and energy-scavenged systems. Warp incorporates 11 state-of-the-art sensor integrated circuits, computation, and an energy-scavenged power supply, all within a miniature system that is just 3.6 cm x 3.3 cm x 0.5 cm. Warp's sensor integrated circuits together contain a total of 21 sensors with a range of precisions and accuracies for measuring eight sensing modalities of acceleration, angular rate, magnetic flux density (compass heading), humidity, atmospheric pressure (elevation), infrared radiation, ambient temperature, and color. Warp uses a combination of analog circuits and digital control to facilitate further tradeoffs between sensor and communication accuracy, energy efficiency, and performance. This article presents the design of Warp and presents an evaluation of our hardware implementation. The results show how Warp's design enables performance and energy efficiency versus ac- curacy tradeoffs.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "The sizes of compressed images depend on their spatial resolution (number of pixels) and on their color resolution (number of color quantization levels). We introduce DaltonQuant, a new color quantization technique for image compression that cloud services can apply to images destined for a specific user with known color vision deficiencies. DaltonQuant improves compression in a user-specific but reversible manner thereby improving a user's network bandwidth and data storage efficiency. DaltonQuant quantizes image data to account for user-specific color perception anomalies, using a new method for incremental color quantization based on a large corpus of color vision acuity data obtained from a popular mobile game. Servers that host images can revert DaltonQuant's image requantization and compression when those images must be transmitted to a different user, making the technique practical to deploy on a large scale. We evaluate DaltonQuant's compression performance on the Kodak PC reference image set and show that it improves compression by an additional 22%-29% over the state-of-the-art compressors TinyPNG and pngquant.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We present the first systematic analysis of the characteristics of patch search spaces for automatic patch generation systems. We analyze the search spaces of two current state-of-the-art systems, SPR and Prophet, with 16 different search space configurations. Our results are derived from an analysis of 1104 different search spaces and 768 patch generation executions. Together these experiments consumed over 9000 hours of CPU time on Amazon EC2.\n  The analysis shows that 1) correct patches are sparse in the search spaces (typically at most one correct patch per search space per defect), 2) incorrect patches that nevertheless pass all of the test cases in the validation test suite are typically orders of magnitude more abundant, and 3) leveraging information other than the test suite is therefore critical for enabling the system to successfully isolate correct patches.\n  We also characterize a key tradeoff in the structure of the search spaces. Larger and richer search spaces that contain correct patches for more defects can actually cause systems to find fewer, not more, correct patches. We identify two reasons for this phenomenon: 1) increased validation times because of the presence of more candidate patches and 2) more incorrect patches that pass the test suite and block the discovery of correct patches. These fundamental properties, which are all characterized for the first time in this paper, help explain why past systems often fail to generate correct patches and help identify challenges, opportunities, and productive future directions for the field.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We prove several decidability and undecidability results for the satisfiability and validity problems for languages that can express solutions to word equations with length constraints. The atomic formulas over this language are equality over string terms (word equations), linear inequality over the length function (length constraints), and membership in regular sets. These questions are important in logic, program analysis, and formal verification. Variants of these questions have been studied for many decades by mathematicians. More recently, practical satisfiability procedures (aka SMT solvers) for these formulas have become increasingly important in the context of security analysis for string-manipulating programs such as web applications.\n  We prove three main theorems. First, we give a new proof of undecidability for the validity problem for the set of sentences written as a forall-exists quantifier alternation applied to positive word equations. A corollary of this undecidability result is that this set is undecidable even with sentences with at most two occurrences of a string variable. Second, we consider Boolean combinations of quantifier-free formulas constructed out of word equations and length constraints. We show that if word equations can be converted to a solved form, a form relevant in practice, then the satisfiability problem for Boolean combinations of word equations and length constraints is decidable. Third, we show that the satisfiability problem for quantifier-free formulas over word equations in regular solved form, length constraints, and the membership predicate over regular expressions is also decidable.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We propose a novel approach to improving software security called Cryptographic Path Hardening, which is aimed at hiding security vulnerabilities in software from attackers through the use of provably secure and obfuscated cryptographic devices to harden paths in programs.\n  By \"harden\" we mean that certain error-checking if-conditionals in a given program P are replaced by equivalent\" we mean that adversaries cannot use semi-automatic program analysis techniques to reason about the hardened program paths and thus cannot discover as-yet-unknown errors along those paths, except perhaps through black-box dictionary attacks or random testing (which we can never prevent).\n  Other than these unpreventable attack methods, we can make program analysis aimed at error-finding \"provably hard\" for a resource-bounded attacker, in the same sense that cryptographic schemes are hard to break. Unlike security-through-obscurity, in Cryptographic Path Hardening we use provably-secure crypto devices to hide errors and our mathematical arguments of security are the same as the standard ones used in cryptography.\n  One application of Cryptographic Path Hardening is that software patches or filters often reveal enough information to an attacker that they can be used to construct error-revealing inputs to exploit an unpatched version of the program. By \"hardening\" the patch we make it difficult for the attacker to analyze the patched program to construct error-revealing inputs, and thus prevent him from potentially constructing exploits.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "Administrative role-based access control (ARBAC) is the first comprehensive administrative model proposed for role-based access control (RBAC). ARBAC has several features for designing highly expressive policies, but current work has not highlighted the utility of these expressive policies. In this report, we present a case study of designing an ARBAC policy for a bank comprising 18 branches. Using this case study we provide an assessment about the features of ARBAC that are likely to be used in realistic policies.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "One of the main challenges in the verification of software systems is the analysis of unbounded data structures with dynamic memory allocation, such as linked data structures and arrays. We describe Bohne, a new analysis for verifying data structures. Bohne verifies data structure operations and shows that 1) the operations preserve data structure invariants and 2) the operations satisfy their specifications expressed in terms of changes to the set of objects stored in the data structure. During the analysis, Bohne infers loop invariants in the form of disjunctions of universally quantified Boolean combinations of formulas. To synthesize loop invariants of this form, Bohne uses a combination of decision procedures for Monadic Second-Order Logic over trees, SMT-LIB decision procedures (currently CVC Lite), and an automated reasoner within the Isabelle interactive theorem prover. This architecture shows that synthesized loop invariants can serve as a useful communication mechanism between different decision procedures. Using Bohne, we have verified operations on data structures such as linked lists with iterators and back pointers, trees with and without parent pointers, two-level skip lists, array data structures, and sorted lists. We have deployed Bohne in the Hob and Jahob data structure analysis systems, enabling us to combine Bohne with analyses of data structure clients and apply it in the context of larger programs. This report describes the Bohne algorithm as well as techniques that Bohne uses to reduce the ammount of annotations and the running time of the analysis.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "Typestate systems ensure many desirable properties of imperative programs, including initialization of object fields and correct use of stateful library interfaces. Abstract sets with cardinality constraints naturally generalize typestate properties: relationships between the typestates of objects can be expressed as subset and disjointness relations on sets, and elements of sets can be represented as sets of cardinality one. Motivated by these applications, this paper presents new algorithms and new complexity results for constraints on sets and their cardinalities. We study several classes of constraints and demonstrate a trade-off between their expressive power and their complexity.\n  Our first result concerns a quantifier-free fragment of Boolean Algebra with Presburger Arithmetic. We give a nondeterministic polynomial-time algorithm for reducing the satisfiability of sets with symbolic cardinalities to constraints on constant cardinalities, and give a polynomial-space algorithm for the resulting problem.\n  In a quest for more efficient fragments, we identify several subclasses of sets with cardinality constraints whose satisfiability is NP-hard. Finally, we identify a class of constraints that has polynomial-time satisfiability and entailment problems and can serve as a foundation for efficient program analysis.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "Spatial conjunction is a powerful construct for reasoning about dynamically allocated data structures, as well as concurrent, distributed and mobile computation. While researchers have identified many uses of spatial conjunction, its precise expressive power compared to traditional logical constructs was not previously known. In this paper we establish the expressive power of spatial conjunction. We construct an embedding from first-order logic with spatial conjunction into second-order logic, and more surprisingly, an embedding from full second order logic into first-order logic with spatial conjunction. These embeddings show that the satisfiability of formulas in first-order logic with spatial conjunction is equivalent to the satisfiability of formulas in second-order logic. These results explain the great expressive power of spatial conjunction and can be used to show that adding unrestricted spatial conjunction to a decidable logic leads to an undecidable logic. As one example, we show that adding unrestricted spatial conjunction to two-variable logic leads to undecidability. On the side of decidability, the embedding into second-order logic immediately implies the decidability of first-order logic with a form of spatial conjunction over trees. The embedding into spatial conjunction also has useful consequences: because a restricted form of spatial conjunction in two-variable logic preserves decidability, we obtain that a correspondingly restricted form of second-order quantification in two-variable logic is decidable. The resulting language generalizes the first-order theory of boolean algebra over sets and is useful in reasoning about the contents of data structures in object-oriented languages.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We have previously introduced role logic as a notation for describing properties of relational structures in shape analysis, databases and knowledge bases. A natural fragment of role logic corresponds to two-variable logic with counting and is therefore decidable. We show how to use role logic to describe open and closed records, as well the dual of records, inverse records. We observe that the spatial conjunction operation of separation logic naturally models record concatenation. Moreover, we show how to eliminate the spatial conjunction of formulas of quantifier depth one in first-order logic with counting. As a result, allowing spatial conjunction of formulas of quantifier depth one preserves the decidability of two-variable logic with counting. This result applies to two-variable role logic fragment as well. The resulting logic smoothly integrates type system and predicate calculus notation and can be viewed as a natural generalization of the notation for constraints arising in role analysis and similar shape analysis approaches.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We present role logic, a notation for describing properties of relational structures in shape analysis, databases, and knowledge bases. We construct role logic using the ideas of de Bruijn's notation for lambda calculus, an encoding of first-order logic in lambda calculus, and a simple rule for implicit arguments of unary and binary predicates. The unrestricted version of role logic has the expressive power of first-order logic with transitive closure. Using a syntactic restriction on role logic formulas, we identify a natural fragment RL^2 of role logic. We show that the RL^2 fragment has the same expressive power as two-variable logic with counting C^2 and is therefore decidable. We present a translation of an imperative language into the decidable fragment RL^2, which allows compositional verification of programs that manipulate relational structures. In addition, we show how RL^2 encodes boolean shape analysis constraints and an expressive description logic.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We show that the first-order theory of structural subtyping of non-recursive types is decidable. Let $\u03a3$ be a language consisting of function symbols (representing type constructors) and $C$ a decidable structure in the relational language $L$ containing a binary relation $\\leq$. $C$ represents primitive types; $\\leq$ represents a subtype ordering. We introduce the notion of $\u03a3$-term-power of $C$, which generalizes the structure arising in structural subtyping. The domain of the $\u03a3$-term-power of $C$ is the set of $\u03a3$-terms over the set of elements of $C$. We show that the decidability of the first-order theory of $C$ implies the decidability of the first-order theory of the $\u03a3$-term-power of $C$. Our decision procedure makes use of quantifier elimination for term algebras and Feferman-Vaught theorem. Our result implies the decidability of the first-order theory of structural subtyping of non-recursive types.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We introduce regular graph constraints and explore their decidability properties. The motivation for regular graph constraints is 1) type checking of changing types of objects in the presence of linked data structures, 2) shape analysis techniques, and 3) generalization of similar constraints over trees and grids. We define a subclass of graphs called heaps as an abstraction of the data structures that a program constructs during its execution. We prove that determining the validity of implication for regular graph constraints over the class of heaps is undecidable. We show undecidability by exhibiting a characterization of certain \"corresponder graphs\" in terms of presence and absence of homomorphisms to a finite number of fixed graphs. The undecidability of implication of regular graph constraints implies that there is no algorithm that will verify that procedure preconditions are met or that the invariants are maintained when these properties are expressed in any specification language at least as expressive as regular graph constraints.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We present a new role system for specifying changing referencing relationships of heap objects. The role of an object depends, in large part, on its aliasing relationships with other objects, with the role of each object changing as its aliasing relationships change. Roles therefore capture important object and data structure properties and provide useful information about how the actions of the program interact with these properties. Our role system enables the programmer to specify the legal aliasing relationships that define the set of roles that objects may play, the roles of procedure parameters and object fields, and the role changes that procedures perform while manipulating objects. We present an interprocedural, compositional, and context-sensitive role analysis algorithm that verifies that a program respects the role constraints.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We show that the decidability of the first-order theory of the language that combines Boolean algebras of sets of uninterpreted elements with Presburger arithmetic operations. We thereby disprove a recent conjecture that this theory is undecidable. Our language allows relating the cardinalities of sets to the values of integer variables, and can distinguish finite and infinite sets. We use quantifier elimination to show the decidability and obtain an elementary upper bound on the complexity.\n  Precise program analyses can use our decidability result to verify representation invariants of data structures that use an integer field to represent the number of stored elements.\n        \u25b3 Less", "author": "Martin Rinard"}, {"abstract": "We present an approximate sampling framework and discuss how risk-limiting audits can compensate for these approximations, while maintaining their \"risk-limiting\" properties. Our framework is general and can compensate for counting mistakes made during audits.\n  Moreover, we present and analyze a simple approximate sampling method,\"$k$-cut\", for picking a ballot randomly from a stack, without counting. Our method involves doing $k$ \"cuts\", each involving moving a random portion of ballots from the top to the bottom of the stack, and then picking the ballot on top. Unlike conventional methods of picking a ballot at random, $k$-cut does not require identification numbers on the ballots or counting many ballots per draw. We analyze how close the distribution of chosen ballots is to the uniform distribution, and design different mitigation procedures. We show that $k=6$ cuts is enough for an risk-limiting election audit, based on empirical data, which would provide a significant increase in efficiency.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "We describe a very simple method for `consistent sampling' that allows for sampling with replacement. The method extends previous approaches to consistent sampling, which assign a pseudorandom real number to each element, and sample those with the smallest associated numbers. When sampling with replacement, our extension gives the item sampled a new, larger, associated pseudorandom number, and returns it to the pool of items being sampled.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "Tabulation audits for an election provide statistical evidence that a reported contest outcome is \"correct\" (meaning that the tabulation of votes was properly performed), or else the tabulation audit determines the correct outcome.\n  Stark proposed risk-limiting tabulation audits for this purpose; such audits are effective and are beginning to be used in practice.\n  We expand the study of election audits based on Bayesian methods, first introduced by Rivest and Shen in 2012. (The risk-limiting audits proposed by Stark are \"frequentist\" rather than Bayesian in character.)\n  We first provide a simplified presentation of Bayesian tabulation audits. A Bayesian tabulation audit begins by drawing a random sample of the votes in that contest, and tallying those votes. It then considers what effect statistical variations of this tally have on the contest outcome. If such variations almost always yield the previously-reported outcome, the audit terminates, accepting the reported outcome. Otherwise the audit is repeated with an enlarged sample.\n  Bayesian audits are attractive because they work with any method for determining the winner (such as ranked-choice voting).\n  We then show how Bayesian audits may be extended to handle more complex situations, such as auditing contests that \\emph{span multiple jurisdictions}, or are otherwise \"stratified.\"\n  We highlight the auditing of such multiple-jurisdiction contests where some of the jurisdictions have an electronic cast vote record (CVR) for each cast paper vote, while the others do not. Complex situations such as this may arise naturally when some counties in a state have upgraded to new equipment, while others have not. Bayesian audits are able to handle such situations in a straightforward manner.\n  We also discuss the benefits and relevant considerations for using Bayesian audits in practice.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "Elections seem simple---aren't they just counting? But they have a unique, challenging combination of security and privacy requirements. The stakes are high; the context is adversarial; the electorate needs to be convinced that the results are correct; and the secrecy of the ballot must be ensured. And they have practical constraints: time is of the essence, and voting systems need to be affordable and maintainable, and usable by voters, election officials, and pollworkers. It is thus not surprising that voting is a rich research area spanning theory, applied cryptography, practical systems analysis, usable security, and statistics. Election integrity involves two key concepts: convincing evidence that outcomes are correct and privacy, which amounts to convincing assurance that there is no evidence about how any given person voted. These are obviously in tension. We examine how current systems walk this tightrope.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "We propose a simple risk-limiting audit for elections, ClipAudit. To determine whether candidate A (the reported winner) actually beat candidate B in a plurality election, ClipAudit draws ballots at random, without replacement, until either all cast ballots have been drawn, or until \\[ a - b \\ge \u03b2\\sqrt{a+b}\n  \\] where $a$ is the number of ballots in the sample for the reported winner A, and $b$ is the number of ballots in the sample for opponent B, and where $\u03b2$ is a constant determined a priori as a function of the number $n$ of ballots cast and the risk-limit $\u03b1$. ClipAudit doesn't depend on the unofficial margin (as does Bravo). We show how to extend ClipAudit to contests with multiple winners or losers, or to multiple contests.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "In the \"correlated sampling\" problem, two players, say Alice and Bob, are given two distributions, say $P$ and $Q$ respectively, over the same universe and access to shared randomness. The two players are required to output two elements, without any interaction, sampled according to their respective distributions, while trying to minimize the probability that their outputs disagree. A well-known protocol due to Holenstein, with close variants (for similar problems) due to Broder, and to Kleinberg and Tardos, solves this task with disagreement probability at most $2 \u03b4/(1+\u03b4)$, where $\u03b4$ is the total variation distance between $P$ and $Q$. This protocol has been used in several different contexts including sketching algorithms, approximation algorithms based on rounding linear programming relaxations, the study of parallel repetition and cryptography.\n  In this note, we give a surprisingly simple proof that this protocol is in fact tight. Specifically, for every $\u03b4\\in (0,1)$, we show that any correlated sampling scheme should have disagreement probability at least $2\u03b4/(1+\u03b4)$. This partially answers a recent question of Rivest.\n  Our proof is based on studying a new problem we call \"constrained agreement\". Here, Alice is given a subset $A \\subseteq [n]$ and is required to output an element $i \\in A$, Bob is given a subset $B \\subseteq [n]$ and is required to output an element $j \\in B$, and the goal is to minimize the probability that $i \\neq j$. We prove tight bounds on this question, which turn out to imply tight bounds for correlated sampling. Though we settle basic questions about the two problems, our formulation also leads to several questions that remain open.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "We explain why the Australian Electoral Commission should perform an audit of the paper Senate ballots against the published preference data files. We suggest four different post-election audit methods appropriate for Australian Senate elections. We have developed prototype code for all of them and tested it on preference data from the 2016 election.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "This paper presents a new crypto scheme whose title promises it to be so boring that no-one will bother reading past the abstract. Because of this, the remainder of the paper is left blank.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "Population protocols are a popular model of distributed computing, in which randomly-interacting agents with little computational power cooperate to jointly perform computational tasks. Inspired by developments in molecular computation, and in particular DNA computing, recent algorithmic work has focused on the complexity of solving simple yet fundamental tasks in the population model, such as leader election (which requires stabilization to a single agent in a special \"leader\" state), and majority (in which agents must stabilize to a decision as to which of two possible initial states had higher initial count). Known results point towards an inherent trade-off between the time complexity of such algorithms, and the space complexity, i.e. size of the memory available to each agent.\n  In this paper, we explore this trade-off and provide new upper and lower bounds for majority and leader election. First, we prove a unified lower bound, which relates the space available per node with the time complexity achievable by a protocol: for instance, our result implies that any protocol solving either of these tasks for $n$ agents using $O( \\log \\log n )$ states must take $\u03a9( n / \\rm{polylog} n )$ expected time. This is the first result to characterize time complexity for protocols which employ super-constant number of states per node, and proves that fast, poly-logarithmic running times require protocols to have relatively large space costs.\n  On the positive side, we give algorithms showing that fast, poly-logarithmic stabilization time can be achieved using $O( \\log^2 n )$ space per node, in the case of both tasks. Overall, our results highlight a time complexity separation between $O(\\log \\log n)$ and $\u0398( \\log^2 n )$ state space size for both majority and leader election in population protocols, and introduce new techniques, which should be applicable more broadly.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "This paper presents DiffSum, a simple post-election risk-limiting ballot-polling audit for two-candidate plurality elections. DiffSum sequentially draws ballots (without replacement) until the numbers $a$, $b$, of votes for candidates $A$, $B$ satisfies $a>b$ and $(a-b)^2 > c(a+b)$, where $A$ is the reported winner and $c$ is a suitably chosen constant. Bounds on the error rate (chance of approving an incorrect election outcome) are obtained via simulations. The method is compared with the Bravo method of Lindeman, Stark, and Yates.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "This pamphlet describes end-to-end election verifiability (E2E-V) for a nontechnical audience: election officials, public policymakers, and anyone else interested in secure, transparent, evidence-based electronic elections.\n  This work is part of the Overseas Vote Foundation's End-to-End Verifiable Internet Voting: Specification and Feasibility Assessment Study (E2E VIV Project), funded by the Democracy Fund.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "Schools with the highest average student performance are often the smallest schools; localities with the highest rates of some cancers are frequently small and the effects observed in clinical trials are likely to be largest for the smallest numbers of subjects. Informal explanations of this \"small-schools phenomenon\" point to the fact that the sample means of smaller samples have higher variances. But this cannot be a complete explanation: If we draw two samples from a diffuse distribution that is symmetric about some point, then the chance that the smaller sample has larger mean is 50\\%. A particular consequence of results proved below is that if one draws three or more samples of different sizes from the same normal distribution, then the sample mean of the smallest sample is most likely to be highest, the sample mean of the second smallest sample is second most likely to be highest, and so on; this is true even though for any pair of samples, each one of the pair is equally likely to have the larger sample mean.\n  Our conclusions are relevant to certain stochastic choice models including the following generalization of Thurstone's Law of Comparative Judgment. There are $n$ items. Item $i$ is preferred to item $j$ if $Z_i < Z_j$, where $Z$ is a random $n$-vector of preference scores. Suppose $\\mathbb{P}\\{Z_i = Z_j\\} = 0$ for $i \\ne j$, so there are no ties. Item $k$ is the favorite if $Z_k < \\min_{i\\ne k} Z_i$. Let $p_i$ denote the chance that item $i$ is the favorite. We characterize a large class of distributions for $Z$ for which $p_1 > p_2 > \\cdots > p_n$. Our results are most surprising when $\\mathbb{P}\\{Z_i < Z_j\\} = \\mathbb{P}\\{Z_i > Z_j\\} = \\frac{1}{2}$ for $i \\ne j$, so neither of any two items is likely to be preferred over the other in a pairwise comparison.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "We show how to hang a picture by wrapping rope around n nails, making a polynomial number of twists, such that the picture falls whenever any k out of the n nails get removed, and the picture remains hanging when fewer than k nails get removed. This construction makes for some fun mathematical magic performances. More generally, we characterize the possible Boolean functions characterizing when the picture falls in terms of which nails get removed as all monotone Boolean functions. This construction requires an exponential number of twists in the worst case, but exponential complexity is almost always necessary for general functions.\n        \u25b3 Less", "author": "Ronald Rivest"}, {"abstract": "We present $O(\\log\\log n)$-round algorithms in the Massively Parallel Computation (MPC) model, with $\\tilde{O}(n)$ memory per machine, that compute a maximal independent set, a $1+\u03b5$ approximation of maximum matching, and a $2+\u03b5$ approximation of minimum vertex cover, for any $n$-vertex graph and any constant $\u03b5>0$. These improve the state of the art as follows:\n  -- Our MIS algorithm leads to a simple $O(\\log\\log \u0394)$-round MIS algorithm in the Congested Clique model of distributed computing. This result improves exponentially on the $\\tilde{O}(\\sqrt{\\log \u0394})$-round algorithm of Ghaffari [PODC'17].\n  -- Our $O(\\log\\log n)$-round $(1+\u03b5)$-approximate maximum matching algorithm simplifies and improves on a rather complex $O(\\log^2\\log n)$-round $(1+\u03b5)$-approximation algorithm of Czumaj et al. [STOC'18].\n  -- Our $O(\\log\\log n)$-round $(2+\u03b5)$-approximate minimum vertex cover algorithm improves on an $O(\\log\\log n)$-round $O(1)$-approximation of Assadi et al. [arXiv'17].\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "Consider a computation on a massive random graph: Does one need to generate the whole random graph up front, prior to performing the computation? Or, is it possible to provide an oracle to answer queries to the random graph \"on-the-fly\" in a much more efficient manner overall? That is, to provide a $local\\ access\\ generator$ which incrementally constructs the random graph locally, at the queried portions, in a manner consistent with the random graph model and all previous choices. Local access generators can be useful when studying the local behavior of specific random graph models. Our goal is to design local access generators whose required resource overhead for answering each query is significantly more efficient than generating the whole random graph.\n  Our results focus on undirected graphs with independent edge probabilities, that is, each edge is chosen as an independent Bernoulli random variable. We provide a general implementation for generators in this model. Then, we use this construction to obtain the first efficient local implementations for the Erd\u00f6s-R\u00e9nyi $G(n,p)$ model, and the Stochastic Block model.\n  As in previous local-access implementations for random graphs, we support $vertex$-$pair$, $next$-$neighbor$ queries, and $all$-$neighbors$ queries. In addition, we introduce a new $random$-$neighbor$ query. We also give the first local-access generation procedure for $all$-$neighbors$ queries in the (sparse and directed) Kleinberg's Small-World model. Note that, in the sparse case, an $all$-$neighbors$ query can be used to simulate the other types of queries efficiently. All of our generators require no pre-processing time, and answer each query using $polylog(n) $ time, random bits, and additional space.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We investigate the problems of identity and closeness testing over a discrete population from random samples. Our goal is to develop efficient testers while guaranteeing Differential Privacy to the individuals of the population. We describe an approach that yields sample-efficient differentially private testers for these problems. Our theoretical results show that there exist private identity and closeness testers that are nearly as sample-efficient as their non-private counterparts. We perform an experimental evaluation of our algorithms on synthetic data. Our experiments illustrate that our private testers achieve small type I and type II errors with sample size sublinear in the domain size of the underlying distributions.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "Constructing a spanning tree of a graph is one of the most basic tasks in graph theory. We consider this problem in the setting of local algorithms: one wants to quickly determine whether a given edge $e$ is in a specific spanning tree, without computing the whole spanning tree, but rather by inspecting the local neighborhood of $e$. The challenge is to maintain consistency. That is, to answer queries about different edges according to the same spanning tree. Since it is known that this problem cannot be solved without essentially viewing all the graph, we consider the relaxed version of finding a spanning subgraph with $(1+\u03b5)n$ edges (where $n$ is the number of vertices and $\u03b5$ is a given sparsity parameter). It is known that this relaxed problem requires inspecting $\u03a9(\\sqrt{n})$ edges in general graphs, which motivates the study of natural restricted families of graphs. One such family is the family of graphs with an excluded minor. For this family there is an algorithm that achieves constant success probability, and inspects $(d/\u03b5)^{poly(h)\\log(1/\u03b5)}$ edges (for each edge it is queried on), where $d$ is the maximum degree in the graph and $h$ is the size of the excluded minor. The distances between pairs of vertices in the spanning subgraph $G'$ are at most a factor of $poly(d, 1/\u03b5, h)$ larger than in $G$.\n  In this work, we show that for an input graph that is $H$-minor free for any $H$ of size $h$, this task can be performed by inspecting only $poly(d, 1/\u03b5, h)$ edges. The distances between pairs of vertices in the spanning subgraph $G'$ are at most a factor of $\\tilde{O}(h\\log(d)/\u03b5)$ larger than in $G$. Furthermore, the error probability of the new algorithm is significantly improved to $\u0398(1/n)$. This algorithm can also be easily adapted to yield an efficient algorithm for the distributed setting.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We study the problem of estimating the value of sums of the form $S_p \\triangleq \\sum \\binom{x_i}{p}$ when one has the ability to sample $x_i \\geq 0$ with probability proportional to its magnitude. When $p=2$, this problem is equivalent to estimating the selectivity of a self-join query in database systems when one can sample rows randomly. We also study the special case when $\\{x_i\\}$ is the degree sequence of a graph, which corresponds to counting the number of $p$-stars in a graph when one has the ability to sample edges randomly.\n  Our algorithm for a $(1 \\pm \\varepsilon)$-multiplicative approximation of $S_p$ has query and time complexities $\u00d8(\\frac{m \\log \\log n}{\u03b5^2 S_p^{1/p}})$. Here, $m=\\sum x_i/2$ is the number of edges in the graph, or equivalently, half the number of records in the database table. Similarly, $n$ is the number of vertices in the graph and the number of unique values in the database table. We also provide tight lower bounds (up to polylogarithmic factors) in almost all cases, even when $\\{x_i\\}$ is a degree sequence and one is allowed to use the structure of the graph to try to get a better estimate. We are not aware of any prior lower bounds on the problem of join selectivity estimation.\n  For the graph problem, prior work which assumed the ability to sample only \\emph{vertices} uniformly gave algorithms with matching lower bounds [Gonen, Ron, and Shavitt. \\textit{SIAM J. Comput.}, 25 (2011), pp. 1365-1411]. With the ability to sample edges randomly, we show that one can achieve faster algorithms for approximating the number of star subgraphs, bypassing the lower bounds in this prior work. For example, in the regime where $S_p\\leq n$, and $p=2$, our upper bound is $\\tilde{O}(n/S_p^{1/2})$, in contrast to their $\u03a9(n/S_p^{1/3})$ lower bound when no random edge queries are available.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We study the question of testing structured properties (classes) of discrete distributions. Specifically, given sample access to an arbitrary distribution $D$ over $[n]$ and a property $\\mathcal{P}$, the goal is to distinguish between $D\\in\\mathcal{P}$ and $\\ell_1(D,\\mathcal{P})>\\varepsilon$. We develop a general algorithm for this question, which applies to a large range of \"shape-constrained\" properties, including monotone, log-concave, $t$-modal, piecewise-polynomial, and Poisson Binomial distributions. Moreover, for all cases considered, our algorithm has near-optimal sample complexity with regard to the domain size and is computationally efficient. For most of these classes, we provide the first non-trivial tester in the literature. In addition, we also describe a generic method to prove lower bounds for this problem, and use it to show our upper bounds are nearly tight. Finally, we extend some of our techniques to tolerant testing, deriving nearly-tight upper and lower bounds for the corresponding questions.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "In many situations, sample data is obtained from a noisy or imperfect source. In order to address such corruptions, this paper introduces the concept of a sampling corrector. Such algorithms use structure that the distribution is purported to have, in order to allow one to make \"on-the-fly\" corrections to samples drawn from probability distributions. These algorithms then act as filters between the noisy data and the end user.\n  We show connections between sampling correctors, distribution learning algorithms, and distribution property testing algorithms. We show that these connections can be utilized to expand the applicability of known distribution learning and property testing algorithms as well as to achieve improved algorithms for those tasks.\n  As a first step, we show how to design sampling correctors using proper learning algorithms. We then focus on the question of whether algorithms for sampling correctors can be more efficient in terms of sample complexity than learning algorithms for the analogous families of distributions. When correcting monotonicity, we show that this is indeed the case when also granted query access to the cumulative distribution function. We also obtain sampling correctors for monotonicity without this stronger type of access, provided that the distribution be originally very close to monotone (namely, at a distance $O(1/\\log^2 n)$). In addition to that, we consider a restricted error model that aims at capturing \"missing data\" corruptions. In this model, we show that distributions that are close to monotone have sampling correctors that are significantly more efficient than achievable by the learning approach.\n  We also consider the question of whether an additional source of independent random bits is required by sampling correctors to implement the correction process.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "In the model of \\emph{local computation algorithms} (LCAs), we aim to compute the queried part of the output by examining only a small (sublinear) portion of the input. Many recently developed LCAs on graph problems achieve time and space complexities with very low dependence on $n$, the number of vertices. Nonetheless, these complexities are generally at least exponential in $d$, the upper bound on the degree of the input graph. Instead, we consider the case where parameter $d$ can be moderately dependent on $n$, and aim for complexities with subexponential dependence on $d$, while maintaining polylogarithmic dependence on $n$. We present: a randomized LCA for computing maximal independent sets whose time and space complexities are quasi-polynomial in $d$ and polylogarithmic in $n$; for constant $\u03b5> 0$, a randomized LCA that provides a $(1-\u03b5)$-approximation to maximum matching whose time and space complexities are polynomial in $d$ and polylogarithmic in $n$.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "Constructing a spanning tree of a graph is one of the most basic tasks in graph theory. Motivated by several recent studies of local graph algorithms, we consider the following variant of this problem. Let G be a connected bounded-degree graph. Given an edge $e$ in $G$ we would like to decide whether $e$ belongs to a connected subgraph $G'$ consisting of $(1+\u03b5)n$ edges (for a prespecified constant $\u03b5>0$), where the decision for different edges should be consistent with the same subgraph $G'$. Can this task be performed by inspecting only a {\\em constant} number of edges in $G$? Our main results are:\n  (1) We show that if every $t$-vertex subgraph of $G$ has expansion $1/(\\log t)^{1+o(1)}$ then one can (deterministically) construct a sparse spanning subgraph $G'$ of $G$ using few inspections. To this end we analyze a \"local\" version of a famous minimum-weight spanning tree algorithm.\n  (2) We show that the above expansion requirement is sharp even when allowing randomization. To this end we construct a family of $3$-regular graphs of high girth, in which every $t$-vertex subgraph has expansion $1/(\\log t)^{1-o(1)}$.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We present simple, self-contained proofs of correctness for algorithms for linearity testing and program checking of linear functions on finite subsets of integers represented as n-bit numbers. In addition we explore a generalization of self-testing to homomorphisms on a multidimensional vector space. We show that our self-testing algorithm for the univariate case can be directly generalized to vector space domains. The number of queries made by our algorithms is independent of domain size.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "Visualizations are frequently used as a means to understand trends and gather insights from datasets, but often take a long time to generate. In this paper, we focus on the problem of rapidly generating approximate visualizations while preserving crucial visual proper- ties of interest to analysts. Our primary focus will be on sampling algorithms that preserve the visual property of ordering; our techniques will also apply to some other visual properties. For instance, our algorithms can be used to generate an approximate visualization of a bar chart very rapidly, where the comparisons between any two bars are correct. We formally show that our sampling algorithms are generally applicable and provably optimal in theory, in that they do not take more samples than necessary to generate the visualizations with ordering guarantees. They also work well in practice, correctly ordering output groups while taking orders of magnitude fewer samples and much less time than conventional sampling schemes.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "In this paper, we analyze and study a hybrid model for testing and learning probability distributions. Here, in addition to samples, the testing algorithm is provided with one of two different types of oracles to the unknown distribution $D$ over $[n]$. More precisely, we define both the dual and cumulative dual access models, in which the algorithm $A$ can both sample from $D$ and respectively, for any $i\\in[n]$,\n  - query the probability mass $D(i)$ (query access); or\n  - get the total mass of $\\{1,\\dots,i\\}$, i.e. $\\sum_{j=1}^i D(j)$ (cumulative access)\n  These two models, by generalizing the previously studied sampling and query oracle models, allow us to bypass the strong lower bounds established for a number of problems in these settings, while capturing several interesting aspects of these problems -- and providing new insight on the limitations of the models. Finally, we show that while the testing algorithms can be in most cases strictly more efficient, some tasks remain hard even with this additional power.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We initiate the study of the problem of designing sublinear-time (local) algorithms that, given an edge $(u,v)$ in a connected graph $G=(V,E)$, decide whether $(u,v)$ belongs to a sparse spanning graph $G' = (V,E')$ of $G$. Namely, $G'$ should be connected and $|E'|$ should be upper bounded by $(1+\u03b5)|V|$ for a given parameter $\u03b5> 0$. To this end the algorithms may query the incidence relation of the graph $G$, and we seek algorithms whose query complexity and running time (per given edge $(u,v)$) is as small as possible. Such an algorithm may be randomized but (for a fixed choice of its random coins) its decision on different edges in the graph should be consistent with the same spanning graph $G'$ and independent of the order of queries.\n  We first show that for general (bounded-degree) graphs, the query complexity of any such algorithm must be $\u03a9(\\sqrt{|V|})$. This lower bound holds for graphs that have high expansion. We then turn to design and analyze algorithms both for graphs with high expansion (obtaining a result that roughly matches the lower bound) and for graphs that are (strongly) non-expanding (obtaining results in which the complexity does not depend on $|V|$). The complexity of the problem for graphs that do not fall into these two categories is left as an open question.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We present a simple adaptation of the Lempel Ziv 78' (LZ78) compression scheme ({\\em IEEE Transactions on Information Theory, 1978}) that supports efficient random access to the input string. Namely, given query access to the compressed string, it is possible to efficiently recover any symbol of the input string. The compression algorithm is given as input a parameter $\\eps >0$, and with very high probability increases the length of the compressed string by at most a factor of $(1+\\eps)$. The access time is $O(\\log n + 1/\\eps^2)$ in expectation, and $O(\\log n/\\eps^2)$ with high probability. The scheme relies on sparse transitive-closure spanners. Any (consecutive) substring of the input string can be retrieved at an additional additive cost in the running time of the length of the substring. We also formally establish the necessity of modifying LZ78 so as to allow efficient random access. Specifically, we construct a family of strings for which $\u03a9(n/\\log n)$ queries to the LZ78-compressed string are required in order to recover a single symbol in the input string. The main benefit of the proposed scheme is that it preserves the online nature and simplicity of LZ78, and that for {\\em every} input string, the length of the compressed string is only a small factor larger than that obtained by running LZ78.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "A local property reconstructor for a graph property is an algorithm which, given oracle access to the adjacency list of a graph that is \"close\" to having the property, provides oracle access to the adjacency matrix of a \"correction\" of the graph, i.e. a graph which has the property and is close to the given graph. For this model, we achieve local property reconstructors for the properties of connectivity and $k$-connectivity in undirected graphs, and the property of strong connectivity in directed graphs. Along the way, we present a method of transforming a local reconstructor (which acts as a \"adjacency matrix oracle\" for the corrected graph) into an \"adjacency list oracle\". This allows us to recursively use our local reconstructor for $(k-1)$-connectivity to obtain a local reconstructor for $k$-connectivity.\n  We also extend this notion of local property reconstruction to parametrized graph properties (for instance, having diameter at most $D$ for some parameter $D$) and require that the corrected graph has the property with parameter close to the original. We obtain a local reconstructor for the low diameter property, where if the original graph is close to having diameter $D$, then the corrected graph has diameter roughly 2D.\n  We also exploit a connection between local property reconstruction and property testing, observed by Brakerski, to obtain new tolerant property testers for all of the aforementioned properties. Except for the one for connectivity, these are the first tolerant property testers for these properties.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We give a nearly optimal sublinear-time algorithm for approximating the size of a minimum vertex cover in a graph G. The algorithm may query the degree deg(v) of any vertex v of its choice, and for each 1 <= i <= deg(v), it may ask for the i-th neighbor of v. Letting VC_opt(G) denote the minimum size of vertex cover in G, the algorithm outputs, with high constant success probability, an estimate VC_estimate(G) such that VC_opt(G) <= VC_estimate(G) <= 2 * VC_opt(G) + epsilon*n, where epsilon is a given additive approximation parameter. We refer to such an estimate as a (2,epsilon)-estimate. The query complexity and running time of the algorithm are ~O(avg_deg * poly(1/epsilon)), where avg_deg denotes the average vertex degree in the graph. The best previously known sublinear algorithm, of Yoshida et al. (STOC 2009), has query complexity and running time O(d^4/epsilon^2), where d is the maximum degree in the graph. Given the lower bound of Omega(avg_deg) (for constant epsilon) for obtaining such an estimate (with any constant multiplicative factor) due to Parnas and Ron (TCS 2007), our result is nearly optimal.\n  In the case that the graph is dense, that is, the number of edges is Theta(n^2), we consider another model, in which the algorithm may ask, for any pair of vertices u and v, whether there is an edge between u and v. We show how to adapt the algorithm that uses neighbor queries to this model and obtain an algorithm that outputs a (2,epsilon)-estimate of the size of a minimum vertex cover whose query complexity and running time are ~O(n) * poly(1/epsilon).\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "Recently Rubinfeld et al. (ICS 2011, pp. 223--238) proposed a new model of sublinear algorithms called \\emph{local computation algorithms}. In this model, a computation problem $F$ may have more than one legal solution and each of them consists of many bits. The local computation algorithm for $F$ should answer in an online fashion, for any index $i$, the $i^{\\mathrm{th}}$ bit of some legal solution of $F$. Further, all the answers given by the algorithm should be consistent with at least one solution of $F$.\n  In this work, we continue the study of local computation algorithms. In particular, we develop a technique which under certain conditions can be applied to construct local computation algorithms that run not only in polylogarithmic time but also in polylogarithmic \\emph{space}. Moreover, these local computation algorithms are easily parallelizable and can answer all parallel queries consistently. Our main technical tools are pseudorandom numbers with bounded independence and the theory of branching processes.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "For input $x$, let $F(x)$ denote the set of outputs that are the \"legal\" answers for a computational problem $F$. Suppose $x$ and members of $F(x)$ are so large that there is not time to read them in their entirety. We propose a model of {\\em local computation algorithms} which for a given input $x$, support queries by a user to values of specified locations $y_i$ in a legal output $y \\in F(x)$. When more than one legal output $y$ exists for a given $x$, the local computation algorithm should output in a way that is consistent with at least one such $y$. Local computation algorithms are intended to distill the common features of several concepts that have appeared in various algorithmic subfields, including local distributed computation, local algorithms, locally decodable codes, and local reconstruction.\n  We develop a technique, based on known constructions of small sample spaces of $k$-wise independent random variables and Beck's analysis in his algorithmic approach to the Lov{\u00e1}sz Local Lemma, which under certain conditions can be applied to construct local computation algorithms that run in {\\em polylogarithmic} time and space. We apply this technique to maximal independent set computations, scheduling radio network broadcasts, hypergraph coloring and satisfying $k$-SAT formulas.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "The {\\em Total Influence} ({\\em Average Sensitivity) of a discrete function is one of its fundamental measures. We study the problem of approximating the total influence of a monotone Boolean function \\ifnum\\plusminus=1 $f: \\{\\pm1\\}^n \\longrightarrow \\{\\pm1\\}$, \\else $f: \\bitset^n \\to \\bitset$, \\fi which we denote by $I[f]$. We present a randomized algorithm that approximates the influence of such functions to within a multiplicative factor of $(1\\pm \\eps)$ by performing $O(\\frac{\\sqrt{n}\\log n}{I[f]} \\poly(1/\\eps)) $ queries. % \\mnote{D: say something about technique?} We also prove a lower bound of % $\u03a9(\\frac{\\sqrt{n/\\log n}}{I[f]})$ $\u03a9(\\frac{\\sqrt{n}}{\\log n \\cdot I[f]})$ on the query complexity of any constant-factor approximation algorithm for this problem (which holds for $I[f] = \u03a9(1)$), % and $I[f] = O(\\sqrt{n}/\\log n)$), hence showing that our algorithm is almost optimal in terms of its dependence on $n$. For general functions we give a lower bound of $\u03a9(\\frac{n}{I[f]})$, which matches the complexity of a simple sampling algorithm.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "Given samples from two distributions over an $n$-element set, we wish to test whether these distributions are statistically close. We present an algorithm which uses sublinear in $n$, specifically, $O(n^{2/3}\u03b5^{-8/3}\\log n)$, independent samples from each distribution, runs in time linear in the sample size, makes no assumptions about the structure of the distributions, and distinguishes the cases when the distance between the distributions is small (less than $\\max\\{\u03b5^{4/3}n^{-1/3}/32, \u03b5n^{-1/2}/4\\}$) or large (more than $\u03b5$) in $\\ell_1$ distance. This result can be compared to the lower bound of $\u03a9(n^{2/3}\u03b5^{-2/3})$ for this problem given by Valiant.\n  Our algorithm has applications to the problem of testing whether a given Markov process is rapidly mixing. We present sublinear for several variants of this problem as well.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We study the problem of estimating the Earth Mover's Distance (EMD) between probability distributions when given access only to samples. We give closeness testers and additive-error estimators over domains in $[0, \u0394]^d$, with sample complexities independent of domain size - permitting the testability even of continuous distributions over infinite domains. Instead, our algorithms depend on other parameters, such as the diameter of the domain space, which may be significantly smaller. We also prove lower bounds showing the dependencies on these parameters to be essentially optimal. Additionally, we consider whether natural classes of distributions exist for which there are algorithms with better dependence on the dimension, and show that for highly clusterable data, this is indeed the case. Lastly, we consider a variant of the EMD, defined over tree metrics instead of the usual L1 metric, and give optimal algorithms.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "We raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time. We study this question in detail for two popular lossless compression schemes: run-length encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for approximating compressibility with respect to both schemes. We also give several lower bounds that show that our algorithms for both schemes cannot be improved significantly.\n  Our investigation of LZ yields results whose interest goes beyond the initial questions we set out to study. In particular, we prove combinatorial structural lemmas that relate the compressibility of a string with respect to Lempel-Ziv to the number of distinct short substrings contained in it. In addition, we show that approximating the compressibility with respect to LZ is related to approximating the support size of a distribution.\n        \u25b3 Less", "author": "Ronitt Rubinfeld"}, {"abstract": "Deep learning has revolutionized the ability to learn \"end-to-end\" autonomous vehicle control directly from raw sensory data. While there have been recent advances on extensions to handle forms of navigation instruction, these works are unable to capture the full distribution of possible actions that could be taken and to reason about localization of the robot within the environment. In this paper, we extend end-to-end driving networks with the ability to understand maps. We define a novel variational network capable of learning from raw camera data of the environment as well as higher level roadmaps to predict (1) a full probability distribution over the possible control commands; and (2) a deterministic control command capable of navigating on the route specified within the map. Additionally, we formulate how our model can be used to localize the robot according to correspondences between the map and the observed visual road topology, inspired by the rough localization that human drivers can perform. We evaluate our algorithms on real-world driving data, and reason about the robustness of the inferred steering commands under various types of rich driving scenarios. In addition, we evaluate our localization algorithm over a new set of roads and intersections which the model has never driven through and demonstrate rough localization in situations without any GPS prior.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "In this paper, we introduce the notion of liquid time-constant (LTC) recurrent neural networks (RNN)s, a subclass of continuous-time RNNs, with varying neuronal time-constant realized by their nonlinear synaptic transmission model. This feature is inspired by the communication principles in the nervous system of small species. It enables the model to approximate continuous mapping with a small number of computational units. We show that any finite trajectory of an $n$-dimensional continuous dynamical system can be approximated by the internal state of the hidden units and $n$ output units of an LTC network. Here, we also theoretically find bounds on their neuronal states and varying time-constant.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and therefore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects including contact and can be seamlessly incorporated into inference, control and co-design systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of control tasks for soft robots, including problems with nearly 3,000 decision variables.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We propose an effective method for creating interpretable control agents, by \\textit{re-purposing} the function of a biological neural circuit model, to govern simulated and real world reinforcement learning (RL) test-beds. Inspired by the structure of the nervous system of the soil-worm, \\emph{C. elegans}, we introduce \\emph{Neuronal Circuit Policies} (NCPs) as a novel recurrent neural network instance with liquid time-constants, universal approximation capabilities and interpretable dynamics. We theoretically show that they can approximate any finite simulation time of a given continuous n-dimensional dynamical system, with $n$ output units and some hidden units. We model instances of the policies and learn their synaptic and neuronal parameters to control standard RL tasks and demonstrate its application for autonomous parking of a real rover robot on a pre-defined trajectory. For reconfiguration of the \\emph{purpose} of the neural circuit, we adopt a search-based RL algorithm. We show that our neuronal circuit policies perform as good as deep neural network policies with the advantage of realizing interpretable dynamics at the cell-level. We theoretically find bounds for the time-varying dynamics of the circuits, and introduce a novel way to reason about networks' dynamics.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "In this paper, we introduce a novel method to interpret recurrent neural networks (RNNs), particularly long short-term memory networks (LSTMs) at the cellular level. We propose a systematic pipeline for interpreting individual hidden state dynamics within the network using response characterization methods. The ranked contribution of individual cells to the network's output is computed by analyzing a set of interpretable metrics of their decoupled step and sinusoidal responses. As a result, our method is able to uniquely identify neurons with insightful dynamics, quantify relationships between dynamical properties and test accuracy through ablation analysis, and interpret the impact of network capacity on a network's dynamical distribution. Finally, we demonstrate generalizability and scalability of our method by evaluating a series of different benchmark sequential datasets.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "End-to-end trained neural networks (NNs) are a compelling approach to autonomous vehicle control because of their ability to learn complex tasks without manual engineering of rule-based decisions. However, challenging road conditions, ambiguous navigation situations, and safety considerations require reliable uncertainty estimation for the eventual adoption of full-scale autonomous vehicles. Bayesian deep learning approaches provide a way to estimate uncertainty by approximating the posterior distribution of weights given a set of training data. Dropout training in deep NNs approximates Bayesian inference in a deep Gaussian process and can thus be used to estimate model uncertainty. In this paper, we propose a Bayesian NN for end-to-end control that estimates uncertainty by exploiting feature map correlation during training. This approach achieves improved model fits, as well as tighter uncertainty estimates, than traditional element-wise dropout. We evaluate our algorithms on a challenging dataset collected over many different road types, times of day, and weather conditions, and demonstrate how uncertainties can be used in conjunction with a human controller in a parallel autonomous setting.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We present an efficient coresets-based neural network compression algorithm that provably sparsifies the parameters of a trained fully-connected neural network in a manner that approximately preserves the network's output. Our approach is based on an importance sampling scheme that judiciously defines a sampling distribution over the neural network parameters, and as a result, retains parameters of high importance while discarding redundant ones. We leverage a novel, empirical notion of sensitivity and extend traditional coreset constructions to the application of compressing parameters. Our theoretical analysis establishes guarantees on the size and accuracy of the resulting compressed neural network and gives rise to new generalization bounds that may provide novel insights on the generalization properties of neural networks. We demonstrate the practical effectiveness of our algorithm on a variety of neural network configurations and real-world data sets.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Autonomous driving requires 3D perception of vehicles and other objects in the in environment. Much of the current methods support 2D vehicle detection. This paper proposes a flexible pipeline to adopt any 2D detection network and fuse it with a 3D point cloud to generate 3D information with minimum changes of the 2D detection networks. To identify the 3D box, an effective model fitting algorithm is developed based on generalised car models and score maps. A two-stage convolutional neural network (CNN) is proposed to refine the detected 3D box. This pipeline is tested on the KITTI dataset using two different 2D detection networks. The 3D detection results based on these two networks are similar, demonstrating the flexibility of the proposed pipeline. The results rank second among the 3D detection algorithms, indicating its competencies in 3D detection.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Given a set $S$ of $n$ $d$-dimensional points, the $k$-nearest neighbors (KNN) is the problem of quickly finding $k$ points in $S$ that are nearest to a query point $q$. The $k$-nearest neighbors problem has applications in machine learning for classification and regression and also in searching. The secure version of KNN where either $q$ or $S$ are encrypted, has applications such as providing services over sensitive (such as medical or localization) data.\n  In this work we present the first scalable and efficient algorithm for solving KNN with Fully Homomorphic Encryption (FHE) that is realized by a polynomial whose degree is independent of $n$, the number of points. We implemented our algorithm in an open source library based on HELib implementation for the Brakerski-Gentry-Vakuntanthan's FHE scheme, and ran experiments on MIT's OpenStack cloud. Our experiments show that given a query point $q$, we can find the set of 20 nearest points out of more than 1000 points in less than an hour.\n  Our result introduces a statistical coreset, which is a data summarization technique that allows statistical functions, such as moments, to be efficiently and scalably computed. As a central tool, we design a new coin toss technique which we use to build the coreset. This coin toss technique and computation of statistical functions may be of independent interest.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Ego-centric data streams provide a unique opportunity to reason about joint behavior by pooling data across individuals. This is especially evident in urban environments teeming with human activities, but which suffer from incomplete and noisy data. Collaborative human activities exhibit common spatial, temporal, and visual characteristics facilitating inference across individuals from multiple sensory modalities as we explore in this paper from the perspective of meetings. We propose a new Bayesian nonparametric model that enables us to efficiently pool video and GPS data towards collaborative activities analysis from multiple individuals. We demonstrate the utility of this model for inference tasks such as activity detection, classification, and summarization. We further demonstrate how spatio-temporal structure embedded in our model enables better understanding of partial and noisy observations such as localization and face detections based on social interactions. We show results on both synthetic experiments and a new dataset of egocentric video and noisy GPS data from multiple individuals.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We present parametric trajectory optimization, a method for simultaneously computing physical parameters, actuation requirements, and robot motions for more efficient robot designs. In this scheme, robot dimensions, masses, and other physical parameters are solved for concurrently with traditional motion planning variables, including dynamically consistent robot states, actuation inputs, and contact forces. Our method requires minimal user domain knowledge, requiring only a coarse guess of the target robot configuration sequence and a parameterized robot topology as input. We demonstrate our results on four simulated robots, one of which we physically fabricated in order to demonstrate physical consistency. We demonstrate that by optimizing robot body parameters alongside robot trajectories, motion planning problems which would otherwise be infeasible can be made feasible, and actuation requirements can be significantly reduced.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We provide a deterministic data summarization algorithm that approximates the mean $\\bar{p}=\\frac{1}{n}\\sum_{p\\in P} p$ of a set $P$ of $n$ vectors in $\\REAL^d$, by a weighted mean $\\tilde{p}$ of a \\emph{subset} of $O(1/\\eps)$ vectors, i.e., independent of both $n$ and $d$. We prove that the squared Euclidean distance between $\\bar{p}$ and $\\tilde{p}$ is at most $\\eps$ multiplied by the variance of $P$. We use this algorithm to maintain an approximated sum of vectors from an unbounded stream, using memory that is independent of $d$, and logarithmic in the $n$ vectors seen so far. Our main application is to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. For example, in the case of mobile networks, we can use GPS traces to identify meetings, in the case of social networks, we can use information exchange to identify friend groups. Our algorithm provably identifies the {\\it Heavy Hitter} entries in a proximity (adjacency) matrix. The Heavy Hitters can be used to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. We evaluate the algorithm on several large data sets.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "When a vehicle observes another one, the two vehicles' poses are correlated by this spatial relative observation, which can be used in cooperative localization for further increasing localization accuracy and precision. To use spatial relative observations, we propose to add them into a pose graph for optimal pose estimation. Before adding them, we need to know the identities of the observed vehicles. The vehicle identification is formulated as a linear assignment problem, which can be solved efficiently. By using pose graph techniques and the start-of-the-art factor composition/decomposition method, our cooperative localization algorithm is robust against communication delay, packet loss, and out-of-sequence packet reception. We demonstrate the usability of our framework and effectiveness of our algorithm through both simulations and real-world experiments using three vehicles on the road.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Expensive specialized systems have hampered development of telerobotic systems for manufacturing systems. In this paper we demonstrate a telerobotic system which can reduce the cost of such system by leveraging commercial virtual reality(VR) technology and integrating it with existing robotics control software. The system runs on a commercial gaming engine using off the shelf VR hardware. This system can be deployed on multiple network architectures from a wired local network to a wireless network connection over the Internet. The system is based on the homunculus model of mind wherein we embed the user in a virtual reality control room. The control room allows for multiple sensor display, dynamic mapping between the user and robot, does not require the production of duals for the robot, or its environment. The control room is mapped to a space inside the robot to provide a sense of co-location within the robot. We compared our system with state of the art automation algorithms for assembly tasks, showing a 100% success rate for our system compared with a 66% success rate for automated systems. We demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "microMVP is an affordable, portable, and open source micro-scale mobile robot platform designed for robotics research and education. As a complete and unique multi-vehicle platform enabled by 3D printing and the maker culture, microMVP can be easily reproduced and requires little maintenance: a set of six micro vehicles, each measuring $8\\times 5\\times 6$ cubic centimeters and weighing under $100$ grams, and the accompanying tracking platform can be fully assembled in under two hours, all from readily available components. In this paper, we describe microMVP's hardware and software architecture, and the design thoughts that go into the making of the platform. The capabilities of microMVP APIs are then demonstrated with several single- and multi-robot path and motion planning algorithms. microMVP supports all common operation systems.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Our lives have been immensely improved by decades of automation research -- we are more comfortable, more productive and safer than ever before. Just imagine a world where familiar automation technologies have failed. In that world, thermostats don't work -- you have to monitor your home heating system manually. Cruise control for your car doesn't exist. Every elevator has to have a human operator to hit the right floor, most manufactured products are assembled by hand, and you have to wash your own dishes. Who would willingly adopt that world -- the world of last century -- today? Physical systems -- elevators, cars, home appliances, manufacturing equipment -- were more troublesome, ore time consuming, less safe, and far less convenient. Now, suppose we put ourselves in the place someone 20 years in the future, a future of autonomous systems. A future where transportation is largely autonomous, more efficient, and far safer; a future where dangerous occupations like mining or disaster response are performed by autonomous systems supervised remotely by humans; a future where manufacturing and healthcare are twice as productive per person-hour by having smart monitoring and readily re-tasked autonomous physical agents; a future where the elderly and infirm have 24 hour in-home autonomous support for the basic activities, both physical and social, of daily life. In a future world where these capabilities are commonplace, why would someone come back to today's world where someone has to put their life at risk to do a menial job, we lose time to mindless activities that have no intrinsic value, or be consumed with worry that a loved one is at risk in their own home? In what follows, and in a series of associated essays, we expand on these ideas, and frame both the opportunities and challenges posed by autonomous physical systems.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "This work introduces a novel technique for fabricating functional robots using 3D printers. Simultaneously depositing photopolymers and a non-curing liquid allows complex, pre-filled fluidic channels to be fabricated. This new printing capability enables complex hydraulically actuated robots and robotic components to be automatically built, with no assembly required. The technique is showcased by printing linear bellows actuators, gear pumps, soft grippers and a hexapod robot, using a commercially-available 3D printer. We detail the steps required to modify the printer and describe the design constraints imposed by this new fabrication approach.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We present a centralized algorithmic framework for solving multi-robot path planning problems in general, two-dimensional, continuous environments while minimizing globally the task completion time. The framework obtains high levels of effectiveness through the composition of an optimal discretization of the continuous environment and the subsequent fast, near-optimal resolution of the resulting discrete planning problem. This principled approach achieves orders of magnitudes better performance with respect to both speed and the supported robot density. For a wide variety of environments, our method is shown to compute globally near-optimal solutions for fifty robots in seconds with robots packed close to each other. In the extreme, the method can consistently solve problems with hundreds of robots that occupy over 30% of the free space.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "In this paper we present a practical solution with performance guarantees to the problem of dimensionality reduction for very large scale sparse matrices. We show applications of our approach to computing the low rank approximation (reduced SVD) of such matrices. Our solution uses coresets, which is a subset of $O(k/\\eps^2)$ scaled rows from the $n\\times d$ input matrix, that approximates the sub of squared distances from its rows to every $k$-dimensional subspace in $\\REAL^d$, up to a factor of $1\\pm\\eps$. An open theoretical problem has been whether we can compute such a coreset that is independent of the input matrix and also a weighted subset of its rows. %An open practical problem has been whether we can compute a non-trivial approximation to the reduced SVD of very large databases such as the Wikipedia document-term matrix in a reasonable time. We answer this question affirmatively. % and demonstrate an algorithm that efficiently computes a low rank approximation of the entire English Wikipedia. Our main technical result is a novel technique for deterministic coreset construction that is based on a reduction to the problem of $\\ell_2$ approximation for item frequencies.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Long-term sensor network deployments demand careful power management. While managing power requires understanding the amount of energy harvestable from the local environment, current solar prediction methods rely only on recent local history, which makes them susceptible to high variability. In this paper, we present a model and algorithms for distributed solar current prediction, based on multiple linear regression to predict future solar current based on local, in-situ climatic and solar measurements. These algorithms leverage spatial information from neighbors and adapt to the changing local conditions not captured by global climatic information. We implement these algorithms on our Fleck platform and run a 7-week-long experiment validating our work. In analyzing our results from this experiment, we determined that computing our model requires an increased energy expenditure of 4.5mJ over simpler models (on the order of 10^{-7}% of the harvested energy) to gain a prediction improvement of 39.7%.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We introduce and study the problem in which a mobile sensing robot (our tourist) is tasked to travel among and gather intelligence at a set of spatially distributed point-of-interests (POIs). The quality of the information collected at each POI is characterized by some non-decreasing reward function over the time spent at the POI. With limited time budget, the robot must balance between spending time traveling to POIs and spending time at POIs for information collection (sensing) so as to maximize the total reward. Alternatively, the robot may be required to acquire a minimum mount of reward and hopes to do so with the least amount of time. We propose a mixed integer programming (MIP) based anytime algorithm for solving these two NP-hard optimization problems to arbitrary precision. The effectiveness of our algorithm is demonstrated using an extensive set of computational experiments including the planning of a realistic itinerary for a first-time tourist in Istanbul.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We propose a novel non-linear extension to the Orienteering Problem (OP), called the Correlated Orienteering Problem (COP). We use COP to model the planning of informative tours for the persistent monitoring of a spatiotemporal field with time-invariant spatial correlations, in which the tours are constrained to have limited length. Our focus in this paper is QCOP a quadratic COP formulation that only looks at correlations between neighboring nodes in a node network. The main feature of QCOP is a quadratic utility function capturing the said spatial correlation. QCOP may be solved using mixed integer quadratic programming (MIQP), with the resulting anytime algorithm capable of planning multiple disjoint tours that maximize the quadratic utility. In particular, our algorithm can quickly plan a near-optimal tour over a network with up to $150$ nodes. Besides performing extensive simulation studies to verify the algorithm's correctness and characterize its performance, we also successfully applied it to two realistic persistent monitoring tasks: (i) estimation over a synthetic spatiotemporal field, and (ii) estimating the temperature distribution in the state of Massachusetts.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "This paper introduces a new mobile sensor scheduling problem, involving a single robot tasked with monitoring several events of interest that occur at different locations. Of particular interest is the monitoring of transient events that can not be easily forecast. Application areas range from natural phenomena ({\\em e.g.}, monitoring abnormal seismic activity around a volcano using a ground robot) to urban activities ({\\em e.g.}, monitoring early formations of traffic congestion using an aerial robot). Motivated by those and many other examples, this paper focuses on problems in which the precise occurrence times of the events are unknown {\\em a priori}, but statistics for their inter-arrival times are available. The robot's task is to monitor the events to optimize the following two objectives: {\\em (i)} maximize the number of events observed and {\\em (ii)} minimize the delay between two consecutive observations of events occurring at the same location. The paper considers the case when a robot is tasked with optimizing the event observations in a balanced manner, following a cyclic patrolling route. First, assuming the cyclic ordering of stations is known, we prove the existence and uniqueness of the optimal solution, and show that the optimal solution has desirable convergence and robustness properties. Our constructive proof also produces an efficient algorithm for computing the unique optimal solution with $O(n)$ time complexity, in which $n$ is the number of stations, with $O(\\log n)$ time complexity for incrementally adding or removing stations. Except for the algorithm, most of the analysis remains valid when the cyclic order is unknown. We then provide a polynomial-time approximation scheme that gives a $(1+\u03b5)$-optimal solution for this more general, NP-hard problem.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "This paper studies the problem of control strategy synthesis for dynamical systems with differential constraints to fulfill a given reachability goal while satisfying a set of safety rules. Particular attention is devoted to goals that become feasible only if a subset of the safety rules are violated. The proposed algorithm computes a control law, that minimizes the level of unsafety while the desired goal is guaranteed to be reached. This problem is motivated by an autonomous car navigating an urban environment while following rules of the road such as \"always travel in right lane'' and \"do not change lanes frequently''. Ideas behind sampling based motion-planning algorithms, such as Probabilistic Road Maps (PRMs) and Rapidly-exploring Random Trees (RRTs), are employed to incrementally construct a finite concretization of the dynamics as a durational Kripke structure. In conjunction with this, a weighted finite automaton that captures the safety rules is used in order to find an optimal trajectory that minimizes the violation of safety rules. We prove that the proposed algorithm guarantees asymptotic optimality, i.e., almost-sure convergence to optimal solutions. We present results of simulation experiments and an implementation on an autonomous urban mobility-on-demand system.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We consider the problem of automatic generation of control strategies for robotic vehicles given a set of high-level mission specifications, such as \"Vehicle x must eventually visit a target region and then return to a base,\" \"Regions A and B must be periodically surveyed,\" or \"None of the vehicles can enter an unsafe region.\" We focus on instances when all of the given specifications cannot be reached simultaneously due to their incompatibility and/or environmental constraints. We aim to find the least-violating control strategy while considering different priorities of satisfying different parts of the mission. Formally, we consider the missions given in the form of linear temporal logic formulas, each of which is assigned a reward that is earned when the formula is satisfied. Leveraging ideas from the automata-based model checking, we propose an algorithm for finding an optimal control strategy that maximizes the sum of rewards earned if this control strategy is applied. We demonstrate the proposed algorithm on an illustrative case study.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "In this paper we study rebalancing strategies for a mobility-on-demand urban transportation system blending customer-driven vehicles with a taxi service. In our system, a customer arrives at one of many designated stations and is transported to any other designated station, either by driving themselves, or by being driven by an employed driver. The system allows for one-way trips, so that customers do not have to return to their origin. When some origins and destinations are more popular than others, vehicles will become unbalanced, accumulating at some stations and becoming depleted at others. This problem is addressed by employing rebalancing drivers to drive vehicles from the popular destinations to the unpopular destinations. However, with this approach the rebalancing drivers themselves become unbalanced, and we need to \"rebalance the rebalancers\" by letting them travel back to the popular destinations with a customer. Accordingly, in this paper we study how to optimally route the rebalancing vehicles and drivers so that stability (in terms of boundedness of the number of waiting customers) is ensured while minimizing the number of rebalancing vehicles traveling in the network and the number of rebalancing drivers needed; surprisingly, these two objectives are aligned, and one can find the optimal rebalancing strategy by solving two decoupled linear programs. Leveraging our analysis, we determine the minimum number of drivers and minimum number of vehicles needed to ensure stability in the system. Interestingly, our simulations suggest that, in Euclidean network topologies, one would need between 1/3 and 1/4 as many drivers as vehicles.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "A case study of the Singapore road network provides empirical evidence that road pricing can significantly affect commuter trip timing behaviors. In this paper, we propose a model of trip timing decisions that reasonably matches the observed commuters' behaviors. Our model explicitly captures the difference in individuals' sensitivity to price, travel time and early or late arrival at destination. New pricing schemes are suggested to better spread peak travel and reduce traffic congestion. Simulation results based on the proposed model are provided in comparison with the real data for the Singapore case study.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Accurate vehicular localization is important for various cooperative vehicle safety (CVS) applications such as collision avoidance, turning assistant, etc. In this paper, we propose a cooperative vehicular distance measurement technique based on the sharing of GPS pseudorange measurements and a weighted least squares method. The classic double difference pseudorange solution, which was originally designed for high-end survey level GPS systems, is adapted to low-end navigation level GPS receivers for its wide availability in ground vehicles. The Carrier to Noise Ratio (CNR) of raw pseudorange measurements are taken into account for noise mitigation. We present a Dedicated Short Range Communications (DSRC) based mechanism to implement the exchange of pseudorange information among neighboring vehicles. As demonstrated in field tests, our proposed technique increases the accuracy of the distance measurement significantly compared with the distance obtained from the GPS fixes.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We study the problem of planning paths for $p$ distinguishable pebbles (robots) residing on the vertices of an $n$-vertex connected graph with $p \\le n$. A pebble may move from a vertex to an adjacent one in a time step provided that it does not collide with other pebbles. When $p = n$, the only collision free moves are synchronous rotations of pebbles on disjoint cycles of the graph. We show that the feasibility of such problems is intrinsically determined by the diameter of a (unique) permutation group induced by the underlying graph. Roughly speaking, the diameter of a group $\\mathbf G$ is the minimum length of the generator product required to reach an arbitrary element of $\\mathbf G$ from the identity element. Through bounding the diameter of this associated permutation group, which assumes a maximum value of $O(n^2)$, we establish a linear time algorithm for deciding the feasibility of such problems and an $O(n^3)$ algorithm for planning complete paths.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We consider the synthesis of control policies from temporal logic specifications for robots that interact with multiple dynamic environment agents. Each environment agent is modeled by a Markov chain whereas the robot is modeled by a finite transition system (in the deterministic case) or Markov decision process (in the stochastic case). Existing results in probabilistic verification are adapted to solve the synthesis problem. To partially address the state explosion issue, we propose an incremental approach where only a small subset of environment agents is incorporated in the synthesis procedure initially and more agents are successively added until we hit the constraints on computational resources. Our algorithm runs in an anytime fashion where the probability that the robot satisfies its specification increases as the algorithm progresses.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "In this paper we present a method for automatically planning optimal paths for a group of robots that satisfy a common high level mission specification. Each robot's motion in the environment is modeled as a weighted transition system. The mission is given as a Linear Temporal Logic formula. In addition, an optimizing proposition must repeatedly be satisfied. The goal is to minimize the maximum time between satisfying instances of the optimizing proposition. Our method is guaranteed to compute an optimal set of robot paths. We utilize a timed automaton representation in order to capture the relative position of the robots in the environment. We then obtain a bisimulation of this timed automaton as a finite transition system that captures the joint behavior of the robots and apply our earlier algorithm for the single robot case to optimize the group motion. We present a simulation of a persistent monitoring task in a road network environment.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We present a method to generate a robot control strategy that maximizes the probability to accomplish a task. The task is given as a Linear Temporal Logic (LTL) formula over a set of properties that can be satisfied at the regions of a partitioned environment. We assume that the probabilities with which the properties are satisfied at the regions are known, and the robot can determine the truth value of a proposition only at the current region. Motivated by several results on partitioned-based abstractions, we assume that the motion is performed on a graph. To account for noisy sensors and actuators, we assume that a control action enables several transitions with known probabilities. We show that this problem can be reduced to the problem of generating a control policy for a Markov Decision Process (MDP) such that the probability of satisfying an LTL formula over its states is maximized. We provide a complete solution for the latter problem that builds on existing results from probabilistic model checking. We include an illustrative case study.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "In this paper, we develop a method to automatically generate a control policy for a dynamical system modeled as a Markov Decision Process (MDP). The control specification is given as a Linear Temporal Logic (LTL) formula over a set of propositions defined on the states of the MDP. We synthesize a control policy such that the MDP satisfies the given specification almost surely, if such a policy exists. In addition, we designate an \"optimizing proposition\" to be repeatedly satisfied, and we formulate a novel optimization criterion in terms of minimizing the expected cost in between satisfactions of this proposition. We propose a sufficient condition for a policy to be optimal, and develop a dynamic programming algorithm that synthesizes a policy that is optimal under some conditions, and sub-optimal otherwise. This problem is motivated by robotic applications requiring persistent tasks, such as environmental monitoring or data gathering, to be performed.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "We present controllers that enable mobile robots to persistently monitor or sweep a changing environment. The changing environment is modeled as a field which grows in locations that are not within range of a robot, and decreases in locations that are within range of a robot. We assume that the robots travel on given closed paths. The speed of each robot along its path is controlled to prevent the field from growing unbounded at any location. We consider the space of speed controllers that can be parametrized by a finite set of basis functions. For a single robot, we develop a linear program that is guaranteed to compute a speed controller in this space to keep the field bounded, if such a controller exists. Another linear program is then derived whose solution is the speed controller that minimizes the maximum field value over the environment. We extend our linear program formulation to develop a multi-robot controller that keeps the field bounded. The multi-robot controller has the unique feature that it does not require communication among the robots. Simulation studies demonstrate the robustness of the controllers to modeling errors, and to stochasticity in the environment.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "In this paper we present a method for automatically generating optimal robot trajectories satisfying high level mission specifications. The motion of the robot in the environment is modeled as a general transition system, enhanced with weighted transitions. The mission is specified by a general linear temporal logic formula. In addition, we require that an optimizing proposition must be repeatedly satisfied. The cost function that we seek to minimize is the maximum time between satisfying instances of the optimizing proposition. For every environment model, and for every formula, our method computes a robot trajectory which minimizes the cost function. The problem is motivated by applications in robotic monitoring and data gathering. In this setting, the optimizing proposition is satisfied at all locations where data can be uploaded, and the entire formula specifies a complex (and infinite horizon) data collection mission. Our method utilizes B\u00fcchi automata to produce an automaton (which can be thought of as a graph) whose runs satisfy the temporal logic specification. We then present a graph algorithm which computes a path corresponding to the optimal robot trajectory. We also present an implementation for a robot performing a data gathering mission in a road network.\n        \u25b3 Less", "author": "Daniela Rus"}, {"abstract": "Transneptunian objects (TNOs) are a source of invaluable information to access the history and evolution of the outer solar system. However, observing these faint objects is a difficult task. As a consequence, important properties such as size and albedo are known for only a small fraction of them. Now, with the results from deep sky surveys and the Gaia space mission, a new exciting era is within reach as accurate predictions of stellar occultations by numerous distant small solar system bodies become available. From them, diameters with kilometer accuracies can be determined. Albedos, in turn, can be obtained from diameters and absolute magnitudes. We use observations from the Dark Energy Survey (DES) from November 2012 until February 2016, amounting to 4292847 CCD frames. We searched them for all known small solar system bodies and recovered a total of 202 TNOs and Centaurs, 63 of which have been discovered by the DES collaboration until the date of this writing. Their positions were determined using the Gaia Data Release 2 as reference and their orbits were refined. Stellar occultations were then predicted using these refined orbits plus stellar positions from Gaia. These predictions are maintained, and updated, in a dedicated web service. The techniques developed here are also part of an ambitious preparation to use the data from the Large Synoptic Survey Telescope (LSST), that expects to obtain accurate positions and multifilter photometry for tens of thousands of TNOs.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The profile of the longitudinal development of showers produced by ultra-high energy cosmic rays carries information related to the interaction properties of the primary particles with atmospheric nuclei. In this work, we present the first measurement of the average shower profile in traversed atmospheric depth at the Pierre Auger Observatory. The shapes of profiles are well reproduced by the Gaisser-Hillas parametrization within the range studied, for E > 10^{17.8} eV. A detailed analysis of the systematic uncertainties is performed using 10 years of data and a full detector simulation. The average shape is quantified using two variables related to the width and asymmetry of the profile, and the results are compared with predictions of hadronic interaction models for different primary particles.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e. 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that undergone gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "(Abridged) This is the Maunakea Spectroscopic Explorer 2018 book. It is intended as a concise reference guide to all aspects of the scientific and technical design of MSE, for the international astronomy and engineering communities, and related agencies. The current version is a status report of MSE's science goals and their practical implementation, following the System Conceptual Design Review, held in January 2018. MSE is a planned 10-m class, wide-field, optical and near-infrared facility, designed to enable transformative science, while filling a critical missing gap in the emerging international network of large-scale astronomical facilities. MSE is completely dedicated to multi-object spectroscopy of samples of between thousands and millions of astrophysical objects. It will lead the world in this arena, due to its unique design capabilities: it will boast a large (11.25 m) aperture and wide (1.52 sq. degree) field of view; it will have the capabilities to observe at a wide range of spectral resolutions, from R2500 to R40,000, with massive multiplexing (4332 spectra per exposure, with all spectral resolutions available at all times), and an on-target observing efficiency of more than 80%. MSE will unveil the composition and dynamics of the faint Universe and is designed to excel at precision studies of faint astrophysical phenomena. It will also provide critical follow-up for multi-wavelength imaging surveys, such as those of the Large Synoptic Survey Telescope, Gaia, Euclid, the Wide Field Infrared Survey Telescope, the Square Kilometre Array, and the Next Generation Very Large Array.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "This paper presents a Center of Mass (CoM) based manipulation and regrasp planner that implements stability constraints to preserve the robot balance. The planner provides a graph of IK-feasible, collision-free and stable motion sequences, constructed using an energy based motion planning algorithm. It assures that the assembly motions are stable and prevent the robot from falling while performing dexterous tasks in different situations. Furthermore, the constraints are also used to perform an RRT-inspired task-related stability estimation in several simulations. The estimation can be used to select between single-arm and dual-arm regrasping configurations to achieve more stability and robustness for a given manipulation task. To validate the planner and the task-related stability estimations, several tests are performed in simulations and real-world experiments involving the HRP5P humanoid robot, the 5th generation of the HRP robot family. The experiment results suggest that the planner and the task-related stability estimation provide robust behavior for the humanoid robot while performing regrasp tasks.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "In this paper we study empirically the validity of measures of referential success for referring expressions involving gradual properties. More specifically, we study the ability of several measures of referential success to predict the success of a user in choosing the right object, given a referring expression. Experimental results indicate that certain fuzzy measures of success are able to predict human accuracy in reference resolution. Such measures are therefore suitable for the estimation of the success or otherwise of a referring expression produced by a generation algorithm, especially in case the properties in a domain cannot be assumed to have crisp denotations.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Using deep wide-field photometry three-year data (Y3) from the Dark Energy Survey (DES), we present a panoramic study of the Fornax dwarf spheroidal galaxy. The data presented here -- a small subset of the full survey -- uniformly covers a region of 25 square degrees centered on the galaxy to a depth of g ~ 23.5. We use this data to study the structural properties of Fornax, overall stellar population, and its member stars in different evolutionary phases. We also search for possible signs of tidal disturbance. Fornax is found to be significantly more spatially extended than what early studies suggested. No statistically significant distortions or signs of tidal disturbances were found down to a surface brightness limit of ~32.1 mag/arcsec^2. However, there are hints of shell-like features located ~30' - 40' from the center of Fornax that may be stellar debris from past merger events. We also find that intermediate age and young main-sequence populations show different orientation at the galaxy center and have many substructures. The deep DES Y3 data allows us to characterize the age of those substructures with great accuracy, both those previously known and those newly discovered in this work, on the basis of their color-magnitude diagram morphology. We find that the youngest overdensities are all found on the Eastern side of Fornax, where the Fornax field population itself is slightly younger than in the West. The high quality DES Y3 data reveals that Fornax has many rich structures, and provides insights into its complex formation history.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Classical epidemiology has focused on the control of confounding but it is only recently that epidemiologists have started to focus on the bias produced by colliders. A collider for a certain pair of variables (e.g., an outcome Y and an exposure A) is a third variable (C) that is caused by both. In a directed acyclic graph (DAG), a collider is the variable in the middle of an inverted fork (i.e., the variable C in A -> C <- Y). Controlling for, or conditioning an analysis on a collider (i.e., through stratification or regression) can introduce a spurious association between its causes. This potentially explains many paradoxical findings in the medical literature, where established risk factors for a particular outcome appear protective. We use an example from non-communicable disease epidemiology to contextualize and explain the effect of conditioning on a collider. We generate a dataset with 1,000 observations and run Monte-Carlo simulations to estimate the effect of 24-hour dietary sodium intake on systolic blood pressure, controlling for age, which acts as a confounder, and 24-hour urinary protein excretion, which acts as a collider. We illustrate how adding a collider to a regression model introduces bias. Thus, to prevent paradoxical associations, epidemiologists estimating causal effects should be wary of conditioning on colliders. We provide R-code in easy-to-read boxes throughout the manuscript and a GitHub repository (https://github.com/migariane/ColliderApp) for the reader to reproduce our example. We also provide an educational web application allowing real-time interaction to visualize the paradoxical effect of conditioning on a collider http://watzilei.com/shiny/collider/.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The Large Synoptic Survey Telescope (LSST) Dark Energy Science Collaboration (DESC) will use five cosmological probes: galaxy clusters, large scale structure, supernovae, strong lensing, and weak lensing. This Science Requirements Document (SRD) quantifies the expected dark energy constraining power of these probes individually and together, with conservative assumptions about analysis methodology and follow-up observational resources based on our current understanding and the expected evolution within the field in the coming years. We then define requirements on analysis pipelines that will enable us to achieve our goal of carrying out a dark energy analysis consistent with the Dark Energy Task Force definition of a Stage IV dark energy experiment. This is achieved through a forecasting process that incorporates the flowdown to detailed requirements on multiple sources of systematic uncertainty. Future versions of this document will include evolution in our software capabilities and analysis plans along with updates to the LSST survey strategy.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The black hole transient V404 Cygni exhibited a bright outburst in June 2015 that was intensively followed over a wide range of wavelengths. Our team obtained high time resolution optical spectroscopy (~90 s), which included a detailed coverage of the most active phase of the event. We present a database consisting of 651 optical spectra obtained during this event, that we combine with 58 spectra gathered during the fainter December 2015 sequel outburst, as well as with 57 spectra from the 1989 event. We previously reported the discovery of wind-related features (P-Cygni and broad-wing line profiles) during both 2015 outbursts. Here, we build diagnostic diagrams that enable us to study the evolution of typical emission line parameters, such as line fluxes and equivalent widths, and develop a technique to systematically detect outflow signatures. We find that these are present throughout the outburst, even at very low optical fluxes, and that both types of outflow features are observed simultaneously in some spectra, confirming the idea of a common origin. We also show that the nebular phases depict loop patterns in many diagnostic diagrams, while P-Cygni profiles are highly variable on time-scales of minutes. The comparison between the three outbursts reveals that the spectra obtained during June and December 2015 share many similarities, while those from 1989 exhibit narrower emission lines and lower wind terminal velocities. The diagnostic diagrams presented in this work have been produced using standard measurement techniques and thus may be applied to other active low-mass X-ray binaries.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present a detailed study of the large-scale anisotropies of cosmic rays with energies above 4 EeV measured using the Pierre Auger Observatory. For the energy bins [4,8] EeV and $E\\geq 8$ EeV, the most significant signal is a dipolar modulation in right ascension at energies above 8 EeV, as previously reported. In this paper we further scrutinize the highest-energy bin by splitting it into three energy ranges. We find that the amplitude of the dipole increases with energy above 4 EeV. The growth can be fitted with a power law with index $\u03b2=0.79\\pm 0.19$. The directions of the dipoles are consistent with an extragalactic origin of these anisotropies at all the energies considered. Additionally we have estimated the quadrupolar components of the anisotropy: they are not statistically significant. We discuss the results in the context of the predictions from different models for the distribution of ultrahigh-energy sources and cosmic magnetic fields.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present a structural and morphological catalogue for 45 million objects selected from the first year of data from the Dark Energy Survey (DES). Single Sersic fits and non-parametric measurements are produced for g, r and i filters. The parameters from the best-fitting Sersic model (total magnitude, half-light radius, Sersic index, axis ratio and position angle) are measured with Galfit; the non-parametric coefficients (concentration, asymmetry, clumpiness, Gini, M20) are provided using the Zurich Estimator of Structural Types (ZEST+). To study the statistical uncertainties, we consider a sample of state-of-the-art image simulations with a realistic distribution in the input parameter space and then process and analyse them as we do with real data: this enables us to quantify the observational biases due to PSF blurring and magnitude effects and correct the measurements as a function of magnitude, galaxy size, Sersic index (concentration for the analysis of the non-parametric measurements) and ellipticity. We present the largest structural catalogue to date: we find that accurate and complete measurements for all the structural parameters are typically obtained for galaxies with SExtractor MAG AUTO I < 21. Indeed, the parameters in the filters i and r can be overall well recovered up to MAG AUTO < 21.5, corresponding to a fitting completeness of ~90% below this threshold, for a total of 25 million galaxies. The combination of parametric and non-parametric structural measurements makes this catalogue an important instrument to explore and understand how galaxies form and evolve. The catalogue described in this paper will be publicly released alongside the Dark Energy Survey collaboration Y1 cosmology data products at the following URL: https://des.ncsa.illinois.edu/releases/y1a1/gold/morphology.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We have measured the low-frequency time instability known as charge offset drift of Si/SiO$_2$ single electron devices (SEDs) with and without an overall poly-Si top gate. We find that SEDs with a poly-Si top gate have significantly less charge offset drift, exhibiting fewer isolated jumps and a factor of two reduction in fluctuations about a stable mean value. The observed reduction can be accounted for by the electrostatic reduction in the mutual capacitance $C_m$ between defects and the quantum dot, and increase in the total defect capacitance $C_d$ due to the top gate. These results depart from the accepted understanding that the level of charge offset drift in SEDs is determined by the intrinsic material properties, forcing consideration of the device design as well. We expect these results to be of importance in developing SEDs for applications from quantum information to metrology or wherever charge noise or integrability of devices is a challenge.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "In van der Waals (vdW) heterostructures formed by stacking two monolayer semiconductors, lattice mismatch or rotational misalignment introduces an in-plane moir\u00e9 superlattice. While it is widely recognized that a moir\u00e9 superlattice can modulate the electronic band structure and lead to novel transport properties including unconventional superconductivity and insulating behavior driven by correlations, its influence on optical properties has not been investigated experimentally. We present spectroscopic evidence that interlayer excitons are confined by the moir\u00e9 potential in a high-quality MoSe2/WSe2 heterobilayer with small rotational twist. A series of interlayer exciton resonances with either positive or negative circularly polarized emission is observed in photoluminescence, consistent with multiple exciton states confined within the moir\u00e9 potential. The recombination dynamics and temperature dependence of these interlayer exciton resonances are consistent with this interpretation. These results demonstrate the feasibility of engineering artificial excitonic crystals using vdW heterostructures for nanophotonics and quantum information applications.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The $\u03a5(1S)\u03bc^+\u03bc^-$ invariant-mass distribution is investigated for a possible exotic meson state composed of two $b$ quarks and two $\\overline{b}$ quarks, $X_{b\\overline{b}b\\overline{b}}$. The analysis is based on a data sample of $pp$ collisions recorded with the LHCb detector at centre-of-mass energies $\\sqrt{s} =$ 7, 8 and 13 TeV, corresponding to an integrated luminosity of 6.3 fb$^{-1}$. No significant excess is found, and upper limits are set on the product of the production cross-section and the branching fraction as functions of the mass of the $X_{b\\overline{b}b\\overline{b}}$ state. The limits are set in the fiducial volume where all muons have pseudorapidity in the range $[2.0,5.0]$, and the $X_{b\\overline{b}b\\overline{b}}$ state has rapidity in the range $[2.0,4.5]$ and transverse momentum less than 15 GeV/$c$.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "In this work we report on a new mechanism to generate dissipative steady state entanglement in two coupled qubits driven by strong periodic ac fields. We show that steady entanglement can be generated at one side of a multiphoton resonance between a non-entangled ground state and an entangled excited state. The degree of entanglement can be tuned as a function of the amplitude of the periodic drive. A rich dynamic behavior with creation, death and revival of entanglement can be observed for certain parameter regimes, accessible in current experimental devices.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "With the Auger Engineering Radio Array (AERA) of the Pierre Auger Observatory, we have observed the radio emission from 561 extensive air showers with zenith angles between 60$^\\circ$ and 84$^\\circ$. In contrast to air showers with more vertical incidence, these inclined air showers illuminate large ground areas of several km$^2$ with radio signals detectable in the 30 to 80\\,MHz band. A comparison of the measured radio-signal amplitudes with Monte Carlo simulations of a subset of 50 events for which we reconstruct the energy using the Auger surface detector shows agreement within the uncertainties of the current analysis. As expected for forward-beamed radio emission undergoing no significant absorption or scattering in the atmosphere, the area illuminated by radio signals grows with the zenith angle of the air shower. Inclined air showers with EeV energies are thus measurable with sparse radio-antenna arrays with grid sizes of a km or more. This is particularly attractive as radio detection provides direct access to the energy in the electromagnetic cascade of an air shower, which in case of inclined air showers is not accessible by arrays of particle detectors on the ground.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The Deep Underground Neutrino Experiment (DUNE) is a dual-site experiment for long baseline neutrino oscillation studies, and for neutrino astrophysics and nucleon decay searches. The far detector is a 40-kton underground liquid argon time-projection-chamber (LAr TPC), in which the photon detector system adds precise timing capabilities. The ProtoDUNE dual phase detector will consist of a 6x6x6 m^3 LAr TPC to be operated at the CERN Neutrino Platform and the photon detection system will be formed by 8-inch cryogenic photomultipliers from Hamamatsu. The PMT model (R5912-20Mod) performance at cryogenic temperature is studied including dark current, gain, and linearity with the light intensity and pulse rate. In addition, the PMT base design is validated. At cold, a decrease of the PMT amplification, or fatigue effect, is measured as the PMT output current increases, either, due to high gain, light intensity or rate. Also, the characterisation results of the 40 photomultipliers to be used in ProtoDUNE dual phase are presented.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "A 10 kilo-tonne dual-phase liquid argon TPC is one of the detector options considered for the Deep Underground Neutrino Experiment (DUNE). The detector technology relies on amplification of the ionisation charge in ultra-pure argon vapour and oers several advantages compared to the traditional single-phase liquid argon TPCs. A 4.2 tonne dual-phase liquid argon TPC prototype, the largest of its kind, with an active volume of 3x1x1 $m^3$ has been constructed and operated at CERN. In this paper we describe in detail the experimental setup and detector components as well as report on the operation experience. We also present the first results on the achieved charge amplification, prompt scintillation and electroluminescence detection, and purity of the liquid argon from analyses of a collected sample of cosmic ray muons.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The DESI Legacy Imaging Surveys are a combination of three public projects (the Dark Energy Camera Legacy Survey, the Beijing-Arizona Sky Survey, and the Mayall z-band Legacy Survey) that will jointly image ~14,000 square degrees of the extragalactic sky visible from the northern hemisphere in three optical bands (g, r, and z) using telescopes at the Kitt Peak National Observatory and the Cerro Tololo Inter-American Observatory. The combined survey footprint is split into two contiguous areas by the Galactic plane. The optical imaging is conducted using a unique strategy of dynamic observing that results in a survey of nearly uniform depth. In addition to calibrated images, the project is delivering an inference-based catalog which includes photometry from the grz optical bands and from four mid-infrared bands (at 3.4um, 4.6um, 12um and 22um) observed by the Wide-field Infrared Survey Explorer (WISE) satellite during its full operational lifetime. The project plans two public data releases each year. All the software used to generate the catalogs is also released with the data. This paper provides an overview of the Legacy Surveys project.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present weak lensing (WL) mass constraints for a sample of massive galaxy clusters detected by the South Pole Telescope (SPT) via the Sunyaev-Zeldovich effect (SZE). We use $griz$ imaging data obtained from the Science Verification (SV) phase of the Dark Energy Survey (DES) to fit the WL shear signal of 33 clusters in the redshift range $0.25 \\le z \\le 0.8$ with NFW profiles and to constrain a four-parameter SPT mass-observable relation. To account for biases in WL masses, we introduce a WL mass to true mass scaling relation described by a mean bias and an intrinsic, log-normal scatter. We allow for correlated scatter within the WL and SZE mass-observable relations and use simulations to calibrate priors on nuisance parameters related to bias and scatter from WL. We constrain the normalization of the $\u03b6-M_{500}$ relation, $A_\\mathrm{SZ}=12.0_{-6.7}^{+2.6}$ when using a prior on the mass slope $B_\\mathrm{SZ}$ from the latest SPT cluster cosmology analysis. Without this prior, we recover $A_\\mathrm{SZ}=10.8_{-5.2}^{+2.3}$ and $B_\\mathrm{SZ}=1.30_{-0.44}^{+0.22}$. Results in both cases imply lower cluster masses than measured in previous work with and without WL, although the uncertainties are large. The WL derived value of $B_\\mathrm{SZ}$ is $\\approx 20\\%$ lower than the value preferred by the most recent SPT cluster cosmology analysis. The method demonstrated in this work is designed to constrain cluster masses and cosmological parameters simultaneously and will be used for subsequent studies that employ the full SPT cluster sample together with the DES data.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Mapping the underlying density field, including non-visible dark matter, using weak gravitational lensing measurements is now a standard tool in cosmology. Due to its importance to the science results of current and upcoming surveys, the quality of the convergence reconstruction methods should be well understood. We compare three methods: Kaiser-Squires (KS), Wiener filter, and GLIMPSE. KS is a direct inversion, not accounting for survey masks or noise. The Wiener filter is well-motivated for Gaussian density fields in a Bayesian framework. GLIMPSE uses sparsity, aiming to reconstruct non-linearities in the density field. We compare these methods with several tests using public Dark Energy Survey (DES) Science Verification (SV) data and realistic DES simulations. The Wiener filter and GLIMPSE offer substantial improvements over smoothed KS with a range of metrics. Both the Wiener filter and GLIMPSE convergence reconstructions show a 12 per cent improvement in Pearson correlation with the underlying truth from simulations. To compare the mapping methods' abilities to find mass peaks, we measure the difference between peak counts from simulated \u039bCDM shear catalogues and catalogues with no mass fluctuations (a standard data vector when inferring cosmology from peak statistics); the maximum signal-to-noise of these peak statistics is increased by a factor of 3.5 for the Wiener filter and 9 for GLIMPSE. With simulations we measure the reconstruction of the harmonic phases; the phase residuals' concentration is improved 17 per cent by GLIMPSE and 18 per cent by the Wiener filter. The correlation between reconstructions from data and foreground redMaPPer clusters is increased 18 per cent by the Wiener filter and 32 per cent by GLIMPSE.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Psychlab is a simulated psychology laboratory inside the first-person 3D game world of DeepMind Lab (Beattie et al. 2016). Psychlab enables implementations of classical laboratory psychological experiments so that they work with both human and artificial agents. Psychlab has a simple and flexible API that enables users to easily create their own tasks. As examples, we are releasing Psychlab implementations of several classical experimental paradigms including visual search, change detection, random dot motion discrimination, and multiple object tracking. We also contribute a study of the visual psychophysics of a specific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg et al. 2016). This study leads to the surprising conclusion that UNREAL learns more quickly about larger target stimuli than it does about smaller stimuli. In turn, this insight motivates a specific improvement in the form of a simple model of foveal vision that turns out to significantly boost UNREAL's performance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By open-sourcing Psychlab we hope to facilitate a range of future such studies that simultaneously advance deep reinforcement learning and improve its links with cognitive science.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "A new analysis of the dataset from the Pierre Auger Observatory provides evidence for anisotropy in the arrival directions of ultra-high-energy cosmic rays on an intermediate angular scale, which is indicative of excess arrivals from strong, nearby sources. The data consist of 5514 events above 20 EeV with zenith angles up to 80 deg recorded before 2017 April 30. Sky models have been created for two distinct populations of extragalactic gamma-ray emitters: active galactic nuclei from the second catalog of hard Fermi-LAT sources (2FHL) and starburst galaxies from a sample that was examined with Fermi-LAT. Flux-limited samples, which include all types of galaxies from the Swift-BAT and 2MASS surveys, have been investigated for comparison. The sky model of cosmic-ray density constructed using each catalog has two free parameters, the fraction of events correlating with astrophysical objects and an angular scale characterizing the clustering of cosmic rays around extragalactic sources. A maximum-likelihood ratio test is used to evaluate the best values of these parameters and to quantify the strength of each model by contrast with isotropy. It is found that the starburst model fits the data better than the hypothesis of isotropy with a statistical significance of 4.0 sigma, the highest value of the test statistic being for energies above 39 EeV. The three alternative models are favored against isotropy with 2.7-3.2 sigma significance. The origin of the indicated deviation from isotropy is examined and prospects for more sensitive future studies are discussed.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We use photoemission spectroscopy to discover the first topological magnet in three dimensions, the material Co$_2$MnGa.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R&D activities required to prepare for this software upgrade.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Mock catalogues are a crucial tool in the analysis of galaxy surveys data, both for the accurate computation of covariance matrices, and for the optimisation of analysis methodology and validation of data sets. In this paper, we present a set of 1800 galaxy mock catalogues designed to match the Dark Energy Survey Year-1 BAO sample (Crocce et al. 2017) in abundance, observational volume, redshift distribution and uncertainty, and redshift dependent clustering. The simulated samples were built upon HALOGEN (Avila et al. 2015) halo catalogues, based on a $2LPT$ density field with an exponential bias. For each of them, a lightcone is constructed by the superposition of snapshots in the redshift range $0.45<z<1.4$. Uncertainties introduced by so-called photometric redshifts estimators were modelled with a \\textit{double-skewed-Gaussian} curve fitted to the data. We also introduce a hybrid HOD-HAM model with two free parameters that are adjusted to achieve a galaxy bias evolution $b(z_{\\rm ph})$ that matches the data at the 1-$\u03c3$ level in the range $0.6<z_{\\rm ph}<1.0$. We further analyse the galaxy mock catalogues and compare their clustering to the data using the angular correlation function $ w(\u03b8)$, the comoving transverse separation clustering $\u03be_{\u03bc<0.8}(s_{\\perp})$ and the angular power spectrum $C_\\ell$.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present 5 years of optical and infrared data of the black hole candidate MAXI J1659-152 covering its 2010 outburst, decay and quiescence. Combining optical data taken during the outburst decay, we obtain an orbital period of 2.414 $\\pm$ 0.005 h, in perfect agreement with the value previously measured from X-ray dips. In addition, we detect a clear H$\u03b1$ excess in MAXI J1659-152 with data taken during the outburst decay. We also detect a single hump modulation most likely produced by irradiation. Assuming that the maximum occurs at orbital phase 0.5, we constrain the phase of the X-ray dips to be ~ 0.65. We also detect the quiescent optical counterpart at r' = 24.20 $\\pm$ 0.08, I = 23.32 $\\pm$ 0.02 and H = 20.7 $\\pm$ 0.1. These magnitudes provide colour indices implying an M2-M5 donor star assuming 60% contribution from a disc component in the r'-band.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The Dalitz plot analysis technique has become an increasingly important method in heavy flavour physics. The Laura++ fitter has been developed as a flexible tool that can be used for Dalitz plot analyses in different experimental environments. Explicitly designed for three-body decays of heavy-flavoured mesons to spinless final state particles, it is optimised in order to describe all possible resonant or nonresonant contributions, and to accommodate possible CP violation effects.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The first Weyl semimetal was recently discovered in the NbP class of compounds. Although the topology of these novel materials has been identified, the surface properties are not yet fully understood. By means of scanning tunneling spectroscopy, we find that NbPs (001) surface hosts a pair of Dirac cones protected by mirror symmetry. Through our high resolution spectroscopic measurements, we resolve the quantum interference patterns arising from these novel Dirac fermions, and reveal their electronic structure, including the linear dispersions. Our data, in agreement with our theoretical calculations, uncover further interesting features of the Weyl semimetal NbPs already exotic surface. Moreover, we discuss the similarities and distinctions between the Dirac fermions here and those in topological crystalline insulators in terms of symmetry protection and topology.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We combine Dark Energy Survey Year 1 clustering and weak lensing data with Baryon Acoustic Oscillations (BAO) and Big Bang Nucleosynthesis (BBN) experiments to constrain the Hubble constant. Assuming a flat $\u039b$CDM model with minimal neutrino mass ($\\sum m_\u03bd= 0.06$ eV) we find $H_0=67.2^{+1.2}_{-1.0}$ km/s/Mpc (68% CL). This result is completely independent of Hubble constant measurements based on the distance ladder, Cosmic Microwave Background (CMB) anisotropies (both temperature and polarization), and strong lensing constraints. There are now five data sets that: a) have no shared observational systematics; and b) each constrain the Hubble constant with a few percent level precision. We compare these five independent measurements, and find that, as a set, the differences between them are significant at the $2.1\u03c3$ level ($\u03c7^2/dof=20.1/11$, probability to exceed=4%). This difference is low enough that we consider the data sets statistically consistent with each other. The best fit Hubble constant obtained by combining all five data sets is $H_0 = 69.1^{+0.4}_{-0.6}$ km/s/Mpc.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We study the role of cold gas in quenching star formation in the green valley by analysing ALMA $^{12}$CO (1-0) observations of three galaxies with resolved optical spectroscopy from the MaNGA survey. We present resolution-matched maps of the star formation rate and molecular gas mass. These data are used to calculate the star formation efficiency (SFE) and gas fraction ($f_{\\rm~gas}$) for these galaxies separately in the central `bulge' regions and outer disks. We find that, for the two galaxies whose global specific star formation rate (sSFR) deviates most from the star formation main sequence, the gas fraction in the bulges is significantly lower than that in their disks, supporting an `inside-out' model of galaxy quenching. For the two galaxies where SFE can be reliably determined in the central regions, the bulges and disks share similar SFEs. This suggests that a decline in $f_{\\rm~gas}$ is the main driver of lowered sSFR in bulges compared to disks in green valley galaxies. Within the disks, there exist common correlations between the sSFR and SFE and between sSFR and $f_{\\rm~gas}$ on kpc scales -- the local SFE or $f_{\\rm~gas}$ in the disks declines with local sSFR. Our results support a picture in which the sSFR in bulges is primarily controlled by $f_{\\rm~gas}$, whereas both SFE and $f_{\\rm~gas}$ play a role in lowering the sSFR in disks. A larger sample is required to confirm if the trend established in this work is representative of green valley as a whole.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present a new method for probing the hadronic interaction models at ultra-high energy and extracting details about mass composition. This is done using the time profiles of the signals recorded with the water-Cherenkov detectors of the Pierre Auger Observatory. The profiles arise from a mix of the muon and electromagnetic components of air-showers. Using the risetimes of the recorded signals we define a new parameter, which we use to compare our observations with predictions from simulations. We find, firstly, inconsistencies between our data and predictions over a greater energy range and with substantially more events than in previous studies. Secondly, by calibrating the new parameter with fluorescence measurements from observations made at the Auger Observatory, we can infer the depth of shower maximum for a sample of over 81,000 events extending from 0.3 EeV to over 100 EeV. Above 30 EeV, the sample is nearly fourteen times larger than currently available from fluorescence measurements and extending the covered energy range by half a decade. The energy dependence of the average depth of shower maximum is compared to simulations and interpreted in terms of the mean of the logarithmic mass. We find good agreement with previous work and extend the measurement of the mean depth of shower maximum to greater energies than before, reducing significantly the statistical uncertainty associated with the inferences about mass composition.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Splashback refers to the process of matter that is accreting onto a dark matter halo reaching its first orbital apocenter and turning around in its orbit. The cluster-centric radius at which this process occurs, r_sp, defines a halo boundary that is connected to the dynamics of the cluster. A rapid decline in the halo profile is expected near r_sp. We measure the galaxy number density and weak lensing mass profiles around redMaPPer galaxy clusters in the first year Dark Energy Survey (DES) data. For a cluster sample with mean M_200m mass ~2.5 x 10^14 M_sun, we find strong evidence of a splashback-like steepening of the galaxy density profile and measure r_sp=1.13 +/- 0.07 Mpc/h, consistent with earlier SDSS measurements of More et al. (2016) and Baxter et al. (2017). Moreover, our weak lensing measurement demonstrates for the first time the existence of a splashback-like steepening of the matter profile of galaxy clusters. We measure r_sp=1.34 +/- 0.21 Mpc/h from the weak lensing data, in good agreement with our galaxy density measurements. For different cluster and galaxy samples, we find that consistent with LCDM simulations, r_sp scales with R_200m and does not evolve with redshift over the redshift range of 0.3--0.6. We also find that potential systematic effects associated with the redMaPPer algorithm may impact the location of r_sp. We discuss progress needed to understand the systematic uncertainties and fully exploit forthcoming data from DES and future surveys, emphasizing the importance of more realistic mock catalogs and independent cluster samples.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present a study of NGC 4993, the host galaxy of the GW170817 gravitational wave event, the GRB170817A short gamma-ray burst (sGRB) and the AT2017gfo kilonova. We use Dark Energy Camera imaging, AAT spectra and publicly available data, relating our findings to binary neutron star (BNS) formation scenarios and merger delay timescales. NGC4993 is a nearby (40 Mpc) early-type galaxy, with $i$-band S\u00e9rsic index $n=4.0$ and low asymmetry ($A=0.04\\pm 0.01$). These properties are unusual for sGRB hosts. However, NGC4993 presents shell-like structures and dust lanes indicative of a recent galaxy merger, with the optical transient located close to a shell. We constrain the star formation history (SFH) of the galaxy assuming that the galaxy merger produced a star formation burst, but find little to no on-going star formation in either spatially-resolved broadband SED or spectral fitting. We use the best-fit SFH to estimate the BNS merger rate in this type of galaxy, as $R_{NSM}^{gal}= 5.7^{+0.57}_{-3.3} \\times 10^{-6} {\\rm yr}^{-1}$. If star formation is the only considered BNS formation scenario, the expected number of BNS mergers from early-type galaxies detectable with LIGO during its first two observing seasons is $0.038^{+0.004}_{-0.022}$, as opposed to $\\sim 0.5$ from all galaxy types. Hypothesizing that the binary system formed due to dynamical interactions during the galaxy merger, the subsequent time elapsed can constrain the delay time of the BNS coalescence. By using velocity dispersion estimates and the position of the shells, we find that the galaxy merger occurred $t_{\\rm mer}\\lesssim 200~{\\rm Myr}$ prior to the BNS coalescence.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The discovery of a kilonova (KN) associated with the Advanced LIGO (aLIGO)/Virgo event GW170817 opens up new avenues of multi-messenger astrophysics. Here, using realistic simulations, we provide estimates of the number of KNe that could be found in data from past, present and future surveys without a gravitational-wave trigger. For the simulation, we construct a spectral time-series model based on the DES-GW multi-band light-curve from the single known KN event, and we use an average of BNS rates from past studies of $10^3 \\rm{Gpc}^{-3}/\\rm{year}$, consistent with the $1$ event found so far. Examining past and current datasets from transient surveys, the number of KNe we expect to find for ASAS-SN, SDSS, PS1, SNLS, DES, and SMT is between 0 and $0.3$. We predict the number of detections per future survey to be: 8.3 from ATLAS, 10.6 from ZTF, 5.5/69 from LSST (the Deep Drilling / Wide Fast Deep), and 16.0 from WFIRST. The maximum redshift of KNe discovered for each survey is z = 0.8 for WFIRST, z = 0.25 for LSST and z = 0.04 for ZTF and ATLAS. For the LSST survey, we also provide contamination estimates from Type Ia and Core-collapse supernovae: after light-curve and template-matching requirements, we estimate a background of just 2 events. More broadly, we stress that future transient surveys should consider how to optimize their search strategies to improve their detection efficiency, and to consider similar analyses for GW follow-up programs.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The Advanced LIGO and Advanced Virgo observatories recently discovered gravitational waves from a binary neutron star inspiral. A short gamma-ray burst (GRB) that followed the merger of this binary was also recorded by the Fermi Gamma-ray Burst Monitor (Fermi-GBM), and the Anticoincidence Shield for the Spectrometer for the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), indicating particle acceleration by the source. The precise location of the event was determined by optical detections of emission following the merger. We searched for high-energy neutrinos from the merger in the GeV--EeV energy range using the ANTARES, IceCube, and Pierre Auger Observatories. No neutrinos directionally coincident with the source were detected within $\\pm500$ s around the merger time. Additionally, no MeV neutrino burst signal was detected coincident with the merger. We further carried out an extended search in the direction of the source for high-energy neutrinos within the 14-day period following the merger, but found no evidence of emission. We used these results to probe dissipation mechanisms in relativistic outflows driven by the binary neutron star merger. The non-detection is consistent with model predictions of short GRBs observed at a large off-axis angle.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "On 2017 August 17 the merger of two compact objects with masses consistent with two neutron stars was discovered through gravitational-wave (GW170817), gamma-ray (GRB 170817A), and optical (SSS17a/AT 2017gfo) observations. The optical source was associated with the early-type galaxy NGC 4993 at a distance of just $\\sim$40 Mpc, consistent with the gravitational-wave measurement, and the merger was localized to be at a projected distance of $\\sim$2 kpc away from the galaxy's center. We use this minimal set of facts and the mass posteriors of the two neutron stars to derive the first constraints on the progenitor of GW170817 at the time of the second supernova (SN). We generate simulated progenitor populations and follow the three-dimensional kinematic evolution from the binary neutron star (BNS) birth to the merger time, accounting for pre-SN galactic motion, for considerably different input distributions of the progenitor mass, pre-SN semimajor axis, and SN-kick velocity. Though not considerably tight, we find these constraints to be comparable to those for Galactic BNS progenitors. The derived constraints are very strongly influenced by the requirement of keeping the binary bound after the second SN and having the merger occur relatively close to the center of the galaxy. These constraints are insensitive to the galaxy's star formation history, provided the stellar populations are older than 1 Gyr.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The LIGO Scientific and Virgo Collaborations have announced the first detection of gravitational waves from the coalescence of two neutron stars. The merger rate of binary neutron stars estimated from this event suggests that distant, unresolvable binary neutron stars create a significant astrophysical stochastic gravitational-wave background. The binary neutron star background will add to the background from binary black holes, increasing the amplitude of the total astrophysical background relative to previous expectations. In the Advanced LIGO-Virgo frequency band most sensitive to stochastic backgrounds (near 25 Hz), we predict a total astrophysical background with amplitude $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.8_{-1.3}^{+2.7} \\times 10^{-9}$ with $90\\%$ confidence, compared with $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.1_{-0.7}^{+1.2} \\times 10^{-9}$ from binary black holes alone. Assuming the most probable rate for compact binary mergers, we find that the total background may be detectable with a signal-to-noise-ratio of 3 after 40 months of total observation time, based on the expected timeline for Advanced LIGO and Virgo to reach their design sensitivity.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The source of the gravitational-wave signal GW170817, very likely a binary neutron star merger, was also observed electromagnetically, providing the first multi-messenger observations of this type. The two week long electromagnetic counterpart had a signature indicative of an r-process-induced optical transient known as a kilonova. This Letter examines how the mass of the dynamical ejecta can be estimated without a direct electromagnetic observation of the kilonova, using gravitational-wave measurements and a phenomenological model calibrated to numerical simulations of mergers with dynamical ejecta. Specifically, we apply the model to the binary masses inferred from the gravitational-wave measurements, and use the resulting mass of the dynamical ejecta to estimate its contribution (without the effects of wind ejecta) to the corresponding kilonova light curves from various models. The distributions of dynamical ejecta mass range between $M_{ej} = 10^{-3} - 10^{-2} M_{\\odot}$ for various equations of state, assuming the neutron stars are rotating slowly. In addition, we use our estimates of the dynamical ejecta mass and the neutron star merger rates inferred from GW170817 to constrain the contribution of events like this to the r-process element abundance in the Galaxy when ejecta mass from post-merger winds is neglected. We find that if $\\gtrsim10\\%$ of the matter dynamically ejected from BNS mergers is converted to r-process elements, GW170817-like BNS mergers could fully account for the amount of r-process material observed in the Milky Way.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The detection of GW170817 in both gravitational waves and electromagnetic waves heralds the age of gravitational-wave multi-messenger astronomy. On 17 August 2017 the Advanced LIGO and Virgo detectors observed GW170817, a strong signal from the merger of a binary neutron-star system. Less than 2 seconds after the merger, a gamma-ray burst (GRB 170817A) was detected within a region of the sky consistent with the LIGO-Virgo-derived location of the gravitational-wave source. This sky region was subsequently observed by optical astronomy facilities, resulting in the identification of an optical transient signal within $\\sim 10$ arcsec of the galaxy NGC 4993. These multi-messenger observations allow us to use GW170817 as a standard siren, the gravitational-wave analog of an astronomical standard candle, to measure the Hubble constant. This quantity, which represents the local expansion rate of the Universe, sets the overall scale of the Universe and is of fundamental importance to cosmology. Our measurement combines the distance to the source inferred purely from the gravitational-wave signal with the recession velocity inferred from measurements of the redshift using electromagnetic data. This approach does not require any form of cosmic \"distance ladder;\" the gravitational wave analysis can be used to estimate the luminosity distance out to cosmological scales directly, without the use of intermediate astronomical distance measurements. We determine the Hubble constant to be $70.0^{+12.0}_{-8.0} \\, \\mathrm{km} \\, \\mathrm{s}^{-1} \\, \\mathrm{Mpc}^{-1}$ (maximum a posteriori and 68% credible interval). This is consistent with existing measurements, while being completely independent of them. Additional standard-siren measurements from future gravitational-wave sources will provide precision constraints of this important cosmological parameter.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present the Dark Energy Camera (DECam) discovery of the optical counterpart of the first binary neutron star merger detected through gravitational wave emission, GW170817. Our observations commenced 10.5 hours post-merger, as soon as the localization region became accessible from Chile. We imaged 70 deg$^2$ in the $i$ and $z$ bands, covering 93\\% of the initial integrated localization probability, to a depth necessary to identify likely optical counterparts (e.g., a kilonova). At 11.4 hours post-merger we detected a bright optical transient located $10.6''$ from the nucleus of NGC\\,4993 at redshift $z=0.0098$, consistent (for $H_0 = 70$\\, km s$^{-1}$ Mpc$^{-1}$) with the distance of $40 \\pm 8$\\, Mpc reported by the LIGO Scientific Collaboration and the Virgo Collaboration (LVC). At detection the transient had magnitudes $i\\approx 17.30$ and $z\\approx 17.45$, and thus an absolute magnitude of $M_i = -15.7$, in the luminosity range expected for a kilonova. We identified 1,500 potential transient candidates. Applying simple selection criteria aimed at rejecting background events such as supernovae, we find the transient associated with NGC\\,4993 as the only remaining plausible counterpart, and reject chance coincidence at the 99.5\\% confidence level. We therefore conclude that the optical counterpart we have identified near NGC\\,4993 is associated with GW170817. This discovery ushers in the era of multi-messenger astronomy with gravitational waves, and demonstrates the power of DECam to identify the optical counterparts of gravitational-wave sources.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We derive cosmological constraints from the probability distribution function (PDF) of evolved large-scale matter density fluctuations. We do this by splitting lines of sight by density based on their count of tracer galaxies, and by measuring both gravitational shear around and counts-in-cells in overdense and underdense lines of sight, in Dark Energy Survey (DES) First Year and Sloan Digital Sky Survey (SDSS) data. Our analysis uses a perturbation theory model (see companion paper Friedrich at al.) and is validated using N-body simulation realizations and log-normal mocks. It allows us to constrain cosmology, bias and stochasticity of galaxies w.r.t. matter density and, in addition, the skewness of the matter density field.\n  From a Bayesian model comparison, we find that the data weakly prefer a connection of galaxies and matter that is stochastic beyond Poisson fluctuations on <=20 arcmin angular smoothing scale. The two stochasticity models we fit yield DES constraints on the matter density $\u03a9_m=0.26^{+0.04}_{-0.03}$ and $\u03a9_m=0.28^{+0.05}_{-0.04}$ that are consistent with each other. These values also agree with the DES analysis of galaxy and shear two-point functions (3x2pt) that only uses second moments of the PDF. Constraints on $\u03c3_8$ are model dependent ($\u03c3_8=0.97^{+0.07}_{-0.06}$ and $0.80^{+0.06}_{-0.07}$ for the two stochasticity models), but consistent with each other and with the 3x2pt results if stochasticity is at the low end of the posterior range.\n  As an additional test of gravity, counts and lensing in cells allow to compare the skewness $S_3$ of the matter density PDF to its LCDM prediction. We find no evidence of excess skewness in any model or data set, with better than 25 per cent relative precision in the skewness estimate from DES alone.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present the largest catalogue of HI single dish observations of isolated galaxies to date and the corresponding HI scaling relations, as part of the multi-wavelength project AMIGA (Analysis of the interstellar Medium in Isolated GAlaxies). Despite numerous studies of the HI content of galaxies, no revision has been made for the most isolated L* galaxies since 1984. In total we have measurements or constraints on the HI masses of 844 galaxies from the Catalogue of Isolated Galaxies (CIG), obtained with our own observations at Arecibo, Effelsberg, Nancay and GBT, and spectra from the literature. Cuts are made to this sample to ensure isolation and a high level of completeness. We then fit HI scaling relations based on luminosity, optical diameter and morphology. Our regression model incorporates all the data, including upper limits, and accounts for uncertainties in both variables, as well as distance uncertainties. The scaling relation of HI mass with optical diameter is in good agreement with that of Haynes & Giovanelli 1984, but our relation with luminosity is considerably steeper. This is attributed to the large uncertainties in the luminosities, which introduce a bias when using OLS regression (used previously), and the different morphology distributions of the samples. We find that the main effect of morphology on the relations is to increase the intercept and flatten the slope towards later types. These trends were not evident in previous works due to the small number of detected early-type galaxies. The HI scaling relations of the AMIGA sample define an up-to-date metric of the HI content of almost \"nurture free\" galaxies. These relations allow the expected HI mass, in the absence of interactions, of a galaxy to be predicted to within 0.25 dex, and are thus suitable for use as statistical measures of the impact of interactions on the neutral gas content of galaxies. (Abridged)\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Cosmic rays are atomic nuclei arriving from outer space that reach the highest energies observed in nature. Clues to their origin come from studying the distribution of their arrival directions. Using $3 \\times 10^4$ cosmic rays above $8 \\times 10^{18}$ electron volts, recorded with the Pierre Auger Observatory from a total exposure of 76,800 square kilometers steradian year, we report an anisotropy in the arrival directions. The anisotropy, detected at more than the 5.2$\u03c3$ level of significance, can be described by a dipole with an amplitude of $6.5_{-0.9}^{+1.3}$% towards right ascension $\u03b1_{d} = 100 \\pm 10$ degrees and declination $\u03b4_{d} = -24_{-13}^{+12}$ degrees. That direction indicates an extragalactic origin for these ultra-high energy particles.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "List of contributions from the Cherenkov Telescope Array Consortium presented at the 35th International Cosmic Ray Conference, July 12-20 2017, Busan, Korea.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present a novel method to measure precisely the relative spectral response of the fluorescence telescopes of the Pierre Auger Observatory. We used a portable light source based on a xenon flasher and a monochromator to measure the relative spectral efficiencies of eight telescopes in steps of 5 nm from 280 nm to 440 nm. Each point in a scan had approximately 2 nm FWHM out of the monochromator. Different sets of telescopes in the observatory have different optical components, and the eight telescopes measured represent two each of the four combinations of components represented in the observatory. We made an end-to-end measurement of the response from different combinations of optical components, and the monochromator setup allowed for more precise and complete measurements than our previous multi-wavelength calibrations. We find an overall uncertainty in the calibration of the spectral response of most of the telescopes of 1.5% for all wavelengths; the six oldest telescopes have larger overall uncertainties of about 2.2%. We also report changes in physics measureables due to the change in calibration, which are generally small.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "The notion of mutual unbiasedness for coarse-grained measurements of quantum continuous variable systems is considered. It is shown that while the procedure of \"standard\" coarse graining breaks the mutual unbiasedness between conjugate variables, this desired feature can be theoretically established and experimentally observed in periodic coarse graining. We illustrate our results in an optics experiment implementing Fraunhofer diffraction through a periodic diffraction grating, finding excellent agreement with the derived theory. Our results are an important step in developing a formal connection between discrete and continuous variable quantum mechanics.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Contributions of the Pierre Auger Collaboration to the 35th International Cosmic Ray Conference (ICRC 2017), 12-20 July 2017, Bexco, Busan, Korea.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "We present chemical abundance measurements of three stars in the ultra-faint dwarf galaxy Horologium I, a Milky Way satellite discovered by the Dark Energy Survey. Using high resolution spectroscopic observations we measure the metallicity of the three stars as well as abundance ratios of several $\u03b1$-elements, iron-peak elements, and neutron-capture elements. The abundance pattern is relatively consistent among all three stars, which have a low average metallicity of [Fe/H] $\\sim -2.6$ and are not $\u03b1$-enhanced ([$\u03b1$/Fe] $\\sim 0.0$). This result is unexpected when compared to other low-metallicity stars in the Galactic halo and other ultra-faint dwarfs and hints at an entirely different mechanism for the enrichment of Hor I compared to other satellites. We discuss possible scenarios that could lead to this observed nucleosynthetic signature including extended star formation, a Population III supernova, and a possible association with the Large Magellanic Cloud.\n        \u25b3 Less", "author": "Daniel Sanchez"}, {"abstract": "Clustering is a fundamental tool in data mining. It partitions points into groups (clusters) and may be used to make decisions for each point based on its group. However, this process may harm protected (minority) classes if the clustering algorithm does not adequately represent them in desirable clusters -- especially if the data is already biased.\n  At NIPS 2017, Chierichetti et al. proposed a model for fair clustering requiring the representation in each cluster to (approximately) preserve the global fraction of each protected class. Restricting to two protected classes, they developed both a 4-approximation for the fair $k$-center problem and a $O(t)$-approximation for the fair $k$-median problem, where $t$ is a parameter for the fairness model. For multiple protected classes, the best known result is a 14-approximation for fair $k$-center.\n  We extend and improve the known results. Firstly, we give a 5-approximation for the fair $k$-center problem with multiple protected classes. Secondly, we propose a relaxed fairness notion under which we can give bicriteria constant-factor approximations for all of the classical clustering objectives $k$-center, $k$-supplier, $k$-median, $k$-means and facility location. The latter approximations are achieved by a framework that takes an arbitrary existing unfair (integral) solution and a fair (fractional) LP solution and combines them into an essentially fair clustering with a weakly supervised rounding scheme. In this way, a fair clustering can be established belatedly, in a situation where the centers are already fixed.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A search for $CP$ violation in the Cabibbo-suppressed $D^0 \\rightarrow K^+ K^- \u03c0^+ \u03c0^-$ decay mode is performed using an amplitude analysis. The measurement uses a sample of $pp$ collisions recorded by the LHCb experiment during 2011 and 2012, corresponding to an integrated luminosity of 3.0 fb$^{-1}$. The $D^0$ mesons are reconstructed from semileptonic $b$-hadron decays into $D^0\u03bc^- X$ final states. The selected sample contains more than 160000 signal decays, allowing the most precise amplitude modelling of this $D^0$ decay to date. The obtained amplitude model is used to perform the search for $CP$ violation. The result is compatible with $CP$ symmetry, with a sensitivity ranging from 1% to 15% depending on the amplitude considered.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The recent discovery by Advanced LIGO and Advanced Virgo of a gravitational wave signal from a binary neutron star inspiral has enabled tests of general relativity (GR) with this new type of source. This source, for the first time, permits tests of strong-field dynamics of compact binaries in presence of matter. In this paper, we place constraints on the dipole radiation and possible deviations from GR in the post-Newtonian coefficients that govern the inspiral regime. Bounds on modified dispersion of gravitational waves are obtained; in combination with information from the observed electromagnetic counterpart we can also constrain effects due to large extra dimensions. Finally, the polarization content of the gravitational wave signal is studied. The results of all tests performed here show good agreement with GR.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "Astrophysical sources of gravitational waves, such as binary neutron star and black hole mergers or core-collapse supernovae, can drive relativistic outflows, giving rise to non-thermal high-energy emission. High-energy neutrinos are signatures of such outflows. The detection of gravitational waves and high-energy neutrinos from common sources could help establish the connection between the dynamics of the progenitor and the properties of the outflow. We searched for associated emission of gravitational waves and high-energy neutrinos from astrophysical transients with minimal assumptions using data from Advanced LIGO from its first observing run O1, and data from the ANTARES and IceCube neutrino observatories from the same time period. We focused on candidate events whose astrophysical origin could not be determined from a single messenger. We found no significant coincident candidate, which we used to constrain the rate density of astrophysical sources dependent on their gravitational wave and neutrino emission processes.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The first measurement of heavy-flavour production by the LHCb experiment in its fixed-target mode is presented. The production of $J/\u03c8$ and $D^0$ mesons is studied with beams of protons of different energies colliding with gaseous targets of helium and argon with nucleon-nucleon centre-of-mass energies of $\\sqrt{s_{NN}} = 86.6 $ and $ 110.4$ ${\\rm GeV}$, respectively. The $J/\u03c8$ and $D^0$ production cross-sections in $p{\\rm He}$ collisions in the rapidity range $[2,4.6]$ are found to be $\u03c3_{J/\u03c8} = 652 \\pm 33$ (stat) $\\pm 42$ (syst) nb$/$nucleon and $\u03c3_{D^0} = 80.8 \\pm 2.4$ (stat) $\\pm 6.3$ (syst) $\u03bc$b$/$nucleon, where the first uncertainty is statistical and the second is systematic. No evidence for a substantial intrinsic charm content of the nucleon is observed in the large Bjorken-$x$ region.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The production of $\u03a5(nS)$ mesons ($n=1,2,3$) in $p$Pb and Pb$p$ collisions at a centre-of-mass energy per nucleon pair $\\sqrt{s_{NN}}=8.16$ TeV is measured by the LHCb experiment, using a data sample corresponding to an integrated luminosity of 31.8 nb$^{-1}$. The $\u03a5(nS)$ mesons are reconstructed through their decays into two opposite-sign muons. The measurements comprise the differential production cross-sections of the $\u03a5(1S)$ and $\u03a5(2S)$ states, their forward-to-backward ratios and nuclear modification factors, performed as a function of the transverse momentum \\pt and rapidity in the nucleon-nucleon centre-of-mass frame $y^*$ of the $\u03a5(nS)$ states, in the kinematic range $p_{\\rm{T}}<25$ GeV/$c$ and $1.5<y^*<4.0$ ($-5.0<y^*<-2.5$) for $p$Pb (Pb$p$) collisions. In addition, production cross-sections for $\u03a5(3S)$ are measured integrated over phase space and the production ratios between all three $\u03a5(nS)$ states are determined. The measurements are compared to theoretical predictions and suppressions for quarkonium in $p$Pb collisions are observed.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A measurement of the charm-mixing parameter $y_{CP}$ using $D^0 \\to K^+ K^-$, $D^0 \\to \u03c0^+ \u03c0^-$, and $D^0 \\to K^- \u03c0^+$ decays is reported. The $D^0$ mesons are required to originate from semimuonic decays of $B^-$ and $\\overline{B}^0$ mesons. These decays are partially reconstructed in a data set of proton-proton collisions at center-of-mass energies of 7 and 8 TeV collected with the LHCb experiment and corresponding to an integrated luminosity of 3 fb$^{-1}$. The $y_{CP}$ parameter is measured to be $(0.57 \\pm 0.13(\\rm{stat.}) \\pm 0.09(\\rm{syst.}))\\%$, in agreement with, and as precise as, the current world-average value.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "In this paper, we present our approach for solving the DEBS Grand Challenge 2018. The challenge asks to provide a prediction for (i) a destination and the (ii) arrival time of ships in a streaming-fashion using Geo-spatial data in the maritime context. Novel aspects of our approach include the use of ensemble learning based on Random Forest, Gradient Boosting Decision Trees (GBDT), XGBoost Trees and Extremely Randomized Trees (ERT) in order to provide a prediction for a destination while for the arrival time, we propose the use of Feed-forward Neural Networks. In our evaluation, we were able to achieve an accuracy of 97% for the port destination classification problem and 90% (in mins) for the ETA prediction.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The branching fractions of the doubly Cabibbo-suppressed decays $D^+\\rightarrow K^-K^+K^+$, $D^+\\rightarrow \u03c0^-\u03c0^+K^+$ and $D^+_s\\rightarrow\u03c0^-K^+K^+$ are measured using the decays $D^+\\rightarrow K^-\u03c0^+\u03c0^+$ and $D^+_s\\rightarrow K^-K^+\u03c0^+$ as normalisation channels. The measurements are performed using proton-proton collision data collected with the LHCb detector at a centre-of-mass energy of 8 TeV, corresponding to an integrated luminosity of 2.0 fb$^{-1}$. The results are\n  \\begin{align}\n  \\frac {\\mathcal{B}(D^+\\rightarrow K^-K^+K^+)} {\\mathcal{B}(D^+\\rightarrow K^-\u03c0^+\u03c0^+)}& = (6.541 \\pm 0.025 \\pm 0.042) \\times 10^{-4},\\nonumber\n  \\frac {\\mathcal{B}(D^+\\rightarrow \u03c0^-\u03c0^+K^+)} {\\mathcal{B}(D^+\\rightarrow K^-\u03c0^+\u03c0^+)}& = (5.231 \\pm 0.009 \\pm 0.023) \\times 10^{-3}, \\nonumber\n  \\frac {\\mathcal{B}(D^+_s\\rightarrow\u03c0^-K^+K^+)} {\\mathcal{B}(D^+_s\\rightarrow K^-K^+\u03c0^+)}& = (2.372 \\pm 0.024 \\pm 0.025) \\times 10^{-3},\\nonumber\n  \\end{align} where the uncertainties are statistical and systematic, respectively. These are the most precise measurements up to date.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "We present a search for prompt gamma-ray counterparts to compact binary coalescence gravitational wave (GW) candidates from Advanced LIGO's first observing run (O1). As demonstrated by the multimessenger observations of GW170817/GRB 170817A, electromagnetic and GW observations provide complementary information about the astrophysical source and, in the case of weaker candidates, may strengthen the case for an astrophysical origin. Here we investigate low-significance GW candidates from the O1 compact-binary coalescence searches using the Fermi Gamma-ray Burst Monitor (GBM), leveraging its all-sky and broad energy coverage. Candidates are ranked and compared to background to measure significance. Those with false alarm rates of less than 10^-5 Hz (about one per day) are used as the search sample for gamma-ray follow-up. No GW candidates were found to be coincident with gamma-ray transients independently identified by blind searches of the GBM data. In addition, GW candidate event times were followed up by a separate targeted search of GBM data. Among the resulting GBM events, the two with lowest false alarm rates were the gamma-ray transient GW150914-GBM presented in Connaughton et al. (2016) and a solar flare in chance coincidence with a GW candidate.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "One unanswered question about the binary neutron star coalescence GW170817 is the nature of its post-merger remnant. A previous search for post-merger gravitational waves targeted high-frequency signals from a possible neutron star remnant with a maximum signal duration of 500 s. Here we revisit the neutron star remnant scenario with a focus on longer signal durations up until the end of the Second Advanced LIGO-Virgo Observing run, 8.5 days after the coalescence of GW170817. The main physical scenario for such emission is the power-law spindown of a massive magnetar-like remnant. We use four independent search algorithms with varying degrees of restrictiveness on the signal waveformand different ways of dealing with noise artefacts. In agreement with theoretical estimates, we find no significant signal candidates. Through simulated signals, we quantify that with the current detector sensitivity, nowhere in the studied parameter space are we sensitive to a signal from more than 1 Mpc away, compared to the actual distance of 40 Mpc. This study however serves as a prototype for post-merger analyses in future observing runs with expected higher sensitivity.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The first observation of two structures consistent with resonances in the final states $\u039b_b^0 \u03c0^-$ and $\u039b_b^0 \u03c0^+$ is reported using samples of $pp$ collision data collected by the LHCb experiment at $\\sqrt{s} = 7$ and $8$ TeV, corresponding to an integrated luminosity of 3 $\\mathrm{fb}^{-1}$. The ground states $\u03a3_b^\\pm$ and $\u03a3_b^{*\\pm}$ are also confirmed and their masses and widths are precisely measured.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A Dalitz plot analysis of $B^0 \\to \u03b7_c(1S) K^+\u03c0^-$ decays is performed using data samples of $pp$ collisions collected with the LHCb detector at centre-of-mass energies of $\\sqrt{s}=7,~8$ and $13$ TeV, corresponding to a total integrated luminosity of $4.7~\\text{fb}^{-1}$. A satisfactory description of the data is obtained when including a contribution representing an exotic $\u03b7_c(1S) \u03c0^-$ resonant state. The significance of this exotic resonance is more than three standard deviations, while its mass and width are $4096 \\pm 20~^{+18}_{-22}$ MeV and $152 \\pm 58~^{+60}_{-35}$ MeV, respectively. The spin-parity assignments $J^P=0^+$ and $J^{P}=1^-$ are both consistent with the data. In addition, the first measurement of the $B^0 \\to \u03b7_c(1S) K^+\u03c0^-$ branching fraction is performed and gives $\\displaystyle \\mathcal{B}(B^0 \\to \u03b7_c(1S) K^+\u03c0^-) = (5.73 \\pm 0.24 \\pm 0.13 \\pm 0.66) \\times 10^{-4}$, where the first uncertainty is statistical, the second systematic, and the third is due to limited knowledge of external branching fractions.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The production of $\u039b^+_c$ baryons produced directly at the interacting point is studied in proton-lead collisions collected with the LHCb detector at the LHC. The data sample corresponds to an integrated luminosity of $1.58\\mathrm{nb}^{-1}$ recorded at a nucleon-nucleon centre-of-mass energy of $\\sqrt{s_{NN}}=5.02$ TeV. Measurements of the differential cross-section and the forward-backward production ratio are reported for $\u039b^+_c$ baryons with transverse momenta in the range $2<p_{T}<10$GeV/$c$ and rapidities in the ranges $1.5<y^*<4.0$ and $-4.5<y^*<-2.5$ in the nucleon-nucleon centre-of-mass system. The ratio of cross-sections of $\u039b^+_c$ baryons and $D^0$ mesons is also reported. The results are compared with next-to-leading order calculations that use nuclear parton distribution functions.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The LHCb Upgrade II will fully exploit the flavour-physics opportunities of the HL-LHC, and study additional physics topics that take advantage of the forward acceptance of the LHCb spectrometer. The LHCb Upgrade I will begin operation in 2020. Consolidation will occur, and modest enhancements of the Upgrade I detector will be installed, in Long Shutdown 3 of the LHC (2025) and these are discussed here. The main Upgrade II detector will be installed in long shutdown 4 of the LHC (2030) and will build on the strengths of the current LHCb experiment and the Upgrade I. It will operate at a luminosity up to $ 2 \\times 10^{34} \\rm cm^{-2}s^{-1}$, ten times that of the Upgrade I detector. New detector components will improve the intrinsic performance of the experiment in certain key areas. An Expression Of Interest proposing Upgrade II was submitted in February 2017. The physics case for the Upgrade II is presented here in more depth. $CP$-violating phases will be measured with precisions unattainable at any other envisaged facility. The experiment will probe $b\\to s \\ell^+\\ell^-$ and $b\\to d \\ell^+\\ell^-$ transitions in both muon and electron decays in modes not accessible at Upgrade I. Minimal flavour violation will be tested with a precision measurement of the ratio of $B(B^0\\to\u03bc^+\u03bc^-)/B(B_s^0\\to \u03bc^+\u03bc^-)$. Probing charm $CP$ violation at the $10^{-5}$ level may result in its long sought discovery. Major advances in hadron spectroscopy will be possible, which will be powerful probes of low energy QCD. Upgrade II potentially will have the highest sensitivity of all the LHC experiments on the Higgs to charm-quark couplings. Generically, the new physics mass scale probed, for fixed couplings, will almost double compared with the pre-HL-LHC era; this extended reach for flavour physics is similar to that which would be achieved by the HE-LHC proposal for the energy frontier.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "We analyze the impact of a proposed tidal instability coupling $p$-modes and $g$-modes within neutron stars on GW170817. This non-resonant instability transfers energy from the orbit of the binary to internal modes of the stars, accelerating the gravitational-wave driven inspiral. We model the impact of this instability on the phasing of the gravitational wave signal using three parameters per star: an overall amplitude, a saturation frequency, and a spectral index. Incorporating these additional parameters, we compute the Bayes Factor ($\\ln B^{pg}_{!pg}$) comparing our $p$-$g$ model to a standard one. We find that the observed signal is consistent with waveform models that neglect $p$-$g$ effects, with $\\ln B^{pg}_{!pg} = 0.03^{+0.70}_{-0.58}$ (maximum a posteriori and 90% credible region). By injecting simulated signals that do not include $p$-$g$ effects and recovering them with the $p$-$g$ model, we show that there is a $\\simeq 50\\%$ probability of obtaining similar $\\ln B^{pg}_{!pg}$ even when $p$-$g$ effects are absent. We find that the $p$-$g$ amplitude for 1.4 $M_\\odot$ neutron stars is constrained to $\\lesssim \\text{few}\\times10^{-7}$, with maxima a posteriori near $\\sim 10^{-7}$ and $p$-$g$ saturation frequency $\\sim 70\\, \\mathrm{Hz}$. This suggests that there are less than a few hundred excited modes, assuming they all saturate by wave breaking. For comparison, theoretical upper bounds suggest a $p$-$g$ amplitude $\\lesssim 10^{-6}$ and $\\lesssim 10^{3}$ modes saturating by wave breaking. Thus, the measured constraints only rule out extreme values of the $p$-$g$ parameters. They also imply that the instability dissipates $\\lesssim 10^{51}\\, \\mathrm{ergs}$ over the entire inspiral, i.e., less than a few percent of the energy radiated as gravitational waves.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A search is presented for a Higgs-like boson with mass in the range 45 to 195 GeV/$c^2$ decaying into a muon and a tau lepton. The dataset consists of proton-proton interactions at a centre-of-mass energy of 8 TeV, collected by the LHCb experiment, corresponding to an integrated luminosity of 2 fb$^{-1}$. The tau leptons are reconstructed in both leptonic and hadronic decay channels. An upper limit on the production cross-section multiplied by the branching fraction at 95% confidence level is set and ranges from 22 pb for a boson mass of 45 GeV/$c^2$ to 4 pb for a mass of 195 GeV/$c^2$.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The cross-section for prompt antiproton production in collisions of protons with an energy of $6.5$ TeV incident on helium nuclei at rest is measured with the LHCb experiment from a data set corresponding to an integrated luminosity of $0.5\\,nb^{-1}$. The target is provided by injecting helium gas into the LHC beam line at the LHCb interaction point. The reported results, covering antiproton momenta between $12$ and $110\\,\\mathrm{GeV/}c$, represent the first direct determination of the antiproton production cross-section in ${\\rm p He}$ collisions, and impact the interpretation of recent results on antiproton cosmic rays from space-borne experiments.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "We present the first Advanced LIGO and Advanced Virgo search for ultracompact binary systems with component masses between 0.2 $M_\\odot$ - 1.0 $M_\\odot$ using data taken between September 12, 2015 and January 19, 2016. We find no viable gravitational wave candidates. Our null result constrains the coalescence rate of monochromatic (delta function) distributions of non-spinning (0.2 $M_\\odot$, 0.2 $M_\\odot$) ultracompact binaries to be less than $1.0 \\times 10^6 \\text{Gpc}^{-3} \\text{yr}^{-1}$ and the coalescence rate of a similar distribution of (1.0 $M_\\odot$, 1.0 $M_\\odot$) ultracompact binaries to be less than $1.9 \\times 10^4 \\text{Gpc}^{-3} \\text{yr}^{-1}$ (at 90 percent confidence). Neither black holes nor neutron stars are expected to form below ~ 1 solar mass through conventional stellar evolution, though it has been proposed that similarly low mass black holes could be formed primordially through density fluctuations in the early universe. Under a particular primordial black hole binary formation scenario, we constrain monochromatic primordial black hole populations of 0.2 $M_\\odot$ to be less than $33\\%$ of the total dark matter density and monochromatic populations of 1.0 $M_\\odot$ to be less than $5\\%$ of the dark matter density. The latter strengthens the presently placed bounds from micro-lensing surveys of MAssive Compact Halo Objects (MACHOs) provided by the MACHO and EROS collaborations.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "(abridged) Using the particularly long gravitational microlensing event OGLE-2014-BLG-1186 with a time-scale $t_\\mathrm{E}$ ~ 300 d, we present a methodology for identifying the nature of localised deviations from single-lens point-source light curves, which ensures that 1) the claimed signal is substantially above the noise floor, 2) the inferred properties are robustly determined and their estimation not subject to confusion with systematic noise in the photometry, 3) there are no alternative viable solutions within the model framework that might have been missed. Annual parallax and binarity could be separated and robustly measured from the wing and the peak data, respectively. We find matching model light curves that involve either a binary lens or a binary source. Our binary-lens models indicate a planet of mass $M_2$ = (45 $\\pm$ 9) $M_\\oplus$, orbiting a star of mass $M_1$ = (0.35 $\\pm$ 0.06) $M_\\odot$, located at a distance $D_\\mathrm{L}$ = (1.7 $\\pm$ 0.3) kpc from Earth, whereas our binary-source models suggest a brown-dwarf lens of $M$ = (0.046 $\\pm$ 0.007) $M_\\odot$, located at a distance $D_\\mathrm{L}$ = (5.7 $\\pm$ 0.9) kpc, with the source potentially being a (partially) eclipsing binary involving stars predicted to be of similar colour given the ratios between the luminosities and radii. The ambiguity in the interpretation would be resolved in favour of a lens binary by observing the luminous lens star separating from the source at the predicted proper motion of $\u03bc$ = (1.6 $\\pm$ 0.3) mas yr$^{-1}$, whereas it would be resolved in favour of a source binary if the source could be shown to be a (partially) eclipsing binary matching the obtained model parameters. We experienced that close binary source stars pose a challenge for claiming the detection of planets by microlensing in events where the source passes very close to the lens star hosting the planet.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "An analysis of the angular distribution of the decay $\u039b_b^0 \\rightarrow \u039b\u03bc^{+} \u03bc^{-}$ is presented, using data collected with the LHCb detector between 2011 and 2016 and corresponding to an integrated luminosity of approximately $5\\,fb^{-1}$. Angular observables are determined using a moment analysis of the angular distribution at low hadronic recoil, corresponding to the dimuon invariant mass squared range $15 < q^{2} < 20\\, GeV^2/c^4$. The full basis of observables is measured for the first time. The lepton-side, hadron-side and combined forward-backward asymmetries of the decay are determined to be \\begin{align} A_{FB}^{l} & = -0.39 \\pm 0.04\\,\\rm{stat} \\pm 0.01\\, \\rm{syst}, \\nonumber\\\\ A_{FB}^{h} & = -0.30 \\pm 0.05\\,\\rm{stat} \\pm 0.02\\, \\rm{syst}, \\nonumber\\\\ A_{FB}^{lh} & = +0.25 \\pm 0.04\\,\\rm{stat} \\pm 0.01\\, \\rm{syst}. \\nonumber \\end{align} The measurements are consistent with Standard Model predictions.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The decay of the narrow resonance $\\overline{B}{}_{s2}^{*0}\\!\\rightarrow B^- K^+$ can be used to determine the $B^-$ momentum in partially reconstructed decays without any assumptions on the decay products of the $B^-$ meson. This technique is employed for the first time to distinguish contributions from $D^0$, $D^{*0}$, and higher-mass charmed states ($D^{**0}$) in semileptonic $B^-$ decays by using the missing-mass distribution. The measurement is performed using a data sample corresponding to an integrated luminosity of 3.0 fb${}^{-1}$ collected with the LHCb detector in $pp$ collisions at center-of-mass energies of 7 and 8 TeV. The resulting branching fractions relative to the inclusive $B^- \\!\\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc$ are $f_{D^0} = \\mathcal{B}( B^- \\rightarrow D^0\u03bc^-\\overline\u03bd_\u03bc)/\\mathcal{B}( B^- \\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc) = 0.25 \\pm 0.06$, $f_{D^{**0}} = \\mathcal{B}( B^- \\rightarrow ( D^{**0} \\rightarrow D^0 X)\u03bc^-\\overline\u03bd_\u03bc)/\\mathcal{B}( B^- \\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc) = 0.21 \\pm 0.07$, with $f_{D^{*0}} = 1 - f_{D^0} - f_{D^{**0}}$ making up the remainder.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A search for $C\\!P$ violation in $\u039b^0_b \\to p K^-$ and $\u039b^0_b \\to p \u03c0^-$ decays is presented using a sample of $pp$ collisions collected with the LHCb detector and corresponding to an integrated luminosity of 3.0 fb$^{-1}$. The $C\\!P$-violating asymmetries are measured to be $A_{\\mathrm{CP}}^{pK^-} = -0.020 \\pm 0.013\\pm 0.019$ and $A_{\\mathrm{CP}}^{p\u03c0^-} = -0.035 \\pm 0.017 \\pm 0.020 $, and their difference $A_{\\mathrm{CP}}^{pK^-}-A_{\\mathrm{CP}}^{p\u03c0^-} = 0.014 \\pm 0.022 \\pm 0.010$, where the first uncertainties are statistical and the second systematic. These are the most precise measurements of such asymmetries to date.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "We report a measurement of the lifetime of the $\u03a9_c^0$ baryon using proton-proton collision data at center-of-mass energies of 7 and 8~TeV, corresponding to an integrated luminosity of 3.0 fb$^{-1}$ collected by the LHCb experiment. The sample consists of about 1000 $\u03a9_b^-\\to\u03a9_c^0\u03bc^-\\bar\u03bd_\u03bc X$ signal decays, where the $\u03a9_c^0$ baryon is detected in the $pK^-K^-\u03c0^+$ final state and $X$ represents possible additional undetected particles in the decay. The $\u03a9_c^0$ lifetime is measured to be $\u03c4_{\u03a9_c^0} = 268\\pm24\\pm10\\pm2$ fs, where the uncertainties are statistical, systematic, and from the uncertainty in the $D^+$ lifetime, respectively. This value is nearly four times larger than, and inconsistent with, the current world-average value.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The doubly charmed baryon decay $\u039e_{cc}^{++} \\rightarrow \u039e_{c}^{+} \u03c0^{+}$ is observed for the first time, with a statistical significance of $5.9\u03c3$, confirming a recent observation of the baryon in the $\u039b_c^{+} K^{-} \u03c0^{+} \u03c0^{+}$ final state. The data sample used corresponds to an integrated luminosity of $1.7\\,\\mathrm{fb}^{-1}$, collected by the LHCb experiment in $pp$ collisions at a center-of-mass energy of $13\\mathrm{\\,Te\\kern -0.1em V}$. The $\u039e_{cc}^{++}$ mass is measured to be\n  \\begin{equation}\\nonumber\n  3620.6\\pm 1.5~(\\text{stat})\\pm 0.4~(\\text{syst}) \\pm 0.3~(\u039e_{c}^{+})~\\text{MeV}/\\it{c}^{2},\n  \\end{equation}\n  and is consistent with the previous result. The ratio of branching fractions between the decay modes is measured to be\n  \\begin{equation}\\nonumber\n  \\frac{\\mathcal{B} (\u039e_{cc}^{++} \\rightarrow \u039e_{c}^{+} \u03c0^{+}) \\times \\mathcal{B}(\u039e_{c}^{+} \\rightarrow pK^{-}\u03c0^{+})}\n  {\\mathcal{B} (\u039e_{cc}^{++} \\rightarrow \u039b_c^{+} K^{-} \u03c0^{+} \u03c0^{+}) \\times \\mathcal{B}(\u039b_c^{+} \\rightarrow pK^{-}\u03c0^{+})}\n  = 0.035\\pm 0.009~(\\text{stat}) \\pm 0.003~(\\text{syst}).\n  \\end{equation}\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The first observation of the $B_s^0 \\to \\overline{D}^{*0} \u03c6$ decay is reported, with a significance of more than seven standard deviations, from an analysis of $pp$ collision data corresponding to an integrated luminosity of 3 fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of $7$ and $8$ TeV. The branching fraction is measured relative to that of the topologically similar decay $B^0 \\to \\overline{D}^0 \u03c0^+\u03c0^-$ and is found to be $\\mathcal{B}(B_s^0 \\to \\overline{D}^{*0} \u03c6) = (3.7 \\pm 0.5 \\pm 0.3 \\pm 0.2) \\times 10^{-5}$, where the first uncertainty is statistical, the second systematic, and the third from the branching fraction of the $B^0 \\to \\overline{D}^0 \u03c0^+\u03c0^-$ decay. The fraction of longitudinal polarisation in this decay is measured to be ${f_{\\rm L} =(73 \\pm 15 \\pm 3)\\%}$. The most precise determination of the branching fraction for the $B_s^0 \\to \\overline{D}^{0} \u03c6$ decay is also obtained, $\\mathcal{B}(B_s^0 \\to \\overline{D}^{0} \u03c6) = (3.0 \\pm 0.3 \\pm 0.2 \\pm 0.2) \\times 10^{-5}$. An upper limit, $\\mathcal{B}(B^0 \\to \\overline{D}^{0} \u03c6) < 2.0 \\ (2.2) \\times 10^{-6}$ at $90\\%$ (95\\%) confidence level is set. A constraint on the $\u03c9-\u03c6$ mixing angle $\u03b4$ is set at $|\u03b4| < 5.2^\\circ~ (5.5^\\circ)$ at $90\\%$ ($95\\%$) confidence level.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The first observation of the $B_s^0 \\to \\overline{D}^0 K^+ K^-$ decay is reported, together with the most precise branching fraction measurement of the mode $B^0 \\to \\overline{D}^0 K^+ K^-$. The results are obtained from an analysis of $pp$ collision data corresponding to an integrated luminosity of $3.0~\\textrm{fb}^{-1}$. The data were collected with the LHCb detector at centre-of-mass energies of $7$ and $8$ TeV. The branching fraction of the $B^0 \\to \\overline{D}^0 K^+ K^-$ decay is measured relative to that of the decay $B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-$ to be $$\\frac{\\mathcal{B}(B^0 \\to \\overline{D}^0 K^+ K^-)}{\\mathcal{B}(B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-)} = (6.9 \\pm 0.4 \\pm 0.3)\\%,$$ where the first uncertainty is statistical and the second is systematic. The measured branching fraction of the $B_s^0 \\to \\overline{D}^0 K^+ K^-$ decay mode relative to that of the corresponding $B^0$ decay is $$\\frac{\\mathcal{B}(B_s^0 \\to \\overline{D}^0 K^+ K^-)}{\\mathcal{B}(B^0 \\to \\overline{D}^0 K^+ K^-)} = (93.0 \\pm 8.9 \\pm 6.9)\\%.$$ Using the known branching fraction of ${B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-}$, the values of ${{\\mathcal B}(B^0 \\to \\overline{D}^0 K^+ K^- )=(6.1 \\pm 0.4 \\pm 0.3 \\pm 0.3) \\times 10^{-5}}$, and ${{\\cal B}(B_s^0 \\to \\overline{D}^0 K^+ K^-)=}$ $(5.7 \\pm 0.5 \\pm 0.4 \\pm 0.5) \\times 10^{-5}$ are obtained, where the third uncertainties arise from the branching fraction of the decay modes ${B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-}$ and $B^0 \\to \\overline{D}^0 K^+ K^-$, respectively.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The first measurements of the forward-backward asymmetry of the dimuon pair ($A_{FB}$), the triple-product asymmetry ($A_{2\u03c6}$), and the charge-parity-conjugation asymmetry ($A_{CP}$), in $D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-$ and $D^0\\to K^+K^-\u03bc^+\u03bc^-$ decays are reported. They are performed using data from proton-proton collisions collected with the LHCb experiment from 2011 to 2016, corresponding to a total integrated luminosity of 5 fb$^{-1}$. The asymmetries are measured to be \\begin{align*} A_{FB}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-) &= (\\phantom{-}3.3\\pm3.7\\pm0.6)\\%,\\\\ A_{2\u03c6}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-)&= (-0.6\\pm3.7\\pm0.6)\\%,\\\\ A_{CP}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-) &= (\\phantom{-}4.9\\pm3.8\\pm0.7)\\%,\\\\ A_{FB}(D^0\\to K^+K^-\u03bc^+\u03bc^-) &= (0\\pm11\\pm2)\\%,\\\\ A_{2\u03c6}(D^0\\to K^+K^-\u03bc^+\u03bc^-)&= (9\\pm11\\pm1)\\%,\\\\ A_{CP}(D^0\\to K^+K^-\u03bc^+\u03bc^-) &= (0\\pm11\\pm2)\\%, \\end{align*} where the first uncertainty is statistical and the second systematic. The asymmetries are also measured as a function of the dimuon invariant mass. The results are consistent with the Standard Model predictions.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The $\\overline{B_s^0} \\rightarrow \u03c7_{c2} K^+ K^- $ decay mode is observed and its branching fraction relative to the corresponding $\u03c7_{c1}$ decay mode, in a $\\pm 15 \\textrm{MeV}/c^2$ window around the $\u03c6$ mass, is found to be $\\frac{\\mathcal{B}(\\overline{B_s^0} \\rightarrow \u03c7_{c2} K^+ K^-) }{ \\mathcal{B}(\\overline{B_s^0} \\rightarrow \u03c7_{c1} K^+ K^-)} = (17.1 \\pm 3.1 \\pm 0.4 \\pm 0.9)\\%,$ where the first uncertainty is statistical, the second systematic and the third due to the knowledge of the branching fractions of radiative $\u03c7_c$ decays. The decay mode $\\overline{B_s^0} \\rightarrow \u03c7_{c1} K^+ K^- $ allows the $ B_s^0$ mass to be measured as $m(B_s^0) = 5366.83 \\pm 0.25 \\pm 0.27 \\, \\textrm{MeV}/c^2,$ where the first uncertainty is statistical and the second systematic. A combination of this result with other LHCb determinations of the $B_s^0$ mass is made.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The $\u03a5(1S)\u03bc^+\u03bc^-$ invariant-mass distribution is investigated for a possible exotic meson state composed of two $b$ quarks and two $\\overline{b}$ quarks, $X_{b\\overline{b}b\\overline{b}}$. The analysis is based on a data sample of $pp$ collisions recorded with the LHCb detector at centre-of-mass energies $\\sqrt{s} =$ 7, 8 and 13 TeV, corresponding to an integrated luminosity of 6.3 fb$^{-1}$. No significant excess is found, and upper limits are set on the product of the production cross-section and the branching fraction as functions of the mass of the $X_{b\\overline{b}b\\overline{b}}$ state. The limits are set in the fiducial volume where all muons have pseudorapidity in the range $[2.0,5.0]$, and the $X_{b\\overline{b}b\\overline{b}}$ state has rapidity in the range $[2.0,4.5]$ and transverse momentum less than 15 GeV/$c$.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "Magnetic resonance (MR) imaging offers a wide variety of imaging techniques. A large amount of data is created per examination which needs to be checked for sufficient quality in order to derive a meaningful diagnosis. This is a manual process and therefore time- and cost-intensive. Any imaging artifacts originating from scanner hardware, signal processing or induced by the patient may reduce the image quality and complicate the diagnosis or any image post-processing. Therefore, the assessment or the ensurance of sufficient image quality in an automated manner is of high interest. Usually no reference image is available or difficult to define. Therefore, classical reference-based approaches are not applicable. Model observers mimicking the human observers (HO) can assist in this task. Thus, we propose a new machine-learning-based reference-free MR image quality assessment framework which is trained on HO-derived labels to assess MR image quality immediately after each acquisition. We include the concept of active learning and present an efficient blinded reading platform to reduce the effort in the HO labeling procedure. Derived image features and the applied classifiers (support-vector-machine, deep neural network) are investigated for a cohort of 250 patients. The MR image quality assessment framework can achieve a high test accuracy of 93.7$\\%$ for estimating quality classes on a 5-point Likert-scale. The proposed MR image quality assessment framework is able to provide an accurate and efficient quality estimation which can be used as a prospective quality assurance including automatic acquisition adaptation or guided MR scanner operation, and/or as a retrospective quality assessment including support of diagnostic decisions or quality control in cohort studies.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The Cabibbo-suppressed decay $\u039b^0_b\\rightarrow\u03c8(2S)p\u03c0^-$ is observed for the first time using a data sample collected by the LHCb experiment in proton-proton collisions corresponding to 1.0, 2.0 and 1.9fb$^{-1}$ of integrated luminosity at centre-of-mass energies of 7, 8 and 13TeV, respectively. The $\u03c8(2S)$ mesons are reconstructed in the $\u03bc^+\u03bc^-$ final state. The~branching fraction with respect to that of the $\u039b^0_b\\rightarrow\u03c8(2S)pK^-$ decay mode is measured to be $$\\frac{\\mathcal{B}\\left(\u039b^0_b\\rightarrow\u03c8(2S)p\u03c0^- \\right)} {\\mathcal{B}\\left(\u039b^0_b\\rightarrow\u03c8(2S)pK^-\\right)}=\\left(11.4 \\pm 1.3 \\pm 0.2\\right)\\!\\%\\,,$$ where the first uncertainty is statistical and the second is systematic. The $\u03c8(2S)p$ and $\u03c8(2S)\u03c0^-$ mass spectra are investigated and no evidence for exotic resonances is found.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The grand challenges of contemporary fundamental physics---dark matter, dark energy, vacuum energy, inflation and early universe cosmology, singularities and the hierarchy problem---all involve gravity as a key component. And of all gravitational phenomena, black holes stand out in their elegant simplicity, while harbouring some of the most remarkable predictions of General Relativity: event horizons, singularities and ergoregions. The hitherto invisible landscape of the gravitational Universe is being unveiled before our eyes: the historical direct detection of gravitational waves by the LIGO-Virgo collaboration marks the dawn of a new era of scientific exploration. Gravitational-wave astronomy will allow us to test models of black hole formation, growth and evolution, as well as models of gravitational-wave generation and propagation. It will provide evidence for event horizons and ergoregions, test the theory of General Relativity itself, and may reveal the existence of new fundamental fields. The synthesis of these results has the potential to radically reshape our understanding of the cosmos and of the laws of Nature. The purpose of this work is to present a concise, yet comprehensive overview of the state of the art in the relevant fields of research, summarize important open problems, and lay out a roadmap for future progress.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A measurement of $Z\\rightarrow\u03c4^+\u03c4^-$ production cross-section is presented using data, corresponding to an integrated luminosity of 2 fb$^{-1}$, from $pp$ collisions at $\\sqrt{s}=8$ TeV collected by the LHCb experiment. The $\u03c4^+\u03c4^-$ candidates are reconstructed in final states with the first tau lepton decaying leptonically, and the second decaying either leptonically or to one or three charged hadrons. The production cross-section is measured for $Z$ bosons with invariant mass between 60 and 120 GeV/$c^2$, which decay to tau leptons with transverse momenta greater than 20 GeV/$c$ and pseudorapidities between 2.0 and 4.5. The cross-section is determined to be $\u03c3_{pp\\rightarrow{}Z\\rightarrow{}\u03c4^+\u03c4^-} = 95.8 \\pm 2.1 \\pm 4.6 \\pm 0.2 \\pm 1.1 \\mathrm{pb}$, where the first uncertainty is statistical, the second is systematic, the third is due to the LHC beam energy uncertainty, and the fourth to the integrated luminosity uncertainty. This result is compatible with NNLO Standard model predictions. The ratio of the cross-sections for $Z\\rightarrow\u03c4^+\u03c4^-$ to $Z\\rightarrow\u03bc^+\u03bc^-$ ($Z\\rightarrow{}e^+e^-$), determined to be $1.01 \\pm 0.05$ ($1.02 \\pm 0.06$), is consistent with the lepton-universality hypothesis in $Z$ decays.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "Measurements are reported of the central exclusive production of \\jpsi and \\psitwos mesons in $pp$ collisions at a centre-of-mass energy of 13 TeV. Backgrounds are significantly reduced compared to previous measurements made at lower energies through the use of new forward shower counters. The products of the cross-sections and the branching fractions for the decays to dimuons, where both muons are within the pseudorapidity range $2.0<\u03b7<4.5$, are measured to be $$\n  \\begin{array}{rcl} \u03c3_{J/\u03c8\\rightarrow\u03bc^+\u03bc^-}&=&435 \\pm 18 \\pm 17 \\pm 16 {\\rm \\ pb},\\\\ \u03c3_{\u03c8(2S)\\rightarrow\u03bc^+\u03bc^-}&=&11.1 \\pm 1.1 \\pm 0.3 \\pm 0.4 {\\rm \\ pb}.\\\\ \\end{array} $$ The first uncertainties are statistical, the second are systematic, and the third are due to the luminosity determination. The cross-sections are also measured differentially for meson rapidities between 2.0 and 4.5. Good agreement is observed with theoretical predictions. Photoproduction cross-sections are derived and compared to previous experiments, and a deviation from a pure power-law extrapolation of lower energy data is observed.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The first measurement of the lifetime of the doubly charmed baryon $\u039e_{cc}^{++}$ is presented, with the signal reconstructed in the final state $\u039b_c^+ K^- \u03c0^+ \u03c0^+$. The data sample used corresponds to an integrated luminosity of $1.7\\,\\mathrm{fb}^{-1}$, collected by the LHCb experiment in proton-proton collisions at a centre-of-mass energy of $13\\mathrm{\\,Te\\kern -0.1em V}$. The $\u039e_{cc}^{++}$ lifetime is measured to be $0.256\\,^{+0.024}_{-0.022}{\\,\\rm (stat)\\,} \\pm 0.014 {\\,\\rm(syst)}\\mathrm{\\,ps}$.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A measurement of the time-integrated $CP$ asymmetry in $D^0\\rightarrow K^0_S K^0_S$ decays is reported. The data correspond to an integrated luminosity of about $2$ fb$^{-1}$ collected in 2015-2016 by the LHCb collaboration in $pp$ collisions at a centre-of-mass energy of $13$ TeV. The $D^0$ candidate is required to originate from a $D^{\\ast +} \\rightarrow D^0 \u03c0^+$ decay, allowing the determination of the flavour of the $D^0$ meson using the pion charge. The $D^0 \\rightarrow K^{+}K^{-}$ decay, which has a well measured $CP$ asymmetry, is used as a calibration channel. The $CP$ asymmetry for $D^0\\rightarrow K^0_S K^0_S$ is measured to be \\begin{equation*} \\mathcal{A}^{CP}(D^0\\rightarrow K^0_S K^0_S) = (4.3\\pm 3.4\\pm 1.0)\\%, \\end{equation*} where the first uncertainty is statistical and the second is systematic. This result is combined with the previous LHCb measurement at lower centre-of-mass energies to obtain \\begin{equation*} \\mathcal{A}^{CP}(D^0\\rightarrow K^0_S K^0_S) = (2.3\\pm 2.8\\pm 0.9)\\%. \\end{equation*}\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A binned Dalitz plot analysis of $B^\\pm \\to D K^\\pm$ decays, with $D\\to K_\\text{S}^0\u03c0^+\u03c0^-$ and $D\\to K_\\text{S}^0K^+K^-$, is used to perform a measurement of the CP-violating observables $x_{\\pm}$ and $y_{\\pm}$, which are sensitive to the Cabibbo-Kobayashi-Maskawa angle $\u03b3$. The analysis is performed without assuming any $D$ decay model, through the use of information on the strong-phase variation over the Dalitz plot from the CLEO collaboration. Using a sample of proton-proton collision data collected with the LHCb experiment in 2015 and 2016, and corresponding to an integrated luminosity of 2.0$\\,\\text{fb}^{-1}$, the values of the CP violation parameters are found to be $x_- = ( 9.0 \\pm 1.7 \\pm 0.7 \\pm 0.4) \\times 10^{-2}$, $y_- = ( 2.1 \\pm 2.2 \\pm 0.5 \\pm 1.1) \\times 10^{-2}$, $x_+ = (- 7.7 \\pm 1.9 \\pm 0.7 \\pm 0.4) \\times 10^{-2}$, and $y_+ = (- 1.0 \\pm 1.9 \\pm 0.4 \\pm 0.9) \\times 10^{-2}$. The first uncertainty is statistical, the second is systematic, and the third is due to the uncertainty on the strong-phase measurements. These values are used to obtain $\u03b3= \\left(87\\,^{+11}_{-12}\\right)^\\circ$, $r_B = 0.086^{+ 0.013}_{-0.014}$, and $\u03b4_B = (101 \\pm 11)^\\circ$, where $r_B$ is the ratio between the suppressed and favoured $B$-decay amplitudes and $\u03b4_B$ is the corresponding strong-interaction phase difference. This measurement is combined with the result obtained using 2011 and 2012 data collected with the \\lhcb experiment, to give $\u03b3= \\left(80\\,^{+10}_{\\,-9}\\right)^\\circ$, $r_B = 0.080 \\pm 0.011$, and $\u03b4_B = (110 \\pm 10)^\\circ$.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "On 17 August 2017, the LIGO and Virgo observatories made the first direct detection of gravitational waves from the coalescence of a neutron star binary system. The detection of this gravitational-wave signal, GW170817, offers a novel opportunity to directly probe the properties of matter at the extreme conditions found in the interior of these stars. The initial, minimal-assumption analysis of the LIGO and Virgo data placed constraints on the tidal effects of the coalescing bodies, which were then translated to constraints on neutron star radii. Here, we expand upon previous analyses by working under the hypothesis that both bodies were neutron stars that are described by the same equation of state and have spins within the range observed in Galactic binary neutron stars. Our analysis employs two methods: the use of equation-of-state-insensitive relations between various macroscopic properties of the neutron stars and the use of an efficient parametrization of the defining function $p(\u03c1)$ of the equation of state itself. From the LIGO and Virgo data alone and the first method, we measure the two neutron star radii as $R_1=10.8^{+2.0}_{-1.7}$ km for the heavier star and $R_2= 10.7^{+2.1}_{-1.5}$ km for the lighter star at the 90% credible level. If we additionally require that the equation of state supports neutron stars with masses larger than $1.97 \\,M_\\odot$ as required from electromagnetic observations and employ the equation-of-state parametrization, we further constrain $R_1= 11.9^{+1.4}_{-1.4}$ km and $R_2= 11.9^{+1.4}_{-1.4}$ km at the 90% credible level. Finally, we obtain constraints on $p(\u03c1)$ at supranuclear densities, with pressure at twice nuclear saturation density measured at $3.5^{+2.7}_{-1.7}\\times 10^{34} \\,\\mathrm{dyn}/\\mathrm{cm}^{2}$ at the 90% level.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "On August 17, 2017, the Advanced LIGO and Advanced Virgo gravitational-wave detectors observed a low-mass compact binary inspiral. The initial sky localization of the source of the gravitational-wave signal, GW170817, allowed electromagnetic observatories to identify NGC 4993 as the host galaxy. In this work we improve initial estimates of the binary's properties, including component masses, spins, and tidal parameters, using the known source location, improved modeling, and re-calibrated Virgo data. We extend the range of gravitational-wave frequencies considered down to 23 Hz, compared to 30 Hz in the initial analysis. We also compare results inferred using several signal models, which are more accurate and incorporate additional physical effects as compared to the initial analysis. We improve the localization of the gravitational-wave source to a 90% credible region of $16~\\mathrm{deg}^2$. We find tighter constraints on the masses, spins, and tidal parameters, and continue to find no evidence for non-zero component spins. The component masses are inferred to lie between 1.00 and 1.89 $M_\\odot$ when allowing for large component spins, and to lie between 1.16 and 1.60 $M_\\odot$ (with a total mass $2.73^{+0.04}_{-0.01} \\, M_\\odot$) when the spins are restricted to be within the range observed in Galactic binary neutron stars. Under minimal assumptions about the nature of the compact objects, our constraints for the tidal deformability parameter $\\tilde \u039b$ are $(0,630)$ when we allow for large component spins, and $300^{+420}_{-230}$ (using a 90% highest posterior density interval) when restricting the magnitude of the component spins, ruling out several equation of state models at the 90% credible level. Finally, with LIGO and GEO600 data, we use a Bayesian analysis to place upper limits on the amplitude and spectral energy density of a possible post-merger signal. (Abridged)\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The inclusive $D_s^{\\pm}$ production asymmetry is measured in $pp$ collisions collected by the LHCb experiment at centre-of-mass energies of $\\sqrt{s} =7$ and 8 TeV. Promptly produced $D_s^{\\pm}$ mesons are used, which decay as $D_s^{\\pm}\\to\u03c6\u03c0^{\\pm}$, with $\u03c6\\to K^+K^-$. The measurement is performed in bins of transverse momentum, $p_{\\rm T}$, and rapidity, $y$, covering the range $2.5<p_{\\rm T}<25.0$ GeV$/c$ and $2.0<y<4.5$. No kinematic dependence is observed. Evidence of nonzero $D_s^{\\pm}$ production asymmetry is found with a significance of 3.3 standard deviations.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A search is performed for a spin-0 boson, $\u03c6$, produced in proton-proton collisions at centre-of-mass energies of 7 and 8 TeV, using prompt $\u03c6\\rightarrow\u03bc^+\u03bc^-$ decays and a data sample corresponding to an integrated luminosity of approximately 3.0 ${\\rm fb}^{-1}$ collected with the LHCb detector. No evidence is found for a signal in the mass range from 5.5 to 15 GeV. Upper limits are placed on the product of the production cross-section and the branching fraction into the dimuon final state. The limits are comparable to the best existing over most of the mass region considered and are the first to be set near the $\u03a5$ resonances.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "From samples of $pp$ collision data collected by the LHCb experiment at $\\sqrt{s}=7$, $8$ and $13$ TeV corresponding to integrated luminosities of 1.0, 2.0 and 1.5 fb$^{-1}$, respectively, a peak in both the $\u039b_b^0K^-$ and $\u039e_b^0\u03c0^-$ invariant mass spectra is observed. In the quark model, radially and orbitally excited $\u039e_b^-$ resonances with quark content $bds$ are expected. Referring to this peak as $\u039e_b(6227)^-$, the mass and natural width are measured to be $m_{\u039e_{b}(6227)^-}=6226.9\\pm2.0\\pm0.3\\pm0.2$ MeV/$c^2$ and $\u0393_{\u039e_b(6227)^-}=18.1\\pm5.4\\pm1.8$ MeV/$c^2$, where the first uncertainty is statistical, the second is systematic, and the third, on $m_{\u039e_b(6227)^-}$, is due to the knowledge of the $\u039b_b^0$ baryon mass. Relative production rates of the ${\u039e_b(6227)^-\\to\u039b_b^0K^-}$ and ${\u039e_b(6227)^-\\to\u039e_b^0\u03c0^-}$ decays are also reported.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The time-dependent $C\\!P$ asymmetries in $B^0\\to\u03c0^+\u03c0^-$ and $B_s^0\\to K^+\\!K^-$ decays are measured using a data sample of $pp$ collisions corresponding to an integrated luminosity of 3.0 fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of 7 and 8 TeV. The same data sample is used to measure the time-integrated $C\\!P$ asymmetries in $B^0\\to K^+\u03c0^-$ and $B_s^0\\to\u03c0^+ K^-$ decays. The results are $C_{\u03c0^+\u03c0^-} = -0.34 \\pm 0.06 \\pm 0.01$, $S_{\u03c0^+\u03c0^-} = -0.63 \\pm 0.05 \\pm 0.01$, $C_{K^+\\!K^-} = 0.20 \\pm 0.06 \\pm 0.02$, $S_{K^+\\!K^-} = 0.18 \\pm 0.06 \\pm 0.02$, $C_{K^+\\!K^-}^{\u0394\u0393} = -0.79 \\pm 0.07 \\pm 0.10$, $A_{C\\!P}^{B^0} = -0.084 \\pm 0.004 \\pm 0.003$, and $A_{C\\!P}^{B_s^0} = 0.213 \\pm 0.015 \\pm 0.007$, where the first uncertainties are statistical and the second systematic. Evidence for $C\\!P$ violation is found in the $B_s^0\\to K^+\\!K^-$ decay for the first time.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A search for $C$P and $P$ violation using triple-product asymmetries is performed with $\u039b^{0}_{b}\\to pK^{-}\u03c0^{+}\u03c0^{-}$, $\u039b^{0}_{b}\\to pK^{-}K^{+}K^{-}$ and $\u039e^{0}_{b}\\to pK^{-}K^{-}\u03c0^{+}$ decays. The data sample corresponds to integrated luminosities of 1.0fb$^{-1}$ and 2.0fb$^{-1}$, recorded with the LHCb detector at centre-of-mass energies of 7TeV and 8TeV, respectively. The $CP$- and $P$-violating asymmetries are measured both integrating over all phase space and in specific phase-space regions. No significant deviation from $CP$ or $P$ symmetry is found. The first observation of $\u039b^{0}_{b}\\to pK^{-}\u03c7_{c0}(1P)(\\to\u03c0^{+}\u03c0^{-}, K^{+}K^{-})$ decay is also reported.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A measurement of the $CP$ asymmetries $S_{f}$ and $S_{\\bar{f}}$ in $B^0\\to D^{\\mp}\u03c0^{\\pm}$ decays is reported. The decays are reconstructed in a dataset collected with the LHCb experiment in proton-proton collisions at centre-of-mass energies of 7 and 8 TeV and corresponding to an integrated luminosity of $3.0 \\rm{ fb}^{-1}$. The $CP$ asymmetries are measured to be $S_{f} = 0.058 \\pm 0.020 (\\rm{stat}) \\pm 0.011(\\rm{syst})$ and $S_{\\bar{f}} = 0.038\\pm 0.020 (\\text{stat})\\pm 0.007 (\\text{syst})$. These results are in agreement with, and more precise than, previous determinations. They are used to constrain $|\\sin\\left(2\u03b2+\u03b3\\right)|$ and $\u03b3$ to intervals that are consistent with the current world-average values.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The decay $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ is observed using $pp$ collision data collected with the LHCb detector at centre-of-mass energies of $\\sqrt{s}=$ 7 and 8 TeV, corresponding to an integrated luminosity of 3 $fb^{-1}$. The ratio of branching fractions between $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ and $\u039b_b^0 \\to \u039b_c^+ \u03c0^-$ decays is measured to be \\begin{equation*}\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ \u03c0^-)} = 0.0540 \\pm 0.0023 \\pm 0.0032. \\end{equation*} Two resonant structures are observed in the $ \u039b_c^+ \u03c0^-$ mass spectrum of the ${\u039b_b^0 \\to \u039b_c^+ p\\overline{p} \u03c0^-}$ decays, corresponding to the $\u03a3_c(2455)^0$ and $\u03a3_c^{*}(2520)^0$ states. The ratios of branching fractions with respect to the decay $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ are \\begin{align*}\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u03a3_c^0 p\\overline{p})\\times\\mathcal{B}(\u03a3_c^0\\to \u039b_c^+ \u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)} = 0.089\\pm0.015\\pm0.006,\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u03a3_c^{*0} p\\overline{p})\\times\\mathcal{B}(\u03a3_c^{*0}\\to \u039b_c^+ \u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)} = 0.119\\pm0.020\\pm0.014. \\end{align*} In all of the above results, the first uncertainty is statistical and the second is systematic. The phase space is also examined for the presence of dibaryon resonances. No evidence for such resonances is found.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "The production cross-sections of $\u03a5(1S)$, $\u03a5(2S)$ and $\u03a5(3S)$ mesons in proton-proton collisions at $\\sqrt{s}$= 13 TeV are measured with a data sample corresponding to an integrated luminosity of $277 \\pm 11$ $\\rm pb^{-1}$ recorded by the LHCb experiment in 2015. The $\u03a5$ mesons are reconstructed in the decay mode $\u03a5\\to\u03bc^{+}\u03bc^{-}$. The differential production cross-sections times the dimuon branching fractions are measured as a function of the $\u03a5$ transverse momentum, $p_{\\rm T}$, and rapidity, $y$, over the range $0 < p_{\\rm T}< 30$ GeV/c and $2.0 < y < 4.5$. The ratios of the cross-sections with respect to the LHCb measurement at $\\sqrt{s}$= 8 TeV are also determined. The measurements are compared with theoretical predictions based on NRQCD.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "A search for the decay $B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-$ is presented using data sets corresponding to 1.0, 2.0 and 1.6 $\\text{fb}^{-1}$ of integrated luminosity collected during $pp$ collisions with the LHCb experiment at centre-of-mass energies of 7, 8 and 13 TeV, respectively. An excess is found over the background-only hypothesis with a significance of 3.4 standard deviations. The branching fraction of the $B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-$ decay is determined to be $\\mathcal{B}(B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-) = [2.9 \\pm 1.0~(\\text{stat}) \\pm 0.2~(\\text{syst}) \\pm 0.3~(\\text{norm})] \\times 10^{-8}$, where the first and second uncertainties are statistical and systematic, respectively. The third uncertainty is due to limited knowledge of external parameters used to normalise the branching fraction measurement.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "Ultrafast non-equilibrium dynamics offer a route to study the microscopic interactions that govern macroscopic behavior. In particular, photo-induced phase transitions (PIPTs) in solids provide a test case for how forces, and the resulting atomic motion along a reaction coordinate, originate from a non-equilibrium population of excited electronic states. Utilizing femtosecond photoemission we obtain access to the transient electronic structure during an ultrafast PIPT in a model system: indium nanowires on a silicon(111) surface. We uncover a detailed reaction pathway, allowing a direct comparison with the dynamics predicted by ab initio simulations. This further reveals the crucial role played by localized photo-holes in shaping the potential energy landscape, and enables a combined momentum and real space description of PIPTs, including the ultrafast formation of chemical bonds.\n        \u25b3 Less", "author": "Martin A. Schmidt"}, {"abstract": "We consider recommendation systems that need to operate under wireless bandwidth constraints, measured as number of broadcast transmissions, and demonstrate a (tight for some instances) tradeoff between regret and bandwidth for two scenarios: the case of multi-armed bandit with context, and the case where there is a latent structure in the message space that we can exploit to reduce the learning phase.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "The goal of deconvolution is in estimating the distribution of a random variable based on its noisy observations. The goal of matrix estimation is in estimating the entries of a large matrix from observed entries, which are noisy versions of entries in a small fraction of the entire matrix. We study the rate of convergence for estimation of matrices with a certain monotonicity property. It turns out to be equivalent to solving a robust version of the deconvolution problem. As the main result of this paper, we provide a simple, intuitive algorithm for matrix estimation which extends the works by Fan (1991) and Delaigle et al. (2008). We show that our computationally efficient method achieves near optimal minimax rate for the matrix estimation as well as robust deconvolution. This rate is within a constant factor to the rate achieved by the kernel deconvolution estimator in the classical setup.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We propose an algorithm to impute and forecast a time series by transforming the observed time series into a matrix, utilizing matrix estimation to recover missing values and de-noise observed entries, and performing linear regression to make predictions. At the core of our analysis is a representation result, which states that for a large model class, the transformed time series matrix is (approximately) low-rank. In effect, this generalizes the widely used Singular Spectrum Analysis (SSA) in time series literature, and allows us to establish a rigorous link between time series analysis and matrix estimation. The key to establishing this link is constructing a Page matrix with non-overlapping entries rather than a Hankel matrix as is commonly done in the literature (e.g., SSA). This particular matrix structure allows us to provide finite sample analysis for imputation and prediction, and prove the asymptotic consistency of our method. Another salient feature of our algorithm is that it is model agnostic with respect to both the underlying time dynamics and the noise distribution in the observations. The noise agnostic property of our approach allows us to recover the latent states when only given access to noisy and partial observations a la a Hidden Markov Model; e.g., recovering the time-varying parameter of a Poisson process without knowing that the underlying process is Poisson. Furthermore, since our forecasting algorithm requires regression with noisy features, our approach suggests a matrix estimation based method - coupled with a novel, non-standard matrix estimation error metric - to solve the error-in-variable regression problem, which could be of interest in its own right. Through synthetic and real-world datasets, we demonstrate that our algorithm outperforms standard software packages (including R libraries) in the presence of missing data as well as high levels of noise.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available. We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a $d$-dimensional state space and the discounted factor $\u03b3\\in (0,1)$, given an arbitrary sample path with \"covering time\" $ L $, we establish that the algorithm is guaranteed to output an $\\varepsilon$-accurate estimate of the optimal Q-function using $\\tilde{O}\\big(L/(\\varepsilon^3(1-\u03b3)^7)\\big)$ samples. For instance, for a well-behaved MDP, the covering time of the sample path under the purely random policy scales as $ \\tilde{O}\\big(1/\\varepsilon^d\\big),$ so the sample complexity scales as $\\tilde{O}\\big(1/\\varepsilon^{d+3}\\big).$ Indeed, we establish a lower bound that argues that the dependence of $ \\tilde\u03a9\\big(1/\\varepsilon^{d+2}\\big)$ is necessary.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "The sparse matrix estimation problem consists of estimating the distribution of an $n\\times n$ matrix $Y$, from a sparsely observed single instance of this matrix where the entries of $Y$ are independent random variables. This captures a wide array of problems; special instances include matrix completion in the context of recommendation systems, graphon estimation, and community detection in (mixed membership) stochastic block models. Inspired by classical collaborative filtering for recommendation systems, we propose a novel iterative, collaborative filtering-style algorithm for matrix estimation in this generic setting. We show that the mean squared error (MSE) of our estimator converges to $0$ at the rate of $O(d^2 (pn)^{-2/5})$ as long as $\u03c9(d^5 n)$ random entries from a total of $n^2$ entries of $Y$ are observed (uniformly sampled), $\\mathbb{E}[Y]$ has rank $d$, and the entries of $Y$ have bounded support. The maximum squared error across all entries converges to $0$ with high probability as long as we observe a little more, $\u03a9(d^5 n \\ln^2(n))$ entries. Our results are the best known sample complexity results in this generality.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method, we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. To begin, we establish the condition under which the fundamental assumption in synthetic control-like approaches holds, i.e. when the linear relationship between the treatment unit and the donor pool prevails in both the pre- and post-intervention periods. We provide the first finite sample analysis for a broader class of models, the Latent Variable Model, in contrast to Factor Models previously considered in the literature. Further, we show that our de-noising procedure accurately imputes missing entries, producing a consistent estimator of the underlying signal matrix provided $p = \u03a9( T^{-1 + \u03b6})$ for some $\u03b6> 0$; here, $p$ is the fraction of observed data and $T$ is the time interval of interest. Under the same setting, we prove that the mean-squared-error (MSE) in our prediction estimation scales as $O(\u03c3^2/p + 1/\\sqrt{T})$, where $\u03c3^2$ is the noise variance. Using a data aggregation method, we show that the MSE can be made as small as $O(T^{-1/2+\u03b3})$ for any $\u03b3\\in (0, 1/2)$, leading to a consistent estimator. We also introduce a Bayesian framework to quantify the model uncertainty through posterior probabilities. Our experiments, using both real-world and synthetic datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the problem of designing a packet-level congestion control and scheduling policy for datacenter networks. Current datacenter networks primarily inherit the principles that went into the design of Internet, where congestion control and scheduling are distributed. While distributed architecture provides robustness, it suffers in terms of performance. Unlike Internet, data center is fundamentally a \"controlled\" environment. This raises the possibility of designing a centralized architecture to achieve better performance. Recent solutions such as Fastpass and Flowtune have provided the proof of this concept. This raises the question: what is theoretically optimal performance achievable in a data center?\n  We propose a centralized policy that guarantees a per-flow end-to-end flow delay bound of $O$(#hops $\\times$ flow-size $/$ gap-to-capacity). Effectively such an end-to-end delay will be experienced by flows even if we removed congestion control and scheduling constraints as the resulting queueing networks can be viewed as the classical reversible multi-class queuing network, which has a product-form stationary distribution. In the language of Harrison et al., we establish that baseline performance for this model class is achievable.\n  Indeed, as the key contribution of this work, we propose a method to emulate such a reversible queuing network while satisfying congestion control and scheduling constraints. Precisely, our policy is an emulation of Store-and-Forward (SFA) congestion control in conjunction with Last-Come-First-Serve Preemptive-Resume (LCFS-PR) scheduling policy.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the setup of nonparametric 'blind regression' for estimating the entries of a large $m \\times n$ matrix, when provided with a small, random fraction of noisy measurements. We assume that all rows $u \\in [m]$ and columns $i \\in [n]$ of the matrix are associated to latent features $x_1(u)$ and $x_2(i)$ respectively, and the $(u,i)$-th entry of the matrix, $A(u, i)$ is equal to $f(x_1(u), x_2(i))$ for a latent function $f$. Given noisy observations of a small, random subset of the matrix entries, our goal is to estimate the unobserved entries of the matrix as well as to \"de-noise\" the observed entries.\n  As the main result of this work, we introduce a neighbor-based estimation algorithm inspired by the classical Taylor's series expansion. We establish its consistency when the underlying latent function $f$ is Lipschitz, the latent features belong to a compact domain, and the fraction of observed entries in the matrix is at least $\\max \\left(m^{-1 + \u03b4}, n^{-1/2 + \u03b4} \\right)$, for any $\u03b4> 0$. As an important byproduct, our analysis sheds light into the performance of the classical collaborative filtering (CF) algorithm for matrix completion, which has been widely utilized in practice. Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides a principled improvement over basic CF and is competitive with matrix factorization methods.\n  Our algorithm has a natural extension to tensor completion. For a $t$-order balanced tensor with total of $N$ entries, we prove that our approach provides a consistent estimator when at least $N^{-\\frac{\\lfloor 2t/3 \\rfloor}{2t}+ \u03b4}$ fraction of entries are observed, for any $\u03b4> 0$. When applied to the setting of image in-painting (a tensor of order 3), we find that our approach is competitive with respect to state-of-art tensor completion algorithms across benchmark images.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Inferring the correct answers to binary tasks based on multiple noisy answers in an unsupervised manner has emerged as the canonical question for micro-task crowdsourcing or more generally aggregating opinions. In graphon estimation, one is interested in estimating edge intensities or probabilities between nodes using a single snapshot of a graph realization. In the recent literature, there has been exciting development within both of these topics. In the context of crowdsourcing, the key intellectual challenge is to understand whether a given task can be more accurately denoised by aggregating answers collected from other different tasks. In the context of graphon estimation, precise information limits and estimation algorithms remain of interest. In this paper, we utilize a statistical reduction from crowdsourcing to graphon estimation to advance the state-of-art for both of these challenges. We use concepts from graphon estimation to design an algorithm that achieves better performance than the {\\em majority voting} scheme for a setup that goes beyond the {\\em rank one} models considered in the literature. We use known explicit lower bounds for crowdsourcing to provide refined lower bounds for graphon estimation.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Despite the popularity and empirical success of patch-based nearest-neighbor and weighted majority voting approaches to medical image segmentation, there has been no theoretical development on when, why, and how well these nonparametric methods work. We bridge this gap by providing a theoretical performance guarantee for nearest-neighbor and weighted majority voting segmentation under a new probabilistic model for patch-based image segmentation. Our analysis relies on a new local property for how similar nearby patches are, and fuses existing lines of work on modeling natural imagery patches and theory for nonparametric classification. We use the model to derive a new patch-based segmentation algorithm that iterates between inferring local label patches and merging these local segmentations to produce a globally consistent image segmentation. Many existing patch-based algorithms arise as special cases of the new algorithm.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "There is much empirical evidence that item-item collaborative filtering works well in practice. Motivated to understand this, we provide a framework to design and analyze various recommendation algorithms. The setup amounts to online binary matrix completion, where at each time a random user requests a recommendation and the algorithm chooses an entry to reveal in the user's row. The goal is to minimize regret, or equivalently to maximize the number of +1 entries revealed at any time. We analyze an item-item collaborative filtering algorithm that can achieve fundamentally better performance compared to user-user collaborative filtering. The algorithm achieves good \"cold-start\" performance (appropriately defined) by quickly making good recommendations to new users about whom there is little information.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. We first observe that the notoriously difficult problem of learning parities with noise can be captured as a special case of learning graphical models. This leads to an unconditional computational lower bound of $\u03a9(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of so-called statistical algorithms recently introduced by Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime required to exhaustively search over neighborhoods cannot be significantly improved without restricting the class of models.\n  Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari (2009) showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time $O(p^2)$. We provide an algorithm whose performance interpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the repulsion.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the \"online\" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of $n$ users either likes or dislikes each of $m$ items. We assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly $\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We present a distributed asynchronous algorithm for approximating a single component of the solution to a system of linear equations $Ax = b$, where $A$ is a positive definite real matrix, and $b \\in \\mathbb{R}^n$. This is equivalent to solving for $x_i$ in $x = Gx + z$ for some $G$ and $z$ such that the spectral radius of $G$ is less than 1. Our algorithm relies on the Neumann series characterization of the component $x_i$, and is based on residual updates. We analyze our algorithm within the context of a cloud computation model, in which the computation is split into small update tasks performed by small processors with shared access to a distributed file system. We prove a robust asymptotic convergence result when $\u03c1(\\tilde{G}) < 1$, regardless of the precise order and frequency in which the update tasks are performed, where $\\tilde{G}_{ij} = |G_{ij}|$. We provide convergence rate bounds which depend on the order of update tasks performed, analyzing both deterministic update rules via counting weighted random walks, as well as probabilistic update rules via concentration bounds. The probabilistic analysis requires analyzing the product of random matrices which are drawn from distributions that are time and path dependent. We specifically consider the setting where $n$ is large, yet $G$ is sparse, e.g., each row has at most $d$ nonzero entries. This is motivated by applications in which $G$ is derived from the edge structure of an underlying graph. Our results prove that if the local neighborhood of the graph does not grow too quickly as a function of $n$, our algorithm can provide significant reduction in computation cost as opposed to any algorithm which computes the global solution vector $x$. Our algorithm obtains an $\u03b5\\|x\\|_2$ additive approximation for $x_i$ in constant time with respect to the size of the matrix when $d = O(1)$ and $1/(1-\\|G\\|_2) = O(1)$.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Motivated by generating personalized recommendations using ordinal (or preference) data, we study the question of learning a mixture of MultiNomial Logit (MNL) model, a parameterized class of distributions over permutations, from partial ordinal or preference data (e.g. pair-wise comparisons). Despite its long standing importance across disciplines including social choice, operations research and revenue management, little is known about this question. In case of single MNL models (no mixture), computationally and statistically tractable learning from pair-wise comparisons is feasible. However, even learning mixture with two MNL components is infeasible in general.\n  Given this state of affairs, we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner. We present a sufficient condition as well as an efficient algorithm for learning mixed MNL models from partial preferences/comparisons data. In particular, a mixture of $r$ MNL components over $n$ objects can be learnt using samples whose size scales polynomially in $n$ and $r$ (concretely, $r^{3.5}n^3(log n)^4$, with $r\\ll n^{2/7}$ when the model parameters are sufficiently incoherent). The algorithm has two phases: first, learn the pair-wise marginals for each component using tensor decomposition; second, learn the model parameters for each component using Rank Centrality introduced by Negahban et al. In the process of proving these results, we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "In this paper we consider the problem of learning undirected graphical models from data generated according to the Glauber dynamics. The Glauber dynamics is a Markov chain that sequentially updates individual nodes (variables) in a graphical model and it is frequently used to sample from the stationary distribution (to which it converges given sufficient time). Additionally, the Glauber dynamics is a natural dynamical model in a variety of settings. This work deviates from the standard formulation of graphical model learning in the literature, where one assumes access to i.i.d. samples from the distribution.\n  Much of the research on graphical model learning has been directed towards finding algorithms with low computational cost. As the main result of this work, we establish that the problem of reconstructing binary pairwise graphical models is computationally tractable when we observe the Glauber dynamics. Specifically, we show that a binary pairwise graphical model on $p$ nodes with maximum degree $d$ can be learned in time $f(d)p^2\\log p$, for a function $f(d)$, using nearly the information-theoretic minimum number of samples.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "In this paper, we discuss the method of Bayesian regression and its efficacy for predicting price variation of Bitcoin, a recently popularized virtual, cryptographic currency. Bayesian regression refers to utilizing empirical data as proxy to perform Bayesian inference. We utilize Bayesian regression for the so-called \"latent source model\". The Bayesian regression for \"latent source model\" was introduced and discussed by Chen, Nikolov and Shah (2013) and Bresler, Chen and Shah (2014) for the purpose of binary classification. They established theoretical as well as empirical efficacy of the method for the setting of binary classification.\n  In this paper, instead we utilize it for predicting real-valued quantity, the price of Bitcoin. Based on this price prediction method, we devise a simple strategy for trading Bitcoin. The strategy is able to nearly double the investment in less than 60 day period when run against real data trace.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "These are notes from the lecture of Devavrat Shah given at the autumn school \"Statistical Physics, Optimization, Inference, and Message-Passing Algorithms\", that took place in Les Houches, France from Monday September 30th, 2013, till Friday October 11th, 2013. The school was organized by Florent Krzakala from UPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka Zdeborova from CEA Saclay & CNRS, and Riccardo Zecchina from Politecnico Torino. This lecture of Devavrat Shah (MIT) covers the basics of inference and learning. It explains how inference problems are represented within structures known as graphical models. The theoretical basis of the belief propagation algorithm is then explained and derived. This lecture sets the stage for generalizations and applications of message passing algorithms.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see the monograph by Wainwright and Jordan (2008)) but no proof was known.\n  Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We study the optimal scaling of the expected total queue size in an $n\\times n$ input-queued switch, as a function of the number of ports $n$ and the load factor $\u03c1$, which has been conjectured to be $\u0398(n/(1-\u03c1))$. In a recent work, the validity of this conjecture has been established for the regime where $1-\u03c1= O(1/n^2)$. In this paper, we make further progress in the direction of this conjecture. We provide a new class of scheduling policies under which the expected total queue size scales as $O(n^{1.5}(1-\u03c1)^{-1}\\log(1/(1-\u03c1)))$ when $1-\u03c1= O(1/n)$. This is an improvement over the state of the art; for example, for $\u03c1= 1 - 1/n$ the best known bound was $O(n^3)$, while ours is $O(n^{2.5}\\log n)$.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "In this paper, we present a novel iterative Monte Carlo method for approximating the stationary probability of a single state of a positive recurrent Markov chain. We utilize the characterization that the stationary probability of a state $i$ is inversely proportional to the expected return time of a random walk beginning at $i$. Our method obtains an $\u03b5$-multiplicative close estimate with probability greater than $1 - \u03b1$ using at most $\\tilde{O}\\left(t_{\\text{mix}} \\ln(1/\u03b1) / \u03c0_i \u03b5^2 \\right)$ simulated random walk steps on the Markov chain across all iterations, where $t_{\\text{mix}}$ is the standard mixing time and $\u03c0_i$ is the stationary probability. In addition, the estimate at each iteration is guaranteed to be an upper bound with high probability, and is decreasing in expectation with the iteration count, allowing us to monitor the progress of the algorithm and design effective termination criteria. We propose a termination criteria which guarantees a $\u03b5(1 + 4 \\ln(2) t_{\\text{mix}})$ multiplicative error performance for states with stationary probability larger than $\u0394$, while providing an additive error for states with stationary probability less than $\u0394\\in (0,1)$. The algorithm along with this termination criteria uses at most $\\tilde{O}\\left(\\frac{\\ln(1/\u03b1)}{\u03b5^2} \\min\\left(\\frac{t_{\\text{mix}}}{\u03c0_i}, \\frac{1}{\u03b5\u0394}\\right)\\right)$ simulated random walk steps, which is bounded by a constant with respect to the Markov Chain. We provide a tight analysis of our algorithm based on a locally weighted variant of the mixing time. Our results naturally extend for countably infinite state space Markov chains via Lyapunov function analysis.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "This paper presents a novel meta algorithm, Partition-Merge (PM), which takes existing centralized algorithms for graph computation and makes them distributed and faster. In a nutshell, PM divides the graph into small subgraphs using our novel randomized partitioning scheme, runs the centralized algorithm on each partition separately, and then stitches the resulting solutions to produce a global solution. We demonstrate the efficiency of the PM algorithm on two popular problems: computation of Maximum A Posteriori (MAP) assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity optimization for community detection. We show that the resulting distributed algorithms for these problems essentially run in time linear in the number of nodes in the graph, and perform as well -- or even better -- than the original centralized algorithm as long as the graph has geometric structures. Here we say a graph has geometric structures, or polynomial growth property, when the number of nodes within distance r of any given node grows no faster than a polynomial function of r. More precisely, if the centralized algorithm is a C-factor approximation with constant C \\ge 1, the resulting distributed algorithm is a (C+\u03b4)-factor approximation for any small \u03b4>0; but if the centralized algorithm is a non-constant (e.g. logarithmic) factor approximation, then the resulting distributed algorithm becomes a constant factor approximation. For general graphs, we compute explicit bounds on the loss of performance of the resulting distributed algorithm with respect to the centralized algorithm.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren't actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a \"weighted majority voting\" classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such \"trending topics\" in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "The question of aggregating pair-wise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR's TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining a ranking, finding `scores' for each object (e.g. player's rating) is of interest for understanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation algorithm for discovering scores for objects (or items) from pair-wise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with an edge present between a pair of objects if they are compared; the score, which we call Rank Centrality, of an object turns out to be its stationary probability under this random walk. To study the efficacy of the algorithm, we consider the popular Bradley-Terry-Luce (BTL) model (equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which each object has an associated score which determines the probabilistic outcomes of pair-wise comparisons between objects. In terms of the pair-wise marginal probabilities, which is the main subject of this paper, the MNL model and the BTL model are identical. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. In particular, the number of samples required to learn the score well with high probability depends on the structure of the comparison graph. When the Laplacian of the comparison graph has a strictly positive spectral gap, e.g. each item is compared to a subset of randomly chosen items, this leads to dependence on the number of samples that is nearly order-optimal.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "This paper presents an analysis of spinal codes, a class of rateless codes proposed recently. We prove that spinal codes achieve Shannon capacity for the binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN) channel with an efficient polynomial-time encoder and decoder. They are the first rateless codes with proofs of these properties for BSC and AWGN. The key idea in the spinal code is the sequential application of a hash function over the message bits. The sequential structure of the code turns out to be crucial for efficient decoding. Moreover, counter to the wisdom of having an expander structure in good codes, we show that the spinal code, despite its sequential structure, achieves capacity. The pseudo-randomness provided by a hash function suffices for this purpose. Our proof introduces a variant of Gallager's result characterizing the error exponent of random codes for any memoryless channel. We present a novel application of these error-exponent results within the framework of an efficient sequential code. The application of a hash function over the message bits provides a methodical and effective way to de-randomize Shannon's random codebook construction.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the problem of detecting the source of a rumor which has spread in a network using only observations about which set of nodes are infected with\n  the rumor and with no information as to \\emph{when} these nodes became infected. In a recent work \\citep{ref:rc} this rumor source detection problem was introduced and studied. The authors proposed the graph score function {\\em rumor centrality} as an estimator for detecting the source. They establish it to be the maximum likelihood estimator with respect to the popular Susceptible Infected (SI) model with exponential spreading times for regular trees. They showed that as the size of the infected graph increases, for a path graph (2-regular tree), the probability of source detection goes to $0$ while for $d$-regular trees with $d \\geq 3$ the probability of detection, say $\u03b1_d$, remains bounded away from $0$ and is less than $1/2$. However, their results stop short of providing insights for the performance of the rumor centrality estimator in more general settings such as irregular trees or the SI model with non-exponential spreading times.\n  This paper overcomes this limitation and establishes the effectiveness of rumor centrality for source detection for generic random trees and the SI model with a generic spreading time distribution. The key result is an interesting connection between a continuous time branching process and the effectiveness of rumor centrality. Through this, it is possible to quantify the detection probability precisely. As a consequence, we recover all previous results as a special case and obtain a variety of novel results including the {\\em universality} of rumor centrality in the context of tree-like graphs and the SI model with a generic spreading time distribution.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Crowdsourcing systems, in which numerous tasks are electronically distributed to numerous \"information piece-workers\", have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all such systems must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in an appropriate manner, e.g. majority voting.\n  In this paper, we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm, inspired by belief propagation and low-rank matrix approximation, significantly outperforms majority voting and, in fact, is optimal through comparison to an oracle that knows the reliability of every worker. Further, we compare our approach with a more general class of algorithms which can dynamically assign tasks. By adaptively deciding which questions to ask to the next arriving worker, one might hope to reduce uncertainty more efficiently. We show that, perhaps surprisingly, the minimum price necessary to achieve a target reliability scales in the same manner under both adaptive and non-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under both scenarios. This strongly relies on the fact that workers are fleeting and can not be exploited. Therefore, architecturally, our results suggest that building a reliable worker-reputation system is essential to fully harnessing the potential of adaptive designs.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the problem of static assortment optimization, where the goal is to find the assortment of size at most $C$ that maximizes revenues. This is a fundamental decision problem in the area of Operations Management. It has been shown that this problem is provably hard for most of the important families of parametric of choice models, except the multinomial logit (MNL) model. In addition, most of the approximation schemes proposed in the literature are tailored to a specific parametric structure. We deviate from this and propose a general algorithm to find the optimal assortment assuming access to only a subroutine that gives revenue predictions; this means that the algorithm can be applied with any choice model. We prove that when the underlying choice model is the MNL model, our algorithm can find the optimal assortment efficiently.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Consider a wireless network of n nodes represented by a graph G=(V, E) where an edge (i,j) models the fact that transmissions of i and j interfere with each other, i.e. simultaneous transmissions of i and j become unsuccessful. Hence it is required that at each time instance a set of non-interfering nodes (corresponding to an independent set in G) access the wireless medium. To utilize wireless resources efficiently, it is required to arbitrate the access of medium among interfering nodes properly. Moreover, to be of practical use, such a mechanism is required to be totally distributed as well as simple. As the main result of this paper, we provide such a medium access algorithm. It is randomized, totally distributed and simple: each node attempts to access medium at each time with probability that is a function of its local information. We establish efficiency of the algorithm by showing that the corresponding network Markov chain is positive recurrent as long as the demand imposed on the network can be supported by the wireless network (using any algorithm). In that sense, the proposed algorithm is optimal in terms of utilizing wireless resources. The algorithm is oblivious to the network graph structure, in contrast with the so-called `polynomial back-off' algorithm by Hastad-Leighton-Rogoff (STOC '87, SICOMP '96) that is established to be optimal for the complete graph and bipartite graphs (by Goldberg-MacKenzie (SODA '96, JCSS '99)).\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Choice models, which capture popular preferences over objects of interest, play a key role in making decisions whose eventual outcome is impacted by human choice behavior. In most scenarios, the choice model, which can effectively be viewed as a distribution over permutations, must be learned from observed data. The observed data, in turn, may frequently be viewed as (partial, noisy) information about marginals of this distribution over permutations. As such, the search for an appropriate choice model boils down to learning a distribution over permutations that is (near-)consistent with observed information about this distribution.\n  In this work, we pursue a non-parametric approach which seeks to learn a choice model (i.e. a distribution over permutations) with {\\em sparsest} possible support, and consistent with observed data. We assume that the data observed consists of noisy information pertaining to the marginals of the choice model we seek to learn. We establish that {\\em any} choice model admits a `very' sparse approximation in the sense that there exists a choice model whose support is small relative to the dimension of the observed data and whose marginals approximately agree with the observed marginal information. We further show that under, what we dub, `signature' conditions, such a sparse approximation can be found in a computationally efficiently fashion relative to a brute force approach. An empirical study using the American Psychological Association election data-set suggests that our approach manages to unearth useful structural properties of the underlying choice model using the sparse approximation found. Our results further suggest that the signature condition is a potential alternative to the recently popularized Restricted Null Space condition for efficient recovery of sparse models.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Communities in social networks or graphs are sets of well-connected, overlapping vertices. The effectiveness of a community detection algorithm is determined by accuracy in finding the ground-truth communities and ability to scale with the size of the data. In this work, we provide three contributions. First, we show that a popular measure of accuracy known as the F1 score, which is between 0 and 1, with 1 being perfect detection, has an information lower bound is 0.5. We provide a trivial algorithm that produces communities with an F1 score of 0.5 for any graph! Somewhat surprisingly, we find that popular algorithms such as modularity optimization, BigClam and CESNA have F1 scores less than 0.5 for the popular IMDB graph. To rectify this, as the second contribution we propose a generative model for community formation, the sequential community graph, which is motivated by the formation of social networks. Third, motivated by our generative model, we propose the leader-follower algorithm (LFA). We prove that it recovers all communities for sequential community graphs by establishing a structural result that sequential community graphs are chordal. For a large number of popular social networks, it recovers communities with a much higher F1 score than other popular algorithms. For the IMDB graph, it obtains an F1 score of 0.81. We also propose a modification to the LFA called the fast leader-follower algorithm (FLFA) which in addition to being highly accurate, is also fast, with a scaling that is almost linear in the network size.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider a queueing network in which there are constraints on which queues may be served simultaneously; such networks may be used to model input-queued switches and wireless networks. The scheduling policy for such a network specifies which queues to serve at any point in time. We consider a family of scheduling policies, related to the maximum-weight policy of Tassiulas and Ephremides [IEEE Trans. Automat. Control 37 (1992) 1936--1948], for single-hop and multihop networks. We specify a fluid model and show that fluid-scaled performance processes can be approximated by fluid model solutions. We study the behavior of fluid model solutions under critical load, and characterize invariant states as those states which solve a certain network-wide optimization problem. We use fluid model results to prove multiplicative state space collapse. A notable feature of our results is that they do not assume complete resource pooling.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Message passing type algorithms such as the so-called Belief Propagation algorithm have recently gained a lot of attention in the statistics, signal processing and machine learning communities as attractive algorithms for solving a variety of optimization and inference problems. As a decentralized, easy to implement and empirically successful algorithm, BP deserves attention from the theoretical standpoint, and here not much is known at the present stage. In order to fill this gap we consider the performance of the BP algorithm in the context of the capacitated minimum-cost network flow problem - the classical problem in the operations research field. We prove that BP converges to the optimal solution in the pseudo-polynomial time, provided that the optimal solution of the underlying problem is unique and the problem input is integral. Moreover, we present a simple modification of the BP algorithm which gives a fully polynomial-time randomized approximation scheme (FPRAS) for the same problem, which no longer requires the uniqueness of the optimal solution. This is the first instance where BP is proved to have fully-polynomial running time. Our results thus provide a theoretical justification for the viability of BP as an attractive method to solve an important class of optimization problems.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider a switched network, a fairly general constrained  queueing network model that has been used successfully to model the detailed packet-level dynamics in communication networks, such as input-queued switches and wireless networks. The main operational issue in this model is that of deciding which queues to serve, subject to  certain constraints. In this paper, we study qualitative performance properties of the well known $\u03b1$-weighted scheduling policies. The stability, in the  sense of positive recurrence, of these policies has been well understood. We establish exponential upper bounds on the tail of the steady-state distribution  of the backlog. Along the way, we prove finiteness of the expected steady-state backlog when $\u03b1<1$, a property that was  known only for $\u03b1\\geq 1$. Finally, we analyze the excursions of the maximum backlog over a finite time horizon for $\u03b1\\geq 1$. As a consequence, for $\u03b1\\geq 1$, we establish the full state space collapse property.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Recently there has been considerable interest in the design of efficient carrier sense multiple access(CSMA) protocol for wireless network. The basic assumption underlying recent results is availability of perfect carrier sense information. This allows for design of continuous time algorithm under which collisions are avoided. The primary purpose of this note is to show how these results can be extended in the case when carrier sense information may not be perfect, or equivalently delayed. Specifically, an adaptation of algorithm in Rajagopalan, Shah, Shin (2009) is presented here for time slotted setup with carrier sense information available only at the end of the time slot. To establish its throughput optimality, in additon to method developed in Rajagopalan, Shah, Shin (2009), understanding properties of stationary distribution of a certain non-reversible Markov chain as well as bound on its mixing time is essential. This note presents these key results. A longer version of this note will provide detailed account of how this gets incorporated with methods of Rajagopalan, Shah, Shin (2009) to provide the positive recurrence of underlying network Markov process. In addition, these results will help design optimal rate control in conjunction with CSMA in presence of collision building upon the method of Jiang, Shah, Shin, Walrand (2009).\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "The packet is the fundamental unit of transportation in modern communication networks such as the Internet. Physical layer scheduling decisions are made at the level of packets, and packet-level models with exogenous arrival processes have long been employed to study network performance, as well as design scheduling policies that more efficiently utilize network resources. On the other hand, a user of the network is more concerned with end-to-end bandwidth, which is allocated through congestion control policies such as TCP. Utility-based flow-level models have played an important role in understanding congestion control protocols. In summary, these two classes of models have provided separate insights for flow-level and packet-level dynamics of a network.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the recovery of a nonnegative vector x from measurements y = Ax, where A is an m-by-n matrix whos entries are in {0, 1}. We establish that when A corresponds to the adjacency matrix of a bipartite graph with sufficient expansion, a simple message-passing algorithm produces an estimate \\hat{x} of x satisfying ||x-\\hat{x}||_1 \\leq O(n/k) ||x-x(k)||_1, where x(k) is the best k-sparse approximation of x. The algorithm performs O(n (log(n/k))^2 log(k)) computation in total, and the number of measurements required is m = O(k log(n/k)). In the special case when x is k-sparse, the algorithm recovers x exactly in time O(n log(n/k) log(k)). Ultimately, this work is a further step in the direction of more formally developing the broader role of message-passing algorithms in solving compressed sensing problems.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the problem of recovering a function over the space of permutations (or, the symmetric group) over $n$ elements from given partial information; the partial information we consider is related to the group theoretic Fourier Transform of the function. This problem naturally arises in several settings such as ranked elections, multi-object tracking, ranking systems, and recommendation systems. Inspired by the work of Donoho and Stark in the context of discrete-time functions, we focus on non-negative functions with a sparse support (support size $\\ll$ domain size). Our recovery method is based on finding the sparsest solution (through $\\ell_0$ optimization) that is consistent with the available information. As the main result, we derive sufficient conditions for functions that can be recovered exactly from partial information through $\\ell_0$ optimization. Under a natural random model for the generation of functions, we quantify the recoverability conditions by deriving bounds on the sparsity (support size) for which the function satisfies the sufficient conditions with a high probability as $n \\to \\infty$. $\\ell_0$ optimization is computationally hard. Therefore, the popular compressive sensing literature considers solving the convex relaxation, $\\ell_1$ optimization, to find the sparsest solution. However, we show that $\\ell_1$ optimization fails to recover a function (even with constant sparsity) generated using the random model with a high probability as $n \\to \\infty$. In order to overcome this problem, we propose a novel iterative algorithm for the recovery of functions that satisfy the sufficient conditions. Finally, using an Information Theoretic framework, we study necessary conditions for exact recovery to be possible.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "A central push in operations models over the last decade has been the incorporation of models of customer choice. Real world implementations of many of these models face the formidable stumbling block of simply identifying the `right' model of choice to use. Thus motivated, we visit the following problem: For a `generic' model of consumer choice (namely, distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal information about these distributions), how may one predict revenues from offering a particular assortment of choices? We present a framework to answer such questions and design a number of tractable algorithms from a data and computational standpoint for the same. This paper thus takes a significant step towards `automating' the crucial task of choice model selection in the context of operational decision problems.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We provide a systematic study of the problem of finding the source of a rumor in a network. We model rumor spreading in a network with a variant of the popular SIR model and then construct an estimator for the rumor source. This estimator is based upon a novel topological quantity which we term \\textbf{rumor centrality}. We establish that this is an ML estimator for a class of graphs. We find the following surprising threshold phenomenon: on trees which grow faster than a line, the estimator always has non-trivial detection probability, whereas on trees that grow like a line, the detection probability will go to 0 as the network grows. Simulations performed on synthetic networks such as the popular small-world and scale-free networks, and on real networks such as an internet AS network and the U.S. electric power grid network, show that the estimator either finds the source exactly or within a few hops of the true source across different network topologies. We compare rumor centrality to another common network centrality notion known as distance centrality. We prove that on trees, the rumor center and distance center are equivalent, but on general networks, they may differ. Indeed, simulations show that rumor centrality outperforms distance centrality in finding rumor sources in networks which are not tree-like.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Motivated by applications of distributed linear estimation, distributed control and distributed optimization, we consider the question of designing linear iterative algorithms for computing the average of numbers in a network. Specifically, our interest is in designing such an algorithm with the fastest rate of convergence given the topological constraints of the network. As the main result of this paper, we design an algorithm with the fastest possible rate of convergence using a non-reversible Markov chain on the given network graph. We construct such a Markov chain by transforming the standard Markov chain, which is obtained using the Metropolis-Hastings method. We call this novel transformation pseudo-lifting. We apply our method to graphs with geometry, or graphs with doubling dimension. Specifically, the convergence time of our algorithm (equivalently, the mixing time of our Markov chain) is proportional to the diameter of the network graph and hence optimal. As a byproduct, our result provides the fastest mixing Markov chain given the network topological constraints, and should naturally find their applications in the context of distributed optimization, estimation and control.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "There has recently been considerable interest in design of low-complexity, myopic, distributed and stable scheduling policies for constrained queueing network models that arise in the context of emerging communication networks. Here, we consider two representative models. One, a model for the collection of wireless nodes communicating through a shared medium, that represents randomly varying number of packets in the queues at the nodes of networks. Two, a buffered circuit switched network model for an optical core of future Internet, to capture the randomness in calls or flows present in the network. The maximum weight scheduling policy proposed by Tassiulas and Ephremide in 1992 leads to a myopic and stable policy for the packet-level wireless network model. But computationally it is very expensive (NP-hard) and centralized. It is not applicable to the buffered circuit switched network due to the requirement of non-premption of the calls in the service. As the main contribution of this paper, we present a stable scheduling algorithm for both of these models. The algorithm is myopic, distributed and performs few logical operations at each node per unit time.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the problem of delivering content cached in a wireless network of n nodes randomly located on a square of area n. The network performance is described by the n2^n-dimensional caching capacity region of the wireless network. We provide an inner bound on this caching capacity region, and, in the high path-loss regime, a matching (in the scaling sense) outer bound. For large path-loss exponent, this provides an information-theoretic scaling characterization of the entire caching capacity region. The proposed communication scheme achieving the inner bound shows that the problems of cache selection and channel coding can be solved separately without loss of order-optimality. On the other hand, our results show that the common architecture of nearest-neighbor cache selection can be arbitrarily bad, implying that cache selection and load balancing need to be performed jointly.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "This paper provides proofs of the rate stability, Harris recurrence, and epsilon-optimality of CSMA algorithms where the backoff parameter of each node is based on its backlog. These algorithms require only local information and are easy to implement.\n  The setup is a network of wireless nodes with a fixed conflict graph that identifies pairs of nodes whose simultaneous transmissions conflict. The paper studies two algorithms. The first algorithm schedules transmissions to keep up with given arrival rates of packets. The second algorithm controls the arrivals in addition to the scheduling and attempts to maximize the sum of the utilities of the flows of packets at the different nodes. For the first algorithm, the paper proves rate stability for strictly feasible arrival rates and also Harris recurrence of the queues. For the second algorithm, the paper proves the epsilon-optimality. Both algorithms operate with strictly local information in the case of decreasing step sizes, and operate with the additional information of the number of nodes in the network in the case of constant step size.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Current approaches to the practical implementation of network coding are batch-based, and often do not use feedback, except possibly to signal completion of a file download. In this paper, the various benefits of using feedback in a network coded system are studied. It is shown that network coding can be performed in a completely online manner, without the need for batches or generations, and that such online operation does not affect the throughput. Although these ideas are presented in a single-hop packet erasure broadcast setting, they naturally extend to more general lossy networks which employ network coding in the presence of feedback. The impact of feedback on queue size at the sender and decoding delay at the receivers is studied. Strategies for adaptive coding based on feedback are presented, with the goal of minimizing the queue size and delay. The asymptotic behavior of these metrics is characterized, in the limit of the traffic load approaching capacity. Different notions of decoding delay are considered, including an order-sensitive notion which assumes that packets are useful only when delivered in order. Our work may be viewed as a natural extension of Automatic Repeat reQuest (ARQ) schemes to coded networks.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "The random assignment problem asks for the minimum-cost perfect matching in the complete $n\\times n$ bipartite graph $\\Knn$ with i.i.d. edge weights, say uniform on $[0,1]$. In a remarkable work by Aldous (2001), the optimal cost was shown to converge to $\u03b6(2)$ as $n\\to\\infty$, as conjectured by M\u00e9zard and Parisi (1987) through the so-called cavity method. The latter also suggested a non-rigorous decentralized strategy for finding the optimum, which turned out to be an instance of the Belief Propagation (BP) heuristic discussed by Pearl (1987). In this paper we use the objective method to analyze the performance of BP as the size of the underlying graph becomes large. Specifically, we establish that the dynamic of BP on $\\Knn$ converges in distribution as $n\\to\\infty$ to an appropriately defined dynamic on the Poisson Weighted Infinite Tree, and we then prove correlation decay for this limiting dynamic. As a consequence, we obtain that BP finds an asymptotically correct assignment in $O(n^2)$ time only. This contrasts with both the worst-case upper bound for convergence of BP derived by Bayati, Shah and Sharma (2005) and the best-known computational cost of $\u0398(n^3)$ achieved by Edmonds and Karp's algorithm (1972).\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We propose a mechanism that incorporates network coding into TCP with only minor changes to the protocol stack, thereby allowing incremental deployment. In our scheme, the source transmits random linear combinations of packets currently in the congestion window. At the heart of our scheme is a new interpretation of ACKs - the sink acknowledges every degree of freedom (i.e., a linear combination that reveals one unit of new information) even if it does not reveal an original packet immediately. Such ACKs enable a TCP-like sliding-window approach to network coding. Our scheme has the nice property that packet losses are essentially masked from the congestion control algorithm. Our algorithm therefore reacts to packet drops in a smooth manner, resulting in a novel and effective approach for congestion control over networks involving lossy links such as wireless links. Our experiments show that our algorithm achieves higher throughput compared to TCP in the presence of lossy wireless links. We also establish the soundness and fairness properties of our algorithm.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the question of determining the scaling of the $n^2$-dimensional balanced unicast and the $n 2^n$-dimensional balanced multicast capacity regions of a wireless network with $n$ nodes placed uniformly at random in a square region of area $n$ and communicating over Gaussian fading channels. We identify this scaling of both the balanced unicast and multicast capacity regions in terms of $\u0398(n)$, out of $2^n$ total possible, cuts. These cuts only depend on the geometry of the locations of the source nodes and their destination nodes and the traffic demands between them, and thus can be readily evaluated. Our results are constructive and provide optimal (in the scaling sense) communication schemes.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We consider the problem of designing a fair scheduling algorithm for discrete-time constrained queuing networks. Each queue has dedicated exogenous packet arrivals. There are constraints on which queues can be served simultaneously. This model effectively describes important special instances like network switches, interference in wireless networks, bandwidth sharing for congestion control and traffic scheduling in road roundabouts. Fair scheduling is required because it provides isolation to different traffic flows; isolation makes the system more robust and enables providing quality of service. Existing work on fairness for constrained networks concentrates on flow based fairness. As a main result, we describe a notion of packet based fairness by establishing an analogy with the ranked election problem: packets are voters, schedules are candidates and each packet ranks the schedules based on its priorities. We then obtain a scheduling algorithm that achieves the described notion of fairness by drawing upon the seminal work of Goodman and Markowitz (1952). This yields the familiar Maximum Weight (MW) style algorithm. As another important result we prove that algorithm obtained is throughput optimal. There is no reason a priori why this should be true, and the proof requires non-traditional methods.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "We investigate the use of message-passing algorithms for the problem of finding the max-weight independent set (MWIS) in a graph. First, we study the performance of the classical loopy max-product belief propagation. We show that each fixed point estimate of max-product can be mapped in a natural way to an extreme point of the LP polytope associated with the MWIS problem. However, this extreme point may not be the one that maximizes the value of node weights; the particular extreme point at final convergence depends on the initialization of max-product. We then show that if max-product is started from the natural initialization of uninformative messages, it always solves the correct LP -- if it converges. This result is obtained via a direct analysis of the iterative algorithm, and cannot be obtained by looking only at fixed points.\n  The tightness of the LP relaxation is thus necessary for max-product optimality, but it is not sufficient. Motivated by this observation, we show that a simple modification of max-product becomes gradient descent on (a convexified version of) the dual of the LP, and converges to the dual optimum. We also develop a message-passing algorithm that recovers the primal MWIS solution from the output of the descent algorithm. We show that the MWIS estimate obtained using these two algorithms in conjunction is correct when the graph is bipartite and the MWIS is unique.\n  Finally, we show that any problem of MAP estimation for probability distributions over finite domains can be reduced to an MWIS problem. We believe this reduction will yield new insights and algorithms for MAP estimation.\n        \u25b3 Less", "author": "Devavrat Shah"}, {"abstract": "Two-way Gaussian protocols have the potential to increase quantum key distribution (QKD) protocols' secret-key rates by orders of magnitudes~[Phys.~Rev.~A {\\bf 94}, 012322 (2016)]. Security proofs for two-way protocols, however, are underdeveloped at present. In this paper, we establish a security proof framework for the general coherent attack on two-way Gaussian protocols in the asymptotic regime. We first prove that coherent-attack security can be reduced to collective-attack security for all two-way QKD protocols. Next, we identify two different constraints that each provide intrusion parameters which bound an eavesdropper's coherent-attack information gain for any two-way Gaussian QKD protocol. Finally, we apply our results to two such protocols.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Floodlight quantum key distribution (FL-QKD) has realized a 1.3\\,Gbit/s secret-key rate (SKR) over a 10-dB-loss channel against a frequency-domain collective attack [Quantum~Sci.~Technol. {\\bf 3}, 025007 (2018)]. It achieved this remarkable SKR by means of binary phase-shift keying (BPSK) of multiple optical modes. Moreover, it did so with available technology, and without space-division or wavelength-division multiplexing. In this paper we explore whether replacing FL-QKD's BPSK modulation with a high-order encoding can further increase that protocol's SKR. First, we show that going to $K$-ary phase-shift keying with $K = 32$ doubles---from 2.0 to 4.5\\,Gbit/s---the theoretical prediction from [Phys.~Rev.~A {\\bf 94}, 012322 (2016)] for FL-QKD's BPSK SKR on a 50-km-long fiber link. Second, we show that $2d\\times 2d$ quadrature amplitude modulation (QAM) does not offer any SKR improvement beyond what its $d=1$ case---which is equivalent to quadrature phase-shift keying---provides.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Non-Gaussian states and operations are crucial for various continuous-variable quantum information processing tasks. To quantitatively understand non-Gaussianity beyond states, we establish a resource theory for non-Gaussian operations. In our framework, we consider Gaussian operations as free operations, and non-Gaussian operations as resources. We define entanglement-assisted non-Gaussianity generating power and show that it is a monotone that is non-increasing under the set of free super-operations, i.e., concatenation and tensoring with Gaussian channels. For conditional unitary maps, this monotone can be analytically calculated. As examples, we show that the non-Gaussianity of ideal photon-number subtraction and photon-number addition equal the non-Gaussianity of the single-photon Fock state. Based on our non-Gaussianity monotone, we divide non-Gaussian operations into two classes: (1) the finite non-Gaussianity class, e.g., photon-number subtraction, photon-number addition and all Gaussian-dilatable non-Gaussian channels; and (2) the diverging non-Gaussianity class, e.g., the binary phase-shift channel and the Kerr nonlinearity. This classification also implies that not all non-Gaussian channels are exactly Gaussian-dilatable. Our resource theory enables a quantitative characterization and a first classification of non-Gaussian operations, paving the way towards the full understanding of non-Gaussianity.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "By precisely monitoring the \"ticks\" of Nature's most precise clocks (millisecond pulsars), scientists are trying to detect the \"ripples in spacetime\" (gravitational waves) produced by the inspirals of supermassive black holes in the centers of distant merging galaxies. Here we describe a relatively simple demonstration that uses two metronomes and a microphone to illustrate several techniques used by pulsar astronomers to search for gravitational waves. An adapted version of this demonstration could be used as an instructional laboratory investigation at the undergraduate level.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "The ability to see around corners, i.e., recover details of a hidden scene from its reflections in the surrounding environment, is of considerable interest in a wide range of applications. However, the diffuse nature of light reflected from typical surfaces leads to mixing of spatial information in the collected light, precluding useful scene reconstruction. Here, we employ a computational imaging technique that opportunistically exploits the presence of occluding objects, which obstruct probe-light propagation in the hidden scene, to undo the mixing and greatly improve scene recovery. Importantly, our technique obviates the need for the ultrafast time-of-flight measurements employed by most previous approaches to hidden-scene imaging. Moreover, it does so in a photon-efficient manner based on an accurate forward model and a computational algorithm that, together, respect the physics of three-bounce light propagation and single-photon detection. Using our methodology, we demonstrate reconstruction of hidden-surface reflectivity patterns in a meter-scale environment from non-time-resolved measurements. Ultimately, our technique represents an instance of a rich and promising new imaging modality with important potential implications for imaging science.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We analyze the fundamental quantum limit of the resolution of an optical imaging system from the perspective of the detection problem of deciding whether the optical field in the image plane is generated by one incoherent on-axis source with brightness $\u03b5$ or by two $\u03b5/2$-brightness incoherent sources that are symmetrically disposed about the optical axis. Using the exact thermal-state model of the field, we derive the quantum Chernoff bound for the detection problem, which specifies the optimum rate of decay of the error probability with increasing number of collected photons that is allowed by quantum mechanics. We then show that recently proposed linear-optic schemes approach the quantum Chernoff bound---the method of binary spatial-mode demultiplexing (B-SPADE) is quantum-optimal for all values of separation, while a method using image-inversion interferometry (SLIVER) is near-optimal for sub-Rayleigh separations. We then simplify our model using a low-brightness approximation that is very accurate for optical microscopy and astronomy, derive quantum Chernoff bounds conditional on the number of photons detected, and show the optimality of our schemes in this conditional detection paradigm. For comparison, we analytically demonstrate the superior scaling of the Chernoff bound for our schemes with source separation relative to that of spatially-resolved direct imaging. Our schemes have the advantages over the quantum-optimal (Helstrom) measurement in that they do not involve joint measurements over multiple modes, and that they do not require the angular separation for the two-source hypothesis to be given \\emph{a priori} and can offer that information as a bonus in the event of a successful detection.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "The Gemini Planet Imager Exoplanet Survey (GPIES) is a multi-year direct imaging survey of 600 stars to discover and characterize young Jovian exoplanets and their environments. We have developed an automated data architecture to process and index all data related to the survey uniformly. An automated and flexible data processing framework, which we term the Data Cruncher, combines multiple data reduction pipelines together to process all spectroscopic, polarimetric, and calibration data taken with GPIES. With no human intervention, fully reduced and calibrated data products are available less than an hour after the data are taken to expedite follow-up on potential objects of interest. The Data Cruncher can run on a supercomputer to reprocess all GPIES data in a single day as improvements are made to our data reduction pipelines. A backend MySQL database indexes all files, which are synced to the cloud, and a front-end web server allows for easy browsing of all files associated with GPIES. To help observers, quicklook displays show reduced data as they are processed in real-time, and chatbots on Slack post observing information as well as reduced data products. Together, the GPIES automated data processing architecture reduces our workload, provides real-time data reduction, optimizes our observing strategy, and maintains a homogeneously reduced dataset to study planet occurrence and instrument performance.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum key distribution (QKD) enables unconditionally secure communication ensured by the laws of physics, opening a promising route to security infrastructure for the coming age of quantum computers. QKD's demonstrated secret-key rates (SKRs), however, fall far short of the gigabit-per-second rates of classical communication, hindering QKD's widespread deployment. QKD's low SKRs are largely due to existing single-photon-based protocols' vulnerability to channel loss. Floodlight QKD (FL-QKD) boosts SKR by transmitting many photons per encoding, while offering security against collective attacks. Here, we report an FL-QKD experiment operating at a 1.3 Gbit/s SKR over a 10-dB-loss channel. To the best of our knowledge, this is the first QKD demonstration that achieves a gigabit-per-second-class SKR, representing a critical advance toward high-rate QKD at metropolitan-area distances.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Distributed quantum sensing uses quantum correlations between multiple sensors to enhance the measurement of unknown parameters beyond the limits of unentangled systems. We describe a sensing scheme that uses continuous-variable multipartite entanglement to enhance distributed sensing of field-quadrature displacement. By dividing a squeezed-vacuum state between multiple homodyne-sensor nodes using a lossless beam-splitter array, we obtain a root-mean-square (rms) estimation error that scales inversely with the number of nodes (Heisenberg scaling), whereas the rms error of a distributed sensor that does not exploit entanglement is inversely proportional to the square root of number of nodes (standard quantum limit scaling). Our sensor's scaling advantage is destroyed by loss, but it nevertheless retains an rms-error advantage in settings in which there is moderate loss. Our distributed sensing scheme can be used to calibrate continuous-variable quantum key distribution networks, to perform multiple-sensor cold-atom temperature measurements, and to do distributed interferometric phase sensing.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Active non-line-of-sight imaging systems are of growing interest for diverse applications. The most commonly proposed approaches to date rely on exploiting time-resolved measurements, i.e., measuring the time it takes for short light pulses to transit the scene. This typically requires expensive, specialized, ultrafast lasers and detectors that must be carefully calibrated. We develop an alternative approach that exploits the valuable role that natural occluders in a scene play in enabling accurate and practical image formation in such settings without such hardware complexity. In particular, we demonstrate that the presence of occluders in the hidden scene can obviate the need for collecting time-resolved measurements, and develop an accompanying analysis for such systems and their generalizations. Ultimately, the results suggest the potential to develop increasingly sophisticated future systems that are able to identify and exploit diverse structural features of the environment to reconstruct scenes hidden from view.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We establish a symmetry-operator framework for designing quantum error correcting~(QEC) codes based on fundamental properties of the underlying system dynamics. Based on this framework, we propose three hardware-efficient bosonic QEC codes that are suitable for $\u03c7^{(2)}$-interaction based quantum computation: the $\u03c7^{(2)}$ parity-check code, the $\u03c7^{(2)}$ embedded error-correcting code, and the $\u03c7^{(2)}$ binomial code, all of which detect photon-loss or photon-gain errors by means of photon-number parity measurements and then correct them via $\u03c7^{(2)}$ Hamiltonian evolutions and linear-optics transformations. Our symmetry-operator framework provides a systematic procedure for finding QEC codes that are not stabilizer codes. The $\u03c7^{(2)}$ binomial code is of special interest because, with $m\\le N$ identified from channel monitoring, it can correct $m$-photon loss errors, $m$-photon gain errors, and $(m-1)$th-order dephasing errors using logical qudits that are encoded in $O(N)$ photons. In comparison, other bosonic QEC codes require $O(N^2)$ photons to correct the same degree of bosonic errors. Such improved photon-efficiency underscores the additional error-correction power that can be provided by channel monitoring. We develop quantum Hamming bounds for photon-loss errors in the code subspaces associated with the $\u03c7^{(2)}$ parity-check code and the $\u03c7^{(2)}$ embedded error-correcting code, and we prove that these codes saturate their respective bounds. Our $\u03c7^{(2)}$ QEC codes exhibit hardware efficiency in that they address the principal error mechanisms and exploit the available physical interactions of the underlying hardware, thus reducing the physical resources required for implementing their encoding, decoding, and error-correction operations, and their universal encoded-basis gate sets.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum illumination (QI) is an entanglement-enhanced sensing system whose performance advantage over a comparable classical system survives its usage in an entanglement-breaking scenario plagued by loss and noise. In particular, QI's error-probability exponent for discriminating between equally-likely hypotheses of target absence or presence is 6 dB higher than that of the optimum classical system using the same transmitted power. This performance advantage, however, presumes that the target return, when present, has known amplitude and phase, a situation that seldom occurs in lidar applications. At lidar wavelengths, most target surfaces are sufficiently rough that their returns are speckled, i.e., they have Rayleigh-distributed amplitudes and uniformly-distributed phases. QI's optical parametric amplifier receiver -- which affords a 3 dB better-than-classical error-probability exponent for a return with known amplitude and phase -- fails to offer any performance gain for Rayleigh-fading targets. We show that the sum-frequency generation receiver [Phys. Rev. Lett. 118, 040801 (2017)] -- whose error-probability exponent for a nonfading target achieves QI's full 6 dB advantage over optimum classical operation -- outperforms the classical system for Rayleigh-fading targets. In this case, QI's advantage is subexponential: its error probability is lower than the classical system's by a factor of $1/\\ln(M\\bar\u03baN_S/N_B)$, when $M\\bar\u03baN_S/N_B \\gg 1$, with $M\\gg 1$ being the QI transmitter's time-bandwidth product, $N_S \\ll 1$ its brightness, $\\bar\u03ba$ the target's average reflectivity, and $N_B$ the background light's brightness.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Lidar is a well known optical technology for measuring a target's range and radial velocity. We describe two lidar systems that use entanglement between transmitted signals and retained idlers to obtain significant quantum enhancements in simultaneous measurement of these parameters. The first entanglement-enhanced lidar circumvents the Arthurs-Kelly uncertainty relation for simultaneous measurement of range and radial velocity from detection of a single photon returned from the target. This performance presumes there is no extraneous (background) light, but is robust to the roundtrip loss incurred by the signal photons. The second entanglement-enhanced lidar---which requires a lossless, noiseless environment---realizes Heisenberg-limited accuracies for both its range and radial-velocity measurements, i.e., their root-mean-square estimation errors are both proportional to $1/M$ when $M$ signal photons are transmitted. These two lidars derive their entanglement-based enhancements from use of a unitary transformation that takes a signal-idler photon pair with frequencies $\u03c9_S$ and $\u03c9_I$ and converts it to a signal-idler photon pair whose frequencies are $(\u03c9_S + \u03c9_I)/2$ and $\u03c9_S-\u03c9_I$. Insight into how this transformation provides its benefits is provided through an analogy to superdense coding.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We use the noisy entanglement-assisted classical capacity formula [arXiv:1609.08592] to create a coherent-attack security framework for Gaussian two-way quantum key distribution protocols in the asymptotic region.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We prove that universal quantum computation can be realized---using only linear optics and $\u03c7^{(2)}$ (three-wave mixing) interactions---in any $(n+1)$-dimensional qudit basis of the $n$-pump-photon subspace. First, we exhibit a strictly universal gate set for the qubit basis in the one-pump-photon subspace. Next, we demonstrate qutrit-basis universality by proving that $\u03c7^{(2)}$ Hamiltonians and photon-number operators generate the full $\\mathfrak{u}(3)$ Lie algebra in the two-pump-photon subspace, and showing how the qutrit controlled-$Z$ gate can be implemented with only linear optics and $\u03c7^{(2)}$ interactions. We then use proof by induction to obtain our general qudit result. Our induction proof relies on coherent photon injection/subtraction, a technique enabled by $\u03c7^{(2)}$ interaction between the encoding modes and ancillary modes. Finally, we show that coherent photon injection is more than a conceptual tool in that it offers a route to preparing high-photon-number Fock states from single-photon Fock states.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum illumination (QI) provides entanglement-based target detection---in an entanglement-breaking environment---whose performance is significantly better than that of optimum classical-illumination target detection. QI's performance advantage was established in a Bayesian setting with the target presumed equally likely to be absent or present and error probability employed as the performance metric. Radar theory, however, eschews that Bayesian approach, preferring the Neyman-Pearson performance criterion to avoid the difficulties of accurately assigning prior probabilities to target absence and presence and appropriate costs to false-alarm and miss errors. We have recently reported an architecture---based on sum-frequency generation (SFG) and feedforward (FF) processing---for minimum error-probability QI target detection with arbitrary prior probabilities for target absence and presence. In this paper, we use our results for FF-SFG reception to determine the receiver operating characteristic---detection probability versus false-alarm probability---for optimum QI target detection under the Neyman-Pearson criterion.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Floodlight quantum key distribution (FL-QKD) uses binary phase-shift keying (BPSK) of multiple optical modes to achieve Gbps secret-key rates (SKRs) at metropolitan-area distances. We show that FL-QKD's SKR can be doubled by using 32-ary PSK.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Spectrally unentangled biphotons with high single-spatiotemporal-mode purity are highly desirable for many quantum information processing tasks. We generate biphotons with an inferred heralded-state spectral purity of 99%, the highest to date without any spectral filtering, by pulsed spontaneous parametric downconversion in a custom-fabricated periodically-poled KTiOPO$_4$ crystal under extended Gaussian phase-matching conditions. To efficiently characterize the joint spectral intensity of the generated biphotons at high spectral resolution, we employ a commercially available dispersion compensation module (DCM) with a dispersion equivalent to 100 km of standard optical fiber and with an insertion loss of only 2.8 dB. Compared with the typical method of using two temperature-stabilized equal-length fibers that incurs an insertion loss of 20 dB per fiber, the DCM approach achieves high spectral resolution in a much shorter measurement time. Because the dispersion amount and center wavelengths of DCMs can be easily customized, spectral characterization in a wide range of quantum photonic applications should benefit significantly from this technique.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum key distribution (QKD) exploits the quantum nature of light to share provably secure keys, allowing secure communication in the presence of an eavesdropper. The first QKD schemes used photons encoded in two states, such as polarization. Recently, much effort has turned to large-alphabet QKD schemes, which encode photons in high-dimensional basis states. Compared to binary-encoded QKD, large-alphabet schemes can encode more secure information per detected photon, boosting secure communication rates, and also provide increased resilience to noise and loss. High-dimensional encoding may also improve the efficiency of other quantum information processing tasks, such as performing Bell tests and implementing quantum gates. Here, we demonstrate a large-alphabet QKD protocol based on high-dimensional temporal encoding. We achieve record secret-key rates and perform the first field demonstration of large-alphabet QKD. This demonstrates a new, practical way to optimize secret-key rates and marks an important step towards transmission of high-dimensional quantum states in deployed networks.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum metrology utilizes nonclassical resources, such as entanglement or squeezed light, to realize sensors whose performance exceeds that afforded by classical-state systems. Environmental loss and noise, however, easily destroy nonclassical resources, and thus nullify the performance advantages of most quantum-enhanced sensors. Quantum illumination (QI) is different. It is a robust entanglement-enhanced sensing scheme whose 6 dB performance advantage over a coherent-state sensor of the same average transmitted photon number survives the initial entanglement's eradication by loss and noise. Unfortunately, an implementation of the optimum quantum receiver that would reap QI's full performance advantage has remained elusive, owing to its having to deal with a huge number of very noisy optical modes. We show how sum-frequency generation (SFG) can be fruitfully applied to optimum multi-mode Gaussian-mixed-state discrimination. Applied to QI, our analysis and numerical evaluations demonstrate that our SFG receiver saturates QI's quantum Chernoff bound. Moreover, augmenting our SFG receiver with a feed-forward (FF) mechanism pushes its performance to the Helstrom bound in the limit of low signal brightness. The FF-SFG receiver thus opens the door to optimum quantum-enhanced imaging, radar detection, state and channel tomography, and communication in practical Gaussian-state situations.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We consider the problem, where a camera is tasked with determining one of two hypotheses: first with an incoherently-radiating quasi-monochromatic point source and the second with two identical closely spaced point sources. We are given that the total number of photons collected over an integration time is assumed to be the same under either hypothesis. For the one-source hypothesis, the source is taken to be on-axis along the line of sight and for the two-source hypothesis, we give ourselves the prior knowledge of the angular separation of the sources, and they are assumed to be identical and located symmetrically off-axis. This problem was studied by Helstrom in 1973, who evaluated the probability of error achievable using a sub-optimal optical measurement, with an unspecified structured realization. In this paper, we evaluate the quantum Chernoff bound, a lower bound on the minimum probability of error achievable by any physically-realizable receiver, which is exponentially tight in the regime that the integration time is high. We give an explicit structured receiver that separates three orthogonal spatial modes of the aperture field followed by quantum-noise-limited time-resolved photon measurement and show that this achieves the quantum Chernoff bound. In other words, the classical Chernoff bound of our mode-resolved detector exactly matches the quantum Chernoff bound for this problem. Finally, we evaluate the classical Chernoff bound on the error probability achievable using an ideal focal plane array---a signal shot-noise limited continuum photon-detection receiver with infinitely many infinitesimally-tiny pixels---and quantify its performance gap with the quantum limit.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "A quantum random number generator (QRNG) generates genuine randomness from the intrinsic probabilistic nature of quantum mechanics. The central problems for most QRNGs are estimating the entropy of the genuine randomness and producing such randomness at high rates. Here we propose and demonstrate a proof-of-concept QRNG that operates at a high rate of 24 Mbit/s by means of a high-dimensional entanglement system, in which the user monitors the entropy in real time via the observation of a nonlocal quantum interference, without a detailed characterization of the devices. Our work provides an important approach to a robust QRNG with trusted but error-prone devices.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Floodlight quantum key distribution (FL-QKD) is a radically different QKD paradigm that can achieve Gbit/s secret-key rates over metropolitan area distances without multiplexing [Phys. Rev. A 94, 012322 (2016)]. It is a two-way protocol that transmits many photons per bit duration and employs a high-gain optical amplifier, neither of which can be utilized by existing QKD protocols to mitigate channel loss. FL-QKD uses an optical bandwidth that is substantially larger than the modulation rate and performs decoding with a unique broadband homodyne receiver. Essential to FL-QKD is Alice's injection of photons from a photon-pair source--in addition to the light used for key generation--into the light she sends to Bob. This injection enables Alice and Bob to quantify Eve's intrusion and thus secure FL-QKD against collective attacks. Our proof-of-concept experiment included 10 dB propagation loss--equivalent to 50 km of low-loss fiber--and achieved a 55 Mbit/s secret-key rate (SKR) for a 100 Mbit/s modulation rate, as compared to the state-of-the-art system's 1 Mbit/s SKR for a 1 Gbit/s modulation rate [Opt. Express 21, 24550-24565 (2013)], representing ~500-fold and ~50-fold improvements in secret-key efficiency (SKE) (bits per channel use) and SKR (bits per second), respectively.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Time-energy high-dimensional quantum key distribution (HD-QKD) leverages the high-dimensional nature of time-energy entangled biphotons and the loss tolerance of single-photon detection to achieve long-distance key distribution with high photon information efficiency. To date, the general-attack security of HD-QKD has only been proven in the asymptotic regime, while HD-QKD's finite-key security has only been established for a limited set of attacks. Here we fill this gap by providing a rigorous HD-QKD security proof for general attacks in the finite-key regime. Our proof relies on a novel entropic uncertainty relation that we derive for time and conjugate-time measurements using dispersive optics, and our analysis includes an efficient decoy-state protocol in its parameter estimation. We present numerically-evaluated secret-key rates illustrating the feasibility of secure and composable HD-QKD over metropolitan-area distances when the system is subjected to the most powerful eavesdropping attack.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Many partially-successful attempts have been made to find the most natural discrete-variable version of Shannon's entropy power inequality (EPI). We develop an axiomatic framework from which we deduce the natural form of a discrete-variable EPI and an associated entropic monotonicity in a discrete-variable central limit theorem. In this discrete EPI, the geometric distribution, which has the maximum entropy among all discrete distributions with a given mean, assumes a role analogous to the Gaussian distribution in Shannon's EPI. The entropy power of $X$ is defined as the mean of a geometric random variable with entropy $H(X)$. The crux of our construction is a discrete-variable version of Lieb's scaled addition $X \\boxplus_\u03b7Y$ of two discrete random variables $X$ and $Y$ with $\u03b7\\in (0, 1)$. We discuss the relationship of our discrete EPI with recent work of Yu and Johnson who developed an EPI for a restricted class of random variables that have ultra-log-concave (ULC) distributions. Even though we leave open the proof of the aforesaid natural form of the discrete EPI, we show that this discrete EPI holds true for variables with arbitrary discrete distributions when the entropy power is redefined as $e^{H(X)}$ in analogy with the continuous version. Finally, we show that our conjectured discrete EPI is a special case of the yet-unproven Entropy Photon-number Inequality (EPnI), which assumes a role analogous to Shannon's EPI in capacity proofs for Gaussian bosonic (quantum) channels.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "The secret key rate attained by a free-space QKD system in the {\\em near-field} propagation regime (relevant for $1$-$10$ km range using $\\approx 7$ cm radii transmit and receive apertures and $1.55~\u03bc$m transmission center wavelenght) can benefit from the use of multiple spatial modes. A suite of theoretical research in recent years have suggested the use of orbital-angular-momentum (OAM) bearing spatial modes of light to obtain this improvement in rate. We show that most of the aforesaid rate improvement in the near field afforded by spatial-mode multiplexing can be realized by a simple-to-build overlapping Gaussian beam array (OGBA) and a pixelated detector array. With the current state-of-the-art in OAM-mode-sorting efficiencies, the key-rate performance of our OGBA architecture could come very close to, if not exceed, that of a system employing OAM modes, but at a fraction of the cost.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We propose an optical scheme, employing optical parametric down-converters interlaced with nonlinear sign gates (NSGs), that completely converts an $n$-photon Fock-state pump to $n$ signal-idler photon pairs when the down-converters' crystal lengths are chosen appropriately. The proof of this assertion relies on amplitude amplification, analogous to that employed in Grover search, applied to the full quantum dynamics of single-mode parametric down-conversion. When we require that all Grover iterations use the same crystal, and account for potential experimental limitations on crystal-length precision, our optimized conversion efficiencies reach unity for $1\\le n \\le 5$, after which they decrease monotonically for $n$ values up to 50, which is the upper limit of our numerical dynamics evaluations. Nevertheless, our conversion efficiencies remain higher than those for a conventional (no NSGs) down-converter.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We determine the ultimate classical information capacity of a linear time-invariant bosonic channel with additive phase-insensitive Gaussian noise. This channel can model fiber-optic communication at power levels below the threshold for significant nonlinear effects. We provide a general continuous-time result that gives the ultimate capacity for such a channel operating in the quasimonochromatic regime under an average power constraint. This ultimate capacity is compared with corresponding results for heterodyne and homodyne detection over the same channel.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Unlike optical CCDs, near-infrared detectors, which are based on CMOS hybrid readout technology, typically suffer from electrical crosstalk between the pixels. The interpixel capacitance (IPC) responsible for the crosstalk affects the point-spread function (PSF) of the telescope, increasing the size and modifying the shape of all objects in the images while correlating the Poisson noise. Upcoming weak lensing surveys that use these detectors, such as WFIRST, place stringent requirements on the PSF size and shape (and the level at which these are known), which in turn must be translated into requirements on IPC. To facilitate this process, we present a first study of the effect of IPC on WFIRST PSF sizes and shapes. Realistic PSFs are forward-simulated from physical principles for each WFIRST bandpass. We explore how the PSF size and shape depends on the range of IPC coupling with pixels that are connected along an edge or corner; for the expected level of IPC in WFIRST, IPC increases the PSF sizes by $\\sim$5\\%. We present a linear fitting formula that describes the uncertainty in the PSF size or shape due to uncertainty in the IPC, which could arise for example due to unknown time evolution of IPC as the detectors age or due to spatial variation of IPC across the detector. We also study of the effect of a small anisotropy in the IPC, which further modifies the PSF shapes. Our results are a first, critical step in determining the hardware and characterization requirements for the detectors used in the WFIRST survey.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "The channel loss incurred in long-distance transmission places a significant burden on quantum key distribution (QKD) systems: they must defeat a passive eavesdropper who detects all the light lost in the quantum channel and does so without disturbing the light that reaches the intended destination. The current QKD implementation with the highest long-distance secret-key rate meets this challenge by transmitting no more than one photon per bit [Opt. Express 21, 24550-24565 (2013)]. As a result, it cannot achieve the Gbps secret-key rate needed for one-time pad encryption of large data files unless an impractically large amount of multiplexing is employed. We introduce floodlight QKD (FL-QKD), which floods the quantum channel with a high number of photons per bit distributed over a much greater number of optical modes. FL-QKD offers security against the optimum frequency-domain collective attack by transmitting less than one photon per mode and using photon-coincidence channel monitoring, and it is completely immune to passive eavesdropping. More importantly, FL-QKD is capable of a 2 Gbps secret-key rate over a 50 km fiber link, without any multiplexing, using available equipment, i.e., no new technology need be developed. FL-QKD achieves this extraordinary secret-key rate by virtue of its unprecedented secret-key efficiency, in bits per channel use, which exceeds those of state-of-the-art systems by two orders of magnitude.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "The binary (one-bit-per-photon) encoding that most existing quantum key distribution (QKD) protocols employ puts a fundamental limit on their achievable key rates, especially under high channel loss conditions associated with long-distance fiber-optic or satellite-to-ground links. Inspired by the pulse-position-modulation (PPM) approach to photon-starved classical communications, we design and demonstrate the first PPM-QKD, whose security against collective attacks is established through continuous-variable entanglement measurements that also enable a novel decoy-state protocol performed conveniently in post processing. We achieve a throughput of 8.0 Mbit/s (2.5 Mbit/s for loss equivalent to 25 km of fiber) and secret-key capacity up to 4.0 bits per detected photon, thus demonstrating the significant enhancement afforded by high-dimensional encoding. These results point to a new avenue for realizing high-throughput satellite-based or long-haul fiber-optic quantum communications beyond their photon-reception-rate limits.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We propose a two-way secure-communication protocol in which Alice uses an amplified spontaneous emission source while Bob employs binary phase-shift keying and an optical amplifier. Against an eavesdropper who captures all the light lost in fibers linking Alice and Bob, this protocol is capable of 3.5 Gbps quantum-secured direct communication at 50 km range. If Alice augments her terminal with a spontaneous parametric downconverter and both Alice and Bob add channel monitors, they can realize 2 Gbps quantum key distribution at that range against an eavesdropper who injects her own light into Bob's terminal. Compared with prevailing quantum key distribution methods, this protocol has the potential to significantly increase secure key rates at long distances by employing many ultrabroadband photons per key bit to mitigate channel loss.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Light detection and ranging systems reconstruct scene depth from time-of-flight measurements. For low light-level depth imaging applications, such as remote sensing and robot vision, these systems use single-photon detectors that resolve individual photon arrivals. Even so, they must detect a large number of photons to mitigate Poisson shot noise and reject anomalous photon detections from background light. We introduce a novel framework for accurate depth imaging using a small number of detected photons in the presence of an unknown amount of background light that may vary spatially. It employs a Poisson observation model for the photon detections plus a union-of-subspaces constraint on the discrete-time flux from the scene at any single pixel. Together, they enable a greedy signal-pursuit algorithm to rapidly and simultaneously converge on accurate estimates of scene depth and background flux, without any assumptions on spatial correlations of the depth or background flux. Using experimental single-photon data, we demonstrate that our proposed framework recovers depth features with 1.7 cm absolute error, using 15 photons per image pixel and an illumination pulse with 6.7-cm scaled root-mean-square length. We also show that our framework outperforms the conventional pixelwise log-matched filtering, which is a computationally-efficient approximation to the maximum-likelihood solution, by a factor of 6.1 in absolute depth error.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum entanglement is a fundamental resource for secure information processing and communications, where hyperentanglement or high-dimensional entanglement has been separately proposed towards high data capacity and error resilience. The continuous-variable nature of the energy-time entanglement makes it an ideal candidate for efficient high-dimensional coding with minimal limitations. Here we demonstrate the first simultaneous high-dimensional hyperentanglement using a biphoton frequency comb to harness the full potential in both energy and time domain. The long-postulated Hong-Ou-Mandel quantum revival is exhibited, with up to 19 time-bins, 96.5% visibilities. We further witness the high-dimensional energy-time entanglement through Franson revivals, which is observed periodically at integer time-bins, with 97.8% visibility. This qudit state is observed to simultaneously violate the generalized Bell inequality by up to 10.95 deviations while observing recurrent Clauser-Horne-Shimony-Holt S-parameters up to 2.76. Our biphoton frequency comb provides a platform in photon-efficient quantum communications towards the ultimate channel capacity through energy-time-polarization high-dimensional encoding.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum illumination is a quantum-optical sensing technique in which an entangled source is exploited to improve the detection of a low-reflectivity object that is immersed in a bright thermal background. Here we describe and analyze a system for applying this technique at microwave frequencies, a more appropriate spectral region for target detection than the optical, due to the naturally-occurring bright thermal background in the microwave regime. We use an electro-optomechanical converter to entangle microwave signal and optical idler fields, with the former being sent to probe the target region and the latter being retained at the source. The microwave radiation collected from the target region is then phase conjugated and upconverted into an optical field that is combined with the retained idler in a joint-detection quantum measurement. The error probability of this microwave quantum-illumination system, or quantum radar, is shown to be superior to that of any classical microwave radar of equal transmitted energy.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Nonclassical states are essential for optics-based quantum information processing, but their fragility limits their utility for practical scenarios in which loss and noise inevitably degrade, if not destroy, nonclassicality. Exploiting nonclassical states in quantum metrology yields sensitivity advantages over all classical schemes delivering the same energy per measurement interval to the sample being probed. These enhancements, almost without exception, are severely diminished by quantum decoherence. Here, we experimentally demonstrate an entanglement-enhanced sensing system that is resilient to quantum decoherence. We employ entanglement to realize a 20% signal-to-noise ratio improvement over the optimum classical scheme in an entanglement-breaking environment plagued by 14 dB of loss and a noise background 75 dB stronger than the returned probe light. Our result suggests that advantageous quantum-sensing technology could be developed for practical situations.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "High-dimensional quantum key distribution (HD-QKD) allows two parties to generate multiple secure bits of information per detected photon. In this work, we show that decoy state protocols can be practically implemented for HD-QKD using only one or two decoy states. HD-QKD with two decoy states, under realistic experimental constraints, can generate multiple secure bits per coincidence at distances over 200 km and at rates similar to those achieved by a protocol with infinite decoy states. Furthermore, HD-QKD with only one decoy state is practical at short distances, where it is almost as secure as a protocol with two decoy states. HD-QKD with only one or two decoy states can therefore be implemented to optimize the rate of secure quantum communications.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Quantum illumination is a quantum-optical sensing technique in which an entangled source is exploited to improve the detection of a low-reflectivity object that is immersed in a bright thermal background. Here we describe and analyze a system for applying this technique at microwave frequencies, a more appropriate spectral region for target detection than the optical, due to the naturally-occurring bright thermal background in the microwave regime. We use an electro-optomechanical converter to entangle microwave signal and optical idler fields, with the former being sent to probe the target region and the latter being retained at the source. The microwave radiation collected from the target region is then phase conjugated and upconverted into an optical field that is combined with the retained idler in a joint-detection quantum measurement. The error probability of this microwave quantum-illumination system, or quantum radar, is shown to be superior to that of any classical microwave radar of equal transmitted energy.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "A framework is established for evaluating {\\sc cphase} gates that use single-photon cross-phase modulation (XPM) originating from the Kerr nonlinearity. Prior work Phys. Rev. A {\\bf 73,} 062305 (2006)], which assumed that the control and target pulses propagated at the same group velocity, showed that the causality-induced phase noise required by a non-instantaneous XPM response function precluded the possibility of high-fidelity $\u03c0$-radian conditional phase shifts. The framework presented herein incorporates the more realistic case of group-velocity disparity between the control and target pulses, as employed in existing XPM-based fiber-optical switches. Nevertheless, the causality-induced phase noise identified in [Phys. Rev. A {\\bf 73,} 062305 (2006)] still rules out high-fidelity $\u03c0$-radian conditional phase shifts. This is shown to be so for both a reasonable theoretical model for the XPM response function and for the experimentally-measured XPM response function of silica-core fiber.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Recently, Rodenburg et al (2014 New J. Phys. 16 033020) presented an approach for simulating propagation over a long path of uniformly distributed Kolmogorov-spectrum turbulence by means of a compact laboratory arrangement that used two carefully placed and controlled spatial light modulators. We show that their simulation approach mimics the behavior of plane-wave propagation, rather than general beam-wave propagation. Thus, the regime in which their orbital angular momentum (OAM) cross-talk results accurately represent the behavior to be expected in horizontal-path propagation through turbulence may be limited to collimated-beam OAM modes whose diameters are sufficient that turbulence-induced beam spread is negligible.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Capturing depth and reflectivity images at low light levels from active illumination of a scene has wide-ranging applications. Conventionally, even with single-photon detectors, hundreds of photon detections are needed at each pixel to mitigate Poisson noise. We develop a robust method for estimating depth and reflectivity using on the order of 1 detected photon per pixel averaged over the scene. Our computational imager combines physically accurate single-photon counting statistics with exploitation of the spatial correlations present in real-world reflectivity and 3D structure. Experiments conducted in the presence of strong background light demonstrate that our computational imager is able to accurately recover scene depth and reflectivity, while traditional maximum-likelihood based imaging methods lead to estimates that are highly noisy. Our framework increases photon efficiency 100-fold over traditional processing and also improves, somewhat, upon first-photon imaging under a total acquisition time constraint in raster-scanned operation. Thus our new imager will be useful for rapid, low-power, and noise-tolerant active optical imaging, and its fixed dwell time will facilitate parallelization through use of a detector array.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We present a security analysis against collective attacks for the recently proposed time-energy entanglement-based quantum key distribution protocol, given the practical constraints of single photon detector efficiency, channel loss, and finite-key considerations. We find a positive secure-key capacity when the key length increases beyond $10^4$ for eight-dimensional systems. The minimum key length required is reduced by the ability to post-select on coincident single-photon detection events. Including finite-key effects, we show the ability to establish a shared secret key over a 200 km fiber link.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "High-dimensional quantum key distribution (HDQKD) offers the possibility of high secure-key rate with high photon-information efficiency. We consider HDQKD based on the time-energy entanglement produced by spontaneous parametric downconversion, and show that it is secure against collective attacks. Its security rests upon visibility data -- obtained from Franson and conjugate-Franson interferometers -- that probe photon-pair frequency correlations and arrival-time correlations. From these measurements an upper bound can be established on the eavesdropper's Holevo information by translating the Gaussian-state security analysis for continuous-variable quantum key distribution so that it applies to our protocol. We show that visibility data from just the Franson interferometer provides a weaker, but nonetheless useful, secure-key rate lower bound. To handle multiple-pair emissions, we incorporate the decoy-state approach into our protocol. Our results show that over 200\\,km transmission distance in optical fiber, time-energy entanglement HDQKD could permit a 700 bit/sec secure-key rate, and a photon information efficiency of 2 secure-key bits per photon coincidence in the key-generation phase using receivers with 15% system efficiency.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "The locking effect is a phenomenon which is unique to quantum information theory and represents one of the strongest separations between the classical and quantum theories of information. The Fawzi-Hayden-Sen (FHS) locking protocol harnesses this effect in a cryptographic context, whereby one party can encode n bits into n qubits while using only a constant-size secret key. The encoded message is then secure against any measurement that an eavesdropper could perform in an attempt to recover the message, but the protocol does not necessarily meet the composability requirements needed in quantum key distribution applications. In any case, the locking effect represents an extreme violation of Shannon's classical theorem, which states that information-theoretic security holds in the classical case if and only if the secret key is the same size as the message. Given this intriguing phenomenon, it is of practical interest to study the effect in the presence of noise, which can occur in the systems of both the legitimate receiver and the eavesdropper. This paper formally defines the locking capacity of a quantum channel as the maximum amount of locked information that can be reliably transmitted to a legitimate receiver by exploiting many independent uses of a quantum channel and an amount of secret key sublinear in the number of channel uses. We provide general operational bounds on the locking capacity in terms of other well-known capacities from quantum Shannon theory. We also study the important case of bosonic channels, finding limitations on these channels' locking capacity when coherent-state encodings are employed and particular locking protocols for these channels that might be physically implementable.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Entanglement is essential to many quantum information applications, but it is easily destroyed by quantum decoherence arising from interaction with the environment. We report the first experimental demonstration of an entanglement-based protocol that is resilient to loss and noise which destroy entanglement. Specifically, despite channel noise 8.3 dB beyond the threshold for entanglement breaking, eavesdropping-immune communication is achieved between Alice and Bob when an entangled source is used, but no such immunity is obtainable when their source is classical. The results prove that entanglement can be utilized beneficially in lossy and noisy situations, i.e., in practical scenarios.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We present an experimental comparison between different iterative ghost imaging algorithms. Our experimental setup utilizes a spatial light modulator for generating known random light fields to illuminate a partially-transmissive object. We adapt the weighting factor used in the traditional ghost imaging algorithm to account for changes in the efficiency of the generated light field. We show that our normalized weighting algorithm can match the performance of differential ghost imaging.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Ghost imaging has been receiving increasing interest for possible use as a remote-sensing system. There has been little comparison, however, between ghost imaging and the imaging laser radars with which it would be competing. Toward that end, this paper presents a performance comparison between a pulsed, computational ghost imager and a pulsed, floodlight-illumination imaging laser radar. Both are considered for range-resolving (3D) imaging of a collection of rough-surfaced objects at standoff ranges in the presence of atmospheric turbulence. Their spatial resolutions and signal-to-noise ratios are evaluated as functions of the system parameters, and these results are used to assess each system's performance trade-offs. Scenarios in which a reflective ghost-imaging system has advantages over a laser radar are identified.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We propose a high-dimensional quantum key distribution (QKD) protocol that employs temporal correlations of entangled photons. The security of the protocol relies on measurements by Alice and Bob in one of two conjugate bases, implemented using dispersive optics. We show that this dispersion-based approach is secure against general coherent attacks. The protocol is additionally compatible with standard fiber telecommunications channels and wavelength division multiplexers. We offer multiple implementations to enhance the transmission rate and describe a heralded qudit source that is easy to implement and enables secret-key generation at up to 100 Mbps at over 2 bits per photon.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We give analytic upper bounds to the channel capacity C for transmission of classical information in electromagnetic channels (bosonic channels with thermal noise). In the practically relevant regimes of high noise and low transmissivity, by comparison with know lower bounds on C, our inequalities determine the value of the capacity up to corrections which are irrelevant for all practical purposes. Examples of such channels are radio communication, infrared or visible-wavelength free space channels. We also provide bounds to active channels that include amplification.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "We show that nature imposes no fundamental upper limit to the number of information bits per expended photon that can, in principle, be read reliably when classical data is encoded in a medium that can only passively modulate the amplitude and phase of the probe light. We show that with a coherent-state (laser) source, an on-off (amplitude-modulation) pixel encoding, and shot-noise-limited direct detection (an overly-optimistic model for commercial CD/DVD drives), the highest photon information efficiency achievable in principle is about 0.5 bit per transmitted photon. We then show that a coherent-state probe can read unlimited bits per photon when the receiver is allowed to make joint (inseparable) measurements on the reflected light from a large block of phase-modulated memory pixels. Finally, we show an example of a spatially-entangled non-classical light probe and a receiver design---constructable using a single-photon source, beam splitters, and single-photon detectors---that can in principle read any number of error-free bits of information. The probe is a single photon prepared in a uniform coherent superposition of multiple orthogonal spatial modes, i.e., a W-state. The code, target, and joint-detection receiver complexity required by a coherent-state transmitter to achieve comparable photon efficiency performance is shown to be much higher in comparison to that required by the W-state transceiver.\n        \u25b3 Less", "author": "Jeffrey Shapiro"}, {"abstract": "Searches for neutrino-less double-beta decay ($0\\nu2\u03b2$) place an important constraint on models where light fields beyond the Standard Model participate in the neutrino mass mechanism. While $0\\nu2\u03b2$ experimental collaborations often consider various massless majoron models, including various forms of majoron couplings and multi-majoron final-state processes, none of these searches considered the scenario where the \"majoron\" $\u03c6$ is not massless, $m_\u03c6\\sim$~MeV, of the same order as the $Q$-value of the $0\\nu2\u03b2$ reaction. We consider this parameter region and estimate $0\\nu2\u03b2\u03c6$ constraints for $m_\u03c6$ of order MeV. The constraints are affected not only by kinematical phase space suppression but also by a change in the signal to background ratio characterizing the search. As a result, $0\\nu2\u03b2\u03c6$ constraints for $m_\u03c6>0$ diminish significantly below the reaction threshold. This has phenomenological implications, which we illustrate focusing on high-energy neutrino telescopes. Our results motivate a dedicated analysis by $0\\nu2\u03b2$ collaborations, analogous to the dedicated analyses targeting massless majoron models.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Deep learning algorithms for connectomics rely upon localized classification, rather than overall morphology. This leads to a high incidence of erroneously merged objects. Humans, by contrast, can easily detect such errors by acquiring intuition for the correct morphology of objects. Biological neurons have complicated and variable shapes, which are challenging to learn, and merge errors take a multitude of different forms. We present an algorithm, MergeNet, that shows 3D ConvNets can, in fact, detect merge errors from high-level neuronal morphology. MergeNet follows unsupervised training and operates across datasets. We demonstrate the performance of MergeNet both on a variety of connectomics data and on a dataset created from merged MNIST images.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. We demonstrate remarkably high test performance after training on corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly-labeled examples. Such behavior holds across multiple patterns of label noise, even when erroneous labels are biased towards confusing classes. We show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. Finally, we provide an analysis of our results that shows how increasing noise decreases the effective batch size.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length coding schemes.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Connectomics is an emerging field in neuroscience that aims to reconstruct the 3-dimensional morphology of neurons from electron microscopy (EM) images. Recent studies have successfully demonstrated the use of convolutional neural networks (ConvNets) for segmenting cell membranes to individuate neurons. However, there has been comparatively little success in high-throughput identification of the intercellular synaptic connections required for deriving connectivity graphs.\n  In this study, we take a compositional approach to segmenting synapses, modeling them explicitly as an intercellular cleft co-located with an asymmetric vesicle density along a cell membrane. Instead of requiring a deep network to learn all natural combinations of this compositionality, we train lighter networks to model the simpler marginal distributions of membranes, clefts and vesicles from just 100 electron microscopy samples. These feature maps are then combined with simple rules-based heuristics derived from prior biological knowledge.\n  Our approach to synapse detection is both more accurate than previous state-of-the-art (7% higher recall and 5% higher F1-score) and yields a 20-fold speed-up compared to the previous fastest implementations. We demonstrate by reconstructing the first complete, directed connectome from the largest available anisotropic microscopy dataset (245 GB) of mouse somatosensory cortex (S1) in just 9.7 hours on a single shared-memory CPU system. We believe that this work marks an important step toward the goal of a microscope-pace streaming connectomics pipeline.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "The field of connectomics faces unprecedented \"big data\" challenges. To reconstruct neuronal connectivity, automated pixel-level segmentation is required for petabytes of streaming electron microscopy data. Existing algorithms provide relatively good accuracy but are unacceptably slow, and would require years to extract connectivity graphs from even a single cubic millimeter of neural tissue. Here we present a viable real-time solution, a multi-pass pipeline optimized for shared-memory multicore systems, capable of processing data at near the terabyte-per-hour pace of multi-beam electron microscopes. The pipeline makes an initial fast-pass over the data, and then makes a second slow-pass to iteratively correct errors in the output of the fast-pass. We demonstrate the accuracy of a sparse slow-pass reconstruction algorithm and suggest new methods for detecting morphological errors. Our fast-pass approach provided many algorithmic challenges, including the design and implementation of novel shallow convolutional neural nets and the parallelization of watershed and object-merging techniques. We use it to reconstruct, from image stack to skeletons, the full dataset of Kasthuri et al. (463 GB capturing 120,000 cubic microns) in a matter of hours on a single multicore machine rather than the weeks it has taken in the past on much larger distributed systems.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of spatiotemporal features. These networks have improved performance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Existing CPU implementations overcome this constraint but are impractically slow. Here we extend and optimize the faster Winograd-class of convolutional algorithms to the $N$-dimensional case and specifically for CPU hardware. First, we remove the need to manually hand-craft algorithms by exploiting the relaxed constraints and cheap sparse access of CPU memory. Second, we maximize CPU utilization and multicore scalability by transforming data matrices to be cache-aware, integer multiples of AVX vector widths. Treating 2-dimensional ConvNets as a special (and the least beneficial) case of our approach, we demonstrate a 5 to 25-fold improvement in throughput compared to previous state-of-the-art.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "For many years, Herlihy's elegant computability based Consensus Hierarchy has been our best explanation of the relative power of various types of multiprocessor synchronization objects when used in deterministic algorithms. However, key to this hierarchy is treating synchronization instructions as distinct objects, an approach that is far from the real-world, where multiprocessor programs apply synchronization instructions to collections of arbitrary memory locations. We were surprised to realize that, when considering instructions applied to memory locations, the computability based hierarchy collapses. This leaves open the question of how to better capture the power of various synchronization instructions.\n  In this paper, we provide an approach to answering this question. We present a hierarchy of synchronization instructions, classified by their space complexity in solving obstruction-free consensus. Our hierarchy provides a classification of combinations of known instructions that seems to fit with our intuition of how useful some are in practice, while questioning the effectiveness of others. We prove an essentially tight characterization of the power of buffered read and write instructions.Interestingly, we show a similar result for multi-location atomic assignments.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Johnson-Lindenstrauss (JL) matrices implemented by sparse random synaptic connections are thought to be a prime candidate for how convergent pathways in the brain compress information. However, to date, there is no complete mathematical support for such implementations given the constraints of real neural tissue. The fact that neurons are either excitatory or inhibitory implies that every so implementable JL matrix must be sign-consistent (i.e., all entries in a single column must be either all non-negative or all non-positive), and the fact that any given neuron connects to a relatively small subset of other neurons implies that the JL matrix had better be sparse.\n  We construct sparse JL matrices that are sign-consistent, and prove that our construction is essentially optimal. Our work answers a mathematical question that was triggered by earlier work and is necessary to justify the existence of JL compression in the brain, and emphasizes that inhibition is crucial if neurons are to perform efficient, correlation-preserving compression.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "All consensus hierarchies in the literature assume that we have, in addition to copies of a given object, an unbounded number of registers. But why do we really need these registers?\n  This paper considers what would happen if one attempts to solve consensus using various objects but without any registers. We show that under a reasonable assumption, objects like queues and stacks cannot emulate the missing registers. We also show that, perhaps surprisingly, initialization, shown to have no computational consequences when registers are readily available, is crucial in determining the synchronization power of objects when no registers are allowed. Finally, we show that without registers, the number of available objects affects the level of consensus that can be solved.\n  Our work thus raises the question of whether consensus hierarchies which assume an unbounded number of registers truly capture synchronization power, and begins a line of research aimed at better understanding the interaction between read-write memory and the powerful synchronization operations available on modern architectures.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Several Hybrid Transactional Memory (HyTM) schemes have recently been proposed to complement the fast, but best-effort, nature of Hardware Transactional Memory (HTM) with a slow, reliable software backup. However, the fundamental limitations of building a HyTM with nontrivial concurrency between hardware and software transactions are still not well understood.\n  In this paper, we propose a general model for HyTM implementations, which captures the ability of hardware transactions to buffer memory accesses, and allows us to formally quantify and analyze the amount of overhead (instrumentation) of a HyTM scheme. We prove the following: (1) it is impossible to build a strictly serializable HyTM implementation that has both uninstrumented reads and writes, even for weak progress guarantees, and (2) under reasonable assumptions, in any opaque progressive HyTM, a hardware transaction must incur instrumentation costs linear in the size of its data set. We further provide two upper bound implementations whose instrumentation costs are optimal with respect to their progress guarantees. In sum, this paper captures for the first time an inherent trade-off between the degree of concurrency a HyTM provides between hardware and software transactions, and the amount of instrumentation overhead the implementation must incur.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "The long-lived renaming problem appears in shared-memory systems where a set of threads need to register and deregister frequently from the computation, while concurrent operations scan the set of currently registered threads. Instances of this problem show up in concurrent implementations of transactional memory, flat combining, thread barriers, and memory reclamation schemes for lock-free data structures. In this paper, we analyze a randomized solution for long-lived renaming. The algorithmic technique we consider, called the LevelArray, has previously been used for hashing and one-shot (single-use) renaming. Our main contribu- tion is to prove that, in long-lived executions, where processes may register and deregister polynomially many times, the technique guarantees constant steps on average and O(log log n) steps with high probability for registering, unit cost for deregistering, and O(n) steps for collect queries, where n is an upper bound on the number of processes that may be active at any point in time. We also show that the algorithm has the surprising property that it is self-healing: under reasonable assumptions on the schedule, operations running while the data structure is in a degraded state implicitly help the data structure re-balance itself. This subtle mechanism obviates the need for expensive periodic rebuilding procedures. Our benchmarks validate this approach, showing that, for typical use parameters, the average number of steps a process takes to register is less than two and the worst-case number of steps is bounded by six, even in executions with billions of operations. We contrast this with other randomized implementations, whose worst-case behavior we show to be unreliable, and with deterministic implementations, whose cost is linear in n.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "Lock-free concurrent algorithms guarantee that some concurrent operation will always make progress in a finite number of steps. Yet programmers prefer to treat concurrent code as if it were wait-free, guaranteeing that all operations always make progress. Unfortunately, designing wait-free algorithms is generally a very complex task, and the resulting algorithms are not always efficient. While obtaining efficient wait-free algorithms has been a long-time goal for the theory community, most non-blocking commercial code is only lock-free.\n  This paper suggests a simple solution to this problem. We show that, for a large class of lock- free algorithms, under scheduling conditions which approximate those found in commercial hardware architectures, lock-free algorithms behave as if they are wait-free. In other words, programmers can keep on designing simple lock-free algorithms instead of complex wait-free ones, and in practice, they will get wait-free progress.\n  Our main contribution is a new way of analyzing a general class of lock-free algorithms under a stochastic scheduler. Our analysis relates the individual performance of processes with the global performance of the system using Markov chain lifting between a complex per-process chain and a simpler system progress chain. We show that lock-free algorithms are not only wait-free with probability 1, but that in fact a general subset of lock-free algorithms can be closely bounded in terms of the average number of steps required until an operation completes.\n  To the best of our knowledge, this is the first attempt to analyze progress conditions, typically stated in relation to a worst case adversary, in a stochastic model capturing their expected asymptotic behavior.\n        \u25b3 Less", "author": "Nir Shavit"}, {"abstract": "One viable solution for continuous reduction in energy-per-operation is to rethink functionality to cope with uncertainty by adopting computational approaches that are inherently robust to uncertainty. It requires a novel look at data representations, associated operations, and circuits, and at materials and substrates that enable them. 3D integrated nanotechnologies combined with novel brain-inspired computational paradigms that support fast learning and fault tolerance could lead the way. Recognizing the very size of the brain's circuits, hyperdimensional (HD) computing can model neural activity patterns with points in a HD space, that is, with hypervectors as large randomly generated patterns. At its very core, HD computing is about manipulating and comparing these patterns inside memory. Emerging nanotechnologies such as carbon nanotube field effect transistors (CNFETs) and resistive RAM (RRAM), and their monolithic 3D integration offer opportunities for hardware implementations of HD computing through tight integration of logic and memory, energy-efficient computation, and unique device characteristics. We experimentally demonstrate and characterize an end-to-end HD computing nanosystem built using monolithic 3D integration of CNFETs and RRAM. With our nanosystem, we experimentally demonstrate classification of 21 languages with measured accuracy of up to 98% on >20,000 sentences (6.4 million characters), training using one text sample (~100,000 characters) per language, and resilient operation (98% accuracy) despite 78% hardware errors in HD representation (outputs stuck at 0 or 1). By exploiting the unique properties of the underlying nanotechnologies, we show that HD computing, when implemented with monolithic 3D integration, can be up to 420X more energy-efficient while using 25X less area compared to traditional silicon CMOS implementations.\n        \u25b3 Less", "author": "Max Shulaker"}, {"abstract": "Carbon nanotube field-effect transistors (CNFETs) are promising candidates for building energy-efficient digital systems at highly-scaled technology nodes. However, carbon nanotubes (CNTs) are inherently subject to variations that reduce circuit yield, increase susceptibility to noise, and severely degrade their anticipated energy and speed benefits. Joint exploration and optimization of CNT processing options and CNFET circuit design are required to overcome this outstanding challenge. Unfortunately, existing approaches for such exploration and optimization are computationally expensive, and mostly rely on trial-and-error-based ad hoc techniques. In this paper, we present a framework that quickly evaluates the impact of CNT variations on circuit delay and noise margin, and systematically explores the large space of CNT processing options to derive optimized CNT processing and CNFET circuit design guidelines. We demonstrate that our framework: 1) runs over 100x faster than existing approaches, and 2) accurately identifies the most important CNT processing parameters, together with CNFET circuit design parameters (e.g., for CNFET sizing and standard cell layouts), to minimize the impact of CNT variations on CNFET circuit speed with less than 5% energy cost, while simultaneously meeting circuit-level noise margin and yield constraints.\n        \u25b3 Less", "author": "Max Shulaker"}, {"abstract": "Distributed transactions on high-overhead TCP/IP-based networks were conventionally considered to be prohibitively expensive and thus were avoided at all costs. To that end, the primary goal of almost any existing partitioning scheme is to minimize the number of cross-partition transactions. However, with the next generation of fast RDMA-enabled networks, this assumption is no longer valid. In fact, recent work has shown that distributed databases can scale even when the majority of transactions are cross-partition.\n  In this paper, we first make the case that the new bottleneck which hinders truly scalable transaction processing in modern RDMA-enabled databases is data contention, and that optimizing for data contention leads to different partitioning layouts than optimizing for the number of distributed transactions. We then present Chiller, a new approach to data partitioning and transaction execution, which minimizes data contention for both local and distributed transactions. Finally, we evaluate Chiller using TPC-C and a real-world workload, and show that our partitioning and execution strategy outperforms traditional partitioning techniques which try to avoid distributed transactions, by up to a factor of 2 under the same conditions.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "In this paper we show that many sequential randomized incremental algorithms are in fact parallel. We consider algorithms for several problems including Delaunay triangulation, linear programming, closest pair, smallest enclosing disk, least-element lists, and strongly connected components.\n  We analyze the dependences between iterations in an algorithm, and show that the dependence structure is shallow with high probability, or that by violating some dependences the structure is shallow and the work is not increased significantly. We identify three types of algorithms based on their dependences and present a framework for analyzing each type. Using the framework gives work-efficient polylogarithmic-depth parallel algorithms for most of the problems that we study.\n  This paper shows the first incremental Delaunay triangulation algorithm with optimal work and polylogarithmic depth, which is an open problem for over 30 years. This result is important since most implementations of parallel Delaunay triangulation use the incremental approach. Our results also improve bounds on strongly connected components and least-elements lists, and significantly simplify parallel algorithms for several problems.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "In this paper, we design parallel write-efficient geometric algorithms that perform asymptotically fewer writes than standard algorithms for the same problem. This is motivated by emerging non-volatile memory technologies with read performance being close to that of random access memory but writes being significantly more expensive in terms of energy and latency. We design algorithms for planar Delaunay triangulation, $k$-d trees, and static and dynamic augmented trees. Our algorithms are designed in the recently introduced Asymmetric Nested-Parallel Model, which captures the parallel setting in which there is a small symmetric memory where reads and writes are unit cost as well as a large asymmetric memory where writes are $\u03c9$ times more expensive than reads. In designing these algorithms, we introduce several techniques for obtaining write-efficiency, including DAG tracing, prefix doubling, reconstruction-based rebalancing and $\u03b1$-labeling, which we believe will be useful for designing other parallel write-efficient algorithms.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "We consider a parallel computational model that consists of $P$ processors, each with a fast local ephemeral memory of limited size, and sharing a large persistent memory. The model allows for each processor to fault with bounded probability, and possibly restart. On faulting all processor state and local ephemeral memory are lost, but the persistent memory remains. This model is motivated by upcoming non-volatile memories that are as fast as existing random access memory, are accessible at the granularity of cache lines, and have the capability of surviving power outages. It is further motivated by the observation that in large parallel systems, failure of processors and their caches is not unusual.\n  Within the model we develop a framework for developing locality efficient parallel algorithms that are resilient to failures. There are several challenges, including the need to recover from failures, the desire to do this in an asynchronous setting (i.e., not blocking other processors when one fails), and the need for synchronization primitives that are robust to failures. We describe approaches to solve these challenges based on breaking computations into what we call capsules, which have certain properties, and developing a work-stealing scheduler that functions properly within the context of failures. The scheduler guarantees a time bound of $O(W/P_A + D(P/P_A) \\lceil\\log_{1/f} W\\rceil)$ in expectation, where $W$ and $D$ are the work and depth of the computation (in the absence of failures), $P_A$ is the average number of processors available during the computation, and $f \\le 1/2$ is the probability that a capsule fails. Within the model and using the proposed methods, we develop efficient algorithms for parallel sorting and other primitives.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "There has been significant interest in parallel graph processing recently due to the need to quickly analyze the large graphs available today. Many graph codes have been designed for distributed memory or external memory. However, today even the largest publicly-available real-world graph (the Hyperlink Web graph with over 3.5 billion vertices and 128 billion edges) can fit in the memory of a single commodity multicore server. Nevertheless, most experimental work in the literature report results on much smaller graphs, and the ones that use the Hyperlink graph are done in distributed or external memory. Therefore it is natural to ask whether we can efficiently solve a broad class of graph problems on this graph in memory.\n  This paper shows that theoretically-efficient parallel graph algorithms can scale to the largest publicly-available graphs using a single machine with a terabyte of RAM, processing them in minutes. We give implementations of theoretically-efficient parallel algorithms for 13 important graph problems. We also present the optimizations and techniques that we used in our implementations, which were crucial in enabling us to process these large graphs quickly. We show that the running times of our implementations outperform existing state-of-the-art implementations on the largest real-world graphs. For many of the problems that we consider, this is the first time they have been solved on graphs at this scale. We have created a problem-based benchmark suite containing these problems that will be made publicly-available.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. Programmers must try different combinations of a large set of techniques to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks lack flexibility, supporting only a limited set of optimizations.\n  This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a scheduling language. The algorithm language simplifies expressing the algorithms. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together optimizations. We also built an autotuner to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8$\\times$, and is never more than 43% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "We develop a new method, which is based on the optimal redshift weighting scheme, to extract the maximal tomographic information of baryonic acoustic oscillations (BAO) and redshift space distortions (RSD) from the extended Baryon Oscillation Spectroscopic Survey (eBOSS) Data Release 14 quasar (DR14Q) survey. We validate our method using the EZ mocks, and apply our pipeline to the eBOSS DR14Q sample in the redshift range of $0.8<z<2.2$. We report a joint measurement of $f\u03c3_8$ and two-dimensional BAO parameters $D_{\\rm A}$ and $H$ at four effective redshifts of $z_{\\rm eff}=0.98, 1.23, 1.52$ and $1.94$, and provide the full data covariance matrix. Using our measurement combined with BOSS DR12, MGS and 6dFGS BAO measurements, we find that the existence of dark energy is supported by observations at a $7.4\u03c3$ significance level. Combining our measurement with BOSS DR12 and Planck observations, we constrain the gravitational growth index to be $\u03b3=0.580\\pm0.082$, which is fully consistent with the prediction of general relativity. This paper is part of a set that analyses the eBOSS DR14 quasar sample.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "The future of main memory appears to lie in the direction of new technologies that provide strong capacity-to-performance ratios, but have write operations that are much more expensive than reads in terms of latency, bandwidth, and energy. Motivated by this trend, we propose sequential and parallel algorithms to solve graph connectivity problems using significantly fewer writes than conventional algorithms. Our primary algorithmic tool is the construction of an $o(n)$-sized \"implicit decomposition\" of a bounded-degree graph $G$ on $n$ nodes, which combined with read-only access to $G$ enables fast answers to connectivity and biconnectivity queries on $G$. The construction breaks the linear-write \"barrier\", resulting in costs that are asymptotically lower than conventional algorithms while adding only a modest cost to querying time. For general non-sparse graphs on $m$ edges, we also provide the first $o(m)$ writes and $O(m)$ operations parallel algorithms for connectivity and biconnectivity. These algorithms provide insight into how applications can efficiently process computations on large graphs in systems with read-write asymmetry.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "Existing parallel algorithms for wavelet tree construction have a work complexity of $O(n\\log\u03c3)$. This paper presents parallel algorithms for the problem with improved work complexity. Our first algorithm is based on parallel integer sorting and has either $O(n\\log\\log n\\lceil\\log\u03c3/\\sqrt{\\log n\\log\\log n}\\rceil)$ work and polylogarithmic depth, or $O(n\\lceil\\log\u03c3/\\sqrt{\\log n}\\rceil)$ work and sub-linear depth. We also describe another algorithm that has $O(n\\lceil\\log\u03c3/\\sqrt{\\log n} \\rceil)$ work and $O(\u03c3+\\log n)$ depth. We then show how to use similar ideas to construct variants of wavelet trees (arbitrary-shaped binary trees and multiary trees) as well as wavelet matrices in parallel with lower work complexity than prior algorithms. Finally, we show that the rank and select structures on binary sequences and multiary sequences, which are stored on wavelet tree nodes, can be constructed in parallel with improved work bounds, matching those of the best existing sequential algorithms for constructing rank and select structures.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "Graph clustering has many important applications in computing, but due to growing sizes of graphs, even traditionally fast clustering methods such as spectral partitioning can be computationally expensive for real-world graphs of interest. Motivated partly by this, so-called local algorithms for graph clustering have received significant interest due to the fact that they can find good clusters in a graph with work proportional to the size of the cluster rather than that of the entire graph. This feature has proven to be crucial in making such graph clustering and many of its downstream applications efficient in practice. While local clustering algorithms are already faster than traditional algorithms that touch the entire graph, they are sequential and there is an opportunity to make them even more efficient via parallelization. In this paper, we show how to parallelize many of these algorithms in the shared-memory multicore setting, and we analyze the parallel complexity of these algorithms. We present comprehensive experiments on large-scale graphs showing that our parallel algorithms achieve good parallel speedups on a modern multicore machine, thus significantly speeding up the analysis of local graph clusters in the very large-scale setting.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "Emerging memory technologies have a significant gap between the cost, both in time and in energy, of writing to memory versus reading from memory. In this paper we present models and algorithms that account for this difference, with a focus on write-efficient sorting algorithms. First, we consider the PRAM model with asymmetric write cost, and show that sorting can be performed in $O\\left(n\\right)$ writes, $O\\left(n \\log n\\right)$ reads, and logarithmic depth (parallel time). Next, we consider a variant of the External Memory (EM) model that charges $\u03c9> 1$ for writing a block of size $B$ to the secondary memory, and present variants of three EM sorting algorithms (multi-way mergesort, sample sort, and heapsort using buffer trees) that asymptotically reduce the number of writes over the original algorithms, and perform roughly $\u03c9$ block reads for every block write. Finally, we define a variant of the Ideal-Cache model with asymmetric write costs, and present write-efficient, cache-oblivious parallel algorithms for sorting, FFTs, and matrix multiplication. Adapting prior bounds for work-stealing and parallel-depth-first schedulers to the asymmetric setting, these yield parallel cache complexity bounds for machines with private caches or with a shared cache, respectively.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "Modern graph clustering applications require the analysis of large graphs and this can be computationally expensive. In this regard, local spectral graph clustering methods aim to identify well-connected clusters around a given \"seed set\" of reference nodes without accessing the entire graph. The celebrated Approximate Personalized PageRank (APPR) algorithm in the seminal paper by Andersen et al. is one such method. APPR was introduced and motivated purely from an algorithmic perspective. In other words, there is no a priori notion of objective function/optimality conditions that characterizes the steps taken by APPR. Here, we derive a novel variational formulation which makes explicit the actual optimization problem solved by APPR. In doing so, we draw connections between the local spectral algorithm of and an iterative shrinkage-thresholding algorithm (ISTA). In particular, we show that, appropriately initialized ISTA applied to our variational formulation can recover the sought-after local cluster in a time that only depends on the number of non-zeros of the optimal solution instead of the entire graph. In the process, we show that an optimization algorithm which apparently requires accessing the entire graph, can be made to behave in a completely local manner by accessing only a small number of nodes. This viewpoint builds a bridge across two seemingly disjoint fields of graph processing and numerical optimization, and it allows one to leverage well-studied, numerically robust, and efficient optimization algorithms for processing today's large graphs.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "In several emerging technologies for computer memory (main memory), the cost of reading is significantly cheaper than the cost of writing. Such asymmetry in memory costs poses a fundamentally different model from the RAM for algorithm design. In this paper we study lower and upper bounds for various problems under such asymmetric read and write costs. We consider both the case in which all but $O(1)$ memory has asymmetric cost, and the case of a small cache of symmetric memory. We model both cases using the $(M,\u03c9)$-ARAM, in which there is a small (symmetric) memory of size $M$ and a large unbounded (asymmetric) memory, both random access, and where reading from the large memory has unit cost, but writing has cost $\u03c9\\gg 1$.\n  For FFT and sorting networks we show a lower bound cost of $\u03a9(\u03c9n\\log_{\u03c9M} n)$, which indicates that it is not possible to achieve asymptotic improvements with cheaper reads when $\u03c9$ is bounded by a polynomial in $M$. Also, there is an asymptotic gap (of $\\min(\u03c9,\\log n)/\\log(\u03c9M)$) between the cost of sorting networks and comparison sorting in the model. This contrasts with the RAM, and most other models. We also show a lower bound for computations on an $n\\times n$ diamond DAG of $\u03a9(\u03c9n^2/M)$ cost, which indicates no asymptotic improvement is achievable with fast reads. However, we show that for the edit distance problem (and related problems), which would seem to be a diamond DAG, there exists an algorithm with only $O(\u03c9n^2/(M\\min(\u03c9^{1/3},M^{1/2})))$ cost. To achieve this we make use of a \"path sketch\" technique that is forbidden in a strict DAG computation. Finally, we show several interesting upper bounds for shortest path problems, minimum spanning trees, and other problems. A common theme in many of the upper bounds is to have redundant computation to tradeoff between reads and writes.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "Motivated by the observation that FIFO-based push-relabel algorithms are able to outperform highest label-based variants on modern, large maximum flow problem instances, we introduce an efficient implementation of the algorithm that uses coarse-grained parallelism to avoid the problems of existing parallel approaches. We demonstrate good relative and absolute speedups of our algorithm on a set of large graph instances taken from real-world applications. On a modern 40-core machine, our parallel implementation outperforms existing sequential implementations by up to a factor of 12 and other parallel implementations by factors of up to 3.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "We present parallel algorithms for wavelet tree construction with polylogarithmic depth, improving upon the linear depth of the recent parallel algorithms by Fuentes-Sepulveda et al. We experimentally show on a 40-core machine with two-way hyper-threading that we outperform the existing parallel algorithms by 1.3--5.6x and achieve up to 27x speedup over the sequential algorithm on a variety of real-world and artificial inputs. Our algorithms show good scalability with increasing thread count, input size and alphabet size. We also discuss extensions to variants of the standard wavelet tree.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "The greedy sequential algorithm for maximal independent set (MIS) loops over the vertices in arbitrary order adding a vertex to the resulting set if and only if no previous neighboring vertex has been added. In this loop, as in many sequential loops, each iterate will only depend directly on a subset of the previous iterates (i.e. knowing that any one of a vertices neighbors is in the MIS or knowing that it has no previous neighbors is sufficient to decide its fate). This leads to a dependence structure among the iterates. If this structure is shallow then running the iterates in parallel while respecting the dependencies can lead to an efficient parallel implementation mimicking the sequential algorithm.\n  In this paper, we show that for any graph, and for a random ordering of the vertices, the dependence depth of the sequential greedy MIS algorithm is polylogarithmic (O(log^2 n) with high probability). Our results extend previous results that show polylogarithmic bounds only for random graphs. We show similar results for a greedy maximal matching (MM). For both problems we describe simple linear work parallel algorithms based on the approach. The algorithms allow for a smooth tradeoff between more parallelism and reduced work, but always return the same result as the sequential greedy algorithms. We present experimental results that demonstrate efficiency and the tradeoff between work and parallelism.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "We review mathematically tractable models for connected networks on random points in the plane, emphasizing the class of proximity graphs which deserves to be better known to applied probabilists and statisticians. We introduce and motivate a particular statistic $R$ measuring shortness of routes in a network. We illustrate, via Monte Carlo in part, the trade-off between normalized network length and $R$ in a one-parameter family of proximity graphs. How close this family comes to the optimal trade-off over all possible networks remains an intriguing open question. The paper is a write-up of a talk developed by the first author during 2007--2009.\n        \u25b3 Less", "author": "Julian Shun"}, {"abstract": "A search for $CP$ violation in the Cabibbo-suppressed $D^0 \\rightarrow K^+ K^- \u03c0^+ \u03c0^-$ decay mode is performed using an amplitude analysis. The measurement uses a sample of $pp$ collisions recorded by the LHCb experiment during 2011 and 2012, corresponding to an integrated luminosity of 3.0 fb$^{-1}$. The $D^0$ mesons are reconstructed from semileptonic $b$-hadron decays into $D^0\u03bc^- X$ final states. The selected sample contains more than 160000 signal decays, allowing the most precise amplitude modelling of this $D^0$ decay to date. The obtained amplitude model is used to perform the search for $CP$ violation. The result is compatible with $CP$ symmetry, with a sensitivity ranging from 1% to 15% depending on the amplitude considered.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "We report on the confirmation of a transiting giant planet around the relatively hot (Teff = 6801 $\\pm$ 56 K) star HD2685, whose transit signal was detected in Sector 1 data of the TESS mission. We confirmed the planetary nature of the transit signal by using Doppler velocimetric measurements with CHIRON, CORALIE and FEROS, as well as photometric data with CHAT and LCOGT. From the photometry and radial velocities joint analysis, we derived the following parameters for HD2685 $b$: $P$=4.12692$\\pm$0.00004 days, M$_P$=1.18 $\\pm$ 0.09 $M_J$ and $R_P$=1.44 $\\pm$ 0.01 $R_J$. This system is a typical example of an inflated transiting Hot-Jupiter in a circular orbit. Given the host star apparent visual magnitude ($V$ = 9.6 mag), this is one of the brightest known stars hosting a transiting Hot-Jupiter, and a good example of the upcoming systems that will be detected by TESS during the two-year primary mission. This is also an excellent target for future ground and space based atmospheric characterization as well as a good candidate for measuring the projected spin-orbit misalignment angle via the Rossiter-McLaughlin effect.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The first measurement of heavy-flavour production by the LHCb experiment in its fixed-target mode is presented. The production of $J/\u03c8$ and $D^0$ mesons is studied with beams of protons of different energies colliding with gaseous targets of helium and argon with nucleon-nucleon centre-of-mass energies of $\\sqrt{s_{NN}} = 86.6 $ and $ 110.4$ ${\\rm GeV}$, respectively. The $J/\u03c8$ and $D^0$ production cross-sections in $p{\\rm He}$ collisions in the rapidity range $[2,4.6]$ are found to be $\u03c3_{J/\u03c8} = 652 \\pm 33$ (stat) $\\pm 42$ (syst) nb$/$nucleon and $\u03c3_{D^0} = 80.8 \\pm 2.4$ (stat) $\\pm 6.3$ (syst) $\u03bc$b$/$nucleon, where the first uncertainty is statistical and the second is systematic. No evidence for a substantial intrinsic charm content of the nucleon is observed in the large Bjorken-$x$ region.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The production of $\u03a5(nS)$ mesons ($n=1,2,3$) in $p$Pb and Pb$p$ collisions at a centre-of-mass energy per nucleon pair $\\sqrt{s_{NN}}=8.16$ TeV is measured by the LHCb experiment, using a data sample corresponding to an integrated luminosity of 31.8 nb$^{-1}$. The $\u03a5(nS)$ mesons are reconstructed through their decays into two opposite-sign muons. The measurements comprise the differential production cross-sections of the $\u03a5(1S)$ and $\u03a5(2S)$ states, their forward-to-backward ratios and nuclear modification factors, performed as a function of the transverse momentum \\pt and rapidity in the nucleon-nucleon centre-of-mass frame $y^*$ of the $\u03a5(nS)$ states, in the kinematic range $p_{\\rm{T}}<25$ GeV/$c$ and $1.5<y^*<4.0$ ($-5.0<y^*<-2.5$) for $p$Pb (Pb$p$) collisions. In addition, production cross-sections for $\u03a5(3S)$ are measured integrated over phase space and the production ratios between all three $\u03a5(nS)$ states are determined. The measurements are compared to theoretical predictions and suppressions for quarkonium in $p$Pb collisions are observed.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A measurement of the charm-mixing parameter $y_{CP}$ using $D^0 \\to K^+ K^-$, $D^0 \\to \u03c0^+ \u03c0^-$, and $D^0 \\to K^- \u03c0^+$ decays is reported. The $D^0$ mesons are required to originate from semimuonic decays of $B^-$ and $\\overline{B}^0$ mesons. These decays are partially reconstructed in a data set of proton-proton collisions at center-of-mass energies of 7 and 8 TeV collected with the LHCb experiment and corresponding to an integrated luminosity of 3 fb$^{-1}$. The $y_{CP}$ parameter is measured to be $(0.57 \\pm 0.13(\\rm{stat.}) \\pm 0.09(\\rm{syst.}))\\%$, in agreement with, and as precise as, the current world-average value.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The branching fractions of the doubly Cabibbo-suppressed decays $D^+\\rightarrow K^-K^+K^+$, $D^+\\rightarrow \u03c0^-\u03c0^+K^+$ and $D^+_s\\rightarrow\u03c0^-K^+K^+$ are measured using the decays $D^+\\rightarrow K^-\u03c0^+\u03c0^+$ and $D^+_s\\rightarrow K^-K^+\u03c0^+$ as normalisation channels. The measurements are performed using proton-proton collision data collected with the LHCb detector at a centre-of-mass energy of 8 TeV, corresponding to an integrated luminosity of 2.0 fb$^{-1}$. The results are\n  \\begin{align}\n  \\frac {\\mathcal{B}(D^+\\rightarrow K^-K^+K^+)} {\\mathcal{B}(D^+\\rightarrow K^-\u03c0^+\u03c0^+)}& = (6.541 \\pm 0.025 \\pm 0.042) \\times 10^{-4},\\nonumber\n  \\frac {\\mathcal{B}(D^+\\rightarrow \u03c0^-\u03c0^+K^+)} {\\mathcal{B}(D^+\\rightarrow K^-\u03c0^+\u03c0^+)}& = (5.231 \\pm 0.009 \\pm 0.023) \\times 10^{-3}, \\nonumber\n  \\frac {\\mathcal{B}(D^+_s\\rightarrow\u03c0^-K^+K^+)} {\\mathcal{B}(D^+_s\\rightarrow K^-K^+\u03c0^+)}& = (2.372 \\pm 0.024 \\pm 0.025) \\times 10^{-3},\\nonumber\n  \\end{align} where the uncertainties are statistical and systematic, respectively. These are the most precise measurements up to date.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The first observation of two structures consistent with resonances in the final states $\u039b_b^0 \u03c0^-$ and $\u039b_b^0 \u03c0^+$ is reported using samples of $pp$ collision data collected by the LHCb experiment at $\\sqrt{s} = 7$ and $8$ TeV, corresponding to an integrated luminosity of 3 $\\mathrm{fb}^{-1}$. The ground states $\u03a3_b^\\pm$ and $\u03a3_b^{*\\pm}$ are also confirmed and their masses and widths are precisely measured.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A Dalitz plot analysis of $B^0 \\to \u03b7_c(1S) K^+\u03c0^-$ decays is performed using data samples of $pp$ collisions collected with the LHCb detector at centre-of-mass energies of $\\sqrt{s}=7,~8$ and $13$ TeV, corresponding to a total integrated luminosity of $4.7~\\text{fb}^{-1}$. A satisfactory description of the data is obtained when including a contribution representing an exotic $\u03b7_c(1S) \u03c0^-$ resonant state. The significance of this exotic resonance is more than three standard deviations, while its mass and width are $4096 \\pm 20~^{+18}_{-22}$ MeV and $152 \\pm 58~^{+60}_{-35}$ MeV, respectively. The spin-parity assignments $J^P=0^+$ and $J^{P}=1^-$ are both consistent with the data. In addition, the first measurement of the $B^0 \\to \u03b7_c(1S) K^+\u03c0^-$ branching fraction is performed and gives $\\displaystyle \\mathcal{B}(B^0 \\to \u03b7_c(1S) K^+\u03c0^-) = (5.73 \\pm 0.24 \\pm 0.13 \\pm 0.66) \\times 10^{-4}$, where the first uncertainty is statistical, the second systematic, and the third is due to limited knowledge of external branching fractions.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The production of $\u039b^+_c$ baryons produced directly at the interacting point is studied in proton-lead collisions collected with the LHCb detector at the LHC. The data sample corresponds to an integrated luminosity of $1.58\\mathrm{nb}^{-1}$ recorded at a nucleon-nucleon centre-of-mass energy of $\\sqrt{s_{NN}}=5.02$ TeV. Measurements of the differential cross-section and the forward-backward production ratio are reported for $\u039b^+_c$ baryons with transverse momenta in the range $2<p_{T}<10$GeV/$c$ and rapidities in the ranges $1.5<y^*<4.0$ and $-4.5<y^*<-2.5$ in the nucleon-nucleon centre-of-mass system. The ratio of cross-sections of $\u039b^+_c$ baryons and $D^0$ mesons is also reported. The results are compared with next-to-leading order calculations that use nuclear parton distribution functions.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The LHCb Upgrade II will fully exploit the flavour-physics opportunities of the HL-LHC, and study additional physics topics that take advantage of the forward acceptance of the LHCb spectrometer. The LHCb Upgrade I will begin operation in 2020. Consolidation will occur, and modest enhancements of the Upgrade I detector will be installed, in Long Shutdown 3 of the LHC (2025) and these are discussed here. The main Upgrade II detector will be installed in long shutdown 4 of the LHC (2030) and will build on the strengths of the current LHCb experiment and the Upgrade I. It will operate at a luminosity up to $ 2 \\times 10^{34} \\rm cm^{-2}s^{-1}$, ten times that of the Upgrade I detector. New detector components will improve the intrinsic performance of the experiment in certain key areas. An Expression Of Interest proposing Upgrade II was submitted in February 2017. The physics case for the Upgrade II is presented here in more depth. $CP$-violating phases will be measured with precisions unattainable at any other envisaged facility. The experiment will probe $b\\to s \\ell^+\\ell^-$ and $b\\to d \\ell^+\\ell^-$ transitions in both muon and electron decays in modes not accessible at Upgrade I. Minimal flavour violation will be tested with a precision measurement of the ratio of $B(B^0\\to\u03bc^+\u03bc^-)/B(B_s^0\\to \u03bc^+\u03bc^-)$. Probing charm $CP$ violation at the $10^{-5}$ level may result in its long sought discovery. Major advances in hadron spectroscopy will be possible, which will be powerful probes of low energy QCD. Upgrade II potentially will have the highest sensitivity of all the LHC experiments on the Higgs to charm-quark couplings. Generically, the new physics mass scale probed, for fixed couplings, will almost double compared with the pre-HL-LHC era; this extended reach for flavour physics is similar to that which would be achieved by the HE-LHC proposal for the energy frontier.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A search is presented for a Higgs-like boson with mass in the range 45 to 195 GeV/$c^2$ decaying into a muon and a tau lepton. The dataset consists of proton-proton interactions at a centre-of-mass energy of 8 TeV, collected by the LHCb experiment, corresponding to an integrated luminosity of 2 fb$^{-1}$. The tau leptons are reconstructed in both leptonic and hadronic decay channels. An upper limit on the production cross-section multiplied by the branching fraction at 95% confidence level is set and ranges from 22 pb for a boson mass of 45 GeV/$c^2$ to 4 pb for a mass of 195 GeV/$c^2$.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The cross-section for prompt antiproton production in collisions of protons with an energy of $6.5$ TeV incident on helium nuclei at rest is measured with the LHCb experiment from a data set corresponding to an integrated luminosity of $0.5\\,nb^{-1}$. The target is provided by injecting helium gas into the LHC beam line at the LHCb interaction point. The reported results, covering antiproton momenta between $12$ and $110\\,\\mathrm{GeV/}c$, represent the first direct determination of the antiproton production cross-section in ${\\rm p He}$ collisions, and impact the interpretation of recent results on antiproton cosmic rays from space-borne experiments.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "An analysis of the angular distribution of the decay $\u039b_b^0 \\rightarrow \u039b\u03bc^{+} \u03bc^{-}$ is presented, using data collected with the LHCb detector between 2011 and 2016 and corresponding to an integrated luminosity of approximately $5\\,fb^{-1}$. Angular observables are determined using a moment analysis of the angular distribution at low hadronic recoil, corresponding to the dimuon invariant mass squared range $15 < q^{2} < 20\\, GeV^2/c^4$. The full basis of observables is measured for the first time. The lepton-side, hadron-side and combined forward-backward asymmetries of the decay are determined to be \\begin{align} A_{FB}^{l} & = -0.39 \\pm 0.04\\,\\rm{stat} \\pm 0.01\\, \\rm{syst}, \\nonumber\\\\ A_{FB}^{h} & = -0.30 \\pm 0.05\\,\\rm{stat} \\pm 0.02\\, \\rm{syst}, \\nonumber\\\\ A_{FB}^{lh} & = +0.25 \\pm 0.04\\,\\rm{stat} \\pm 0.01\\, \\rm{syst}. \\nonumber \\end{align} The measurements are consistent with Standard Model predictions.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The decay of the narrow resonance $\\overline{B}{}_{s2}^{*0}\\!\\rightarrow B^- K^+$ can be used to determine the $B^-$ momentum in partially reconstructed decays without any assumptions on the decay products of the $B^-$ meson. This technique is employed for the first time to distinguish contributions from $D^0$, $D^{*0}$, and higher-mass charmed states ($D^{**0}$) in semileptonic $B^-$ decays by using the missing-mass distribution. The measurement is performed using a data sample corresponding to an integrated luminosity of 3.0 fb${}^{-1}$ collected with the LHCb detector in $pp$ collisions at center-of-mass energies of 7 and 8 TeV. The resulting branching fractions relative to the inclusive $B^- \\!\\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc$ are $f_{D^0} = \\mathcal{B}( B^- \\rightarrow D^0\u03bc^-\\overline\u03bd_\u03bc)/\\mathcal{B}( B^- \\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc) = 0.25 \\pm 0.06$, $f_{D^{**0}} = \\mathcal{B}( B^- \\rightarrow ( D^{**0} \\rightarrow D^0 X)\u03bc^-\\overline\u03bd_\u03bc)/\\mathcal{B}( B^- \\rightarrow D^0 X \u03bc^- \\overline\u03bd_\u03bc) = 0.21 \\pm 0.07$, with $f_{D^{*0}} = 1 - f_{D^0} - f_{D^{**0}}$ making up the remainder.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The DUNE IDR describes the proposed physics program and technical designs of the DUNE far detector modules in preparation for the full TDR to be published in 2019. It is intended as an intermediate milestone on the path to a full TDR, justifying the technical choices that flow down from the high-level physics goals through requirements at all levels of the Project. These design choices will enable the DUNE experiment to make the ground-breaking discoveries that will help to answer fundamental physics questions. Volume 3 describes the dual-phase module's subsystems, the technical coordination required for its design, construction, installation, and integration, and its organizational structure.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The DUNE IDR describes the proposed physics program and technical designs of the DUNE Far Detector modules in preparation for the full TDR to be published in 2019. It is intended as an intermediate milestone on the path to a full TDR, justifying the technical choices that flow down from the high-level physics goals through requirements at all levels of the Project. These design choices will enable the DUNE experiment to make the ground-breaking discoveries that will help to answer fundamental physics questions. Volume 1 contains an executive summary that describes the general aims of this document. The remainder of this first volume provides a more detailed description of the DUNE physics program that drives the choice of detector technologies. It also includes concise outlines of two overarching systems that have not yet evolved to consortium structures: computing and calibration. Volumes 2 and 3 of this IDR describe, for the single-phase and dual-phase technologies, respectively, each detector module's subsystems, the technical coordination required for its design, construction, installation, and integration, and its organizational structure.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The DUNE IDR describes the proposed physics program and technical designs of the DUNE far detector modules in preparation for the full TDR to be published in 2019. It is intended as an intermediate milestone on the path to a full TDR, justifying the technical choices that flow down from the high-level physics goals through requirements at all levels of the Project. These design choices will enable the DUNE experiment to make the ground-breaking discoveries that will help to answer fundamental physics questions. Volume 2 describes the single-phase module's subsystems, the technical coordination required for its design, construction, installation, and integration, and its organizational structure.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A search for $C\\!P$ violation in $\u039b^0_b \\to p K^-$ and $\u039b^0_b \\to p \u03c0^-$ decays is presented using a sample of $pp$ collisions collected with the LHCb detector and corresponding to an integrated luminosity of 3.0 fb$^{-1}$. The $C\\!P$-violating asymmetries are measured to be $A_{\\mathrm{CP}}^{pK^-} = -0.020 \\pm 0.013\\pm 0.019$ and $A_{\\mathrm{CP}}^{p\u03c0^-} = -0.035 \\pm 0.017 \\pm 0.020 $, and their difference $A_{\\mathrm{CP}}^{pK^-}-A_{\\mathrm{CP}}^{p\u03c0^-} = 0.014 \\pm 0.022 \\pm 0.010$, where the first uncertainties are statistical and the second systematic. These are the most precise measurements of such asymmetries to date.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "We report a measurement of the lifetime of the $\u03a9_c^0$ baryon using proton-proton collision data at center-of-mass energies of 7 and 8~TeV, corresponding to an integrated luminosity of 3.0 fb$^{-1}$ collected by the LHCb experiment. The sample consists of about 1000 $\u03a9_b^-\\to\u03a9_c^0\u03bc^-\\bar\u03bd_\u03bc X$ signal decays, where the $\u03a9_c^0$ baryon is detected in the $pK^-K^-\u03c0^+$ final state and $X$ represents possible additional undetected particles in the decay. The $\u03a9_c^0$ lifetime is measured to be $\u03c4_{\u03a9_c^0} = 268\\pm24\\pm10\\pm2$ fs, where the uncertainties are statistical, systematic, and from the uncertainty in the $D^+$ lifetime, respectively. This value is nearly four times larger than, and inconsistent with, the current world-average value.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The doubly charmed baryon decay $\u039e_{cc}^{++} \\rightarrow \u039e_{c}^{+} \u03c0^{+}$ is observed for the first time, with a statistical significance of $5.9\u03c3$, confirming a recent observation of the baryon in the $\u039b_c^{+} K^{-} \u03c0^{+} \u03c0^{+}$ final state. The data sample used corresponds to an integrated luminosity of $1.7\\,\\mathrm{fb}^{-1}$, collected by the LHCb experiment in $pp$ collisions at a center-of-mass energy of $13\\mathrm{\\,Te\\kern -0.1em V}$. The $\u039e_{cc}^{++}$ mass is measured to be\n  \\begin{equation}\\nonumber\n  3620.6\\pm 1.5~(\\text{stat})\\pm 0.4~(\\text{syst}) \\pm 0.3~(\u039e_{c}^{+})~\\text{MeV}/\\it{c}^{2},\n  \\end{equation}\n  and is consistent with the previous result. The ratio of branching fractions between the decay modes is measured to be\n  \\begin{equation}\\nonumber\n  \\frac{\\mathcal{B} (\u039e_{cc}^{++} \\rightarrow \u039e_{c}^{+} \u03c0^{+}) \\times \\mathcal{B}(\u039e_{c}^{+} \\rightarrow pK^{-}\u03c0^{+})}\n  {\\mathcal{B} (\u039e_{cc}^{++} \\rightarrow \u039b_c^{+} K^{-} \u03c0^{+} \u03c0^{+}) \\times \\mathcal{B}(\u039b_c^{+} \\rightarrow pK^{-}\u03c0^{+})}\n  = 0.035\\pm 0.009~(\\text{stat}) \\pm 0.003~(\\text{syst}).\n  \\end{equation}\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The first observation of the $B_s^0 \\to \\overline{D}^{*0} \u03c6$ decay is reported, with a significance of more than seven standard deviations, from an analysis of $pp$ collision data corresponding to an integrated luminosity of 3 fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of $7$ and $8$ TeV. The branching fraction is measured relative to that of the topologically similar decay $B^0 \\to \\overline{D}^0 \u03c0^+\u03c0^-$ and is found to be $\\mathcal{B}(B_s^0 \\to \\overline{D}^{*0} \u03c6) = (3.7 \\pm 0.5 \\pm 0.3 \\pm 0.2) \\times 10^{-5}$, where the first uncertainty is statistical, the second systematic, and the third from the branching fraction of the $B^0 \\to \\overline{D}^0 \u03c0^+\u03c0^-$ decay. The fraction of longitudinal polarisation in this decay is measured to be ${f_{\\rm L} =(73 \\pm 15 \\pm 3)\\%}$. The most precise determination of the branching fraction for the $B_s^0 \\to \\overline{D}^{0} \u03c6$ decay is also obtained, $\\mathcal{B}(B_s^0 \\to \\overline{D}^{0} \u03c6) = (3.0 \\pm 0.3 \\pm 0.2 \\pm 0.2) \\times 10^{-5}$. An upper limit, $\\mathcal{B}(B^0 \\to \\overline{D}^{0} \u03c6) < 2.0 \\ (2.2) \\times 10^{-6}$ at $90\\%$ (95\\%) confidence level is set. A constraint on the $\u03c9-\u03c6$ mixing angle $\u03b4$ is set at $|\u03b4| < 5.2^\\circ~ (5.5^\\circ)$ at $90\\%$ ($95\\%$) confidence level.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The first observation of the $B_s^0 \\to \\overline{D}^0 K^+ K^-$ decay is reported, together with the most precise branching fraction measurement of the mode $B^0 \\to \\overline{D}^0 K^+ K^-$. The results are obtained from an analysis of $pp$ collision data corresponding to an integrated luminosity of $3.0~\\textrm{fb}^{-1}$. The data were collected with the LHCb detector at centre-of-mass energies of $7$ and $8$ TeV. The branching fraction of the $B^0 \\to \\overline{D}^0 K^+ K^-$ decay is measured relative to that of the decay $B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-$ to be $$\\frac{\\mathcal{B}(B^0 \\to \\overline{D}^0 K^+ K^-)}{\\mathcal{B}(B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-)} = (6.9 \\pm 0.4 \\pm 0.3)\\%,$$ where the first uncertainty is statistical and the second is systematic. The measured branching fraction of the $B_s^0 \\to \\overline{D}^0 K^+ K^-$ decay mode relative to that of the corresponding $B^0$ decay is $$\\frac{\\mathcal{B}(B_s^0 \\to \\overline{D}^0 K^+ K^-)}{\\mathcal{B}(B^0 \\to \\overline{D}^0 K^+ K^-)} = (93.0 \\pm 8.9 \\pm 6.9)\\%.$$ Using the known branching fraction of ${B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-}$, the values of ${{\\mathcal B}(B^0 \\to \\overline{D}^0 K^+ K^- )=(6.1 \\pm 0.4 \\pm 0.3 \\pm 0.3) \\times 10^{-5}}$, and ${{\\cal B}(B_s^0 \\to \\overline{D}^0 K^+ K^-)=}$ $(5.7 \\pm 0.5 \\pm 0.4 \\pm 0.5) \\times 10^{-5}$ are obtained, where the third uncertainties arise from the branching fraction of the decay modes ${B^0 \\to \\overline{D}^0 \u03c0^+ \u03c0^-}$ and $B^0 \\to \\overline{D}^0 K^+ K^-$, respectively.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The first measurements of the forward-backward asymmetry of the dimuon pair ($A_{FB}$), the triple-product asymmetry ($A_{2\u03c6}$), and the charge-parity-conjugation asymmetry ($A_{CP}$), in $D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-$ and $D^0\\to K^+K^-\u03bc^+\u03bc^-$ decays are reported. They are performed using data from proton-proton collisions collected with the LHCb experiment from 2011 to 2016, corresponding to a total integrated luminosity of 5 fb$^{-1}$. The asymmetries are measured to be \\begin{align*} A_{FB}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-) &= (\\phantom{-}3.3\\pm3.7\\pm0.6)\\%,\\\\ A_{2\u03c6}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-)&= (-0.6\\pm3.7\\pm0.6)\\%,\\\\ A_{CP}(D^0\\to\u03c0^+\u03c0^-\u03bc^+\u03bc^-) &= (\\phantom{-}4.9\\pm3.8\\pm0.7)\\%,\\\\ A_{FB}(D^0\\to K^+K^-\u03bc^+\u03bc^-) &= (0\\pm11\\pm2)\\%,\\\\ A_{2\u03c6}(D^0\\to K^+K^-\u03bc^+\u03bc^-)&= (9\\pm11\\pm1)\\%,\\\\ A_{CP}(D^0\\to K^+K^-\u03bc^+\u03bc^-) &= (0\\pm11\\pm2)\\%, \\end{align*} where the first uncertainty is statistical and the second systematic. The asymmetries are also measured as a function of the dimuon invariant mass. The results are consistent with the Standard Model predictions.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The $\\overline{B_s^0} \\rightarrow \u03c7_{c2} K^+ K^- $ decay mode is observed and its branching fraction relative to the corresponding $\u03c7_{c1}$ decay mode, in a $\\pm 15 \\textrm{MeV}/c^2$ window around the $\u03c6$ mass, is found to be $\\frac{\\mathcal{B}(\\overline{B_s^0} \\rightarrow \u03c7_{c2} K^+ K^-) }{ \\mathcal{B}(\\overline{B_s^0} \\rightarrow \u03c7_{c1} K^+ K^-)} = (17.1 \\pm 3.1 \\pm 0.4 \\pm 0.9)\\%,$ where the first uncertainty is statistical, the second systematic and the third due to the knowledge of the branching fractions of radiative $\u03c7_c$ decays. The decay mode $\\overline{B_s^0} \\rightarrow \u03c7_{c1} K^+ K^- $ allows the $ B_s^0$ mass to be measured as $m(B_s^0) = 5366.83 \\pm 0.25 \\pm 0.27 \\, \\textrm{MeV}/c^2,$ where the first uncertainty is statistical and the second systematic. A combination of this result with other LHCb determinations of the $B_s^0$ mass is made.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The $\u03a5(1S)\u03bc^+\u03bc^-$ invariant-mass distribution is investigated for a possible exotic meson state composed of two $b$ quarks and two $\\overline{b}$ quarks, $X_{b\\overline{b}b\\overline{b}}$. The analysis is based on a data sample of $pp$ collisions recorded with the LHCb detector at centre-of-mass energies $\\sqrt{s} =$ 7, 8 and 13 TeV, corresponding to an integrated luminosity of 6.3 fb$^{-1}$. No significant excess is found, and upper limits are set on the product of the production cross-section and the branching fraction as functions of the mass of the $X_{b\\overline{b}b\\overline{b}}$ state. The limits are set in the fiducial volume where all muons have pseudorapidity in the range $[2.0,5.0]$, and the $X_{b\\overline{b}b\\overline{b}}$ state has rapidity in the range $[2.0,4.5]$ and transverse momentum less than 15 GeV/$c$.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The Cabibbo-suppressed decay $\u039b^0_b\\rightarrow\u03c8(2S)p\u03c0^-$ is observed for the first time using a data sample collected by the LHCb experiment in proton-proton collisions corresponding to 1.0, 2.0 and 1.9fb$^{-1}$ of integrated luminosity at centre-of-mass energies of 7, 8 and 13TeV, respectively. The $\u03c8(2S)$ mesons are reconstructed in the $\u03bc^+\u03bc^-$ final state. The~branching fraction with respect to that of the $\u039b^0_b\\rightarrow\u03c8(2S)pK^-$ decay mode is measured to be $$\\frac{\\mathcal{B}\\left(\u039b^0_b\\rightarrow\u03c8(2S)p\u03c0^- \\right)} {\\mathcal{B}\\left(\u039b^0_b\\rightarrow\u03c8(2S)pK^-\\right)}=\\left(11.4 \\pm 1.3 \\pm 0.2\\right)\\!\\%\\,,$$ where the first uncertainty is statistical and the second is systematic. The $\u03c8(2S)p$ and $\u03c8(2S)\u03c0^-$ mass spectra are investigated and no evidence for exotic resonances is found.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A measurement of $Z\\rightarrow\u03c4^+\u03c4^-$ production cross-section is presented using data, corresponding to an integrated luminosity of 2 fb$^{-1}$, from $pp$ collisions at $\\sqrt{s}=8$ TeV collected by the LHCb experiment. The $\u03c4^+\u03c4^-$ candidates are reconstructed in final states with the first tau lepton decaying leptonically, and the second decaying either leptonically or to one or three charged hadrons. The production cross-section is measured for $Z$ bosons with invariant mass between 60 and 120 GeV/$c^2$, which decay to tau leptons with transverse momenta greater than 20 GeV/$c$ and pseudorapidities between 2.0 and 4.5. The cross-section is determined to be $\u03c3_{pp\\rightarrow{}Z\\rightarrow{}\u03c4^+\u03c4^-} = 95.8 \\pm 2.1 \\pm 4.6 \\pm 0.2 \\pm 1.1 \\mathrm{pb}$, where the first uncertainty is statistical, the second is systematic, the third is due to the LHC beam energy uncertainty, and the fourth to the integrated luminosity uncertainty. This result is compatible with NNLO Standard model predictions. The ratio of the cross-sections for $Z\\rightarrow\u03c4^+\u03c4^-$ to $Z\\rightarrow\u03bc^+\u03bc^-$ ($Z\\rightarrow{}e^+e^-$), determined to be $1.01 \\pm 0.05$ ($1.02 \\pm 0.06$), is consistent with the lepton-universality hypothesis in $Z$ decays.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "Measurements are reported of the central exclusive production of \\jpsi and \\psitwos mesons in $pp$ collisions at a centre-of-mass energy of 13 TeV. Backgrounds are significantly reduced compared to previous measurements made at lower energies through the use of new forward shower counters. The products of the cross-sections and the branching fractions for the decays to dimuons, where both muons are within the pseudorapidity range $2.0<\u03b7<4.5$, are measured to be $$\n  \\begin{array}{rcl} \u03c3_{J/\u03c8\\rightarrow\u03bc^+\u03bc^-}&=&435 \\pm 18 \\pm 17 \\pm 16 {\\rm \\ pb},\\\\ \u03c3_{\u03c8(2S)\\rightarrow\u03bc^+\u03bc^-}&=&11.1 \\pm 1.1 \\pm 0.3 \\pm 0.4 {\\rm \\ pb}.\\\\ \\end{array} $$ The first uncertainties are statistical, the second are systematic, and the third are due to the luminosity determination. The cross-sections are also measured differentially for meson rapidities between 2.0 and 4.5. Good agreement is observed with theoretical predictions. Photoproduction cross-sections are derived and compared to previous experiments, and a deviation from a pure power-law extrapolation of lower energy data is observed.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The first measurement of the lifetime of the doubly charmed baryon $\u039e_{cc}^{++}$ is presented, with the signal reconstructed in the final state $\u039b_c^+ K^- \u03c0^+ \u03c0^+$. The data sample used corresponds to an integrated luminosity of $1.7\\,\\mathrm{fb}^{-1}$, collected by the LHCb experiment in proton-proton collisions at a centre-of-mass energy of $13\\mathrm{\\,Te\\kern -0.1em V}$. The $\u039e_{cc}^{++}$ lifetime is measured to be $0.256\\,^{+0.024}_{-0.022}{\\,\\rm (stat)\\,} \\pm 0.014 {\\,\\rm(syst)}\\mathrm{\\,ps}$.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A measurement of the time-integrated $CP$ asymmetry in $D^0\\rightarrow K^0_S K^0_S$ decays is reported. The data correspond to an integrated luminosity of about $2$ fb$^{-1}$ collected in 2015-2016 by the LHCb collaboration in $pp$ collisions at a centre-of-mass energy of $13$ TeV. The $D^0$ candidate is required to originate from a $D^{\\ast +} \\rightarrow D^0 \u03c0^+$ decay, allowing the determination of the flavour of the $D^0$ meson using the pion charge. The $D^0 \\rightarrow K^{+}K^{-}$ decay, which has a well measured $CP$ asymmetry, is used as a calibration channel. The $CP$ asymmetry for $D^0\\rightarrow K^0_S K^0_S$ is measured to be \\begin{equation*} \\mathcal{A}^{CP}(D^0\\rightarrow K^0_S K^0_S) = (4.3\\pm 3.4\\pm 1.0)\\%, \\end{equation*} where the first uncertainty is statistical and the second is systematic. This result is combined with the previous LHCb measurement at lower centre-of-mass energies to obtain \\begin{equation*} \\mathcal{A}^{CP}(D^0\\rightarrow K^0_S K^0_S) = (2.3\\pm 2.8\\pm 0.9)\\%. \\end{equation*}\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A binned Dalitz plot analysis of $B^\\pm \\to D K^\\pm$ decays, with $D\\to K_\\text{S}^0\u03c0^+\u03c0^-$ and $D\\to K_\\text{S}^0K^+K^-$, is used to perform a measurement of the CP-violating observables $x_{\\pm}$ and $y_{\\pm}$, which are sensitive to the Cabibbo-Kobayashi-Maskawa angle $\u03b3$. The analysis is performed without assuming any $D$ decay model, through the use of information on the strong-phase variation over the Dalitz plot from the CLEO collaboration. Using a sample of proton-proton collision data collected with the LHCb experiment in 2015 and 2016, and corresponding to an integrated luminosity of 2.0$\\,\\text{fb}^{-1}$, the values of the CP violation parameters are found to be $x_- = ( 9.0 \\pm 1.7 \\pm 0.7 \\pm 0.4) \\times 10^{-2}$, $y_- = ( 2.1 \\pm 2.2 \\pm 0.5 \\pm 1.1) \\times 10^{-2}$, $x_+ = (- 7.7 \\pm 1.9 \\pm 0.7 \\pm 0.4) \\times 10^{-2}$, and $y_+ = (- 1.0 \\pm 1.9 \\pm 0.4 \\pm 0.9) \\times 10^{-2}$. The first uncertainty is statistical, the second is systematic, and the third is due to the uncertainty on the strong-phase measurements. These values are used to obtain $\u03b3= \\left(87\\,^{+11}_{-12}\\right)^\\circ$, $r_B = 0.086^{+ 0.013}_{-0.014}$, and $\u03b4_B = (101 \\pm 11)^\\circ$, where $r_B$ is the ratio between the suppressed and favoured $B$-decay amplitudes and $\u03b4_B$ is the corresponding strong-interaction phase difference. This measurement is combined with the result obtained using 2011 and 2012 data collected with the \\lhcb experiment, to give $\u03b3= \\left(80\\,^{+10}_{\\,-9}\\right)^\\circ$, $r_B = 0.080 \\pm 0.011$, and $\u03b4_B = (110 \\pm 10)^\\circ$.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The inclusive $D_s^{\\pm}$ production asymmetry is measured in $pp$ collisions collected by the LHCb experiment at centre-of-mass energies of $\\sqrt{s} =7$ and 8 TeV. Promptly produced $D_s^{\\pm}$ mesons are used, which decay as $D_s^{\\pm}\\to\u03c6\u03c0^{\\pm}$, with $\u03c6\\to K^+K^-$. The measurement is performed in bins of transverse momentum, $p_{\\rm T}$, and rapidity, $y$, covering the range $2.5<p_{\\rm T}<25.0$ GeV$/c$ and $2.0<y<4.5$. No kinematic dependence is observed. Evidence of nonzero $D_s^{\\pm}$ production asymmetry is found with a significance of 3.3 standard deviations.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A search is performed for a spin-0 boson, $\u03c6$, produced in proton-proton collisions at centre-of-mass energies of 7 and 8 TeV, using prompt $\u03c6\\rightarrow\u03bc^+\u03bc^-$ decays and a data sample corresponding to an integrated luminosity of approximately 3.0 ${\\rm fb}^{-1}$ collected with the LHCb detector. No evidence is found for a signal in the mass range from 5.5 to 15 GeV. Upper limits are placed on the product of the production cross-section and the branching fraction into the dimuon final state. The limits are comparable to the best existing over most of the mass region considered and are the first to be set near the $\u03a5$ resonances.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "From samples of $pp$ collision data collected by the LHCb experiment at $\\sqrt{s}=7$, $8$ and $13$ TeV corresponding to integrated luminosities of 1.0, 2.0 and 1.5 fb$^{-1}$, respectively, a peak in both the $\u039b_b^0K^-$ and $\u039e_b^0\u03c0^-$ invariant mass spectra is observed. In the quark model, radially and orbitally excited $\u039e_b^-$ resonances with quark content $bds$ are expected. Referring to this peak as $\u039e_b(6227)^-$, the mass and natural width are measured to be $m_{\u039e_{b}(6227)^-}=6226.9\\pm2.0\\pm0.3\\pm0.2$ MeV/$c^2$ and $\u0393_{\u039e_b(6227)^-}=18.1\\pm5.4\\pm1.8$ MeV/$c^2$, where the first uncertainty is statistical, the second is systematic, and the third, on $m_{\u039e_b(6227)^-}$, is due to the knowledge of the $\u039b_b^0$ baryon mass. Relative production rates of the ${\u039e_b(6227)^-\\to\u039b_b^0K^-}$ and ${\u039e_b(6227)^-\\to\u039e_b^0\u03c0^-}$ decays are also reported.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The time-dependent $C\\!P$ asymmetries in $B^0\\to\u03c0^+\u03c0^-$ and $B_s^0\\to K^+\\!K^-$ decays are measured using a data sample of $pp$ collisions corresponding to an integrated luminosity of 3.0 fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of 7 and 8 TeV. The same data sample is used to measure the time-integrated $C\\!P$ asymmetries in $B^0\\to K^+\u03c0^-$ and $B_s^0\\to\u03c0^+ K^-$ decays. The results are $C_{\u03c0^+\u03c0^-} = -0.34 \\pm 0.06 \\pm 0.01$, $S_{\u03c0^+\u03c0^-} = -0.63 \\pm 0.05 \\pm 0.01$, $C_{K^+\\!K^-} = 0.20 \\pm 0.06 \\pm 0.02$, $S_{K^+\\!K^-} = 0.18 \\pm 0.06 \\pm 0.02$, $C_{K^+\\!K^-}^{\u0394\u0393} = -0.79 \\pm 0.07 \\pm 0.10$, $A_{C\\!P}^{B^0} = -0.084 \\pm 0.004 \\pm 0.003$, and $A_{C\\!P}^{B_s^0} = 0.213 \\pm 0.015 \\pm 0.007$, where the first uncertainties are statistical and the second systematic. Evidence for $C\\!P$ violation is found in the $B_s^0\\to K^+\\!K^-$ decay for the first time.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A search for $C$P and $P$ violation using triple-product asymmetries is performed with $\u039b^{0}_{b}\\to pK^{-}\u03c0^{+}\u03c0^{-}$, $\u039b^{0}_{b}\\to pK^{-}K^{+}K^{-}$ and $\u039e^{0}_{b}\\to pK^{-}K^{-}\u03c0^{+}$ decays. The data sample corresponds to integrated luminosities of 1.0fb$^{-1}$ and 2.0fb$^{-1}$, recorded with the LHCb detector at centre-of-mass energies of 7TeV and 8TeV, respectively. The $CP$- and $P$-violating asymmetries are measured both integrating over all phase space and in specific phase-space regions. No significant deviation from $CP$ or $P$ symmetry is found. The first observation of $\u039b^{0}_{b}\\to pK^{-}\u03c7_{c0}(1P)(\\to\u03c0^{+}\u03c0^{-}, K^{+}K^{-})$ decay is also reported.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A measurement of the $CP$ asymmetries $S_{f}$ and $S_{\\bar{f}}$ in $B^0\\to D^{\\mp}\u03c0^{\\pm}$ decays is reported. The decays are reconstructed in a dataset collected with the LHCb experiment in proton-proton collisions at centre-of-mass energies of 7 and 8 TeV and corresponding to an integrated luminosity of $3.0 \\rm{ fb}^{-1}$. The $CP$ asymmetries are measured to be $S_{f} = 0.058 \\pm 0.020 (\\rm{stat}) \\pm 0.011(\\rm{syst})$ and $S_{\\bar{f}} = 0.038\\pm 0.020 (\\text{stat})\\pm 0.007 (\\text{syst})$. These results are in agreement with, and more precise than, previous determinations. They are used to constrain $|\\sin\\left(2\u03b2+\u03b3\\right)|$ and $\u03b3$ to intervals that are consistent with the current world-average values.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The decay $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ is observed using $pp$ collision data collected with the LHCb detector at centre-of-mass energies of $\\sqrt{s}=$ 7 and 8 TeV, corresponding to an integrated luminosity of 3 $fb^{-1}$. The ratio of branching fractions between $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ and $\u039b_b^0 \\to \u039b_c^+ \u03c0^-$ decays is measured to be \\begin{equation*}\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ \u03c0^-)} = 0.0540 \\pm 0.0023 \\pm 0.0032. \\end{equation*} Two resonant structures are observed in the $ \u039b_c^+ \u03c0^-$ mass spectrum of the ${\u039b_b^0 \\to \u039b_c^+ p\\overline{p} \u03c0^-}$ decays, corresponding to the $\u03a3_c(2455)^0$ and $\u03a3_c^{*}(2520)^0$ states. The ratios of branching fractions with respect to the decay $\u039b_b^0 \\to \u039b_c^+ p \\overline{p} \u03c0^-$ are \\begin{align*}\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u03a3_c^0 p\\overline{p})\\times\\mathcal{B}(\u03a3_c^0\\to \u039b_c^+ \u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)} = 0.089\\pm0.015\\pm0.006,\n  \\frac{\\mathcal{B}(\u039b_b^0 \\to \u03a3_c^{*0} p\\overline{p})\\times\\mathcal{B}(\u03a3_c^{*0}\\to \u039b_c^+ \u03c0^-)}{\\mathcal{B}(\u039b_b^0 \\to \u039b_c^+ p \\overline{p}\u03c0^-)} = 0.119\\pm0.020\\pm0.014. \\end{align*} In all of the above results, the first uncertainty is statistical and the second is systematic. The phase space is also examined for the presence of dibaryon resonances. No evidence for such resonances is found.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The production cross-sections of $\u03a5(1S)$, $\u03a5(2S)$ and $\u03a5(3S)$ mesons in proton-proton collisions at $\\sqrt{s}$= 13 TeV are measured with a data sample corresponding to an integrated luminosity of $277 \\pm 11$ $\\rm pb^{-1}$ recorded by the LHCb experiment in 2015. The $\u03a5$ mesons are reconstructed in the decay mode $\u03a5\\to\u03bc^{+}\u03bc^{-}$. The differential production cross-sections times the dimuon branching fractions are measured as a function of the $\u03a5$ transverse momentum, $p_{\\rm T}$, and rapidity, $y$, over the range $0 < p_{\\rm T}< 30$ GeV/c and $2.0 < y < 4.5$. The ratios of the cross-sections with respect to the LHCb measurement at $\\sqrt{s}$= 8 TeV are also determined. The measurements are compared with theoretical predictions based on NRQCD.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A search for the decay $B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-$ is presented using data sets corresponding to 1.0, 2.0 and 1.6 $\\text{fb}^{-1}$ of integrated luminosity collected during $pp$ collisions with the LHCb experiment at centre-of-mass energies of 7, 8 and 13 TeV, respectively. An excess is found over the background-only hypothesis with a significance of 3.4 standard deviations. The branching fraction of the $B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-$ decay is determined to be $\\mathcal{B}(B_{s}^0 \\rightarrow \\overline{K}{}^{*0}\u03bc^+\u03bc^-) = [2.9 \\pm 1.0~(\\text{stat}) \\pm 0.2~(\\text{syst}) \\pm 0.3~(\\text{norm})] \\times 10^{-8}$, where the first and second uncertainties are statistical and systematic, respectively. The third uncertainty is due to limited knowledge of external parameters used to normalise the branching fraction measurement.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The Asteroid Terrestrial-impact Last Alert System (ATLAS) carries out its primary planetary defense mission by surveying about 13000 deg^2 at least four times per night. The resulting data set is useful for the discovery of variable stars to a magnitude limit fainter than r~18, with amplitudes down to 0.01 mag for bright objects. Here we present a Data Release One catalog of variable stars based on analyzing 142 million stars measured at least 100 times in the first two years of ATLAS operations. Using a Lomb-Scargle periodogram and other variability metrics, we identify 4.7 million candidate variables which we analyze in detail. Through Space Telescope Science Institute, we publicly release lightcurves for all of them, together with a vector of 169 classification features for each star. We do this at the level of unconfirmed candidate variables in order to provide the community with a large set of homogeneously analyzed photometry and avoid pre-judging which types of objects others may find most interesting. We use machine learning to classify the candidates into fifteen different broad categories based on lightcurve morphology. About 10% (430,000 stars) pass extensive tests designed to screen out spurious variability detections: we label these as `probable' variables. Of these, 230,000 receive specific classifications as eclipsing binaries, pulsating, Mira-type, or sinusoidal variables: these are the `classified' variables. New discoveries among the probable variables number more than 300,000, while 150,000 of the classified variables are new, including about 10,000 pulsating variables, 2,000 Mira stars, and 70,000 eclipsing binaries.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The $CP$ asymmetry in $B^-\\to D_s^-D^0$ and $B^-\\to D^-D^0$ decays is measured using LHCb data corresponding to an integrated luminosity of 3.0 fb$^{-1}$, collected in $pp$ collisions at centre-of-mass energies of 7 and 8 TeV. The results are $A^{CP}(B^-\\to D_s^-D^0)=(-0.4\\pm 0.5\\pm 0.5)\\%$ and $A^{CP}(B^-\\to D^-D^0)=( 2.3\\pm 2.7\\pm 0.4)\\%$, where the first uncertainties are statistical and the second systematic. This is the first measurement of $A^{CP}(B^-\\to D_s^-D^0)$ and the most precise determination of $A^{CP}(B^-\\to D^-D^0)$. Neither result shows evidence of $CP$ violation.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The cross-section for inelastic proton-proton collisions at a centre-of-mass energy of 13\\,TeV is measured with the LHCb detector. The fiducial cross-section for inelastic interactions producing at least one prompt long-lived charged particle with momentum $p>2$\\,GeV/$c$ in the pseudorapidity range $2<\u03b7<5$ is determined to be $\u03c3_{\\rm acc}= 62.2 \\pm 0.2 \\pm 2.5$\\,mb. The first uncertainty is the intrinsic systematic uncertainty of the measurement, the second is due to the uncertainty on the integrated luminosity. The statistical uncertainty is negligible. Extrapolation to full phase space yields the total inelastic proton-proton cross-section $\u03c3_{\\rm inel}= 75.4 \\pm 3.0 \\pm 4.5$\\,mb, where the first uncertainty is experimental and the second due to the extrapolation. An updated value of the inelastic cross-section at a centre-of-mass energy of 7\\,TeV is also reported.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "Forward top quark pair production is studied in $pp$ collisions in the $\u03bceb$ final state using a data sample corresponding to an integrated luminosity of 1.93 fb$^{-1}$ collected with the LHCb experiment at a centre-of-mass energy of 13 TeV. The cross-section is measured in a fiducial region where both leptons have a transverse momentum greater than 20 GeV and a pseudorapidity between 2.0 and 4.5. The quadrature sum of the azimuthal separation and the difference in pseudorapidities, denoted $\u0394R$, between the two leptons must be larger than 0.1. The $b$-jet axis is required to be separated from both leptons by a $\u0394R$ of 0.5, and to have a transverse momentum in excess of 20 GeV and a pseudorapidity between 2.2 and 4.2. The cross-section is measured to be $$\u03c3_{t\\bar{t}}= 126\\pm19\\,(\\mathrm{stat})\\pm16\\,(\\mathrm{syst})\\pm5\\,(\\mathrm{lumi})\\,\\,\\mathrm{ fb}$$ where the first uncertainty is statistical, the second is systematic, and the third is due to the luminosity determination. The measurement is compatible with the Standard Model prediction.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "The time-integrated Dalitz plot of the three-body hadronic charmless decay ${\\overline{B}^0 \\to K_{\\mathrm{\\scriptscriptstyle S}}^0 \u03c0^+ \u03c0^-}$ is studied using a $pp$ collision data sample recorded with the LHCb detector, corresponding to an integrated luminosity of $3.0\\;\\mathrm{fb}^{-1}$. The decay amplitude is described with an isobar model. Relative contributions of the isobar amplitudes to the ${\\overline{B}^0 \\to K_{\\mathrm{\\scriptscriptstyle S}}^0 \u03c0^+ \u03c0^-}$ decay branching fraction and CP asymmetries of the flavour-specific amplitudes are measured. The CP asymmetry between the conjugate ${\\overline{B}^0 \\to K^{*}(892)^{-}\u03c0^+}$ and ${\\overline{B}^0 \\to K^{*}(892)^{+}\u03c0^-}$ decay rates is determined to be $-0.308 \\pm 0.062$.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A flavour-tagged decay-time-dependent amplitude analysis of $B_s^0\\to(K^+\u03c0^-)(K^-\u03c0^+)$ decays is presented in the $K^{\\pm}\u03c0^{\\mp}$ mass range from 750 to 1600 MeV$/c^2$. The analysis uses $pp$ collision data collected with the LHCb detector at centre-of-mass energies of $7$ and $8$ TeV, corresponding to an integrated luminosity of $3.0$ fb$^{-1}$. Several quasi-two-body decay modes are considered, corresponding to $K^{\\pm}\u03c0^{\\mp}$ combinations with spin 0, 1 and 2, which are dominated by the $K_0^*(800)^0$ and $K_0^*(1430)^0$, the $K^*(892)^0$ and the $K_2^*(1430)^0$ resonances, respectively. The longitudinal polarisation fraction for the $B_s^0\\to K^*(892)^0\\overline{K}^*(892)^0$ decay is measured as $f_L=0.208 \\pm 0.032 \\pm 0.046$, where the first uncertainty is statistical and the second is systematic. The first measurement of the mixing-induced $CP$-violating phase, $\u03c6_s^{d\\overline{d}}$, in $b\\to d\\overline{d}s$ transitions is performed, yielding a value of $\u03c6_s^{d\\overline{d}}=-0.10$ $\\pm$ $0.13$ (stat) $\\pm$ $0.14$ (syst) rad.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "Amplitude models are constructed to describe the resonance structure of ${D^{0}\\to K^{-}\u03c0^{+}\u03c0^{+}\u03c0^{-}}$ and ${D^{0} \\to K^{+}\u03c0^{-}\u03c0^{-}\u03c0^{+}}$ decays using $pp$ collision data collected at centre-of-mass energies of 7 and 8 TeV with the LHCb experiment, corresponding to an integrated luminosity of $3.0\\mathrm{fb}^{-1}$. The largest contributions to both decay amplitudes are found to come from axial resonances, with decay modes $D^{0} \\to a_1(1260)^{+} K^{-}$ and $D^{0} \\to K_1(1270/1400)^{+} \u03c0^{-}$ being prominent in ${D^{0}\\to K^{-}\u03c0^{+}\u03c0^{+}\u03c0^{-}}$ and $D^{0}\\to K^{+}\u03c0^{-}\u03c0^{-}\u03c0^{+}$, respectively. Precise measurements of the lineshape parameters and couplings of the $a_1(1260)^{+}$, $K_1(1270)^{-}$ and $K(1460)^{-}$ resonances are made, and a quasi model-independent study of the $K(1460)^{-}$ resonance is performed. The coherence factor of the decays is calculated from the amplitude models to be $R_{K3\u03c0} = 0.459\\pm 0.010\\,(\\mathrm{stat}) \\pm 0.012\\,(\\mathrm{syst}) \\pm 0.020\\,(\\mathrm{model})$, which is consistent with direct measurements. These models will be useful in future measurements of the unitary-triangle angle $\u03b3$ and studies of charm mixing and $C\\!P$ violation.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A search for the rare decay $\u03a3^+ \\to p \u03bc^+ \u03bc^-$ is performed using $pp$ collision data recorded by the LHCb experiment at centre-of-mass energies $\\sqrt{s} = 7$ and $8$ TeV, corresponding to an integrated luminosity of $3 fb^{-1}$. An excess of events is observed with respect to the background expectation, with a signal significance of 4.1 standard deviations. No significant structure is observed in the dimuon invariant mass distribution, in contrast with a previous result from the HyperCP experiment. The measured $\u03a3^+ \\to p \u03bc^+ \u03bc^-$ branching fraction is $(2.2\\,^{+\\,1.8}_{-\\,1.3})\\times 10^{-8}$, where statistical and systematic uncertainties are included, which is consistent with the Standard Model prediction.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "Investigations of the existence of pentaquark states containing a single $b$ (anti)quark decaying weakly into four specific final states J/$\u03c8K^+\u03c0^- p$, J/$\u03c8K^- \u03c0^- p$, J/$\u03c8K^- \u03c0^+ p$, and $J/\u03c8\u03c6(1020) p$ are reported. The data sample corresponds to an integrated luminosity of 3.0/fb in 7 and 8 TeV pp collisions acquired with the LHCb detector. Signals are not observed and upper limits are set on the product of the production cross section times branching fraction with respect to that of the $\u039b_b$.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "A search for the flavor-changing neutral-current decay $\u039b_{c}^{+} \\to p\u03bc^+\u03bc^-$ is reported using a data set corresponding to an integrated luminosity of $3.0\\rm fb^{-1}$ collected by the LHCb collaboration. No significant signal is observed outside of the dimuon mass regions around the $\u03c6$ and $\u03c9$ resonances and an upper limit is placed on the branching fraction of $\\mathcal{B} (\u039b_{c}^{+} \\to p\u03bc^+\u03bc^-) < 7.7~(9.6)\\times 10^{-8}~{\\rm at}~90\\%~(95\\%)$ confidence level. A significant signal is observed in the $\u03c9$ dimuon mass region for the first time.\n        \u25b3 Less", "author": "Henry Smith"}, {"abstract": "Solving nonlinear SMT problems over real numbers has wide applications in robotics and AI. While significant progress is made in solving quantifier-free SMT formulas in the domain, quantified formulas have been much less investigated. We propose the first delta-complete algorithm for solving satisfiability of nonlinear SMT over real numbers with universal quantification and a wide range of nonlinear functions. Our methods combine ideas from counterexample-guided synthesis, interval constraint propagation, and local optimization. In particular, we show how special care is required in handling the interleaving of numerical and symbolic reasoning to ensure delta-completeness. In experiments, we show that the proposed algorithms can handle many new problems beyond the reach of existing SMT solvers.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and(iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "In this paper, we present ReaS, a technique that combines numerical optimization with SAT solving to synthesize unknowns in a program that involves discrete and floating point computation. ReaS makes the program end-to-end differentiable by smoothing any Boolean expression that introduces discontinuity such as conditionals and relaxing the Boolean unknowns so that numerical optimization can be performed. On top of this, ReaS uses a SAT solver to help the numerical search overcome local solutions by incrementally fixing values to the Boolean expressions. We evaluated the approach on 5 case studies involving hybrid systems and show that ReaS can synthesize programs that could not be solved by previous SMT approaches.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an implementation f that meets both a semantic constraint given by a logical formula phi in a background theory T, and a syntactic constraint given by a grammar G, which specifies the allowed set of candidate implementations. Such a synthesis problem can be formally defined in SyGuS-IF, a language that is built on top of SMT-LIB.\n  The Syntax-Guided Synthesis Competition (SyGuS-Comp) is an effort to facilitate, bring together and accelerate research and development of efficient solvers for SyGuS by providing a platform for evaluating different synthesis techniques on a comprehensive set of benchmarks.  In this year's competition six new solvers competed on over 1500 benchmarks. This paper presents and analyses the results of SyGuS-Comp'17.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, mapping the inputs to their corresponding outputs exactly. Due to its precise and combinatorial nature, program synthesis is commonly formulated as a constraint satisfaction problem, where input-output examples are encoded as constraints and solved with a constraint solver. A key challenge of this formulation is scalability: while constraint solvers work well with a few well-chosen examples, a large set of examples can incur significant overhead in both time and memory. We describe a method to discover a subset of examples that is both small and representative: the subset is constructed iteratively, using a neural network to predict the probability of unchosen examples conditioned on the chosen examples in the subset, and greedily adding the least probable example. We empirically evaluate the representativeness of the subsets constructed by our method, and demonstrate such subsets can significantly improve synthesis time and stability.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Delimited continuations are the mother of all monads! So goes the slogan inspired by Filinski's 1994 paper, which showed that delimited continuations can implement any monadic effect, letting the programmer use an effect as easily as if it was built into the language. It's a shame that not many languages have delimited continuations.\n  Luckily, exceptions and state are also the mother of all monads! In this Pearl, we show how to implement delimited continuations in terms of exceptions and state, a construction we call $\\textit{thermometer continuations}$. While traditional implementations of delimited continuations require some way of \"capturing\" an intermediate state of the computation, the insight of thermometer continuations is to reach this intermediate state by replaying the entire computation from the start, guiding it using a recording it so that the same thing happens until the captured point.\n  Along the way, we explain delimited continuations and monadic reflection, show how the Filinski construction lets thermometer continuations express any monadic effect, share an elegant special-case for nondeterminism, and discuss why our construction is not prevented by theoretical results that exceptions and state cannot macro-express continuations.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of \\LaTeX. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are like a trace of the set of primitive commands issued by a graphics program. We learn a model that uses program synthesis techniques to recover a graphics program from that trace. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network, measure similarity between drawings by use of similar high-level geometric structures, and extrapolate drawings. Taken together these results are a step towards agents that induce useful, human-readable programs from perceptual input.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We present a new approach for building source-to-source transformations that can run on multiple programming languages, based on a new way of representing programs called incremental parametric syntax. We implement this approach in Haskell in our Cubix system, and construct incremental parametric syntaxes for C, Java, JavaScript, Lua, and Python. We demonstrate a whole-program refactoring tool that runs on all of them, along with three smaller transformations that each run on several. Our evaluation shows that (1) once a transformation is written, little work is required to configure it for a new language (2) transformations built this way output readable code which preserve the structure of the original, according to participants in our human study, and (3) our transformations can still handle language corner-cases, as validated on compiler test suites.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Maintaining leadership in HPC requires the ability to support simulations at large scales and fidelity. In this study, we detail one of the most significant productivity challenges in achieving this goal, namely the increasing proclivity to bugs, especially in the face of growing hardware and software heterogeneity and sheer system scale. We identify key areas where timely new research must be proactively begun to address these challenges, and create new correctness tools that must ideally play a significant role even while ramping up toward exacale. We close with the proposal for a two-day workshop in which the problems identified in this report can be more broadly discussed, and specific plans to launch these new research thrusts identified.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We consider the problem of diagnosis where a set of simple observations are used to infer a potentially complex hidden hypothesis. Finding the optimal subset of observations is intractable in general, thus we focus on the problem of active diagnosis, where the agent selects the next most-informative observation based on the results of previous observations. We show that under the assumption of uniform observation entropy, one can build an implication model which directly predicts the outcome of the potential next observation conditioned on the results of past observations, and selects the observation with the maximum entropy. This approach enjoys reduced computation complexity by bypassing the complicated hypothesis space, and can be trained on observation data alone, learning how to query without knowledge of the hidden hypothesis.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an implementation f that meets both a semantic constraint given by a logical formula $\\varphi$ in a background theory T, and a syntactic constraint given by a grammar G, which specifies the allowed set of candidate implementations. Such a synthesis problem can be formally defined in SyGuS-IF, a language that is built on top of SMT-LIB.\n  The Syntax-Guided Synthesis Competition (SyGuS-Comp) is an effort to facilitate, bring together and accelerate research and development of efficient solvers for SyGuS by providing a platform for evaluating different synthesis techniques on a comprehensive set of benchmarks. In this year's competition we added a new track devoted to programming by examples. This track consisted of two categories, one using the theory of bit-vectors and one using the theory of strings. This paper presents and analyses the results of SyGuS-Comp'16.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We present a technique for static enforcement of high-level, declarative information flow policies. Given a program that manipulates sensitive data and a set of declarative policies on the data, our technique automatically inserts policy-enforcing code throughout the program to make it provably secure with respect to the policies. We achieve this through a new approach we call type-targeted program synthesis, which enables the application of traditional synthesis techniques in the context of global policy enforcement. The key insight is that, given an appropriate encoding of policy compliance in a type system, we can use type inference to decompose a global policy enforcement problem into a series of small, local program synthesis problems that can be solved independently.\n  We implement this approach in Lifty, a core DSL for data-centric applications. Our experience using the DSL to implement three case studies shows that (1) Lifty's centralized, declarative policy definitions make it easier to write secure data-centric applications, and (2) the Lifty compiler is able to efficiently synthesize all necessary policy-enforcing code, including the code required to prevent several reported real-world information leaks.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We present a novel technique for automatic program correction in MOOCs, capable of fixing both syntactic and semantic errors without manual, problem specific correction strategies. Given an incorrect student program, it generates candidate programs from a distribution of likely corrections, and checks each candidate for correctness against a test suite.\n  The key observation is that in MOOCs many programs share similar code fragments, and the seq2seq neural network model, used in the natural-language processing task of machine translation, can be modified and trained to recover these fragments.\n  Experiment shows our scheme can correct 29% of all incorrect submissions and out-performs state of the art approach which requires manual, problem specific correction strategies.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "This paper addresses the problem of creating simplifiers for logic formulas based on conditional term rewriting. In particular, the paper focuses on a program synthesis application where formula simplifications have been shown to have a significant impact. We show that by combining machine learning techniques with constraint-based synthesis, it is possible to synthesize a formula simplifier fully automatically from a corpus of representative problems, making it possible to create formula simplifiers tailored to specific problem domains. We demonstrate the benefits of our approach for synthesis benchmarks from the SyGuS competition and automated grading.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an implementation f that meets both a semantic constraint given by a logical formula $\\varphi$ in a background theory T, and a syntactic constraint given by a grammar G, which specifies the allowed set of candidate implementations. Such a synthesis problem can be formally defined in SyGuS-IF, a language that is built on top of SMT-LIB.\n  The Syntax-Guided Synthesis Competition (SyGuS-comp) is an effort to facilitate, bring together and accelerate research and development of efficient solvers for SyGuS by providing a platform for evaluating different synthesis techniques on a comprehensive set of benchmarks. In this year's competition we added two specialized tracks: a track for conditional linear arithmetic, where the grammar need not be specified and is implicitly assumed to be that of the LIA logic of SMT-LIB, and a track for invariant synthesis problems, with special constructs conforming to the structure of an invariant synthesis problem. This paper presents and analyzes the results of SyGuS-comp'15.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We present a method for synthesizing recursive functions that provably satisfy a given specification in the form of a polymorphic refinement type. We observe that such specifications are particularly suitable for program synthesis for two reasons. First, they offer a unique combination of expressive power and decidability, which enables automatic verification---and hence synthesis---of nontrivial programs. Second, a type-based specification for a program can often be effectively decomposed into independent specifications for its components, causing the synthesizer to consider fewer component combinations and leading to a combinatorial reduction in the size of the search space. At the core of our synthesis procedure is a new algorithm for refinement type checking, which supports specification decomposition.\n  We have evaluated our prototype implementation on a large set of synthesis problems and found that it exceeds the state of the art in terms of both scalability and usability. The tool was able to synthesize more complex programs than those reported in prior work (several sorting algorithms and operations on balanced search trees), as well as most of the benchmarks tackled by existing synthesizers, often starting from a more concise and intuitive user input.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Recent work has proposed a promising approach to improving scalability of program synthesis by allowing the user to supply a syntactic template that constrains the space of potential programs. Unfortunately, creating templates often requires nontrivial effort from the user, which impedes the usability of the synthesizer. We present a solution to this problem in the context of recursive transformations on algebraic data-types. Our approach relies on polymorphic synthesis constructs: a small but powerful extension to the language of syntactic templates, which makes it possible to define a program space in a concise and highly reusable manner, while at the same time retains the scalability benefits of conventional templates. This approach enables end-users to reuse predefined templates from a library for a wide variety of problems with little effort. The paper also describes a novel optimization that further improves the performance and scalability of the system. We evaluated the approach on a set of benchmarks that most notably includes desugaring functions for lambda calculus, which force the synthesizer to discover Church encodings for pairs and boolean operations.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Sketch-based synthesis, epitomized by the SKETCH tool, lets developers synthesize software starting from a partial program, also called a sketch or template. This paper presents JSKETCH, a tool that brings sketch-based synthesis to Java. JSKETCH's input is a partial Java program that may include holes, which are unknown constants, expression generators, which range over sets of expressions, and class generators, which are partial classes. JSKETCH then translates the synthesis problem into a SKETCH problem; this translation is complex because SKETCH is not object-oriented. Finally, JSKETCH synthesizes an executable Java program by interpreting the output of SKETCH.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We present an approach for dynamic information flow control across the application and database. Our approach reduces the amount of policy code required, yields formal guarantees across the application and database, works with existing relational database implementations, and scales for realistic applications. In this paper, we present a programming model that factors out information flow policies from application code and database queries, a dynamic semantics for the underlying \u03bb^JDB core language, and proofs of termination-insensitive non-interference and policy compliance for the semantics. We implement these ideas in Jacqueline, a Python web framework, and demonstrate feasibility through three application case studies: a course manager, a health record system, and a conference management system used to run an academic workshop. We show that in comparison to traditional applications with hand-coded policy checks, Jacqueline applications have 1) a smaller trusted computing base, 2) fewer lines of policy code, and 2) reasonable, often negligible, additional overheads.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Software synthesis is rapidly developing into an important research area with vast potential for practical application. The SYNT Workshop on Synthesis aims to bringing together researchers interested in synthesis to present both ongoing and mature work on all aspects of automated synthesis and its applications. \n  The second iteration of SYNT took place in Saint Petersburg, Russia, and was co-located with the 25th International Conference on Computer Aided Verification. The workshop included eleven presentations covering the full scope of the emerging synthesis community, as well as a discussion lead by Swen Jacobs on the organization of two new synthesis competitions focusing on reactive synthesis and syntax-guided functional synthesis respectively.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We prove several decidability and undecidability results for the satisfiability and validity problems for languages that can express solutions to word equations with length constraints. The atomic formulas over this language are equality over string terms (word equations), linear inequality over the length function (length constraints), and membership in regular sets. These questions are important in logic, program analysis, and formal verification. Variants of these questions have been studied for many decades by mathematicians. More recently, practical satisfiability procedures (aka SMT solvers) for these formulas have become increasingly important in the context of security analysis for string-manipulating programs such as web applications.\n  We prove three main theorems. First, we give a new proof of undecidability for the validity problem for the set of sentences written as a forall-exists quantifier alternation applied to positive word equations. A corollary of this undecidability result is that this set is undecidable even with sentences with at most two occurrences of a string variable. Second, we consider Boolean combinations of quantifier-free formulas constructed out of word equations and length constraints. We show that if word equations can be converted to a solved form, a form relevant in practice, then the satisfiability problem for Boolean combinations of word equations and length constraints is decidable. Third, we show that the satisfiability problem for quantifier-free formulas over word equations in regular solved form, length constraints, and the membership predicate over regular expressions is also decidable.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "This paper presents a new approach to select events of interest to a user in a social media setting where events are generated by the activities of the user's friends through their mobile devices. We argue that given the unique requirements of the social media setting, the problem is best viewed as an inductive learning problem, where the goal is to first generalize from the users' expressed \"likes\" and \"dislikes\" of specific events, then to produce a program that can be manipulated by the system and distributed to the collection devices to collect only data of interest. The key contribution of this paper is a new algorithm that combines existing machine learning techniques with new program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application. The approach also improves on standard machine learning techniques in that it produces clear programs that can be manipulated to optimize data collection and filtering.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "Developing high-performance applications that interact with databases is a difficult task, as developers need to understand both the details of the language in which their applications are written in, and also the intricacies of the relational model. One popular solution to this problem is the use of object-relational mapping (ORM) libraries that provide transparent access to the database using the same language that the application is written in. Unfortunately, using such frameworks can easily lead to applications with poor performance because developers often end up implementing relational operations in application code, and doing so usually does not take advantage of the optimized implementations of relational operations, efficient query plans, or push down of predicates that database systems provide. In this paper we present QBS, an algorithm that automatically identifies fragments of application logic that can be pushed into SQL queries. The QBS algorithm works by automatically synthesizing invariants and postconditions for the original code fragment. The postconditions and invariants are expressed using a theory of ordered relations that allows us to reason precisely about the contents and order of the records produced even by complex code fragments that compute joins and aggregates. The theory is close in expressiveness to SQL, so the synthesized postconditions can be readily translated to SQL queries. Using 40 code fragments extracted from over 120k lines of open-source code written using the Java Hibernate ORM, we demonstrate that our approach can convert a variety of imperative constructs into relational specifications.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We present a new method for automatically providing feedback for introductory programming problems. In order to use this method, we need a reference implementation of the assignment, and an error model consisting of potential corrections to errors that students might make. Using this information, the system automatically derives minimal corrections to student's incorrect solutions, providing them with a quantifiable measure of exactly how incorrect a given solution was, as well as feedback about what they did wrong.\n  We introduce a simple language for describing error models in terms of correction rules, and formally define a rule-directed translation strategy that reduces the problem of finding minimal corrections in an incorrect program to the problem of synthesizing a correct program from a sketch. We have evaluated our system on thousands of real student attempts obtained from 6.00 and 6.00x. Our results show that relatively simple error models can correct on average 65% of all incorrect submissions.\n        \u25b3 Less", "author": "Armando Solar-Lezama"}, {"abstract": "We consider the tasks of representing, analyzing and manipulating maps between shapes. We model maps as densities over the product manifold of the input shapes; these densities can be treated as scalar functions and therefore are manipulable using the language of signal processing on manifolds. Being a manifold itself, the product space endows the set of maps with a geometry of its own, which we exploit to define map operations in the spectral domain; we also derive relationships with other existing representations (soft maps and functional maps). To apply these ideas in practice, we discretize product manifolds and their Laplace--Beltrami operators, and we introduce localized spectral analysis of the product manifold as a novel tool for map processing. Our framework applies to maps defined between and across 2D and 3D shapes without requiring special adjustment, and it can be implemented efficiently with simple operations on sparse matrices.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Applications in political redistricting demand quantitative measures of geometric compactness to distinguish between simple and contorted shapes of legislative voting districts. While the isoperimetric quotient, or ratio of area to perimeter squared, is commonly used in practice, it is sensitive to noisy data and irrelevant geographic features like coastline. These issues are addressed in theory by the isoperimetric profile, which plots the minimum perimeter needed to inscribe shapes of different prescribed areas within the boundary of a shape; algorithms for computing this profile, however, are not known in practice. Hence, in this paper, we propose a convex Eulerian relaxation of the isoperimetric profile using total variation. We prove theoretical properties of our relaxation, showing that it still satisfies an isoperimetric inequality and yields a convex function of the prescribed area. Furthermore, we provide a discretization of the problem, an optimization technique, and experiments demonstrating the value of our relaxation.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "We propose a technique for interpolating between probability distributions on discrete surfaces, based on the theory of optimal transport. Unlike previous attempts that use linear programming, our method is based on a dynamical formulation of quadratic optimal transport proposed for flat domains by Benamou and Brenier [2000], adapted to discrete surfaces. Our structure-preserving construction yields a Riemannian metric on the (finite-dimensional) space of probability distributions on a discrete surface, which translates the so-called Otto calculus to discrete language. From a practical perspective, our technique provides a smooth interpolation between distributions on discrete surfaces with less diffusion than state-of-the-art algorithms involving entropic regularization. Beyond interpolation, we show how our discrete notion of optimal transport extends to other tasks, such as distribution-valued Dirichlet problems and time integration of gradient flows.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Sparsification is becoming more and more relevant with the proliferation of huge data sets. Coresets are a principled way to construct representative weighted subsets of a data set that have matching performance with the full data set for specific problems. However, coreset language neglects the nature of the underlying data distribution, which is often continuous. In this paper, we address this oversight by introducing a notion of measure coresets that generalizes coreset language to arbitrary probability measures. Our definition reveals a surprising connection to optimal transport theory which we leverage to design a coreset for problems with Lipschitz costs. We validate our construction on support vector machine (SVM) training, k-means clustering, k-median clustering, and linear regression and show that we are competitive with previous coreset constructions.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "We introduce a novel method to circumvent Weinberg's no-go theorem for self-tuning the cosmological vacuum energy: a Lorentz-violating finite-temperature superfluid can counter the effects of an arbitrarily large cosmological constant. Fluctuations of the superfluid result in the graviton acquiring a Lorentz-violating mass and we identify a unique class of theories that are pathology free, phenomenologically viable, and do not suffer from instantaneous modes. This new and hitherto unidentified phase of massive gravity propagates the same degrees of freedom as general relativity with an additional Lorentz-violating scalar that is introduced by higher-derivative operators in a UV insensitive manner. The superfluid is therefore a consistent infrared modification of gravity. We demonstrate how the superfluid can degravitate a cosmological constant and discuss its phenomenology.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "The shape of an electoral district may suggest whether it was drawn with political motivations, or gerrymandered. For this reason, quantifying the shape of districts, in particular their compactness, is a key task in politics and civil rights. A growing body of literature suggests and analyzes compactness measures mathematically, but little consideration has been given to how these scores should be calculated in practice. Here, we consider the effects of a number of decisions that must be made in interpreting and implementing a set of popular compactness scores. We show that the choices made in quantifying compactness may themselves become political tools, with seemingly innocuous decisions leading to disparate scores. We show that when the full range of implementation flexibility is used, it can be abused to make clearly gerrymandered districts appear quantitatively reasonable. This complicates using compactness as a legislative or judicial standard to counteract unfair redistricting practices. This paper accompanies the release of packages in C++, Python, and R which correctly, efficiently, and reproducibly calculate a variety of compactness scores.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "We present a stochastic algorithm to compute the barycenter of a set of probability distributions under the Wasserstein metric from optimal transport. Unlike previous approaches, our method extends to continuous input distributions and allows the support of the barycenter to be adjusted in each iteration. We tackle the problem without regularization, allowing us to recover a sharp output whose support is contained within the support of the true barycenter. We give examples where our algorithm recovers a more meaningful barycenter than previous work. Our method is versatile and can be extended to applications such as generating super samples from a given distribution and recovering blue noise approximations.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Point clouds provide a flexible and scalable geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. Hence, the design of intelligent computational models that act directly on point clouds is critical, especially when efficiency considerations or noise preclude the possibility of expensive denoising and meshing procedures. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv is differentiable and can be plugged into existing architectures. Compared to existing modules operating largely in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked or recurrently applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. Beyond proposing this module, we provide extensive evaluation and analysis revealing that EdgeConv captures and exploits fine-grained geometric properties of point clouds. The proposed approach achieves state-of-the-art performance on standard benchmarks including ModelNet40 and S3DIS.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Inspired by the matching of supply to demand in logistical problems, the optimal transport (or Monge--Kantorovich) problem involves the matching of probability distributions defined over a geometric domain such as a surface or manifold. In its most obvious discretization, optimal transport becomes a large-scale linear program, which typically is infeasible to solve efficiently on triangle meshes, graphs, point clouds, and other domains encountered in graphics and machine learning. Recent breakthroughs in numerical optimal transport, however, enable scalability to orders-of-magnitude larger problems, solvable in a fraction of a second. Here, we discuss advances in numerical optimal transport that leverage understanding of both discrete and smooth aspects of the problem. State-of-the-art techniques in discrete optimal transport combine insight from partial differential equations (PDE) with convex analysis to reformulate, discretize, and optimize transportation problems. The end result is a set of theoretically-justified models suitable for domains with thousands or millions of vertices. Since numerical optimal transport is a relatively new discipline, special emphasis is placed on identifying and explaining open problems in need of mathematical insight and additional research.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Information transfer between triangle meshes is of great importance in computer graphics and geometry processing. To facilitate this process, a smooth and accurate map is typically required between the two meshes. While such maps can sometimes be computed between nearly-isometric meshes, the more general case of meshes with diverse geometries remains challenging. We propose a novel approach for direct map computation between triangle meshes without mapping to an intermediate domain, which optimizes for the harmonicity and reversibility of the forward and backward maps. Our method is general both in the information it can receive as input, e.g. point landmarks, a dense map or a functional map, and in the diversity of the geometries to which it can be applied. We demonstrate that our maps exhibit lower conformal distortion than the state-of-the-art, while succeeding in correctly mapping key features of the input shapes.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Image tracing is a foundational component of the workflow in graphic design, engineering, and computer animation, linking hand-drawn concept images to collections of smooth curves needed for geometry processing and editing. Even for clean line drawings, modern algorithms often fail to faithfully vectorize junctions, or points at which curves meet; this produces vector drawings with incorrect connectivity. This subtle issue undermines the practical application of vectorization tools and accounts for hesitance among artists and engineers to use automatic vectorization software. To address this issue, we propose a novel image vectorization method based on state-of-the-art mathematical algorithms for frame field processing. Our algorithm is tailored specifically to disambiguate junctions without sacrificing quality.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "We propose using the Dirichlet-to-Neumann operator as an extrinsic alternative to the Laplacian for spectral geometry processing and shape analysis. Intrinsic approaches, usually based on the Laplace-Beltrami operator, cannot capture the spatial embedding of a shape up to rigid motion, and many previous extrinsic methods lack theoretical justification. Instead, we consider the Steklov eigenvalue problem, computing the spectrum of the Dirichlet-to-Neumann operator of a surface bounding a volume. A remarkable property of this operator is that it completely encodes volumetric geometry. We use the boundary element method (BEM) to discretize the operator, accelerated by hierarchical numerical schemes and preconditioning; this pipeline allows us to solve eigenvalue and linear problems on large-scale meshes despite the density of the Dirichlet-to-Neumann discretization. We further demonstrate that our operators naturally fit into existing frameworks for geometry processing, making a shift from intrinsic to extrinsic geometry as simple as substituting the Laplace-Beltrami operator with the Dirichlet-to-Neumann operator.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Optimal transportation provides a means of lifting distances between points on a geometric domain to distances between signals over the domain, expressed as probability distributions. On a graph, transportation problems can be used to express challenging tasks involving matching supply to demand with minimal shipment expense; in discrete language, these become minimum-cost network flow problems. Regularization typically is needed to ensure uniqueness for the linear ground distance case and to improve optimization convergence; state-of-the-art techniques employ entropic regularization on the transportation matrix. In this paper, we explore a quadratic alternative to entropic regularization for transport over a graph. We theoretically analyze the behavior of quadratically-regularized graph transport, characterizing how regularization affects the structure of flows in the regime of small but nonzero regularization. We further exploit elegant second-order structure in the dual of this problem to derive an easily-implemented Newton-type optimization algorithm.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "This article introduces a new notion of optimal transport (OT) between tensor fields, which are measures whose values are positive semidefinite (PSD) matrices. This \"quantum\" formulation of OT (Q-OT) corresponds to a relaxed version of the classical Kantorovich transport problem, where the fidelity between the input PSD-valued measures is captured using the geometry of the Von-Neumann quantum entropy. We propose a quantum-entropic regularization of the resulting convex optimization problem, which can be solved efficiently using an iterative scaling algorithm. This method is a generalization of the celebrated Sinkhorn algorithm to the quantum setting of PSD matrices. We extend this formulation and the quantum Sinkhorn algorithm to compute barycenters within a collection of input tensor fields. We illustrate the usefulness of the proposed approach on applications to procedural noise generation, anisotropic meshing, diffusion tensor imaging and spectral texture synthesis.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "Optimal transportation distances are valuable for comparing and analyzing probability distributions, but larger-scale computational techniques for the theoretically favorable quadratic case are limited to smooth domains or regularized approximations. Motivated by fluid flow-based transportation on $\\mathbb{R}^n$, however, this paper introduces an alternative definition of optimal transportation between distributions over graph vertices. This new distance still satisfies the triangle inequality but has better scaling and a connection to continuous theories of transportation. It is constructed by adapting a Riemannian structure over probability distributions to the graph case, providing transportation distances as shortest-paths in probability space. After defining and analyzing theoretical properties of our new distance, we provide a time discretization as well as experiments verifying its effectiveness.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "This paper surveys and discusses recent work adapting partial differential equation (PDE) models to discrete structures.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "We present a generalization of the bilateral filter that can be applied to feature-preserving smoothing of signals on images, meshes, and other domains within a single unified framework. Our discretization is competitive with state-of-the-art smoothing techniques in terms of both accuracy and speed, is easy to implement, and has parameters that are straightforward to understand. Unlike previous bilateral filters developed for meshes and other irregular domains, our construction reduces exactly to the image bilateral on rectangular domains and comes with a rigorous foundation in both the smooth and discrete settings. These guarantees allow us to construct unconditionally convergent mean-shift schemes that handle a variety of extremely noisy signals. We also apply our framework to geometric edge-preserving effects like feature enhancement and show how it is related to local histogram techniques.\n        \u25b3 Less", "author": "Justin Solomon"}, {"abstract": "We consider the problem of image classification for the purpose of aiding doctors in dermatological diagnosis. Dermatological diagnosis poses two major challenges for standard off-the-shelf techniques: First, the data distribution is typically extremely long tailed. Second, intra-class variability is often large. To address the first issue, we formulate the problem as low-shot learning, where once deployed, a base classifier must rapidly generalize to diagnose novel conditions given very few labeled examples. To model diverse classes effectively, we propose Prototypical Clustering Networks (PCN), an extension to Prototypical Networks that learns a mixture of prototypes for each class. Prototypes are initialized for each class via clustering and refined via an online update scheme. Classification is performed by measuring similarity to a weighted combination of prototypes within a class, where the weights are the inferred cluster responsibilities. We demonstrate the strengths of our approach in effective diagnosis on a realistic dataset of dermatological conditions.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "To understand the empirical success of approximate MAP inference, recent work (Lang et al., 2018) has shown that some popular approximation algorithms perform very well when the input instance is stable. The simplest stability condition assumes that the MAP solution does not change at all when some of the pairwise potentials are (adversarially) perturbed. Unfortunately, this strong condition does not seem to be satisfied in practice. In this paper, we introduce a significantly more relaxed condition that only requires blocks (portions) of an input instance to be stable. Under this block stability condition, we prove that the pairwise LP relaxation is persistent on the stable blocks. We complement our theoretical results with an empirical evaluation of real-world MAP inference instances from computer vision. We design an algorithm to find stable blocks, and find that these real instances have large stable regions. Our work gives a theoretical explanation for the widespread empirical phenomenon of persistency for this LP relaxation.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Much attention has been devoted recently to the development of machine learning algorithms with the goal of improving treatment policies in healthcare. Reinforcement learning (RL) is a sub-field within machine learning that is concerned with learning how to make sequences of decisions so as to optimize long-term effects. Already, RL algorithms have been proposed to identify decision-making strategies for mechanical ventilation, sepsis management and treatment of schizophrenia. However, before implementing treatment policies learned by black-box algorithms in high-stakes clinical decision problems, special care must be taken in the evaluation of these policies.\n  In this document, our goal is to expose some of the subtleties associated with evaluating RL algorithms in healthcare. We aim to provide a conceptual starting point for clinical and computational researchers to ask the right questions when designing and evaluating algorithms for new ways of treating patients. In the following, we describe how choices about how to summarize a history, variance of statistical estimators, and confounders in more ad-hoc measures can result in unreliable, even misleading estimates of the quality of a treatment policy. We also provide suggestions for mitigating these effects---for while there is much promise for mining observational health data to uncover better treatment policies, evaluation must be performed thoughtfully.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Predictive models that generalize well under distributional shift are often desirable and sometimes crucial to building robust and reliable machine learning applications. We focus on distributional shift that arises in causal inference from observational data and in unsupervised domain adaptation. We pose both of these problems as prediction under a shift in design. Popular methods for overcoming distributional shift make unrealistic assumptions such as having a well-specified model or knowing the policy that gave rise to the observed data. Other methods are hindered by their need for a pre-specified metric for comparing observations, or by poor asymptotic properties. We devise a bound on the generalization error under design shift, incorporating both representation learning and sample re-weighting. Based on the bound, we propose an algorithmic framework that does not require any of the above assumptions and which is asymptotically consistent. We empirically study the new framework using two synthetic datasets, and demonstrate its effectiveness compared to previous methods.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Approximate algorithms for structured prediction problems---such as LP relaxations and the popular alpha-expansion algorithm (Boykov et al. 2001)---typically far exceed their theoretical performance guarantees on real-world instances. These algorithms often find solutions that are very close to optimal. The goal of this paper is to partially explain the performance of alpha-expansion and an LP relaxation algorithm on MAP inference in Ferromagnetic Potts models (FPMs). Our main results give stability conditions under which these two algorithms provably recover the optimal MAP solution. These theoretical results complement numerous empirical observations of good performance.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process \"grounding\"). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchi- cal predictor. Our approach optimizes an objec- tive function which favors balanced and easily- separable multi-way node partitions. We theoret- ically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the al- gorithm on text classification and language mod- eling, respectively, and show that they compare favorably to common baselines in terms of accu- racy and running time.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "This work proposes a new algorithm for automated and simultaneous phenotyping of multiple co-occurring medical conditions, also referred as comorbidities, using clinical notes from the electronic health records (EHRs). A basic latent factor estimation technique of non-negative matrix factorization (NMF) is augmented with domain specific constraints to obtain sparse latent factors that are anchored to a fixed set of chronic conditions. The proposed anchoring mechanism ensures a one-to-one identifiable and interpretable mapping between the latent factors and the target comorbidities. Qualitative assessment of the empirical results by clinical experts suggests that the proposed model learns clinically interpretable phenotypes while being predictive of 30 day mortality. The proposed method can be readily adapted to any non-negative EHR data across various healthcare institutions.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We describe a method for parameter estimation in bipartite probabilistic graphical models for joint prediction of clinical conditions from the electronic medical record. The method does not rely on the availability of gold-standard labels, but rather uses noisy labels, called anchors, for learning. We provide a likelihood-based objective and a moments-based initialization that are effective at learning the model parameters. The learned model is evaluated in a task of assigning a heldout clinical condition to patients based on retrospective analysis of the records, and outperforms baselines which do not account for the noisiness in the labels or do not model the conditions jointly.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Disparate areas of machine learning have benefited from models that can take raw data with little preprocessing as input and learn rich representations of that raw data in order to perform well on a given prediction task. We evaluate this approach in healthcare by using longitudinal measurements of lab tests, one of the more raw signals of a patient's health state widely available in clinical data, to predict disease onsets. In particular, we train a Long Short-Term Memory (LSTM) recurrent neural network and two novel convolutional neural networks for multi-task prediction of disease onset for 133 conditions based on 18 common lab tests measured over time in a cohort of 298K patients derived from 8 years of administrative claims data. We compare the neural networks to a logistic regression with several hand-engineered, clinically relevant features. We find that the representation-based learning approaches significantly outperform this baseline. We believe that our work suggests a new avenue for patient risk stratification based solely on lab results.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a \"balanced\" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \"Would this patient have lower blood sugar had she received a different medication?\". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Early diagnosis of treatable diseases is essential for improving healthcare, and many diseases' onsets are predictable from annual lab tests and their temporal trends. We introduce a multi-resolution convolutional neural network for early detection of multiple diseases from irregularly measured sparse lab values. Our novel architecture takes as input both an imputed version of the data and a binary observation matrix. For imputing the temporal sparse observations, we develop a flexible, fast to train method for differentiable multivariate kernel regression. Our experiments on data from 298K individuals over 8 years, 18 common lab measurements, and 171 diseases show that the temporal signatures learned via convolution are significantly more predictive than baselines commonly used for early disease diagnosis.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the \"Healing MNIST\" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We present a semi-supervised learning algorithm for learning discrete factor analysis models with arbitrary structure on the latent variables. Our algorithm assumes that every latent variable has an \"anchor\", an observed variable with only that latent variable as its parent. Given such anchors, we show that it is possible to consistently recover moments of the latent variables and use these moments to learn complete models. We also introduce a new technique for improving the robustness of method-of-moment algorithms by optimizing over the marginal polytope or its relaxations. We evaluate our algorithm using two real-world tasks, tag prediction on questions from the Stack Overflow website and medical diagnosis in an emergency department.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We introduce a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational objective over the marginal polytope. The algorithm is based on the conditional gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (MAP) calls. This modular structure enables us to leverage black-box MAP solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation. Theoretically, we bound the sub-optimality for the proposed algorithm despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. Empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to score-based structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, in our related UAI 2013 paper [BS13], we have given empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning. The present paper contains all details of the proofs of the finite-sample complexity results in [BS13] as well as detailed explanation of the computation of the certain error probabilities called beta-values, whose precomputation and tabulation is necessary for the implementation of the algorithm in [BS13].\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Structured prediction tasks in machine learning involve the simultaneous prediction of multiple labels. This is typically done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise elements, each depending on two specific labels. Intuitively, the more pairwise terms are used, the better the expected accuracy. However, there is currently no theoretical account of this intuition. This paper takes a significant step in this direction.\n  We formulate the problem as classifying the vertices of a known graph $G=(V,E)$, where the vertices and edges of the graph are labelled and correlate semi-randomly with the ground truth. We show that the prospects for achieving low expected Hamming error depend on the structure of the graph $G$ in interesting ways. For example, if $G$ is a very poor expander, like a path, then large expected Hamming error is inevitable. Our main positive result shows that, for a wide class of graphs including 2D grid graphs common in machine vision applications, there is a polynomial-time algorithm with small and information-theoretically near-optimal expected error. Our results provide a first step toward a theoretical justification for the empirical success of the efficient approximate inference algorithms that are used for structured prediction in models where exact inference is intractable.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We analyze variational inference for highly symmetric graphical models such as those arising from first-order probabilistic models. We first show that for these graphical models, the tree-reweighted variational objective lends itself to a compact lifted formulation which can be solved much more efficiently than the standard TRW formulation for the ground graphical model. Compared to earlier work on lifted belief propagation, our formulation leads to a convex optimization problem for lifted marginal inference and provides an upper bound on the partition function. We provide two approaches for improving the lifted TRW upper bound. The first is a method for efficiently computing maximum spanning trees in highly symmetric graphs, which can be used to optimize the TRW edge appearance probabilities. The second is a method for tightening the relaxation of the marginal polytope using lifted cycle inequalities and novel exchangeable cluster consistency constraints.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "This paper considers the problem of learning the parameters in Bayesian networks of discrete variables with known structure and hidden variables. Previous approaches in these settings typically use expectation maximization; when the network has high treewidth, the required expectations might be approximated using Monte Carlo or variational methods. We show how to avoid inference altogether during learning by giving a polynomial-time algorithm based on the method-of-moments, building upon recent work on learning discrete-valued mixture models. In particular, we show how to learn the parameters for a family of bipartite noisy-or Bayesian networks. In our experimental results, we demonstrate an application of our algorithm to learning QMR-DT, a large Bayesian network used for medical diagnosis. We show that it is possible to fully learn the parameters of QMR-DT even when only the findings are observed in the training data (ground truth diseases unknown).\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to scorebased structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, we give empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model inference have been based on a maximum likelihood objective. Efficient algorithms exist that approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for topic model inference that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Dual decomposition provides a tractable framework for designing algorithms for finding the most probable (MAP) configuration in graphical models. However, for many real-world inference problems, the typical decomposition has a large integrality gap, due to frustrated cycles. One way to tighten the relaxation is to introduce additional constraints that explicitly enforce cycle consistency. Earlier work showed that cluster-pursuit algorithms, which iteratively introduce cycle and other higherorder consistency constraints, allows one to exactly solve many hard inference problems. However, these algorithms explicitly enumerate a candidate set of clusters, limiting them to triplets or other short cycles. We solve the search problem for cycle constraints, giving a nearly linear time algorithm for finding the most frustrated cycle of arbitrary length. We show how to use this search algorithm together with the dual decomposition framework and clusterpursuit. The new algorithm exactly solves MAP inference problems arising from relational classification and stereo vision.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Linear Programming (LP) relaxations have become powerful tools for finding the most probable (MAP) configuration in graphical models. These relaxations can be solved efficiently using message-passing algorithms such as belief propagation and, when the relaxation is tight, provably find the MAP configuration. The standard LP relaxation is not tight enough in many real-world problems, however, and this has lead to the use of higher order cluster-based LP relaxations. The computational cost increases exponentially with the size of the clusters and limits the number and type of clusters we can use. We propose to solve the cluster selection problem monotonically in the dual LP, iteratively selecting clusters with guaranteed improvement, and quickly re-solving with the added clusters by reusing the existing solution. Our dual message-passing algorithm finds the MAP configuration in protein sidechain placement, protein design, and stereo problems, in cases where the standard LP relaxation fails.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "New checkable criteria for persistence of chemical reaction networks are proposed, which extend and complement those obtained by the authors in previous work. The new results allow the consideration of reaction rates which are time-varying, thus incorporating the effects of external signals, and also relax the assumption of existence of global conservation laws, thus allowing for inflows (production) and outflows (degradation). For time-invariant networks parameter-dependent conditions for persistence of certain classes of networks are provided. As an illustration, two networks arising in the systems biology literature are analyzed, namely a hypoxia and an apoptosis network.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Attractors of cooperative dynamical systems are particularly simple; for example, a nontrivial periodic orbit cannot be an attractor. This paper provides characterizations of attractors for the wider class of coherent systems, defined by the property that no directed feedback loops are negative. Several new results for cooperative systems are obtained in the process.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Oscillatory behavior is a key property of many biological systems. The Small-Gain Theorem (SGT) for input/output monotone systems provides a sufficient condition for global asymptotic stability of an equilibrium and hence its violation is a necessary condition for the existence of periodic solutions. One advantage of the use of the monotone SGT technique is its robustness with respect to all perturbations that preserve monotonicity and stability properties of a very low-dimensional (in many interesting examples, just one-dimensional) model reduction. This robustness makes the technique useful in the analysis of molecular biological models in which there is large uncertainty regarding the values of kinetic and other parameters. However, verifying the conditions needed in order to apply the SGT is not always easy. This paper provides an approach to the verification of the needed properties, and illustrates the approach through an application to a classical model of circadian oscillations, as a nontrivial ``case study,'' and also provides a theorem in the converse direction of predicting oscillations when the SGT conditions fail.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Persistency is the property, for differential equations in $\\R^n$, that solutions starting in the positive orthant do not approach the boundary. For chemical reactions and population models, this translates into the non-extinction property: provided that every species is present at the start of the reaction, no species will tend to be eliminated in the course of the reaction. This paper provides checkable conditions for persistence of chemical species in reaction networks, using concepts and tools from Petri net theory. Nontrivial examples are provided to illustrate the theory.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We show that strongly monotone systems of ordinary differential equations which have a certain translation-invariance property are so that all solutions converge to a unique equilibrium. The result may be seen as a dual of a well-known theorem of Mierczynski for systems that satisfy a conservation law. An application to a reaction of interest in biochemistry is provided as an illustration.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "In this note, we show how certain properties of Goldbeter's 1995 model for circadian oscillations can be proved mathematically, using techniques from the recently developed theory of monotone systems with inputs and outputs. The theory establishes global asymptotic stability, and in particular no oscillations, if the rate of transcription is somewhat smaller than that assumed by Goldbeter. This stability persists even under arbitrary delays in the feedback loop.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "This paper studies the emergence of multi-stability and hysteresis in those systems that arise, under positive feedback, starting from monotone systems with well-defined steady-state responses. Such feedback configurations appear routinely in several fields of application, and especially in biology. Characterizations of global stability behavior are stated in terms of easily checkable graphical conditions. An example of a signaling cascade under positive feedback is presented.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Monotone systems constitute one of the most important classes of dynamical systems used in mathematical biology modeling. The objective of this paper is to extend the notion of monotonicity to systems with inputs and outputs, a necessary first step in trying to understand interconnections, especially including feedback loops, built up out of monotone components. Basic definitions and theorems are provided, as well as an application of a theorem regarding negative feedback loops to the study of a model of one of the cell's most important subsystems (MAPK cascades) .\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We extend the setup in our previous paper to deal with the case in which more than one steady state may exist in feedback configurations. This provides a foundation for the analysis of multi-stability and hysteresis behaviour in high dimensional feedback systems.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "Monotone systems constitute one of the most important classes of dynamical systems used in mathematical biology modeling.\n  The objective of this paper is to extend the notion of monotonicity to systems with inputs and outputs, a necessary first step in trying to understand interconnections, especially including feedback loops, built up out of monotone components.\n  Basic definitions and theorems are provided, as well as an application to the study of a model of one of the cell's most important subsystems.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "This paper continues the study of the integral input-to-state stability (IISS) property. It is shown that the IISS property is equivalent to one which arises from the consideration of mixed norms on states and inputs, as well as to the superposition of a ``bounded energy bounded state'' requirement and the global asymptotic stability of the unforced system. A semiglobal version of IISS is shown to imply the global version, though a counterexample shows that the analogous fact fails for input to state stability (ISS). The results in this note complete the basic theoretical picture regarding IISS and ISS.\n        \u25b3 Less", "author": "David Sontag"}, {"abstract": "We study smooth stochastic optimization problems on Riemannian manifolds. Via adapting the recently proposed SPIDER algorithm \\citep{fang2018spider} (a variance reduced stochastic method) to Riemannian manifold, we can achieve faster rate than known algorithms in both the finite sum and stochastic settings. Unlike previous works, by \\emph{not} resorting to bounding iterate distances, our analysis yields curvature independent convergence rates for both the nonconvex and strongly convex cases.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study universal finite sample expressivity of neural networks, defined as the capability to perfectly memorize arbitrary datasets. For scalar outputs, existing results require a hidden layer as wide as $N$ to memorize $N$ data points. In contrast, we prove that a 3-layer (2-hidden-layer) ReLU network with $4 \\sqrt {N}$ hidden nodes can perfectly fit any arbitrary dataset. For $K$-class classification, we prove that a 4-layer ReLU network with $4 \\sqrt{N} + 4K$ hidden neurons can memorize arbitrary datasets. For example, a 4-layer ReLU network with only 8,000 hidden nodes can memorize datasets with $N$ = 1M and $K$ = 1k (e.g., ImageNet). Our results show that even small networks already have tremendous overfitting capability, admitting zero empirical risk for any dataset. We also extend our results to deeper and narrower networks, and prove converse results showing necessity of $\u03a9(N)$ parameters for shallow networks.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks. Our algorithm receives any parameter value and returns: local minimum, second-order stationary point, or a strict descent direction. The presence of M data points on the nondifferentiability of the ReLU divides the parameter space into at most 2^M regions, which makes analysis difficult. By exploiting polyhedral geometry, we reduce the total computation down to one convex quadratic program (QP) for each hidden node, O(M) (in)equality tests, and one (or a few) nonconvex QP. For the last QP, we show that our specific problem can be solved efficiently, in spite of nonconvexity. In the benign case, we solve one equality constrained QP, and we prove that projected gradient descent solves it exponentially fast. In the bad case, we have to solve a few more inequality constrained QPs, but we prove that the time complexity is exponential only in the number of inequality constraints. Our experiments show that either benign case or bad case with very few inequality constraints occurs, implying that our algorithm is efficient in most cases.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "A long-standing problem in the theory of stochastic gradient descent (SGD) is to prove that its without-replacement version RandomShuffle converges faster than the usual with-replacement version. We present the first (to our knowledge) non-asymptotic solution to this problem, which shows that after a \"reasonable\" number of epochs RandomShuffle indeed converges faster than SGD. Specifically, we prove that under strong convexity and second-order smoothness, the sequence generated by RandomShuffle converges to the optimal solution at the rate O(1/T^2 + n^3/T^3), where n is the number of components in the objective, and T is the total number of iterations. This result shows that after a reasonable number of epochs RandomShuffle is strictly better than SGD (which converges as O(1/T)). The key step toward showing this better dependence on T is the introduction of n into the bound; and as our analysis will show, in general a dependence on n is unavoidable without further changes to the algorithm. We show that for sparse data RandomShuffle has the rate O(1/T^2), again strictly better than SGD. Furthermore, we discuss extensions to nonconvex gradient dominated functions, as well as non-strongly convex settings.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We propose a Riemannian version of Nesterov's Accelerated Gradient algorithm (RAGD), and show that for geodesically smooth and strongly convex problems, within a neighborhood of the minimizer whose radius depends on the condition number as well as the sectional curvature of the manifold, RAGD converges to the minimizer with acceleration. Unlike the algorithm in (Liu et al., 2017) that requires the exact solution to a nonlinear equation which in turn may be intractable, our algorithm is constructive and computationally tractable. Our proof exploits a new estimate sequence and a novel bound on the nonlinear metric distortion, both ideas may be of independent interest.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov's accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-$(s+2)$ differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE converges to the optimal solution at a rate of $\\mathcal{O}({N^{-2\\frac{s}{s+1}}})$, where $s$ is the order of the Runge-Kutta numerical integrator. Furthermore, we introduce a new local flatness condition on the objective, under which rates even faster than $\\mathcal{O}(N^{-2})$ can be achieved with low-order integrators and only gradient information. Notably, this flatness condition is satisfied by several standard loss functions used in machine learning. We provide numerical experiments that verify the theoretical rates predicted by our results.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Representations that can compactly and effectively capture the temporal evolution of semantic content are important to computer vision and machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert space, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective. We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We prove some \"power\" generalizations of Marcus-Lopes-style (including McLeod and Bullen) concavity inequalities for elementary symmetric polynomials, and convexity inequalities (of McLeod and Baston) for complete homogeneous symmetric polynomials. Finally, we present sundry concavity results for elementary symmetric polynomials, of which the main result is a concavity theorem that among other implies a well-known log-convexity result of Muir (1972/74) for positive definite matrices.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Determinantal Point Processes (DPPs) have attracted significant interest from the machine-learning community due to their ability to elegantly and tractably model the delicate balance between quality and diversity of sets. We consider learning DPPs from data, a key task for DPPs; for this task, we introduce a novel optimization problem, Contrastive Estimation (CE), which encodes information about \"negative\" samples into the basic learning model. CE is grounded in the successful use of negative information in machine-vision and language modeling. Depending on the chosen negative distribution (which may be static or evolve during optimization), CE assumes two different forms, which we analyze theoretically and experimentally. We evaluate our new model on real-world datasets; on a challenging dataset, CE learning delivers a considerable improvement in predictive performance over a DPP learned without using contrastive information.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with \"slightest\" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general \"no spurious local minima\" is a property limited to deep linear networks, and insights obtained from linear networks are not robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all (in contrast to previous results) practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on local optimality in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study projection free methods for constrained geodesically convex optimization. In particular, we propose a Riemannian version of the Frank-Wolfe (RFW) method. We analyze RFW's convergence and provide a global, non-asymptotic sublinear convergence rate. We also present a setting under which RFW can attain a linear rate. Later, we specialize RFW to the manifold of positive definite matrices, where we are motivated by the specific task of computing the geometric mean (also known as Karcher mean or Riemannian centroid). For this task, RFW requires access to a \"linear oracle\" that turns out to be a nonconvex semidefinite program. Remarkably, this nonconvex program is shown to admit a closed form solution, which may be of independent interest too. We complement this result by also studying a nonconvex Euclidean Frank-Wolfe approach, along with its global convergence analysis. Finally, we empirically compare Rfw against recently published methods for the Riemannian centroid and observe strong performance gains.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "A central challenge to using first-order methods for optimizing nonconvex problems is the presence of saddle points. First-order methods often get stuck at saddle points, greatly deteriorating their performance. Typically, to escape from saddles one has to use second-order methods. However, most works on second-order methods rely extensively on expensive Hessian-based computations, making them impractical in large-scale settings. To tackle this challenge, we introduce a generic framework that minimizes Hessian based computations while at the same time provably converging to second-order critical points. Our framework carefully alternates between a first-order and a second-order subroutine, using the latter only close to saddle points, and yields convergence results competitive to the state-of-the-art. Empirical results suggest that our strategy also enjoys a good practical performance.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study the error landscape of deep linear and nonlinear neural networks with the squared error loss. Minimizing the loss of a deep linear neural network is a nonconvex problem, and despite recent progress, our understanding of this loss surface is still incomplete. For deep linear networks, we present necessary and sufficient conditions for a critical point of the risk function to be a global minimum. Surprisingly, our conditions provide an efficiently checkable test for global optimality, while such tests are typically intractable in nonconvex optimization. We further extend these results to deep nonlinear neural networks and prove similar sufficient conditions for global optimality, albeit in a more limited function space setting.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We propose a framework for adversarial training that relies on a sample rather than a single sample point as the fundamental unit of discrimination. Inspired by discrepancy measures and two-sample tests between probability distributions, we propose two such distributional adversaries that operate and predict on samples, and show how they can be easily implemented on top of existing models. Various experimental results show that generators trained with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with pointwise prediction discriminators. The application of our framework to domain adaptation also results in considerable improvement over recent state-of-the-art.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We consider maximum likelihood estimation for Gaussian Mixture Models (Gmms). This task is almost invariably solved (in theory and practice) via the Expectation Maximization (EM) algorithm. EM owes its success to various factors, of which is its ability to fulfill positive definiteness constraints in closed form is of key importance. We propose an alternative to EM by appealing to the rich Riemannian geometry of positive definite matrices, using which we cast Gmm parameter estimation as a Riemannian optimization problem. Surprisingly, such an out-of-the-box Riemannian formulation completely fails and proves much inferior to EM. This motivates us to take a closer look at the problem geometry, and derive a better formulation that is much more amenable to Riemannian optimization. We then develop (Riemannian) batch and stochastic gradient algorithms that outperform EM, often substantially. We provide a non-asymptotic convergence analysis for our stochastic method, which is also the first (to our knowledge) such global analysis for Riemannian stochastic gradient. Numerous empirical results are included to demonstrate the effectiveness of our methods.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We revisit the classical problem of optimal experimental design (OED) under a new mathematical model grounded in a geometric motivation. Specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture \"partial volumes\" and offer a graded interpolation between the widely used A-optimal design and D-optimal design models, obtaining each of them as special cases. We analyze properties of our models, and derive both greedy and convex-relaxation algorithms for computing the associated designs. Our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate a curious phenomenon concerning our greedy method. Finally, as a byproduct, we obtain new results on the theory of elementary symmetric polynomials that may be of independent interest.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Representations that can compactly and effectively capture temporal evolution of semantic content are important to machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in an RKHS, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective; we then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study dual volume sampling, a method for selecting k columns from an n x m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume spanned by the rows of the induced submatrix. This method was proposed by Avron and Boutsidis (2013), who showed it to be a promising method for column subset selection and its multiple applications. However, its wider adoption has been hampered by the lack of polynomial time sampling algorithms. We remove this hindrance by developing an exact (randomized) polynomial time sampling algorithm as well as its derandomization. Thereafter, we study dual volume sampling via the theory of real stable polynomials and prove that its distribution satisfies the \"Strong Rayleigh\" property. This result has numerous consequences, including a provably fast-mixing Markov chain sampler that makes dual volume sampling much more attractive to practitioners. This sampler is closely related to classical algorithms for popular experimental design methods that are to date lacking theoretical analysis but are known to empirically work well.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints. We consider the task of rapidly sampling from such constrained measures, and develop fast Markov chain samplers for them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures, for which we present sharp polynomial bounds on the mixing time. As a corollary, this result yields a fast mixing sampler for Determinantal Point Processes (DPPs), yielding (to our knowledge) the first provably fast MCMC sampler for DPPs since their inception over four decades ago. Beyond SR measures, we develop MCMC samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study Frank-Wolfe methods for nonconvex stochastic and finite-sum optimization problems. Frank-Wolfe methods (in the convex case) have gained tremendous recent interest in machine learning and optimization communities due to their projection-free property and their ability to exploit structured constraints. However, our understanding of these algorithms in the nonconvex setting is fairly limited. In this paper, we propose nonconvex stochastic Frank-Wolfe methods and analyze their convergence properties. For objective functions that decompose into a finite-sum, we leverage ideas from variance reduction techniques for convex optimization to obtain new variance reduced nonconvex Frank-Wolfe methods that have provably faster convergence than the classical Frank-Wolfe method. Finally, we show that the faster convergence rates of our variance reduced methods also translate into improved convergence rates for the stochastic setting.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "In this note we consider sampling from (non-homogeneous) strongly Rayleigh probability measures. As an important corollary, we obtain a fast mixing Markov Chain sampler for Determinantal Point Processes.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of $N$ items. They have recently gained prominence in several applications that rely on \"diverse\" subsets. However, their applicability to large problems is still limited due to the $\\mathcal O(N^3)$ complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KronDPP.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study optimization of finite sums of geodesically smooth functions on Riemannian manifolds. Although variance reduction techniques for optimizing finite-sums have witnessed tremendous attention in the recent years, existing work is limited to vector space problems. We introduce Riemannian SVRG (RSVRG), a new variance reduced Riemannian optimization method. We analyze RSVRG for both geodesically convex and nonconvex (smooth) functions. Our analysis reveals that RSVRG inherits advantages of the usual SVRG method, but with factors depending on curvature of the manifold that influence its convergence. To our knowledge, RSVRG is the first provably fast stochastic Riemannian method. Moreover, our paper presents the first non-asymptotic complexity analysis (novel even for the batch setting) for nonconvex Riemannian optimization. Our results have several implications; for instance, they offer a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence analysis.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonconvex part is smooth and the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we show provably faster convergence than batch proximal gradient descent. Finally, we prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, that subsumes several recent works. This paper builds upon our recent series of papers on fast stochastic methods for smooth nonconvex optimization [22, 23], with a novel analysis for nonconvex and nonsmooth functions.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "The modern data analyst must cope with data encoded in various forms, vectors, matrices, strings, graphs, or more. Consequently, statistical and machine learning models tailored to different data encodings are important. We focus on data encoded as normalized vectors, so that their \"direction\" is more important than their magnitude. Specifically, we consider high-dimensional vectors that lie either on the surface of the unit hypersphere or on the real projective plane. For such data, we briefly review common mathematical models prevalent in machine learning, while also outlining some technical aspects, software, applications, and open mathematical challenges.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Topic models have emerged as fundamental tools in unsupervised machine learning. Most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from LDA by passing to the small-variance limit. We minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. In particular, we show that our results are competitive with popular LDA-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to mini-batching in parallel settings.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We analyze a fast incremental aggregated gradient method for optimizing nonconvex problems of the form $\\min_x \\sum_i f_i(x)$. Specifically, we analyze the SAGA algorithm within an Incremental First-order Oracle framework, and show that it converges to a stationary point provably faster than both gradient descent and stochastic gradient descent. We also discuss a Polyak's special class of nonconvex problems for which SAGA converges at a linear rate to the global optimum. Finally, we analyze the practically valuable regularized and minibatch variants of SAGA. To our knowledge, this paper presents the first analysis of fast convergence for an incremental aggregated gradient method for nonconvex problems.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "The Nystr\u00f6m method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystr\u00f6m using Determinantal Point Processes (DPPs), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via DPPs guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of DPPsampling, we show that (under certain conditions) Markov chain DPP sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of DPP-based landmark selection compared with existing approaches.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Geodesic convexity generalizes the notion of (vector space) convexity to nonlinear metric spaces. But unlike convex optimization, geodesically convex (g-convex) optimization is much less developed. In this paper we contribute to the understanding of g-convex optimization by developing iteration complexity analysis for several first-order algorithms on Hadamard manifolds. Specifically, we prove upper bounds for the global complexity of deterministic and stochastic (sub)gradient methods for optimizing smooth and nonsmooth g-convex functions, both with and without strong g-convexity. Our analysis also reveals how the manifold geometry, especially \\emph{sectional curvature}, impacts convergence rates. To the best of our knowledge, our work is the first to provide global complexity analysis for first-order algorithms for general g-convex optimization.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We present a framework for accelerating a spectrum of machine learning algorithms that require computation of bilinear inverse forms $u^\\top A^{-1}u$, where $A$ is a positive definite matrix and $u$ a given vector. Our framework is built on Gauss-type quadrature and easily scales to large, sparse matrices. Further, it allows retrospective computation of lower and upper bounds on $u^\\top A^{-1}u$, which in turn accelerates several algorithms. We prove that these bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge, ours is the first work to demonstrate these key properties of Gauss-type quadrature, which is a classical and deeply studied topic. We illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization, and observe tremendous speedups in several instances.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We consider a dominance order on positive vectors induced by the elementary symmetric polynomials. Under this dominance order we provide conditions that yield simple proofs of several monotonicity questions. Notably, our approach yields a quick (4 line) proof of the so-called \\emph{\"sum-of-squared-logarithms\"} inequality conjectured in (P.~Neff, B.~Eidel, F.~Osterbrink, and R.~Martin, \\emph{Applied Math. \\& Mechanics., 2013}; P.~Neff, Y.~Nakatsukasa, and A.~Fischle; \\emph{SIMAX, 35, 2014}). This inequality has been the subject of several recent articles, and only recently it received a full proof, albeit via a more elaborate complex-analytic approach. We provide an elementary proof, which moreover extends to yield simple proofs of both old and new inequalities for R\u00e9nyi entropy, subentropy, and quantum R\u00e9nyi entropy.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD, nor (b) resort to augmented Lagrangian techniques, nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Determinantal Point Processes (DPPs) are elegant probabilistic models of repulsion and diversity over discrete sets of items. But their applicability to large sets is hindered by expensive cubic-complexity matrix operations for basic tasks such as sampling. In light of this, we propose a new method for approximate sampling from discrete $k$-DPPs. Our method takes advantage of the diversity property of subsets sampled from a DPP, and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter, it efficiently samples subsets based on the constructed coresets. As opposed to previous approaches, our algorithm aims to minimize the total variation distance to the original distribution. Experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets, and yields more accurate samples than previous approaches.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study distributed stochastic convex optimization under the delayed gradient model where the server nodes perform parameter updates, while the worker nodes compute stochastic gradients. We discuss, analyze, and experiment with a setup motivated by the behavior of real-world distributed computation networks, where the machines are differently slow at different time. Therefore, we allow the parameter updates to be sensitive to the actual delays experienced, rather than to worst-case bounds on the maximum delay. This sensitivity leads to larger stepsizes, that can help gain rapid initial convergence without having to wait too long for slower machines, while maintaining the same asymptotic complexity. We obtain encouraging improvements to overall convergence for distributed experiments on real datasets with up to billions of examples and features.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We prove the \\emph{sum of squared logarithms inequality} (SSLI) which states that for nonnegative vectors $x, y \\in \\mathbb{R}^n$ whose elementary symmetric polynomials satisfy $e_k(x)\\le e_k(y)$ (for $1\\le k < n$) and $e_n(x)=e_n(y)$, the inequality $\\sum_i (\\log x_i)^2 \\le \\sum_i (\\log y_i)^2$ holds. Our proof of this inequality follows by a suitable extension to the complex plane. In particular, we show that the function $f\\colon M\\subseteq \\mathbb{C}^n\\to \\mathbb{R}$ with $f(z)=\\sum_i(\\log z_i)^2$ has nonnegative partial derivatives with respect to the elementary symmetric polynomials of $z$. This property leads to our proof. We conclude by providing applications and wider connections of the SSLI.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "This paper is triggered by the preprint \"\\emph{Computing Matrix Squareroot via Non Convex Local Search}\" by Jain et al. (\\textit{\\textcolor{blue}{arXiv:1507.05854}}), which analyzes gradient-descent for computing the square root of a positive definite matrix. Contrary to claims of~\\citet{jain2015}, our experiments reveal that Newton-like methods compute matrix square roots rapidly and reliably, even for highly ill-conditioned matrices and without requiring commutativity. We observe that gradient-descent converges very slowly primarily due to tiny step-sizes and ill-conditioning. We derive an alternative first-order method based on geodesic convexity: our method admits a transparent convergence analysis ($< 1$ page), attains linear rate, and displays reliable convergence even for rank deficient problems. Though superior to gradient-descent, ultimately our method is also outperformed by a well-known scaled Newton method. Nevertheless, the primary value of our work is its conceptual value: it shows that for deriving gradient based methods for the matrix square root, \\emph{the manifold geometric view of positive definite matrices can be much more advantageous than the Euclidean view}.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Data encoded as symmetric positive definite (SPD) matrices frequently arise in many areas of computer vision and machine learning. While these matrices form an open subset of the Euclidean space of symmetric matrices, viewing them through the lens of non-Euclidean Riemannian geometry often turns out to be better suited in capturing several desirable data properties. However, formulating classical machine learning algorithms within such a geometry is often non-trivial and computationally expensive. Inspired by the great success of dictionary learning and sparse coding for vector-valued data, our goal in this paper is to represent data in the form of SPD matrices as sparse conic combinations of SPD atoms from a learned dictionary via a Riemannian geometric approach. To that end, we formulate a novel Riemannian optimization objective for dictionary learning and sparse coding in which the representation loss is characterized via the affine invariant Riemannian metric. We also present a computationally simple algorithm for optimizing our model. Experiments on several computer vision datasets demonstrate superior classification and retrieval performance using our approach when compared to sparse coding via alternative non-Riemannian formulations.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We take a new look at parameter estimation for Gaussian Mixture Models (GMMs). In particular, we propose using \\emph{Riemannian manifold optimization} as a powerful counterpart to Expectation Maximization (EM). An out-of-the-box invocation of manifold optimization, however, fails spectacularly: it converges to the same solution but vastly slower. Driven by intuition from manifold convexity, we then propose a reparamerization that has remarkable empirical consequences. It makes manifold optimization not only match EM---a highly encouraging result in itself given the poor record nonlinear programming methods have had against EM so far---but also outperform EM in many practical settings, while displaying much less variability in running times. We further highlight the strengths of manifold optimization by developing a somewhat tuned manifold LBFGS method that proves even more competitive and reliable than existing manifold optimization tools. We hope that our results encourage a wider consideration of manifold optimization for parameter estimation problems.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms---a crucial requirement for modern large-scale applications---have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "Energy minimization has been an intensely studied core problem in computer vision. With growing image sizes (2D and 3D), it is now highly desirable to run energy minimization algorithms in parallel. But many existing algorithms, in particular, some efficient combinatorial algorithms, are difficult to par-allelize. By exploiting results from convex and submodular theory, we reformulate the quadratic energy minimization problem as a total variation denoising problem, which, when viewed geometrically, enables the use of projection and reflection based convex methods. The resulting min-cut algorithm (and code) is conceptually very simple, and solves a sequence of TV denoising problems. We perform an extensive empirical evaluation comparing state-of-the-art combinatorial algorithms and convex optimization techniques. On small problems the iterative convex methods match the combinatorial max-flow algorithms, while on larger problems they offer other flexibility and important gains: (a) their memory footprint is small; (b) their straightforward parallelizability fits multi-core platforms; (c) they can easily be warm-started; and (d) they quickly reach approximately good solutions, thereby enabling faster \"inexact\" solutions. A key consequence of our approach based on submodularity and convexity is that it is allows to combine any arbitrary combinatorial or convex methods as subroutines, which allows one to obtain hybrid combinatorial and convex optimization algorithms that benefit from the strengths of both.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We prove a conjecture of Cuttler et al.~[2011] [A. Cuttler, C. Greene, and M. Skandera; \\emph{Inequalities for symmetric means}. European J. Combinatorics, 32(2011), 745--761] on the monotonicity of \\emph{normalized Schur functions} under the usual (dominance) partial-order on partitions. We believe that our proof technique may be helpful in obtaining similar inequalities for other symmetric functions.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study a specific \"anti-triangular\" Cesar\u00f3 matrix corresponding to a Markov chain. We derive closed forms for all the eigenvalues and eigenvectors of this matrix.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study \\emph{TV regularization}, a widely used technique for eliciting structured sparsity. In particular, we propose efficient algorithms for computing prox-operators for $\\ell_p$-norm TV. The most important among these is $\\ell_1$-norm TV, for whose prox-operator we present a new geometric analysis which unveils a hitherto unknown connection to taut-string methods. This connection turns out to be remarkably useful as it shows how our geometry guided implementation results in efficient weighted and unweighted 1D-TV solvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the backbone for building more complex (two or higher-dimensional) TV solvers within a modular proximal optimization approach. We review the literature for an array of methods exploiting this strategy, and illustrate the benefits of our modular design through extensive suite of experiments on (i) image denoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and (iv) video denoising. To underscore our claims and permit easy reproducibility, we provide all the reviewed and our new TV solvers in an easy to use multi-threaded C++, Matlab and Python library.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We prove inequalities on symmetric tensor sums of positive definite operators. In particular, we prove multivariable operator inequalities inspired by generalizations to the well-known Hlawka and Popoviciu inequalities. As corollaries, we obtain generalized Hlawka and Popoviciu inequalities for determinants, permanents, and generalized matrix functions. The new operator inequalities and their corollaries contain a few recently published inequalities on positive definite matrices as special cases.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We study modeling and inference with the Elliptical Gamma Distribution (EGD). We consider maximum likelihood (ML) estimation for EGD scatter matrices, a task for which we develop new fixed-point algorithms. Our algorithms are efficient and converge to global optima despite nonconvexity. Moreover, they turn out to be much faster than both a well-known iterative algorithm of Kent & Tyler (1991) and sophisticated manifold optimization algorithms. Subsequently, we invoke our ML algorithms as subroutines for estimating parameters of a mixture of EGDs. We illustrate our methods by applying them to model natural image statistics---the proposed EGD mixture model yields the most parsimonious model among several competing approaches.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "We prove that generalized matrix functions satisfy a block-matrix strong superadditivity inequality over the cone of positive semidefinite matrices. Our result extends a recent result of Paksoy-Turkmen-Zhang (V. Paksoy, R. Turkmen, F. Zhang, Inequalities of generalized matrix functions via tensor products, Electron. J. Linear Algebra 27 (2014) 332-341.). As an application, we obtain a short proof of a classical inequality of Thompson (1961) on block matrix determinants.\n        \u25b3 Less", "author": "Suvrit Sra"}, {"abstract": "The rise in computing hardware choices is driving a reevaluation of operating systems. The traditional role of an operating system controlling the execution of its own hardware is evolving toward a model whereby the controlling processor is distinct from the compute engines that are performing most of the computations. In this context, an operating system can be viewed as software that brokers and tracks the resources of the compute engines and is akin to a database management system. To explore the idea of using a database in an operating system role, this work defines key operating system functions in terms of rigorous mathematical semantics (associative array algebra) that are directly translatable into database operations. These operations possess a number of mathematical properties that are ideal for parallel operating systems by guaranteeing correctness over a wide range of parallel operations. The resulting operating system equations provide a mathematical specification for a Tabular Operating System Architecture (TabulaROSA) that can be implemented on any platform. Simulations of forking in TabularROSA are performed using an associative array implementation and compared to Linux on a 32,000+ core supercomputer. Using over 262,000 forkers managing over 68,000,000,000 processes, the simulations show that TabulaROSA has the potential to perform operating system functions on a massively parallel scale. The TabulaROSA simulations show 20x higher performance as compared to Linux while managing 2000x more processes in fully searchable tables.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "\"How common is interactive visualization on the web?\" \"What is the most popular visualization design?\" \"How prevalent are pie charts really?\" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 86% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare in practice. Our findings also indicate that users may prefer tools that emphasize a succinct set of visualization types, and provide diverse expert visualization examples.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "Data integration has been a long-standing challenge in data management with many applications. A key step in data integration is entity consolidation. It takes a collection of clusters of duplicate records as input and produces a single \"golden record\" for each cluster, which contains the canonical value for each attribute. Truth discovery and data fusion methods, as well as Master Data Management (MDM) systems, can be used for entity consolidation. However, to achieve better results, the variant values (i.e., values that are logically the same with different formats) in the clusters need to be consolidated before applying these methods.\n  For this purpose, we propose a data-driven method to standardize the variant values based on two observations: (1) the variant values usually can be transformed to the same representation (e.g., \"Mary Lee\" and \"Lee, Mary\") and (2) the same transformation often appears repeatedly across different clusters (e.g., transpose the first and last name). Our approach first uses an unsupervised method to generate groups of value pairs that can be transformed in the same way (i.e., they share a transformation). Then the groups are presented to a human for verification and the approved ones are used to standardize the data. In a real-world dataset with 17,497 records, our method achieved 75% recall and 99.5% precision in standardizing variant values by asking a human 100 yes/no questions, which completely outperformed a state of the art data wrangling tool.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "A polystore system is a database management system (DBMS) composed of integrated heterogeneous database engines and multiple programming languages. By matching data to the storage engine best suited to its needs, complex analytics run faster and flexible storage choices helps improve data organization. BigDAWG (Big Data Working Group) is our reference implementation of a polystore system. In this paper, we describe the current BigDAWG software release which supports PostgreSQL, Accumulo and SciDB. We describe the overall architecture, API and initial results of applying BigDAWG to the MIMIC II medical dataset.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "Determining if two sets are related - that is, if they have similar values or if one set contains the other - is an important problem with many applications in data cleaning, data integration, and information retrieval. A particularly popular metric that has been proposed is to measure the relatedness of two sets by treating the elements as vertices of a bipartite graph and calculating the score of the maximum matching pairing between elements. Compared to other metrics which require exact matchings between elements, this metric uses a similarity function to compare elements between the two sets, making it robust to small dissimilarities in elements and more useful for real-world, dirty data. Unfortunately, the metric suffers from expensive computational cost, taking O(n^3) time, where n is the number of elements in sets, for each set-to-set comparison. Thus for applications which try to search for all pairings of related sets in a brute-force manner, the runtime becomes unacceptably large.\n  To address this challenge, we developed SilkMoth, a system capable of rapidly discovering related set pairs in collections of sets. Internally, SilkMoth creates a signature for each set, with the property that any other set which is related must match the signature. SilkMoth then uses these signatures to prune the search space, so only sets which match the signatures are left as candidates. Finally, SilkMoth applies the maximum matching metric on remaining candidates to verify which of these candidates are truly related sets. Thus, a contribution of this paper is the characterization of the space of signatures which enable this property. We show that selecting the optimal signature in this space is NP-complete, and based on insights from the characterization of the space, we propose two novel filters which help to prune the candidates further before verification.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "The Intel Science and Technology Center for Big Data is developing a reference implementation of a Polystore database. The BigDAWG (Big Data Working Group) system supports \"many sizes\" of database engines, multiple programming languages and complex analytics for a variety of workloads. Our recent efforts include application of BigDAWG to an ocean metagenomics problem and containerization of BigDAWG. We intend to release an open source BigDAWG v1.0 in the Spring of 2017. In this article, we will demonstrate a number of polystore applications developed with oceanographic researchers at MIT and describe our forthcoming open source release of the BigDAWG system.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "Organizations are often faced with the challenge of providing data management solutions for large, heterogenous datasets that may have different underlying data and programming models. For example, a medical dataset may have unstructured text, relational data, time series waveforms and imagery. Trying to fit such datasets in a single data management system can have adverse performance and efficiency effects. As a part of the Intel Science and Technology Center on Big Data, we are developing a polystore system designed for such problems. BigDAWG (short for the Big Data Analytics Working Group) is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands and a middleware that provides a uniform multi--island interface. Initial results from a prototype of the BigDAWG system applied to a medical dataset validate polystore concepts. In this article, we will describe polystore databases, the current BigDAWG architecture and its application on the MIMIC II medical dataset, initial performance results and our future development plans.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "BigDAWG is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands of information and a middleware that provides a uniform multi-island interface. In this article, we describe the current architecture of BigDAWG, its application on the MIMIC II medical dataset, and our plans for the mechanics of cross-system queries. During the presentation, we will also deliver a brief demonstration of the current version of BigDAWG.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "Stream processing addresses the needs of real-time applications. Transaction processing addresses the coordination and safety of short atomic computations. Heretofore, these two modes of operation existed in separate, stove-piped systems. In this work, we attempt to fuse the two computational paradigms in a single system called S-Store. In this way, S-Store can simultaneously accommodate OLTP and streaming applications. We present a simple transaction model for streams that integrates seamlessly with a traditional OLTP system. We chose to build S-Store as an extension of H-Store, an open-source, in-memory, distributed OLTP database system. By implementing S-Store in this way, we can make use of the transaction processing facilities that H-Store already supports, and we can concentrate on the additional implementation features that are needed to support streaming. Similar implementations could be done using other main-memory OLTP platforms. We show that we can actually achieve higher throughput for streaming workloads in S-Store than an equivalent deployment in H-Store alone. We also show how this can be achieved within H-Store with the addition of a modest amount of new functionality. Furthermore, we compare S-Store to two state-of-the-art streaming systems, Spark Streaming and Storm, and show how S-Store matches and sometimes exceeds their performance while providing stronger transactional guarantees.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "A group of senior database researchers gathers every few years to assess the state of database research and to point out problem areas that deserve additional focus. This report summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6, 2003 in Lowell, Mass. It observes that information management continues to be a critical component of most complex software systems. It recommends that database researchers increase focus on: integration of text, data, code, and streams; fusion of information from heterogeneous data sources; reasoning about uncertain data; unsupervised data mining for interesting correlations; information privacy; and self-adaptation and repair.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda -- broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.\n        \u25b3 Less", "author": "Michael Stonebraker"}, {"abstract": "In many domains such as medicine, training data is in short supply. In such cases, external knowledge is often helpful in building predictive models. We propose a novel method to incorporate publicly available domain expertise to build accurate models. Specifically, we use word2vec models trained on a domain-specific corpus to estimate the relevance of each feature's text description to the prediction problem. We use these relevance estimates to rescale the features, causing more important features to experience weaker regularization.\n  We apply our method to predict the onset of five chronic diseases in the next five years in two genders and two age groups. Our rescaling approach improves the accuracy of the model, particularly when there are few positive examples. Furthermore, our method selects 60% fewer features, easing interpretation by physicians. Our method is applicable to other domains where feature and outcome descriptions are available.\n        \u25b3 Less", "author": "Collin Stultz"}, {"abstract": "Gravitational-wave interferometers are expected to monitor the last three minutes of inspiral and final coalescence of neutron star and black hole binaries at distances approaching cosmological, where the event rate may be many per year. Because the binary's accumulated orbital phase can be measured to a fractional accuracy $\\ll 10^{-3}$ and relativistic effects are large, the waveforms will be far more complex, carry more information, and be far harder to model theoretically than has been expected. Theorists must begin now to lay a foundation for extracting the waves' information.\n        \u25b3 Less", "author": "Gerald Sussman"}, {"abstract": "Depth sensing is useful in a variety of applications that range from augmented reality to robotics. Time-of-flight (TOF) cameras are appealing because they obtain dense depth measurements with minimal latency. However, for many battery-powered devices, the illumination source of a TOF camera is power hungry and can limit the battery life of the device. To address this issue, we present an algorithm that lowers the power for depth sensing by reducing the usage of the TOF camera and estimating depth maps using concurrently collected images. Our technique also adaptively controls the TOF camera and enables it when an accurate depth map cannot be estimated. To ensure that the overall system power for depth sensing is reduced, we design our algorithm to run on a low power embedded platform, where it outputs 640x480 depth maps at 30 frames per second. We evaluate our approach on the TU Munich RGB-D dataset, where it produces depth maps with an overall mean relative error of 1.0% and lowers the usage of the TOF camera by 6x. When used with commercial TOF cameras, our algorithm can lower the power for depth sensing by up to 3.5x.\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "This paper presents Navion, an energy-efficient accelerator for visual-inertial odometry (VIO) that enables autonomous navigation of miniaturized robots (e.g., nano drones), and virtual/augmented reality on portable devices. The chip uses inertial measurements and mono/stereo images to estimate the drone's trajectory and a 3D map of the environment. This estimate is obtained by running a state-of-the-art VIO algorithm based on non-linear factor graph optimization, which requires large irregularly structured memories and heterogeneous computation flow. To reduce the energy consumption and footprint, the entire VIO system is fully integrated on chip to eliminate costly off-chip processing and storage. This work uses compression and exploits both structured and unstructured sparsity to reduce on-chip memory size by 4.1$\\times$. Parallelism is used under tight area constraints to increase throughput by 43%. The chip is fabricated in 65nm CMOS, and can process 752$\\times$480 stereo images from EuRoC dataset in real-time at 20 frames per second (fps) consuming only an average power of 2mW. At its peak performance, Navion can process stereo images at up to 171 fps and inertial measurements at up to 52 kHz, while consuming an average of 24mW. The chip is configurable to maximize accuracy, throughput and energy-efficiency trade-offs and to adapt to different environments. To the best of our knowledge, this is the first fully integrated VIO system in an ASIC.\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "The design of DNNs has increasingly focused on reducing the computational complexity in addition to improving accuracy. While emerging DNNs tend to have fewer weights and operations, they also reduce the amount of data reuse with more widely varying layer shapes and sizes. This leads to a diverse set of DNNs, ranging from large ones with high reuse (e.g., AlexNet) to compact ones with high bandwidth requirements (e.g., MobileNet). However, many existing DNN processors depend on certain DNN properties, e.g., a large number of channels, to achieve high performance and energy efficiency and do not have sufficient flexibility to efficiently process a diverse set of DNNs. In this work, we present Eyexam, a performance analysis framework that quantitatively identifies the sources of performance loss in DNN processors. It highlights two architectural bottlenecks in many existing designs. First, their dataflows are not flexible enough to adapt to the varying layer shapes and sizes of different DNNs. Second, their network-on-chip (NoC) can't adapt to support both high data reuse and high bandwidth scenarios. Based on this analysis, we present Eyeriss v2, a high-performance DNN accelerator that adapts to a wide range of DNNs. Eyeriss v2 has a new dataflow, called Row-Stationary Plus (RS+), that enables the spatial tiling of data from all dimensions to fully utilize the parallelism for high performance. To support RS+, it has a low-cost and scalable NoC design, called hierarchical mesh, that connects the high-bandwidth global buffer to the array of processing elements (PEs) in a two-level hierarchy. This enables high-bandwidth data delivery while still being able to harness any available data reuse. Compared with Eyeriss, Eyeriss v2 has a performance increase of 10.4x-17.9x for 256 PEs, 37.7x-71.5x for 1024 PEs, and 448.8x-1086.7x for 16384 PEs on DNNs with widely varying amounts of data reuse.\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7$\\times$ speedup in measured inference latency with equal or higher accuracy on MobileNets (V1&V2).\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems.\n  This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry.\n  The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "Computer vision enables a wide range of applications in robotics/drones, self-driving cars, smart Internet of Things, and portable/wearable electronics. For many of these applications, local embedded processing is preferred due to privacy and/or latency concerns. Accordingly, energy-efficient embedded vision hardware delivering real-time and robust performance is crucial. While deep learning is gaining popularity in several computer vision algorithms, a significant energy consumption difference exists compared to traditional hand-crafted approaches. In this paper, we provide an in-depth analysis of the computation, energy and accuracy trade-offs between learned features such as deep Convolutional Neural Networks (CNN) and hand-crafted features such as Histogram of Oriented Gradients (HOG). This analysis is supported by measurements from two chips that implement these algorithms. Our goal is to understand the source of the energy discrepancy between the two approaches and to provide insight about the potential areas where CNNs can be improved and eventually approach the energy-efficiency of HOG while maintaining its outstanding performance accuracy.\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or amount of computation, we find that they do not necessarily result in lower energy consumption, and therefore do not serve as a good metric for energy cost estimation.\n  To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses energy consumption estimation of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements that target realistic battery-powered system setups. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in output feature maps instead of filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is further globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss. Finally, we show that pruning the AlexNet with a reduced number of target classes can greatly decrease the number of weights but the energy reduction is limited.\n  Energy modeling tool and energy-aware pruned models available at http://eyeriss.mit.edu/energy.html\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "This paper presents a programmable, energy-efficient and real-time object detection accelerator using deformable parts models (DPM), with 2x higher accuracy than traditional rigid body models. With 8 deformable parts detection, three methods are used to address the high computational complexity: classification pruning for 33x fewer parts classification, vector quantization for 15x memory size reduction, and feature basis projection for 2x reduction of the cost of each classification. The chip is implemented in 65nm CMOS technology, and can process HD (1920x1080) images at 30fps without any off-chip storage while consuming only 58.6mW (0.94nJ/pixel, 1168 GOPS/W). The chip has two classification engines to simultaneously detect two different classes of objects. With a tested high throughput of 60fps, the classification engines can be time multiplexed to detect even more than two object classes. It is energy scalable by changing the pruning factor or disabling the parts classification.\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "State-of-the-art super-resolution (SR) algorithms require significant computational resources to achieve real-time throughput (e.g., 60Mpixels/s for HD video). This paper introduces FAST (Free Adaptive Super-resolution via Transfer), a framework to accelerate any SR algorithm applied to compressed videos. FAST exploits the temporal correlation between adjacent frames such that SR is only applied to a subset of frames; SR pixels are then transferred to the other frames. The transferring process has negligible computation cost as it uses information already embedded in the compressed video (e.g., motion vectors and residual). Adaptive processing is used to retain accuracy when the temporal correlation is not present (e.g., occlusions). FAST accelerates state-of-the-art SR algorithms by up to 15x with a visual quality loss of 0.2dB. FAST is an important step towards real-time SR algorithms for ultra-HD displays and energy constrained devices (e.g., phones and tablets).\n        \u25b3 Less", "author": "Vivienne Sze"}, {"abstract": "Joint embeddings between medical imaging modalities and associated radiology reports have the potential to offer significant benefits to the clinical community, ranging from cross-domain retrieval to conditional generation of reports to the broader goals of multimodal representation learning. In this work, we establish baseline joint embedding results measured via both local and global retrieval methods on the soon to be released MIMIC-CXR dataset consisting of both chest X-ray images and the associated radiology reports. We examine both supervised and unsupervised methods on this task and show that for document retrieval tasks with the learned representations, only a limited amount of supervision is needed to yield results comparable to those of fully-supervised methods.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "This paper presents a Lisp architecture for a portable NLP system, termed LAPNLP, for processing clinical notes. LAPNLP integrates multiple standard, customized and in-house developed NLP tools. Our system facilitates portability across different institutions and data systems by incorporating an enriched Common Data Model (CDM) to standardize necessary data elements. It utilizes UMLS to perform domain adaptation when integrating generic domain NLP tools. It also features stand-off annotations that are specified by positional reference to the original document. We built an interval tree based search engine to efficiently query and retrieve the stand-off annotations by specifying positional requirements. We also developed a utility to convert an inline annotation format to stand-off annotations to enable the reuse of clinical text datasets with inline annotations. We experimented with our system on several NLP facilitated tasks including computational phenotyping for lymphoma patients and semantic relation extraction for clinical notes. These experiments showcased the broader applicability and utility of LAPNLP.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "In evidence-based medicine (EBM), structured medical questions are always favored for efficient search of the best available evidence for treatments. PICO element detection is widely used to help structurize the clinical studies and question by identifying the sentences in a given medical text that belong to one of the four components: Participants (P), Intervention (I), Comparison (C), and Outcome (O). In this work, we propose a hierarchical deep neural network (DNN) architecture that contains dual bi-directional long short-term memory (bi-LSTM) layers to automatically detect the PICO element in medical texts. Within the model, the lower layer of bi-LSTM is for sentence encoding while the upper one is to contextualize the encoded sentence representation vector. In addition, we adopt adversarial and virtual adversarial training to regularize the model. Overall, we advance the PICO element detection to new state-of-the-art performance, outperforming the previous works by at least 4\\% in F1 score for all P/I/O categories.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "There are established racial disparities in healthcare, including during end-of-life care, when poor communication and trust can lead to suboptimal outcomes for patients and their families. In this work, we find that racial disparities which have been reported in existing literature are also present in the MIMIC-III database. We hypothesize that one underlying cause of this disparity is due to mistrust between patient and caregivers, and we develop multiple possible trust metric proxies (using coded interpersonal variables and clinical notes) to measure this phenomenon more directly. These metrics show even stronger disparities in end-of-life care than race does, and they also tend to demonstrate statistically significant higher levels of mistrust for black patients than white ones. Finally, we demonstrate that these metrics improve performance on three clinical tasks: in-hospital mortality, discharge against medical advice (AMA) and modified care status (e.g., DNR, DNI, etc.).\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "In this work, we characterize the doctor-patient relationship using a machine learning-derived trust score. We show that this score has statistically significant racial associations, and that by modeling trust directly we find stronger disparities in care than by stratifying on race. We further demonstrate that mistrust is indicative of worse outcomes, but is only weakly associated with physiologically-created severity scores. Finally, we describe sentiment analysis experiments indicating patients with higher levels of mistrust have worse experiences and interactions with their caregivers. This work is a step towards measuring fairer machine learning in the healthcare domain.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Mapping and translating professional but arcane clinical jargons to consumer language is essential to improve the patient-clinician communication. Researchers have used the existing biomedical ontologies and consumer health vocabulary dictionary to translate between the languages. However, such approaches are limited by expert efforts to manually build the dictionary, which is hard to be generalized and scalable. In this work, we utilized the embeddings alignment method for the word mapping between unparalleled clinical professional and consumer language embeddings. To map semantically similar words in two different word embeddings, we first independently trained word embeddings on both the corpus with abundant clinical professional terms and the other with mainly healthcare consumer terms. Then, we aligned the embeddings by the Procrustes algorithm. We also investigated the approach with the adversarial training with refinement. We evaluated the quality of the alignment through the similar words retrieval both by computing the model precision and as well as judging qualitatively by human. We show that the Procrustes algorithm can be performant for the professional consumer language embeddings alignment, whereas adversarial training with refinement may find some relations between two languages.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Clinical notes often describe the most important aspects of a patient's physiology and are therefore critical to medical research. However, these notes are typically inaccessible to researchers without prior removal of sensitive protected health information (PHI), a natural language processing (NLP) task referred to as deidentification. Tools to automatically de-identify clinical notes are needed but are difficult to create without access to those very same notes containing PHI. This work presents a first step toward creating a large synthetically-identified corpus of clinical notes and corresponding PHI annotations in order to facilitate the development de-identification tools. Further, one such tool is evaluated against this corpus in order to understand the advantages and shortcomings of this approach.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Clinical notes often describe important aspects of a patient's stay and are therefore critical to medical research. Clinical concept extraction (CCE) of named entities - such as problems, tests, and treatments - aids in forming an understanding of notes and provides a foundation for many downstream clinical decision-making tasks. Historically, this task has been posed as a standard named entity recognition (NER) sequence tagging problem, and solved with feature-based methods using handengineered domain knowledge. Recent advances, however, have demonstrated the efficacy of LSTM-based models for NER tasks, including CCE. This work presents CliNER 2.0, a simple-to-install, open-source tool for extracting concepts from clinical text. CliNER 2.0 uses a word- and character- level LSTM model, and achieves state-of-the-art performance. For ease of use, the tool also includes pre-trained models available for public use.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Glycemic control is essential for critical care. However, it is a challenging task because there has been no study on personalized optimal strategies for glycemic control. This work aims to learn personalized optimal glycemic trajectories for severely ill septic patients by learning data-driven policies to identify optimal targeted blood glucose levels as a reference for clinicians. We encoded patient states using a sparse autoencoder and adopted a reinforcement learning paradigm using policy iteration to learn the optimal policy from data. We also estimated the expected return following the policy learned from the recorded glycemic trajectories, which yielded a function indicating the relationship between real blood glucose values and 90-day mortality rates. This suggests that the learned optimal policy could reduce the patients' estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. The result demonstrates that reinforcement learning with appropriate patient state encoding can potentially provide optimal glycemic trajectories and allow clinicians to design a personalized strategy for glycemic control in septic patients.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Sepsis is a leading cause of mortality in intensive care units and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. In this work, we propose an approach to deduce treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Our model learns clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. The learned policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are noisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper, we integrate data from all available ICU sources (vitals, labs, notes, demographics) and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner to enable \"real-time\" performance, and predictions are made with a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on our predictive tasks using deep architectures. We explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines in intervention prediction, and provide insight into model learning, which is crucial for the adoption of such models in practice.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient's physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient's physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6% over observed clinical policies, from a baseline mortality of 13.7%. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for named-entity recognition (NER). In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difficult to obtain for the dataset on which the user wants to perform NER: label scarcity is particularly pronounced for patient note de-identification, which is an instance of NER. In this work, we analyze to what extent transfer learning may address this issue. In particular, we demonstrate that transferring an ANN model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state-of-the-art results on two different datasets for patient note de-identification.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Named-entity recognition (NER) aims at identifying entities of interest in a text. Artificial neural networks (ANNs) have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert users. In this paper, we present NeuroNER, an easy-to-use named-entity recognition tool based on ANNs. Users can annotate entities using a graphical web-based user interface (BRAT): the annotations are then used to train an ANN, which in turn predict entities' locations and categories in new texts. NeuroNER makes this annotation-training-prediction flow smooth and accessible to anyone.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-identification system has been proposed, yielding state-of-the-art results. Unlike other systems, it does not rely on human-engineered features, which allows it to be quickly deployed, but does not leverage knowledge from human experts or from electronic health records (EHRs). In this work, we explore a method to incorporate human-engineered features as well as features derived from EHRs to a neural-network-based de-identification system. Our results show that the addition of features, especially the EHR-derived features, further improves the state-of-the-art in patient note de-identification, including for some of the most sensitive PHI types such as patient names. Since in a real-life setting patient notes typically come with EHRs, we recommend developers of de-identification systems to leverage the information EHRs contain.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of EHR databases, the limited number of researchers with access to the non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.\n  Materials and Methods: We introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.\n  Results: Our ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall 99.25 and a precision of 99.06.\n  Conclusion: Our findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no feature engineering.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "In this paper we propose a new approach to probabilistic inference on belief networks, global conditioning, which is a simple generalization of Pearl's (1986b) method of loopcutset conditioning.  We show that global conditioning, as well as loop-cutset conditioning, can be thought of as a special case of the method of Lauritzen and Spiegelhalter (1988) as refined by Jensen et al (199Oa; 1990b).  Nonetheless, this approach provides new opportunities for parallel processing and, in the case of sequential processing, a tradeoff of time for memory.  We also show how a hybrid method (Suermondt and others 1990) combining loop-cutset conditioning with Jensen's method can be viewed within our framework.  By exploring the relationships between these methods, we develop a unifying framework in which the advantages of each approach can be combined successfully.\n        \u25b3 Less", "author": "Peter Szolovits"}, {"abstract": "Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called {\\em dataset distillation}: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to {\\em synthesize} a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress $60,000$ MNIST training images into just $10$ synthetic {\\em distilled images} (one per class) and achieve close to original performance with only a few steps of gradient descent, given a particular fixed network initialization. We evaluate our method in a wide range of initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods in most settings.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.\n  In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts with a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We provide open source interpretation tools to help peer researchers and practitioners better understand their GAN models.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "In this paper, we introduce Recipe1M, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M dataset and food and cooking in general. Code, data and models are publicly available.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Real-life control tasks involve matter of various substances---rigid or soft bodies, liquid, gas---each with distinct physical behaviors. This poses challenges to traditional rigid-body physics engines. Particle-based simulators have been developed to model the dynamics of these complex scenes; however, relying on approximation techniques, their simulation often deviates from real world physics, especially in the long term. In this paper, we propose to learn a particle-based simulator for complex control tasks. Combining learning with particle-based systems brings in two major benefits: first, the learned simulator, just like other particle-based systems, acts widely on objects of different materials; second, the particle-based representation poses strong inductive bias for learning: particles of the same type have the same dynamics within. This enables the model to quickly adapt to new environments of unknown dynamics within a few observations. Using the learned simulator, robots have achieved success in complex manipulation tasks, such as manipulating fluids and deformable foam. The effectiveness of our method has also been demonstrated in the real world. Our study helps lay the foundation for robot learning of dynamic scenes with particle-based representations.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "There has been an increasing interest in learning dynamics simulators for model-based control. Compared with off-the-shelf physics engines, a learnable simulator can quickly adapt to unseen objects, scenes, and tasks. However, existing models like interaction networks only work for fully observable systems; they also only consider pairwise interactions within a single time step, both restricting their use in practical systems. We introduce Propagation Networks (PropNet), a differentiable, learnable dynamics model that handles partially observable scenarios and enables instantaneous propagation of signals beyond pairwise interactions. With these innovations, our propagation networks not only outperform current learnable physics engines in forward simulation, but also achieves superior performance on various control tasks. Compared with existing deep reinforcement learning algorithms, model-based control with propagation networks is more accurate, efficient, and generalizable to novel, partially observable scenes and tasks.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous scene representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for semantics, geometry, and appearance into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object-wise representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance supports 3D-aware scene manipulation, e.g., rotating and moving objects freely while keeping the consistent shape and texture, and changing the object appearance without affecting its shape. Experiments demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "In this work, we introduce pose interpreter networks for 6-DoF object pose estimation. In contrast to other CNN-based approaches to pose estimation that require expensively annotated object pose data, our pose interpreter network is trained entirely on synthetic pose data. We use object masks as an intermediate representation to bridge real and synthetic. We show that when combined with a segmentation model trained on RGB images, our synthetically trained pose interpreter network is able to generalize to real data. Our end-to-end system for object pose estimation runs in real-time (20 Hz) on live RGB data, without using depth information or ICP refinement.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to \"drive\" an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classification accuracy, it does lead to significant damage on the accuracy of specific classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We confirm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work \\cite{morcos2018importance}. However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to specific classes. Our results show that units with high selectivity play an important role in network classification power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "In this paper, we explore neural network models that learn to associate segments of spoken audio captions with the semantically relevant portions of natural images that they refer to. We demonstrate that these audio-visual associative localizations emerge from network-internal representations learned as a by-product of training to perform an image-audio retrieval task. Our models operate directly on the image pixels and speech waveform, and do not rely on any conventional supervision in the form of labels, segmentations, or alignments between the modalities during training. We perform analysis using the Places 205 and ADE20k datasets demonstrating that our models implicitly learn semantically-coupled object and word detectors.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Understanding 3D object structure from a single image is an important but challenging task in computer vision, mostly due to the lack of 3D object annotations to real images. Previous research tackled this problem by either searching for a 3D shape that best explains 2D annotations, or training purely on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Networks (3D-INN), an end-to-end trainable framework that sequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses. Our system learns from both 2D-annotated real images and synthetic 3D data. This is made possible mainly by two technical innovations. First, heatmaps of 2D keypoints serve as an intermediate representation to connect real and synthetic data. 3D-INN is trained on real images to estimate 2D keypoint heatmaps from an input image; it then predicts 3D object structure from heatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN benefits from the variation and abundance of synthetic 3D objects, without suffering from the domain difference between real and synthesized images, often due to imperfect rendering. Second, we propose a Projection Layer, mapping estimated 3D structure back to 2D. During training, it ensures 3D-INN to predict 3D structure whose projection is consistent with the 2D annotations to real images. Experiments show that the proposed system performs well on both 2D keypoint estimation and 3D structure recovery. We also demonstrate that the recovered 3D information has wide vision applications, such as image retrieval.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "The ability to see around corners, i.e., recover details of a hidden scene from its reflections in the surrounding environment, is of considerable interest in a wide range of applications. However, the diffuse nature of light reflected from typical surfaces leads to mixing of spatial information in the collected light, precluding useful scene reconstruction. Here, we employ a computational imaging technique that opportunistically exploits the presence of occluding objects, which obstruct probe-light propagation in the hidden scene, to undo the mixing and greatly improve scene recovery. Importantly, our technique obviates the need for the ultrafast time-of-flight measurements employed by most previous approaches to hidden-scene imaging. Moreover, it does so in a photon-efficient manner based on an accurate forward model and a computational algorithm that, together, respect the physics of three-bounce light propagation and single-photon detection. Using our methodology, we demonstrate reconstruction of hidden-surface reflectivity patterns in a meter-scale environment from non-time-resolved measurements. Ultimately, our technique represents an instance of a rich and promising new imaging modality with important potential implications for imaging science.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "This paper describes a procedure for the creation of large-scale video datasets for action classification and localization from unconstrained, realistic web data. The scalability of the proposed procedure is demonstrated by building a novel video benchmark, named SLAC (Sparsely Labeled ACtions), consisting of over 520K untrimmed videos and 1.75M clip annotations spanning 200 action categories. Using our proposed framework, annotating a clip takes merely 8.8 seconds on average. This represents a saving in labeling time of over 95% compared to the traditional procedure of manual trimming and localization of actions. Our approach dramatically reduces the amount of human labeling by automatically identifying hard clips, i.e., clips that contain coherent actions but lead to prediction disagreement between action classifiers. A human annotator can disambiguate whether such a clip truly contains the hypothesized action in a handful of seconds, thus generating labels for highly informative samples at little cost. We show that our large-scale dataset can be used to effectively pre-train action recognition models, significantly improving final metrics on smaller-scale benchmarks after fine-tuning. On Kinetics, UCF-101 and HMDB-51, models pre-trained on SLAC outperform baselines trained from scratch, by 2.0%, 20.1% and 35.4% in top-1 accuracy, respectively when RGB input is used. Furthermore, we introduce a simple procedure that leverages the sparse labels in SLAC to pre-train action localization models. On THUMOS14 and ActivityNet-v1.3, our localization model improves the mAP of baseline model by 8.6% and 2.5%, respectively.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent's actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance, which contains annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "The sound of crashing waves, the roar of fast-moving cars -- sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds. This paper extends an earlier conference paper, Owens et al. 2016, with additional experiments and discussion.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Temporal relational reasoning, the ability to link meaningful transformations of objects or entities over time, is a fundamental property of intelligent species. In this paper, we introduce an effective and interpretable network module, the Temporal Relation Network (TRN), designed to learn and reason about temporal dependencies between video frames at multiple time scales. We evaluate TRN-equipped networks on activity recognition tasks using three recent video datasets - Something-Something, Jester, and Charades - which fundamentally depend on temporal relational reasoning. Our results demonstrate that the proposed TRN gives convolutional neural networks a remarkable capacity to discover temporal relations in videos. Through only sparsely sampled video frames, TRN-equipped networks can accurately predict human-object interactions in the Something-Something dataset and identify various human gestures on the Jester dataset with very competitive performance. TRN-equipped networks also outperform two-stream networks and 3D convolution networks in recognizing daily activities in the Charades dataset. Further analyses show that the models learn intuitive and interpretable visual common sense knowledge in videos.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Active non-line-of-sight imaging systems are of growing interest for diverse applications. The most commonly proposed approaches to date rely on exploiting time-resolved measurements, i.e., measuring the time it takes for short light pulses to transit the scene. This typically requires expensive, specialized, ultrafast lasers and detectors that must be carefully calibrated. We develop an alternative approach that exploits the valuable role that natural occluders in a scene play in enabling accurate and practical image formation in such settings without such hardware complexity. In particular, we demonstrate that the presence of occluders in the hidden scene can obviate the need for collecting time-resolved measurements, and develop an accompanying analysis for such systems and their generalizations. Ultimately, the results suggest the potential to develop increasingly sophisticated future systems that are able to identify and exploit diverse structural features of the environment to reconstruct scenes hidden from view.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. However, CNNs often criticized as being black boxes that lack interpretability, since they have millions of unexplained model parameters. In this work, we describe Network Dissection, a method that interprets networks by providing labels for the units of their deep visual representations. The proposed method quantifies the interpretability of CNN representations by evaluating the alignment between individual hidden units and a set of visual semantic concepts. By identifying the best alignments, units are given human interpretable labels across a range of objects, parts, scenes, textures, materials, and colors. The method reveals that deep representations are more transparent and interpretable than expected: we find that representations are significantly more interpretable than they would be under a random equivalently powerful basis. We apply the method to interpret and compare the latent representations of various network architectures trained to solve different supervised and self-supervised training tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initializations, and the network depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a prediction given by a CNN for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We present a new method for probing the hadronic interaction models at ultra-high energy and extracting details about mass composition. This is done using the time profiles of the signals recorded with the water-Cherenkov detectors of the Pierre Auger Observatory. The profiles arise from a mix of the muon and electromagnetic components of air-showers. Using the risetimes of the recorded signals we define a new parameter, which we use to compare our observations with predictions from simulations. We find, firstly, inconsistencies between our data and predictions over a greater energy range and with substantially more events than in previous studies. Secondly, by calibrating the new parameter with fluorescence measurements from observations made at the Auger Observatory, we can infer the depth of shower maximum for a sample of over 81,000 events extending from 0.3 EeV to over 100 EeV. Above 30 EeV, the sample is nearly fourteen times larger than currently available from fluorescence measurements and extending the covered energy range by half a decade. The energy dependence of the average depth of shower maximum is compared to simulations and interpreted in terms of the mean of the logarithmic mass. We find good agreement with previous work and extend the measurement of the mean depth of shower maximum to greater energies than before, reducing significantly the statistical uncertainty associated with the inferences about mass composition.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "A person's weight status can have profound implications on their life, ranging from mental health, to longevity, to financial income. At the societal level, \"fat shaming\" and other forms of \"sizeism\" are a growing concern, while increasing obesity rates are linked to ever raising healthcare costs. For these reasons, researchers from a variety of backgrounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we show how computer vision can be used to infer a person's BMI from social media images. We hope that our tool, which we release, helps to advance the study of social aspects related to body weight.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Recent robotic manipulation competitions have highlighted that sophisticated robots still struggle to achieve fast and reliable perception of task-relevant objects in complex, realistic scenarios. To improve these systems' perceptive speed and robustness, we present SegICP, a novel integrated solution to object recognition and pose estimation. SegICP couples convolutional neural networks and multi-hypothesis point cloud registration to achieve both robust pixel-wise semantic segmentation as well as accurate and real-time 6-DOF pose estimation for relevant objects. Our architecture achieves 1cm position error and <5^\\circ$ angle error in real time without an initial seed. We evaluate and benchmark SegICP against an annotated dataset generated by motion capture.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Food is an integral part of our life and what and how much we eat crucially affects our health. Our food choices largely depend on how we perceive certain characteristics of food, such as whether it is healthy, delicious or if it qualifies as a salad. But these perceptions differ from person to person and one person's \"single lettuce leaf\" might be another person's \"side salad\". Studying how food is perceived in relation to what it actually is typically involves a laboratory setup. Here we propose to use recent advances in image recognition to tackle this problem. Concretely, we use data for 1.9 million images from Instagram from the US to look at systematic differences in how a machine would objectively label an image compared to how a human subjectively does. We show that this difference, which we call the \"perception gap\", relates to a number of health outcomes observed at the county level. To the best of our knowledge, this is the first time that image recognition is being used to study the \"misalignment\" of how people describe food images vs. what they actually depict.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Following the gaze of people inside videos is an important signal for understanding people and their actions. In this paper, we present an approach for following gaze across views by predicting where a particular person is looking throughout a scene. We collect VideoGaze, a new dataset which we use as a benchmark to both train and evaluate models. Given one view with a person in it and a second view of the scene, our model estimates a density for gaze location in the second view. A key aspect of our approach is an end-to-end model that solves the following sub-problems: saliency, gaze pose, and geometric relationships between views. Although our model is supervised only with gaze, we show that the model learns to solve these subproblems automatically without supervision. Experiments suggest that our approach follows gaze better than standard baselines and produces plausible results for everyday situations.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Recognizing when people have false beliefs is crucial for understanding their actions. We introduce the novel problem of identifying when people in abstract scenes have incorrect beliefs. We present a dataset of scenes, each visually depicting an 8-frame story in which a character has a mistaken belief. We then create a representation of characters' beliefs for two tasks in human action understanding: predicting who is mistaken, and when they are mistaken. Experiments suggest that our method for identifying mistaken characters performs better on these tasks than simple baselines. Diagnostics on our model suggest it learns important cues for recognizing mistaken beliefs, such as gaze. We believe models of people's beliefs will have many applications in action understanding, robotics, and healthcare.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We present the Neural Physics Engine (NPE), a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations. We propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions. Like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions; realized as a neural network, it can be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds. By comparing to less structured architectures, we show that the NPE's compositional representation of the structure in physical interactions improves its ability to predict movement, generalize across variable object count and different scene configurations, and infer latent properties of objects such as mass.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "The sound of crashing waves, the roar of fast-moving cars -- sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on ADE20K can be applied to a wide variety of scenes and objects.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize cross-modal scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We measure the energy emitted by extensive air showers in the form of radio emission in the frequency range from 30 to 80 MHz. Exploiting the accurate energy scale of the Pierre Auger Observatory, we obtain a radiation energy of 15.8 \\pm 0.7 (stat) \\pm 6.7 (sys) MeV for cosmic rays with an energy of 1 EeV arriving perpendicularly to a geomagnetic field of 0.24 G, scaling quadratically with the cosmic-ray energy. A comparison with predictions from state-of-the-art first-principle calculations shows agreement with our measurement. The radiation energy provides direct access to the calorimetric energy in the electromagnetic cascade of extensive air showers. Comparison with our result thus allows the direct calibration of any cosmic-ray radio detector against the well-established energy scale of the Pierre Auger Observatory.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an end-to-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technical innovations. First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as 3D rendering and image retrieval.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "How best to evaluate a saliency model's ability to predict where humans look in images is an open research question. The choice of evaluation metric depends on how saliency is defined and how the ground truth is represented. Metrics differ in how they rank saliency models, and this results from how false positives and false negatives are treated, whether viewing biases are accounted for, whether spatial deviations are factored in, and how the saliency maps are pre-processed. In this paper, we provide an analysis of 8 different evaluation metrics and their properties. With the help of systematic experiments and visualizations of metric computations, we add interpretability to saliency scores and more transparency to the evaluation of saliency models. Building off the differences in metric properties and behaviors, we make recommendations for metric selections under specific assumptions and for specific applications.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "The complex multi-stage architecture of cortical visual pathways provides the neural basis for efficient visual object recognition in humans. However, the stage-wise computations therein remain poorly understood. Here, we compared temporal (magnetoencephalography) and spatial (functional MRI) visual brain representations with representations in an artificial deep neural network (DNN) tuned to the statistics of real-world visual recognition. We showed that the DNN captured the stages of human visual processing in both time and space from early visual areas towards the dorsal and ventral streams. Further investigation of crucial DNN parameters revealed that while model architecture was important, training on real-world categorization was necessary to enforce spatio-temporal hierarchical relationships with the brain. Together our results provide an algorithmically informed view on the spatio-temporal dynamics of visual object recognition in the human visual brain.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- video clips, plots, subtitles, scripts, and DVS. We analyze our data through various statistics and methods. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "The Auger Engineering Radio Array (AERA) is part of the Pierre Auger Observatory and is used to detect the radio emission of cosmic-ray air showers. These observations are compared to the data of the surface detector stations of the Observatory, which provide well-calibrated information on the cosmic-ray energies and arrival directions. The response of the radio stations in the 30 to 80 MHz regime has been thoroughly calibrated to enable the reconstruction of the incoming electric field. For the latter, the energy deposit per area is determined from the radio pulses at each observer position and is interpolated using a two-dimensional function that takes into account signal asymmetries due to interference between the geomagnetic and charge-excess emission components. The spatial integral over the signal distribution gives a direct measurement of the energy transferred from the primary cosmic ray into radio emission in the AERA frequency range. We measure 15.8 MeV of radiation energy for a 1 EeV air shower arriving perpendicularly to the geomagnetic field. This radiation energy -- corrected for geometrical effects -- is used as a cosmic-ray energy estimator. Performing an absolute energy calibration against the surface-detector information, we observe that this radio-energy estimator scales quadratically with the cosmic-ray energy as expected for coherent emission. We find an energy resolution of the radio reconstruction of 22% for the data set and 17% for a high-quality subset containing only events with at least five radio stations with signal.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.\n        \u25b3 Less", "author": "Antonio Torralba"}, {"abstract": "We consider a multi-hop switched network operating under a Max-Weight (MW) scheduling policy, and show that the distance between the queue length process and a fluid solution remains bounded by a constant multiple of the deviation of the cumulative arrival process from its average. We then exploit this result to prove matching upper and lower bounds for the time scale over which additive state space collapse (SSC) takes place. This implies, as two special cases, an additive SSC result in diffusion scaling under non-Markovian arrivals and, for the case of i.i.d. arrivals, an additive SSC result over an exponential time scale.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider a class of continuous-time hybrid dynamical systems that correspond to subgradient flows of a piecewise linear and convex potential function with finitely many pieces, and which include the fluid-level dynamics of the Max-Weight scheduling policy as a special case. We study the effect of an external disturbance/perturbation on the state trajectory, and establish that the magnitude of this effect can be bounded by a constant multiple of the integral of the perturbation. We also discuss the extent to which such a result can be extended.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider the following distributed service model: jobs with unit mean, general distribution, and independent processing times arrive as a renewal process of rate $\u03bbn$, with $0<\u03bb<1$, and are immediately dispatched to one of several queues associated with $n$ identical servers with unit processing rate. We assume that the dispatching decisions are made by a central dispatcher endowed with a finite memory, and with the ability to exchange messages with the servers.\n  We study the fundamental resource requirements (memory bits and message exchange rate), in order to drive the expected queueing delay in steady-state of a typical job to zero, as $n$ increases. We develop a novel approach to show that, within a certain broad class of \"symmetric\" policies, every dispatching policy with a message rate of the order of $n$, and with a memory of the order of $\\log n$ bits, results in an expected queueing delay which is bounded away from zero, uniformly as $n\\to\\infty$.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We formulate a private learning model to study an intrinsic tradeoff between privacy and query complexity in sequential learning. Our model involves a learner who aims to determine a scalar value, $v^*$, by sequentially querying an external database and receiving binary responses. In the meantime, an adversary observes the learner's queries, though not the responses, and tries to infer from them the value of $v^*$. The objective of the learner is to obtain an accurate estimate of $v^*$ using only a small number of queries, while simultaneously protecting her privacy by making $v^*$ provably difficult to learn for the adversary. Our main results provide tight upper and lower bounds on the learner's query complexity as a function of desired levels of privacy and estimation accuracy. We also construct explicit query strategies whose complexity is optimal up to an additive constant.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider the following distributed service model: jobs with unit mean, exponentially distributed, and independent processing times arrive as a Poisson process of rate $\u03bbn$, with $0<\u03bb<1$, and are immediately dispatched by a centralized dispatcher to one of $n$ First-In-First-Out queues associated with $n$ identical servers. The dispatcher is endowed with a finite memory, and with the ability to exchange messages with the servers.\n  We propose and study a resource-constrained \"pull-based\" dispatching policy that involves two parameters: (i) the number of memory bits available at the dispatcher, and (ii) the average rate at which servers communicate with the dispatcher. We establish (using a fluid limit approach) that the asymptotic, as $n\\to\\infty$, expected queueing delay is zero when either (i) the number of memory bits grows logarithmically with $n$ and the message rate grows superlinearly with $n$, or (ii) the number of memory bits grows superlogarithmically with $n$ and the message rate is at least $\u03bbn$. Furthermore, when the number of memory bits grows only logarithmically with $n$ and the message rate is proportional to $n$, we obtain a closed-form expression for the (now positive) asymptotic delay.\n  Finally, we demonstrate an interesting phase transition in the resource-constrained regime where the asymptotic delay is non-zero. In particular, we show that for any given $\u03b1>0$ (no matter how small), if our policy only uses a linear message rate $\u03b1n$, the resulting asymptotic delay is upper bounded, uniformly over all $\u03bb<1$; this is in sharp contrast to the delay obtained when no messages are used ($\u03b1= 0$), which grows as $1/(1-\u03bb)$ when $\u03bb\\uparrow 1$, or when the popular power-of-$d$-choices is used, in which the delay grows as $\\log(1/(1-\u03bb))$.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider an SIS-type epidemic process that evolves on a known graph. We assume that a fixed curing budget can be allocated at each instant to the nodes of the graph, towards the objective of minimizing the expected extinction time of the epidemic. We provide a lower bound on the optimal expected extinction time as a function of the available budget, the epidemic parameters, the maximum degree, and the CutWidth of the graph. For graphs with large CutWidth (close to the largest possible), and under a budget which is sublinear in the number of nodes, our lower bound scales exponentially with the size of the graph.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider the propagation of a contagion process (epidemic) on a network and study the problem of dynamically allocating a fixed curing budget to the nodes of the graph, at each time instant. For bounded degree graphs, we provide a lower bound on the expected time to extinction under any such dynamic allocation policy, in terms of a combinatorial quantity that we call the resistance of the set of initially infected nodes, the available budget, and the number of nodes n. Specifically, we consider the case of bounded degree graphs, with the resistance growing linearly in n. We show that if the curing budget is less than a certain multiple of the resistance, then the expected time to extinction grows exponentially with n. As a corollary, if all nodes are initially infected and the CutWidth of the graph grows linearly, while the curing budget is less than a certain multiple of the CutWidth, then the expected time to extinction grows exponentially in n. The combination of the latter with our prior work establishes a fairly sharp phase transition on the expected time to extinction (sub-linear versus exponential) based on the relation between the CutWidth and the curing budget.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We study a multi-server model with $n$ flexible servers and $n$ queues, connected through a bipartite graph, where the level of flexibility is captured by the graph's average degree, $d_n$. Applications in content replication in data centers, skill-based routing in call centers, and flexible supply chains are among our main motivations.\n  We focus on the scaling regime where the system size $n$ tends to infinity, while the overall traffic intensity stays fixed. We show that a large capacity region and an asymptotically vanishing queueing delay are simultaneously achievable even under limited flexibility ($d_n \\ll n$). Our main results demonstrate that, when $d_n\\gg \\ln n$, a family of expander-graph-based flexibility architectures has a capacity region that is within a constant factor of the maximum possible, while simultaneously ensuring a diminishing queueing delay for all arrival rate vectors in the capacity region. Our analysis is centered around a new class of virtual-queue-based scheduling policies that rely on dynamically constructed job-to-server assignments on the connectivity graph. For comparison, we also analyze a natural family of modular architectures, which is simpler but has provably weaker performance.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We provide a dynamic policy for the rapid containment of a contagion process modeled as an SIS epidemic on a bounded degree undirected graph with n nodes. We show that if the budget $r$ of curing resources available at each time is $\u03a9(W)$, where $W$ is the CutWidth of the graph, and also of order $\u03a9(\\log n)$, then the expected time until the extinction of the epidemic is of order $O(n/r)$, which is within a constant factor from optimal, as well as sublinear in the number of nodes. Furthermore, if the CutWidth increases only sublinearly with n, a sublinear expected time to extinction is possible with a sublinearly increasing budget $r$.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We study the optimal scaling of the expected total queue size in an $n\\times n$ input-queued switch, as a function of the number of ports $n$ and the load factor $\u03c1$, which has been conjectured to be $\u0398(n/(1-\u03c1))$. In a recent work, the validity of this conjecture has been established for the regime where $1-\u03c1= O(1/n^2)$. In this paper, we make further progress in the direction of this conjecture. We provide a new class of scheduling policies under which the expected total queue size scales as $O(n^{1.5}(1-\u03c1)^{-1}\\log(1/(1-\u03c1)))$ when $1-\u03c1= O(1/n)$. This is an improvement over the state of the art; for example, for $\u03c1= 1 - 1/n$ the best known bound was $O(n^3)$, while ours is $O(n^{2.5}\\log n)$.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We analyze the effect of tumor repopulation on optimal dose delivery in radiation therapy. We are primarily motivated by accelerated tumor repopulation towards the end of radiation treatment, which is believed to play a role in treatment failure for some tumor sites. A dynamic programming framework is developed to determine an optimal fractionation scheme based on a model of cell kill due to radiation and tumor growth in between treatment days. We find that faster tumor growth suggests shorter overall treatment duration. In addition, the presence of accelerated repopulation suggests larger dose fractions later in the treatment to compensate for the increased tumor proliferation. We prove that the optimal dose fractions are increasing over time. Numerical simulations indicate potential for improvement in treatment effectiveness.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider an infinite collection of agents who make decisions, sequentially, about an unknown underlying binary state of the world. Each agent, prior to making a decision, receives an independent private signal whose distribution depends on the state of the world. Moreover, each agent also observes the decisions of its last K immediate predecessors. We study conditions under which the agent decisions converge to the correct value of the underlying state. We focus on the case where the private signals have bounded information content and investigate whether learning is possible, that is, whether there exist decision rules for the different agents that result in the convergence of their sequence of individual decisions to the correct state of the world. We first consider learning in the almost sure sense and show that it is impossible, for any value of K. We then explore the possibility of convergence in probability of the decisions to the correct state. Here, a distinction arises: if K equals 1, learning in probability is impossible under any decision rule, while for K greater or equal to 2, we design a decision rule that achieves it. We finally consider a new model, involving forward looking strategic agents, each of which maximizes the discounted sum (over all agents) of the probabilities of a correct decision. (The case, studied in previous literature, of myopic agents who maximize the probability of their own decision being correct is an extreme special case.) We show that for any value of K, for any equilibrium of the associated Bayesian game, and under the assumption that each private signal has bounded information content, learning in probability fails to obtain.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We carry out a delay stability analysis (i.e., determine conditions under which expected steady-state delays at a queue are finite) for a simple 3-queue system operated under the Max-Weight scheduling policy, for the case where one of the queues is fed by heavy-tailed traffic (i.e, when the number of arrivals at each time slot has infinite second moment). This particular system exemplifies an intricate phenomenon whereby heavy-tailed traffic at one queue may or may not result in the delay instability of another queue, depending on the arrival rates.\n  While the ordinary stability region (in the sense of convergence to a steady-state distribution) is straightforward to determine, the determination of the delay stability region is more involved: (i) we use \"fluid-type\" sample path arguments, combined with renewal theory, to prove delay instability outside a certain region; (ii) we use a piecewise linear Lyapunov function to prove delay stability in the interior of that same region; (iii) as an intermediate step in establishing delay stability, we show that the expected workload of a stable M/GI/1 queue scales with time as $\\mathcal{O}(t^{1/(1+\u03b3)})$, assuming that service times have a finite $1+\u03b3$ moment, where $\u03b3\\in (0,1)$.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "In an electric power system, demand fluctuations may result in significant ancillary cost to suppliers. Furthermore, in the near future, deep penetration of volatile renewable electricity generation is expected to exacerbate the variability of demand on conventional thermal generating units. We address this issue by explicitly modeling the ancillary cost associated with demand variability. We argue that a time-varying price equal to the suppliers' instantaneous marginal cost may not achieve social optimality, and that consumer demand fluctuations should be properly priced. We propose a dynamic pricing mechanism that explicitly encourages consumers to adapt their consumption so as to offset the variability of demand on conventional units. Through a dynamic game-theoretic formulation, we show that (under suitable convexity assumptions) the proposed pricing mechanism achieves social optimality asymptotically, as the number of consumers increases to infinity. Numerical results demonstrate that compared with marginal cost pricing, the proposed mechanism creates a stronger incentive for consumers to shift their peak load, and therefore has the potential to reduce the need for long-term investment in peaking plants.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider a Cournot oligopoly model where multiple suppliers (oligopolists) compete by choosing quantities. We compare the social welfare achieved at a Cournot equilibrium to the maximum possible, for the case where the inverse market demand function is convex. We establish a lower bound on the efficiency of Cournot equilibria in terms of a scalar parameter derived from the inverse demand function, namely, the ratio of the slope of the inverse demand function at the Cournot equilibrium to the average slope of the inverse demand function between the Cournot equilibrium and a social optimum. Also, for the case of a single, monopolistic, profit maximizing supplier, or of multiple suppliers who collude to maximize their total profit, we establish a similar but tighter lower bound on the efficiency of the resulting output. Our results provide nontrivial quantitative bounds on the loss of social welfare for several convex inverse demand functions that appear in the economics literature.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We conduct a theoretical study of various solution methods for the adaptive fractionation problem. The two messages of this paper are: (i) dynamic programming (DP) is a useful framework for adaptive radiation therapy, particularly adaptive fractionation, because it allows us to assess how close to optimal different methods are, and (ii) heuristic methods proposed in this paper are near-optimal, and therefore, can be used to evaluate the best possible benefit of using an adaptive fraction size.\n  The essence of adaptive fractionation is to increase the fraction size when the tumor and organ-at-risk (OAR) are far apart (a \"favorable\" anatomy) and to decrease the fraction size when they are close together. Given that a fixed prescribed dose must be delivered to the tumor over the course of the treatment, such an approach results in a lower cumulative dose to the OAR when compared to that resulting from standard fractionation. We first establish a benchmark by using the DP algorithm to solve the problem exactly. In this case, we characterize the structure of an optimal policy, which provides guidance for our choice of heuristics. We develop two intuitive, numerically near-optimal heuristic policies, which could be used for more complex, high-dimensional problems. Furthermore, one of the heuristics requires only a statistic of the motion probability distribution, making it a reasonable method for use in a realistic setting. Numerically, we find that the amount of decrease in dose to the OAR can vary significantly (5 - 85%) depending on the amount of motion in the anatomy, the number of fractions, and the range of fraction sizes allowed. In general, the decrease in dose to the OAR is more pronounced when: (i) we have a high probability of large tumor-OAR distances, (ii) we use many fractions (as in a hyper-fractionated setting), and (iii) we allow large daily fraction size deviations.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider the problem of packet scheduling in single-hop queueing networks, and analyze the impact of heavy-tailed traffic on the performance of Max-Weight scheduling. As a performance metric we use the delay stability of traffic flows: a traffic flow is delay stable if its expected steady-state delay is finite, and delay unstable otherwise. First, we show that a heavy-tailed traffic flow is delay unstable under any scheduling policy. Then, we focus on the celebrated Max-Weight scheduling policy, and show that a light-tailed flow that conflicts with a heavy-tailed flow is also delay unstable. This is true irrespective of the rate or the tail distribution of the light-tailed flow, or other scheduling constraints in the network. Surprisingly, we show that a light-tailed flow can be delay unstable, even when it does not conflict with heavy-tailed traffic. Furthermore, delay stability in this case may depend on the rate of the light-tailed flow. Finally, we turn our attention to the class of Max-Weight-a scheduling policies; we show that if the a-parameters are chosen suitably, then the sum of the a-moments of the steady-state queue lengths is finite. We provide an explicit upper bound for the latter quantity, from which we derive results related to the delay stability of traffic flows, and the scaling of moments of steady-state queue lengths with traffic intensity.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider a consensus algorithm in which every node in a sequence of undirected, B-connected graphs assigns equal weight to each of its neighbors. Under the assumption that the degree of each node is fixed (except for times when the node has no connections to other nodes), we show that consensus is achieved within a given accuracy $\u03b5$ on n nodes in time $B+4n^3 B \\ln(2n/\u03b5)$. Because there is a direct relation between consensus algorithms in time-varying environments and inhomogeneous random walks, our result also translates into a general statement on such random walks. Moreover, we give a simple proof of a result of Cao, Spielman, and Morse that the worst case convergence time becomes exponentially large in the number of nodes $n$ under slight relaxation of the degree constancy assumption.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider continuous-time consensus seeking systems whose time-dependent interactions are cut-balanced, in the following sense: if a group of agents influences the remaining ones, the former group is also influenced by the remaining ones by at least a proportional amount. Models involving symmetric interconnections and models in which a weighted average of the agent values is conserved are special cases. We prove that such systems always converge. We give a sufficient condition on the evolving interaction topology for the limit values of two agents to be the same. Conversely, we show that if our condition is not satisfied, then these limits are generically different. These results allow treating systems where the agent interactions are a priori unknown, e.g., random or determined endogenously by the agent values. We also derive corresponding results for discrete-time systems.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We show that unless P=NP, there exists no polynomial time (or even pseudo-polynomial time) algorithm that can decide whether a multivariate polynomial of degree four (or higher even degree) is globally convex. This solves a problem that has been open since 1992 when N. Z. Shor asked for the complexity of deciding convexity for quartic polynomials. We also prove that deciding strict convexity, strong convexity, quasiconvexity, and pseudoconvexity of polynomials of even degree four or higher is strongly NP-hard. By contrast, we show that quasiconvexity and pseudoconvexity of odd degree polynomials can be decided in polynomial time.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We investigate the asymptotic behavior of the steady-state queue length distribution under generalized max-weight scheduling in the presence of heavy-tailed traffic. We consider a system consisting of two parallel queues, served by a single server. One of the queues receives heavy-tailed traffic, and the other receives light-tailed traffic. We study the class of throughput optimal max-weight-alpha scheduling policies, and derive an exact asymptotic characterization of the steady-state queue length distributions. In particular, we show that the tail of the light queue distribution is heavier than a power-law curve, whose tail coefficient we obtain explicitly. Our asymptotic characterization also contains an intuitively surprising result - the celebrated max-weight scheduling policy leads to the worst possible tail of the light queue distribution, among all non-idling policies. Motivated by the above negative result regarding the max-weight-alpha policy, we analyze a log-max-weight (LMW) scheduling policy. We show that the LMW policy guarantees an exponentially decaying light queue tail, while still being throughput optimal.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We propose a model for deterministic distributed function computation by a network of identical and anonymous nodes. In this model, each node has bounded computation and storage capabilities that do not grow with the network size. Furthermore, each node only knows its neighbors, not the entire graph. Our goal is to characterize the class of functions that can be computed within this model. In our main result, we provide a necessary condition for computability which we show to be nearly sufficient, in the sense that every function that satisfies this condition can at least be approximated. The problem of computing suitably rounded averages in a distributed manner plays a central role in our development; we provide an algorithm that solves it in time that grows quadratically with the size of the network.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider a switched network, a fairly general constrained  queueing network model that has been used successfully to model the detailed packet-level dynamics in communication networks, such as input-queued switches and wireless networks. The main operational issue in this model is that of deciding which queues to serve, subject to  certain constraints. In this paper, we study qualitative performance properties of the well known $\u03b1$-weighted scheduling policies. The stability, in the  sense of positive recurrence, of these policies has been well understood. We establish exponential upper bounds on the tail of the steady-state distribution  of the backlog. Along the way, we prove finiteness of the expected steady-state backlog when $\u03b1<1$, a property that was  known only for $\u03b1\\geq 1$. Finally, we analyze the excursions of the maximum backlog over a finite time horizon for $\u03b1\\geq 1$. As a consequence, for $\u03b1\\geq 1$, we establish the full state space collapse property.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We derive lower bounds on the convergence speed of a widely used class of distributed averaging algorithms. In particular, we prove that any distributed averaging algorithm whose state consists of a single real number and whose (possibly nonlinear) update function satisfies a natural smoothness condition has a worst case running time of at least on the order of $n^2$ on a network of $n$ nodes. Our results suggest that increased memory or expansion of the state space is crucial for improving the running times of distributed averaging algorithms.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We study a simple continuous-time multi-agent system related to Krause's model of opinion dynamics: each agent holds a real value, and this value is continuously attracted by every other value differing from it by less than 1, with an intensity proportional to the difference.\n  We prove convergence to a set of clusters, with the agents in each cluster sharing a common value, and provide a lower bound on the distance between clusters at a stable equilibrium, under a suitable notion of multi-agent system stability.\n  To better understand the behavior of the system for a large number of agents, we introduce a variant involving a continuum of agents. We prove, under some conditions, the existence of a solution to the system dynamics, convergence to clusters, and a non-trivial lower bound on the distance between clusters. Finally, we establish that the continuum model accurately represents the asymptotic behavior of a system with a finite but large number of agents.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We propose a model for deterministic distributed function computation by a network of identical and anonymous nodes, with bounded computation and storage capabilities that do not scale with the network size. Our goal is to characterize the class of functions that can be computed within this model. In our main result, we exhibit a class of non-computable functions, and prove that every function outside this class can at least be approximated. The problem of computing averages in a distributed manner plays a central role in our development.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an $r$-dimensional random vector $\\mathbf{Z} \\in \\mathbb{R}^r$, where $r \\geq 2$. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order $\u0398(r \\sqrt{T})$, by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form $O(r \\sqrt{T} \\log^{3/2} T)$.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We study a model of opinion dynamics introduced by Krause: each agent has an opinion represented by a real number, and updates its opinion by averaging all agent opinions that differ from its own by less than 1. We give a new proof of convergence into clusters of agents, with all agents in the same cluster holding the same opinion. We then introduce a particular notion of equilibrium stability and provide lower bounds on the inter-cluster distances at a stable equilibrium. To better understand the behavior of the system when the number of agents is large, we also introduce and study a variant involving a continuum of agents, obtaining partial convergence results and lower bounds on inter-cluster distances, under some mild assumptions.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider the problem of decentralized detection in a network consisting of a large number of nodes arranged as a tree of bounded height, under the assumption of conditionally independent, identically distributed observations. We characterize the optimal error exponent under a Neyman-Pearson formulation. We show that the Type II error probability decays exponentially fast with the number of nodes, and the optimal error exponent is often the same as that corresponding to a parallel configuration. We provide sufficient, as well as necessary, conditions for this to happen. For those networks satisfying the sufficient conditions, we propose a simple strategy that nearly achieves the optimal error exponent, and in which all non-leaf nodes need only send 1-bit messages.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider a convex unconstrained optimization problem that arises in a network of agents whose goal is to cooperatively optimize the sum of the individual agent objective functions through local computations and communications. For this problem, we use averaging algorithms to develop distributed subgradient methods that can operate over a time-varying topology. Our focus is on the convergence rate of these methods and the degradation in performance when only quantized information is available. Based on our recent results on the convergence time of distributed averaging algorithms, we derive improved upper bounds on the convergence rate of the unquantized subgradient method. We then propose a distributed subgradient method under the additional constraint that agents can only store and communicate quantized information, and we provide bounds on its convergence rate that highlight the dependence on the number of quantization levels.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider distributed iterative algorithms for the averaging problem over time-varying topologies. Our focus is on the convergence time of such algorithms when complete (unquantized) information is available, and on the degradation of performance when only quantized information is available. We study a large and natural class of averaging algorithms, which includes the vast majority of algorithms proposed to date, and provide tight polynomial bounds on their convergence time. We also describe an algorithm within this class whose convergence time is the best among currently available averaging algorithms for time-varying topologies. We then propose and analyze distributed averaging algorithms under the additional constraint that agents can only store and communicate quantized information, so that they can only converge to the average of the initial values of the agents within some error. We establish bounds on the error and tight bounds on the convergence time, as a function of the number of quantization levels.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We provide an example proving that there exists no quadratic Lyapunov function for a certain class of linear agreement/consensus algorithms, a fact that had been numerically verified in [5]. We also briefly discuss sufficient conditions for the existence of such a Lyapunov function.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We study the convergence speed of distributed iterative algorithms for the consensus and averaging problems, with emphasis on the latter. We first consider the case of a fixed communication topology. We show that a simple adaptation of a consensus algorithm leads to an averaging algorithm. We prove lower bounds on the worst-case convergence time for various classes of linear, time-invariant, distributed consensus methods, and provide an algorithm that essentially matches those lower bounds. We then consider the case of a time-varying topology, and provide a polynomial-time averaging algorithm.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We consider a resource allocation problem where individual users wish to send data across a network to maximize their utility, and a cost is incurred at each link that depends on the total rate sent through the link. It is known that as long as users do not anticipate the effect of their actions on prices, a simple proportional pricing mechanism can maximize the sum of users' utilities minus the cost (called aggregate surplus). Continuing previous efforts to quantify the effects of selfish behavior in network pricing mechanisms, we consider the possibility that users anticipate the effect of their actions on link prices. Under the assumption that the links' marginal cost functions are convex, we establish existence of a Nash equilibrium. We show that the aggregate surplus at a Nash equilibrium is no worse than a factor of 4*sqrt{2} - 5 times the optimal aggregate surplus; thus, the efficiency loss when users are selfish is no more than approximately 34%.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "We study the rate of convergence of linear two-time-scale stochastic approximation methods. We consider two-time-scale linear iterations driven by i.i.d. noise, prove some results on their asymptotic covariance and establish asymptotic normality. The well-known result [Polyak, B. T. (1990). Automat.\n  Remote Contr. 51 937-946; Ruppert, D. (1988). Technical Report 781, Cornell Univ.] on the optimality of Polyak-Ruppert averaging techniques specialized to linear stochastic approximation is established as a consequence of the general results in this paper.\n        \u25b3 Less", "author": "John Tsitsiklis"}, {"abstract": "Generative adversarial networks (GANs) are an expressive class of neural generative models with tremendous success in modeling high-dimensional continuous measures. In this paper, we present a scalable method for unbalanced optimal transport (OT) based on the generative-adversarial framework. We formulate unbalanced OT as a problem of simultaneously learning a transport map and a scaling factor that push a source measure to a target measure in a cost-optimal manner. In addition, we propose an algorithm for solving this problem based on stochastic alternating gradient updates, similar in practice to GANs. We also provide theoretical justification for this formulation, showing that it is closely related to an existing static formulation by Liero et al. (2018), and perform numerical experiments demonstrating how this methodology can be applied to population modeling.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Memorization of data in deep neural networks has become a subject of significant research interest. In this paper, we link memorization of images in deep convolutional autoencoders to downsampling through strided convolution. To analyze this mechanism in a simpler setting, we train linear convolutional autoencoders and show that linear combinations of training data are stored as eigenvectors in the linear operator corresponding to the network when downsampling is used. On the other hand, networks without downsampling do not memorize training data. We provide further evidence that the same effect happens in nonlinear networks. Moreover, downsampling in nonlinear networks causes the model to not only memorize linear combinations of images, but individual training images. Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We study nonparametric density estimation for two classes of multivariate distributions that imply strong forms of positive dependence; namely multivariate totally positive distributions of order 2 (MTP$_2$, a.k.a. log-supermodular) and the subclass of log-$L^\\natural$-concave (LLC) distributions. In both cases we impose the additional assumption of log-concavity in order to ensure boundedness of the likelihood function. Given $n$ independent and identically distributed random vectors from a $d$-dimensional MTP$_2$ distribution (LLC distribution, respectively), we show that the maximum likelihood estimator (MLE) exists and is unique with probability one when $n\\geq 3$ ($n\\geq 2$, respectively), independent of the number $d$ of variables. The logarithm of the MLE is a tent function in the binary setting and in $\\mathbb{R}^2$ under MTP$_2$ and in the rational setting under LLC. We provide a conditional gradient algorithm for computing it, and we conjecture that the same convex program also yields the MLE in the remaining cases.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We consider the construction of insurance premiums that are monotonically increasing with respect to a loading parameter. By introducing weight functions that are totally positive of higher order, we derive higher monotonicity properties of weighted transformed premiums. We deduce that the greater the degree of randomness of insured risks, the higher the order of total positivity that should be required for the chosen weight functions. We examine seven classes of weight functions that have appeared in the literature, and we ascertain the higher order total positivity properties of those functions.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We consider the problem of jointly estimating multiple related directed acyclic graph (DAG) models based on high-dimensional data from each graph. This problem is motivated by the task of learning gene regulatory networks based on gene expression data from different tissues, developmental stages or disease states. We prove that under certain regularity conditions, the proposed $\\ell_0$-penalized maximum likelihood estimator converges in Frobenius norm to the adjacency matrices consistent with the data-generating distributions and has the correct sparsity. In particular, we show that this joint estimation procedure leads to a faster convergence rate than estimating each DAG model separately. As a corollary we also obtain high-dimensional consistency results for causal inference from a mix of observational and interventional data. For practical purposes, we propose jointGES consisting of Greedy Equivalence Search (GES) to estimate the union of all DAG models followed by variable selection using lasso to obtain the different DAGs, and we analyze its consistency guarantees. The proposed method is illustrated through an analysis of simulated data as well as epithelial ovarian cancer gene expression data.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationships. However, traditional methods often fail in modern applications, which exhibit a larger number of observed variables than data points. The resulting uncertainty about the underlying network as well as the desire to incorporate prior information recommend a Bayesian approach to learning the BN, but the highly combinatorial structure of BNs poses a striking challenge for inference. The current state-of-the-art methods such as order MCMC are faster than previous methods but prevent the use of many natural structural priors and still have running time exponential in the maximum indegree of the true directed acyclic graph (DAG) of the BN. We here propose an alternative posterior approximation based on the observation that, if we incorporate empirical conditional independence tests, we can focus on a high-probability DAG associated with each order of the vertices. We show that our method allows the desired flexibility in prior specification, removes timing dependence on the maximum indegree and yields provably good posterior approximations; in addition, we show that it achieves superior accuracy, scalability, and sampler mixing on several datasets.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Multivariate distributions are fundamental to modeling. Discrete copulas can be used to construct diverse multivariate joint distributions over random variables from estimated univariate marginals. The space of discrete copulas admits a representation as a convex polytope which can be exploited in entropy-copula methods relevant to hydrology and climatology. To allow for an extensive use of such methods in a wide range of applied fields, it is important to have a geometric representation of discrete copulas with desirable stochastic properties. In this paper, we show that the families of ultramodular discrete copulas and their generalization to convex discrete quasi-copulas admit representations as polytopes. We draw connections to the prominent Birkhoff polytope, alternating sign matrix polytope, and their most extensive generalizations in the discrete geometry literature. In doing so, we generalize some well-known results on these polytopes from both the statistics literature and the discrete geometry literature.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser and B\u00fchlmann (2012) previously characterized the identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes. In this paper, we extend these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily perfect) intervention experiments. We also propose the first provably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We consider the problem of estimating the differences between two causal directed acyclic graph (DAG) models with a shared topological order given i.i.d. samples from each model. This is of interest for example in genomics, where changes in the structure or edge weights of the underlying causal graphs reflect alterations in the gene regulatory networks. We here provide the first provably consistent method for directly estimating the differences in a pair of causal DAGs without separately learning two possibly large and dense DAG models and computing their difference. Our two-step algorithm first uses invariance tests between regression coefficients of the two data sets to estimate the skeleton of the difference graph and then orients some of the edges using invariance tests between regression residual variances. We demonstrate the properties of our method through a simulation study and apply it to the analysis of gene expression data from ovarian cancer and during T-cell activation.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We consider the lattice, $\\mathcal{L}$, of all subsets of a multidimensional contingency table and establish the properties of monotonicity and supermodularity for the marginalization function, $n(\\cdot)$, on $\\mathcal{L}$. We derive from the supermodularity of $n(\\cdot)$ some generalized Fr\u00e9chet inequalities complementing and extending inequalities of Dobra and Fienberg. Further, we construct new monotonic and supermodular functions from $n(\\cdot)$, and we remark on the connection between supermodularity and some correlation inequalities for probability distributions on lattices. We also apply an inequality of Ky Fan to derive a new approach to Fr\u00e9chet inequalities for multidimensional contingency tables.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Gaussian graphical models are used throughout the natural sciences, social sciences, and economics to model the statistical relationships between variables of interest in the form of a graph. We here provide a pedagogic introduction to Gaussian graphical models and review recent results on maximum likelihood estimation for such models. Throughout, we highlight the rich algebraic and geometric properties of Gaussian graphical models and explain how these properties relate to convex optimization and ultimately result in insights on the existence of the maximum likelihood estimator (MLE) and algorithms for computing the MLE.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "DAG models are statistical models satisfying a collection of conditional independence relations encoded by the nonedges of a directed acyclic graph (DAG) $\\mathcal{G}$. Such models are used to model complex cause-effect systems across a variety of research fields. From observational data alone, a DAG model $\\mathcal{G}$ is only recoverable up to Markov equivalence. Combinatorially, two DAGs are Markov equivalent if and only if they have the same underlying undirected graph (i.e. skeleton) and the same set of the induced subDAGs $i\\to j \\leftarrow k$, known as immoralities. Hence it is of interest to study the number and size of Markov equivalence classes (MECs). In a recent paper, the authors introduced a pair of generating functions that enumerate the number of MECs on a fixed skeleton by number of immoralities and by class size, and they studied the complexity of computing these functions. In this paper, we lay the foundation for studying these generating functions by analyzing their structure for trees and other closely related graphs. We describe these polynomials for some important families of graphs including paths, stars, cycles, spider graphs, caterpillars, and complete binary trees. In doing so, we recover important connections to independence polynomials, and extend some classical identities that hold for Fibonacci numbers. We also provide tight lower and upper bounds for the number and size of MECs on any tree. Finally, we use computational methods to show that the number and distribution of high degree nodes in a triangle-free graph dictates the number and size of MECs.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Learning directed acyclic graphs using both observational and interventional data is now a fundamentally important problem due to recent technological developments in genomics that generate such single-cell gene expression data at a very large scale. In order to utilize this data for learning gene regulatory networks, efficient and reliable causal inference algorithms are needed that can make use of both observational and interventional data. In this paper, we present two algorithms of this type and prove that both are consistent under the faithfulness assumption. These algorithms are interventional adaptations of the Greedy SP algorithm and are the first algorithms using both observational and interventional data with consistency guarantees. Moreover, these algorithms have the advantage that they are nonparametric, which makes them useful also for analyzing non-Gaussian data. In this paper, we present these two algorithms and their consistency guarantees, and we analyze their performance on simulated data, protein signaling data, and single-cell gene expression data.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Understanding how a complex machine learning model makes a classification decision is essential for its acceptance in sensitive areas such as health care. Towards this end, we present PatchNet, a method that provides the features indicative of each class in an image using a tradeoff between restricting global image context and classification error. We mathematically analyze this tradeoff, demonstrate Patchnet's ability to construct sharp visual heatmap representations of the learned features, and quantitatively compare these features with features selected by domain experts by applying PatchNet to the classification of benign/malignant skin lesions from the ISBI-ISIC 2017 melanoma classification challenge.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Shape-constrained density estimation is an important topic in mathematical statistics. We focus on densities on $\\mathbb{R}^d$ that are log-concave, and we study geometric properties of the maximum likelihood estimator (MLE) for weighted samples. Cule, Samworth, and Stewart showed that the logarithm of the optimal log-concave density is piecewise linear and supported on a regular subdivision of the samples. This defines a map from the space of weights to the set of regular subdivisions of the samples, i.e. the face poset of their secondary polytope. We prove that this map is surjective. In fact, every regular subdivision arises in the MLE for some set of weights with positive probability, but coarser subdivisions appear to be more likely to arise than finer ones. To quantify these results, we introduce a continuous version of the secondary polytope, whose dual we name the Samworth body. This article establishes a new link between geometric combinatorics and nonparametric statistics, and it suggests numerous open problems.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We analyze the problem of maximum likelihood estimation for Gaussian distributions that are multivariate totally positive of order two (MTP2). By exploiting connections to phylogenetics and single-linkage clustering, we give a simple proof that the maximum likelihood estimator (MLE) for such distributions exists based on at least 2 observations, irrespective of the underlying dimension. Slawski and Hein, who first proved this result, also provided empirical evidence showing that the MTP2 constraint serves as an implicit regularizer and leads to sparsity in the estimated inverse covariance matrix, determining what we name the ML graph. We show that we can find an upper bound for the ML graph by adding edges corresponding to correlations in excess of those explained by the maximum weight spanning forest of the correlation matrix. Moreover, we provide globally convergent coordinate descent algorithms for calculating the MLE under the MTP2 constraint which are structurally similar to iterative proportional scaling. We conclude the paper with a discussion of signed MTP2 distributions.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Bayesian networks, or directed acyclic graph (DAG) models, are widely used to represent complex causal systems. Since the basic task of learning a Bayesian network from data is NP-hard, a standard approach is greedy search over the space of DAGs or Markov equivalent DAGs. Since the space of DAGs on p nodes and the associated space of Markov equivalence classes are both much larger than the space of permutations, it is desirable to consider permutation-based searches. We here provide the first consistency guarantees, both uniform and high-dimensional, of a permutation-based greedy search. Geometrically, this search corresponds to a simplex-type algorithm on a sub-polytope of the permutohedron, the DAG associahedron. Every vertex in this polytope is associated with a DAG, and hence with a collection of permutations that are consistent with the DAG ordering. A walk is performed on the edges of the polytope maximizing the sparsity of the associated DAGs. We show based on simulations that this permutation search is competitive with standard approaches.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Two directed acyclic graphs (DAGs) are called Markov equivalent if and only if they have the same underlying undirected graph (i.e. skeleton) and the same set of immoralities. Using observational data, a DAG model can only be determined up to Markov equivalence, and so it is desirable to understand the size and number of Markov equivalence classes (MECs) combinatorially. In this paper, we address this enumerative question using a pair of generating functions that encode the number and size of MECs on a skeleton $G$, and in doing so we connect this problem to classical problems in combinatorial optimization. The first is a graph polynomial that counts the number of MECs on $G$ by their number of immoralities. Using connections to the independent set problem, we show that computing a DAG on $G$ with the maximum possible number of immoralities is NP-hard. The second generating function counts the MECs on $G$ according to their size. Via computer enumeration, we show that this generating function is distinct for every connected graph on $p$ nodes for all $p\\leq 10$.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "A graphical model encodes conditional independence relations via the Markov properties. For an undirected graph these conditional independence relations can be represented by a simple polytope known as the graph associahedron, which can be constructed as a Minkowski sum of standard simplices. There is an analogous polytope for conditional independence relations coming from a regular Gaussian model, and it can be defined using multiinformation or relative entropy. For directed acyclic graphical models and also for mixed graphical models containing undirected, directed and bidirected edges, we give a construction of this polytope, up to equivalence of normal fans, as a Minkowski sum of matroid polytopes. Finally, we apply this geometric insight to construct a new ordering-based search algorithm for causal inference via directed acyclic graphical models.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We discuss properties of distributions that are multivariate totally positive of order two (MTP2) related to conditional independence. In particular, we show that any independence model generated by an MTP2 distribution is a compositional semigraphoid which is upward-stable and singleton-transitive. In addition, we prove that any MTP2 distribution satisfying an appropriate support condition is faithful to its concentration graph. Finally, we analyze factorization properties of MTP2 distributions and discuss ways of constructing MTP2 distributions; in particular we give conditions on the log-linear parameters of a discrete distribution which ensure MTP2 and characterize conditional Gaussian distributions which satisfy MTP2.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "For a graph $G$ with $p$ vertices the closed convex cone $\\mathbb{S}^p_{\\succeq0}(G)$ consists of all real positive semidefinite $p\\times p$ matrices with zeros in the off-diagonal entries corresponding to nonedges of $G$. The extremal rays of this cone and their associated ranks have applications to matrix completion problems, maximum likelihood estimation in Gaussian graphical models in statistics, and Gauss elimination for sparse matrices. For a graph $G$ without $K_5$ minors, we show that the normal vectors to the facets of the $(\\pm1)$-cut polytope of $G$ specify the off-diagonal entries of extremal matrices in $\\mathbb{S}^p_{\\succeq0}(G)$. We also prove that the constant term of the linear equation of each facet-supporting hyperplane is the rank of its corresponding extremal matrix in $\\mathbb{S}^p_{\\succeq0}(G)$. Furthermore, we show that if $G$ is series-parallel then this gives a complete characterization of all possible extremal ranks of $\\mathbb{S}^p_{\\succeq0}(G)$, consequently solving the sparsity order problem for series-parallel graphs.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Exponential varieties arise from exponential families in statistics. These real algebraic varieties have strong positivity and convexity properties, familiar from toric varieties and their moment maps. Among them are varieties of inverses of symmetric matrices satisfying linear constraints. This class includes Gaussian graphical models. We develop a general theory of exponential varieties. These are derived from hyperbolic polynomials and their integral representations. We compare the multidegrees and ML degrees of the gradient map for hyperbolic polynomials.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "The Ising model is one of the simplest and most famous models of interacting systems. It was originally proposed to model ferromagnetic interactions in statistical physics and is now widely used to model spatial processes in many areas such as ecology, sociology, and genetics, usually without testing its goodness of fit. Here, we propose various test statistics and an exact goodness-of-fit test for the finite-lattice Ising model. The theory of Markov bases has been developed in algebraic statistics for exact goodness-of-fit testing using a Monte Carlo approach. However, finding a Markov basis is often computationally intractable. Thus, we develop a Monte Carlo method for exact goodness-of-fit testing for the Ising model which avoids computing a Markov basis and also leads to a better connectivity of the Markov chain and hence to a faster convergence. We show how this method can be applied to analyze the spatial organization of receptors on the cell membrane.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We study parameter estimation in linear Gaussian covariance models, which are $p$-dimensional Gaussian models with linear constraints on the covariance matrix. Maximum likelihood estimation for this class of models leads to a non-convex optimization problem which typically has many local maxima. Using recent results on the asymptotic distribution of extreme eigenvalues of the Wishart distribution, we provide sufficient conditions for any hill-climbing method to converge to the global maximum. Although we are primarily interested in the case in which $n>\\!\\!>p$, the proofs of our results utilize large-sample asymptotic theory under the scheme $n/p \\to \u03b3> 1$. Remarkably, our numerical simulations indicate that our results remain valid for $p$ as small as $2$. An important consequence of this analysis is that for sample sizes $n \\simeq 14 p$, maximum likelihood estimation for linear Gaussian covariance models behaves as if it were a convex optimization problem.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Following the publication of an attack on genome-wide association studies (GWAS) data proposed by Homer et al., considerable attention has been given to developing methods for releasing GWAS data in a privacy-preserving way. Here, we develop an end-to-end differentially private method for solving regression problems with convex penalty functions and selecting the penalty parameters by cross-validation. In particular, we focus on penalized logistic regression with elastic-net regularization, a method widely used to in GWAS analyses to identify disease-causing genes. We show how a differentially private procedure for penalized logistic regression with elastic-net regularization can be applied to the analysis of GWAS data and evaluate our method's performance.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Gaussian graphical models have received considerable attention during the past four decades from the statistical and machine learning communities. In Bayesian treatments of this model, the G-Wishart distribution serves as the conjugate prior for inverse covariance matrices satisfying graphical constraints. While it is straightforward to posit the unnormalized densities, the normalizing constants of these distributions have been known only for graphs that are chordal, or decomposable. Up until now, it was unknown whether the normalizing constant for a general graph could be represented explicitly, and a considerable body of computational literature emerged that attempted to avoid this apparent intractability. We close this question by providing an explicit representation of the G-Wishart normalizing constant for general graphs.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "The concepts of faithfulness and strong-faithfulness are important for statistical learning of graphical models. Graphs are not sufficient for describing the association structure of a discrete distribution. Hypergraphs representing hierarchical log-linear models are considered instead, and the concept of parametric (strong-) faithfulness with respect to a hypergraph is introduced. Strong-faithfulness ensures the existence of uniformly consistent parameter estimators and enables building uniformly consistent procedures for a hypergraph search. The strength of association in a discrete distribution can be quantified with various measures, leading to different concepts of strong-faithfulness. Lower and upper bounds for the proportions of distributions that do not satisfy strong-faithfulness are computed for different parameterizations and measures of association.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "The protection of privacy of individual-level information in genome-wide association study (GWAS) databases has been a major concern of researchers following the publication of \"an attack\" on GWAS data by Homer et al. (2008) Traditional statistical methods for confidentiality and privacy protection of statistical databases do not scale well to deal with GWAS data, especially in terms of guarantees regarding protection from linkage to external information. The more recent concept of differential privacy, introduced by the cryptographic community, is an approach that provides a rigorous definition of privacy with meaningful privacy guarantees in the presence of arbitrary external information, although the guarantees may come at a serious price in terms of data utility. Building on such notions, Uhler et al. (2013) proposed new methods to release aggregate GWAS data without compromising an individual's privacy. We extend the methods developed in Uhler et al. (2013) for releasing differentially-private $\u03c7^2$-statistics by allowing for arbitrary number of cases and controls, and for releasing differentially-private allelic test statistics. We also provide a new interpretation by assuming the controls' data are known, which is a realistic assumption because some GWAS use publicly available data as controls. We assess the performance of the proposed methods through a risk-utility analysis on a real data set consisting of DNA samples collected by the Wellcome Trust Case Control Consortium and compare the methods with the differentially-private release mechanism proposed by Johnson and Shmatikov (2013).\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "The classical sphere packing problem asks for the best (infinite) arrangement of non-overlapping unit balls which cover as much space as possible. We define a generalized version of the problem, where we allow each ball a limited amount of overlap with other balls. We study two natural choices of overlap measures and obtain the optimal lattice packings in a parameterized family of lattices which contains the FCC, BCC, and integer lattice.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We consider the problem of learning a Bayesian network or directed acyclic graph (DAG) model from observational data. A number of constraint-based, score-based and hybrid algorithms have been developed for this purpose. For constraint-based methods, statistical consistency guarantees typically rely on the faithfulness assumption, which has been show to be restrictive especially for graphs with cycles in the skeleton. However, there is only limited work on consistency guarantees for score-based and hybrid algorithms and it has been unclear whether consistency guarantees can be proven under weaker conditions than the faithfulness assumption.\n  In this paper, we propose the sparsest permutation (SP) algorithm. This algorithm is based on finding the causal ordering of the variables that yields the sparsest DAG. We prove that this new score-based method is consistent under strictly weaker conditions than the faithfulness assumption. We also demonstrate through simulations on small DAGs that the SP algorithm compares favorably to the constraint-based PC and SGS algorithms as well as the score-based Greedy Equivalence Search and hybrid Max-Min Hill-Climbing method. In the Gaussian setting, we prove that our algorithm boils down to finding the permutation of the variables with sparsest Cholesky decomposition for the inverse covariance matrix. Using this connection, we show that in the oracle setting, where the true covariance matrix is known, the SP algorithm is in fact equivalent to $\\ell_0$-penalized maximum likelihood estimation.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "An asymptotic theory is developed for computing volumes of regions in the parameter space of a directed Gaussian graphical model that are obtained by bounding partial correlations. We study these volumes using the method of real log canonical thresholds from algebraic geometry. Our analysis involves the computation of the singular loci of correlation hypersurfaces. Statistical applications include the strong-faithfulness assumption for the PC-algorithm, and the quantification of confounder bias in causal inference. A detailed analysis is presented for trees, bow-ties, tripartite graphs, and complete graphs.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Many algorithms for inferring causality rely heavily on the faithfulness assumption. The main justification for imposing this assumption is that the set of unfaithful distributions has Lebesgue measure zero, since it can be seen as a collection of hypersurfaces in a hypercube. However, due to sampling error the faithfulness condition alone is not sufficient for statistical estimation, and strong-faithfulness has been proposed and assumed to achieve uniform or high-dimensional consistency. In contrast to the plain faithfulness assumption, the set of distributions that is not strong-faithful has nonzero Lebesgue measure and in fact, can be surprisingly large as we show in this paper. We study the strong-faithfulness condition from a geometric and combinatorial point of view and give upper and lower bounds on the Lebesgue measure of strong-faithful distributions for various classes of directed acyclic graphs. Our results imply fundamental limitations for the PC-algorithm and potentially also for other algorithms based on partial correlation testing in the Gaussian case.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Traditional statistical methods for confidentiality protection of statistical databases do not scale well to deal with GWAS (genome-wide association studies) databases especially in terms of guarantees regarding protection from linkage to external information. The more recent concept of differential privacy, introduced by the cryptographic community, is an approach which provides a rigorous definition of privacy with meaningful privacy guarantees in the presence of arbitrary external information, although the guarantees come at a serious price in terms of data utility. Building on such notions, we propose new methods to release aggregate GWAS data without compromising an individual's privacy. We present methods for releasing differentially private minor allele frequencies, chi-square statistics and p-values. We compare these approaches on simulated data and on a GWAS study of canine hair length involving 685 dogs. We also propose a privacy-preserving method for finding genome-wide associations based on a differentially-private approach to penalized logistic regression.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "The problem of packing ellipsoids of different sizes and shapes into an ellipsoidal container so as to minimize a measure of overlap between ellipsoids is considered. A bilevel optimization formulation is given, together with an algorithm for the general case and a simpler algorithm for the special case in which all ellipsoids are in fact spheres. Convergence results are proved and computational experience is described and illustrated. The motivating application - chromosome organization in the human cell nucleus - is discussed briefly, and some illustrative results are presented.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We study maximum likelihood estimation in Gaussian graphical models from a geometric point of view. An algebraic elimination criterion allows us to find exact lower bounds on the number of observations needed to ensure that the maximum likelihood estimator (MLE) exists with probability one. This is applied to bipartite graphs, grids and colored graphs. We also study the ML degree, and we present the first instance of a graph for which the MLE exists with probability one, even when the number of observations equals the treewidth.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "Rapid research progress in genotyping techniques have allowed large genome-wide association studies. Existing methods often focus on determining associations between single loci and a specific phenotype. However, a particular phenotype is usually the result of complex relationships between multiple loci and the environment. In this paper, we describe a two-stage method for detecting epistasis by combining the traditionally used single-locus search with a search for multiway interactions. Our method is based on an extended version of Fisher's exact test. To perform this test, a Markov chain is constructed on the space of multidimensional contingency tables using the elements of a Markov basis as moves. We test our method on simulated data and compare it to a two-stage logistic regression method and to a fully Bayesian method, showing that we are able to detect the interacting loci when other methods fail to do so. Finally, we apply our method to a genome-wide data set consisting of 685 dogs and identify epistasis associated with canine hair length for four pairs of SNPs.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We study multivariate normal models that are described by linear constraints on the inverse of the covariance matrix. Maximum likelihood estimation for such models leads to the problem of maximizing the determinant function over a spectrahedron, and to the problem of characterizing the image of the positive definite cone under an arbitrary linear projection. These problems at the interface of statistics and optimization are here examined from the perspective of convex algebraic geometry.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We use methods from combinatorics and algebraic statistics to study analogues of birth-and-death processes that have as their state space a finite subset of the $m$-dimensional lattice and for which the $m$ matrices that record the transition probabilities in each of the lattice directions commute pairwise. One reason such processes are of interest is that the transition matrix is straightforward to diagonalize, and hence it is easy to compute $n$ step transition probabilities. The set of commuting birth-and-death processes decomposes as a union of toric varieties, with the main component being the closure of all processes whose nearest neighbor transition probabilities are positive. We exhibit an explicit monomial parametrization for this main component, and we explore the boundary components using primary decomposition.\n        \u25b3 Less", "author": "Caroline Uhler"}, {"abstract": "We study secure and undetectable communication in a world where governments can read all encrypted communications of citizens. We consider a world where the only permitted communication method is via a government-mandated encryption scheme, using government-mandated keys. Citizens caught trying to communicate otherwise (e.g., by encrypting strings which do not appear to be natural language plaintexts) will be arrested. The one guarantee we suppose is that the government-mandated encryption scheme is semantically secure against outsiders: a perhaps advantageous feature to secure communication against foreign entities. But what good is semantic security against an adversary that has the power to decrypt?\n  Even in this pessimistic scenario, we show citizens can communicate securely and undetectably. Informally, there is a protocol between Alice and Bob where they exchange ciphertexts that look innocuous even to someone who knows the secret keys and thus sees the corresponding plaintexts. And yet, in the end, Alice will have transmitted her secret message to Bob. Our security definition requires indistinguishability between unmodified use of the mandated encryption scheme, and conversations using the mandated encryption scheme in a modified way for subliminal communication.\n  Our topics may be thought to fall broadly within the realm of steganography: the science of hiding secret communication in innocent-looking messages, or cover objects. However, we deal with the non-standard setting of adversarial cover object distributions (i.e., a stronger-than-usual adversary). We leverage that our cover objects are ciphertexts of a secure encryption scheme to bypass impossibility results which we show for broader classes of steganographic schemes. We give several constructions of subliminal communication schemes based on any key exchange protocol with random messages (e.g., Diffie-Hellman).\n        \u25b3 Less", "author": "Vinod Vaikuntanathan"}, {"abstract": "The growing popularity of cloud-based machine learning raises a natural question about the privacy guarantees that can be provided in such a setting. Our work tackles this problem in the context where a client wishes to classify private images using a convolutional neural network (CNN) trained by a server. Our goal is to build efficient protocols whereby the client can acquire the classification result without revealing their input to the server, while guaranteeing the privacy of the server's neural network.\n  To this end, we design Gazelle, a scalable and low-latency system for secure neural network inference, using an intricate combination of homomorphic encryption and traditional two-party computation techniques (such as garbled circuits). Gazelle makes three contributions. First, we design the Gazelle homomorphic encryption library which provides fast algorithms for basic homomorphic operations such as SIMD (single instruction multiple data) addition, SIMD multiplication and ciphertext permutation. Second, we implement the Gazelle homomorphic linear algebra kernels which map neural network layers to optimized homomorphic matrix-vector multiplication and convolution routines. Third, we design optimized encryption switching protocols which seamlessly convert between homomorphic and garbled circuit encodings to enable implementation of complete neural network inference.\n  We evaluate our protocols on benchmark neural networks trained on the MNIST and CIFAR-10 datasets and show that Gazelle outperforms the best existing systems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint 2017/1164) by 30 times in online runtime. Similarly when compared with fully homomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders of magnitude faster online run-time.\n        \u25b3 Less", "author": "Vinod Vaikuntanathan"}, {"abstract": "In a multiparty message-passing model of communication, there are $k$ players. Each player has a private input, and they communicate by sending messages to one another over private channels. While this model has been used extensively in distributed computing and in multiparty computation, lower bounds on communication complexity in this model and related models have been somewhat scarce. In recent work \\cite{phillips12,woodruff12,woodruff13}, strong lower bounds of the form $\u03a9(n \\cdot k)$ were obtained for several functions in the message-passing model; however, a lower bound on the classical Set Disjointness problem remained elusive.\n  In this paper, we prove tight lower bounds of the form $\u03a9(n \\cdot k)$ for the Set Disjointness problem in the message passing model. Our bounds are obtained by developing information complexity tools in the message-passing model, and then proving an information complexity lower bound for Set Disjointness. As a corollary, we show a tight lower bound for the task allocation problem \\cite{DruckerKuhnOshman} via a reduction from Set Disjointness.\n        \u25b3 Less", "author": "Vinod Vaikuntanathan"}, {"abstract": "Using methods from algebraic graph theory and convex optimization, we study the relationship between local structural features of a network and spectral properties of its Laplacian matrix. In particular, we derive expressions for the so-called spectral moments of the Laplacian matrix of a network in terms of a collection of local structural measurements. Furthermore, we propose a series of semidefinite programs to compute bounds on the spectral radius and the spectral gap of the Laplacian matrix from a truncated sequence of Laplacian spectral moments. Our analysis shows that the Laplacian spectral moments and spectral radius are strongly constrained by local structural features of the network. On the other hand, we illustrate how local structural features are usually not enough to estimate the Laplacian spectral gap.\n        \u25b3 Less", "author": "George Verghese"}, {"abstract": "A ubiquitous building block of signaling pathways is a cycle of covalent modification (e.g., phosphorylation and dephosphorylation in MAPK cascades). Our paper explores the kind of information processing and filtering that can be accomplished by this simple biochemical circuit.\n  Signaling cycles are particularly known for exhibiting a highly sigmoidal (ultrasensitive) input-output characteristic in a certain steady-state regime. Here we systematically study the cycle's steady-state behavior and its response to time-varying stimuli. We demonstrate that the cycle can actually operate in four different regimes, each with its specific input-output characteristics. These results are obtained using the total quasi-steady-state approximation, which is more generally valid than the typically used Michaelis-Menten approximation for enzymatic reactions. We invoke experimental data that suggests the possibility of signaling cycles operating in one of the new regimes.\n  We then consider the cycle's dynamic behavior, which has so far been relatively neglected. We demonstrate that the intrinsic architecture of the cycles makes them act - in all four regimes - as tunable low-pass filters, filtering out high-frequency fluctuations or noise in signals and environmental cues. Moreover, the cutoff frequency can be adjusted by the cell. Numerical simulations show that our analytical results hold well even for noise of large amplitude. We suggest that noise filtering and tunability make signaling cycles versatile components of more elaborate cell signaling pathways.\n        \u25b3 Less", "author": "George Verghese"}, {"abstract": "We develop a sufficient condition for the least-squares measurement (LSM), or the square-root measurement, to minimize the probability of a detection error when distinguishing between a collection of mixed quantum states. Using this condition we derive the optimal measurement for state sets with a broad class of symmetries.\n  We first consider geometrically uniform (GU) state sets with a possibly nonabelian generating group, and show that if the generator satisfies a certain constraint, then the LSM is optimal. In particular, for pure-state GU ensembles the LSM is shown to be optimal. For arbitrary GU state sets we show that the optimal measurement operators are GU with generator that can be computed very efficiently in polynomial time, within any desired accuracy.\n  We then consider compound GU (CGU) state sets which consist of subsets that are GU. When the generators satisfy a certain constraint, the LSM is again optimal. For arbitrary CGU state sets the optimal measurement operators are shown to be CGU with generators that can be computed efficiently in polynomial time.\n        \u25b3 Less", "author": "George Verghese"}, {"abstract": "We consider the problem of designing an optimal quantum detector to minimize the probability of a detection error when distinguishing between a collection of quantum states, represented by a set of density operators. We show that the design of the optimal detector can be formulated as a semidefinite programming problem. Based on this formulation, we derive a set of necessary and sufficient conditions for an optimal quantum measurement. We then show that the optimal measurement can be found by solving a standard (convex) semidefinite program followed by the solution of a set of linear equations or, at worst, a standard linear programming problem. By exploiting the many well-known algorithms for solving semidefinite programs, which are guaranteed to converge to the global optimum, the optimal measurement can be computed very efficiently in polynomial time.\n  Using the semidefinite programming formulation, we also show that the rank of each optimal measurement operator is no larger than the rank of the corresponding density operator. In particular, if the quantum state ensemble is a pure-state ensemble consisting of (not necessarily independent) rank-one density operators, then we show that the optimal measurement is a pure-state measurement consisting of rank-one measurement operators.\n        \u25b3 Less", "author": "George Verghese"}, {"abstract": "Joel Voldman is a professor in the Electrical Engineering and Computer Science Department at MIT. Here he describes his labs efforts to develop microfluidic devices for cell manipulation and analysis.\n        \u25b3 Less", "author": "Joel Voldman"}, {"abstract": "The Simons Observatory (SO) is a new cosmic microwave background experiment being built on Cerro Toco in Chile, due to begin observations in the early 2020s. We describe the scientific goals of the experiment, motivate the design, and forecast its performance. SO will measure the temperature and polarization anisotropy of the cosmic microwave background in six frequency bands: 27, 39, 93, 145, 225 and 280 GHz. The initial configuration of SO will have three small-aperture 0.5-m telescopes (SATs) and one large-aperture 6-m telescope (LAT), with a total of 60,000 cryogenic bolometers. Our key science goals are to characterize the primordial perturbations, measure the number of relativistic species and the mass of neutrinos, test for deviations from a cosmological constant, improve our understanding of galaxy evolution, and constrain the duration of reionization. The SATs will target the largest angular scales observable from Chile, mapping ~10% of the sky to a white noise level of 2 $\u03bc$K-arcmin in combined 93 and 145 GHz bands, to measure the primordial tensor-to-scalar ratio, $r$, at a target level of $\u03c3(r)=0.003$. The LAT will map ~40% of the sky at arcminute angular resolution to an expected white noise level of 6 $\u03bc$K-arcmin in combined 93 and 145 GHz bands, overlapping with the majority of the LSST sky region and partially with DESI. With up to an order of magnitude lower polarization noise than maps from the Planck satellite, the high-resolution sky maps will constrain cosmological parameters derived from the damping tail, gravitational lensing of the microwave background, the primordial bispectrum, and the thermal and kinematic Sunyaev-Zel'dovich effects, and will aid in delensing the large-angle polarization signal to measure the tensor-to-scalar ratio. The survey will also provide a legacy catalog of 16,000 galaxy clusters and more than 20,000 extragalactic sources.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "We derive an analytical expression for the intermediate scattering function of a particle on a flat surface obeying the Generalised Langevin Equation, with exponential memory friction. Numerical simulations based on an extended phase space method confirm the analytical results. The simulated trajectories provide qualitative insight into the effect that introducing a finite memory timescale has on the analytical line shapes. The relative amplitude of the long-time exponential tail of the line shape is suppressed, but its decay rate is unchanged, reflecting the fact that the cutoff frequency of the exponential kernel affects short-time correlations but not the diffusion coefficient which is defined in terms of a long-time limit. The exponential sensitivity of the relative amplitudes to the decay time of the chosen memory kernel is a very strong indicator for the prospect of inferring a friction kernel and the physical insights from experimentally measured intermediate scattering functions.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "Forward top quark pair production is studied in $pp$ collisions in the $\u03bceb$ final state using a data sample corresponding to an integrated luminosity of 1.93 fb$^{-1}$ collected with the LHCb experiment at a centre-of-mass energy of 13 TeV. The cross-section is measured in a fiducial region where both leptons have a transverse momentum greater than 20 GeV and a pseudorapidity between 2.0 and 4.5. The quadrature sum of the azimuthal separation and the difference in pseudorapidities, denoted $\u0394R$, between the two leptons must be larger than 0.1. The $b$-jet axis is required to be separated from both leptons by a $\u0394R$ of 0.5, and to have a transverse momentum in excess of 20 GeV and a pseudorapidity between 2.2 and 4.2. The cross-section is measured to be $$\u03c3_{t\\bar{t}}= 126\\pm19\\,(\\mathrm{stat})\\pm16\\,(\\mathrm{syst})\\pm5\\,(\\mathrm{lumi})\\,\\,\\mathrm{ fb}$$ where the first uncertainty is statistical, the second is systematic, and the third is due to the luminosity determination. The measurement is compatible with the Standard Model prediction.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The ratios of the branching fractions of the decays $\u039b_{c}^{+} \\rightarrow p \u03c0^{-} \u03c0^{+}$, $\u039b_{c}^{+} \\rightarrow p K^{-} K^{+}$, and $\u039b_{c}^{+} \\rightarrow p \u03c0^{-} K^{+}$ with respect to the Cabibbo-favoured $\u039b_{c}^{+} \\rightarrow p K^{-} \u03c0^{+}$ decay are measured using proton-proton collision data collected with the LHCb experiment at a 7 TeV centre-of-mass energy and corresponding to an integrated luminosity of 1.0 fb$^{-1}$:\n  \\begin{align*} \\frac{\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p \u03c0^{-} \u03c0^{+})}{\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p K^{-} \u03c0^{+})} & = (7.44 \\pm 0.08 \\pm 0.18)\\,\\%, \\frac{\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p K^{-} K^{+})}{\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p K^{-} \u03c0^{+})} &= (1.70 \\pm 0.03 \\pm 0.03)\\,\\%, \\frac{\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p \u03c0^{-} K^{+})}{\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p K^{-} \u03c0^{+})} & = (0.165 \\pm 0.015 \\pm 0.005 )\\,\\%, \\end{align*} where the uncertainties are statistical and systematic, respectively. These results are the most precise measurements of these quantities to date. When multiplied by the world-average value for $\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p K^{-} \u03c0^{+})$, the corresponding branching fractions are \\begin{align*} \\mathcal{B}(\u039b_{c}^{+} \\rightarrow p \u03c0^{-} \u03c0^{+}) &= (4.72 \\pm 0.05 \\pm 0.11 \\pm 0.25) \\times 10^{-3}, \\mathcal{B}(\u039b_{c}^{+} \\rightarrow p K^{-} K^{+}) &= (1.08 \\pm 0.02 \\pm 0.02 \\pm 0.06) \\times 10^{-3}, \\mathcal{B}(\u039b_{c}^{+} \\rightarrow p \u03c0^{-} K^{+}) &= (1.04 \\pm 0.09 \\pm 0.03 \\pm 0.05) \\times 10^{-4}, \\end{align*} where the final uncertainty is due to $\\mathcal{B}(\u039b_{c}^{+} \\rightarrow p K^{-} \u03c0^{+})$.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "This white paper summarizes the workshop \"U.S. Cosmic Visions: New Ideas in Dark Matter\" held at University of Maryland on March 23-25, 2017.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "Optical tomographic reconstruction of a 3D nanoscale specimen is hindered by the axial diffraction limit, which is 2-3 times worse than the focal plane resolution. We propose and experimentally demonstrate an axial super-resolution evanescent wave tomography (AxSET) method that enables the use of regular evanescent wave microscopes like Total Internal Reflection Fluorescence Microscope (TIRF) beyond surface imaging, and achieve tomographic reconstruction with axial super-resolution. Our proposed method based on Fourier reconstruction achieves axial super-resolution by extracting information from multiple sets of three-dimensional fluorescence images when the sample is illuminated by an evanescent wave. We propose a procedure to extract super-resolution features from the incremental penetration of an evanescent wave and support our theory by 1D (along the optical axis) and 3D simulations. We validate our claims by experimentally demonstrating tomographic reconstruction of microtubules in HeLa cells with an axial resolution of $\\sim$130 nm. Our method does not require any additional optical components or sample preparation. The proposed method can be combined with focal plane super-resolution techniques like STORM and can also be adapted for THz and microwave near-field tomography.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The first observation of the rare decay$B^0_s \\to \u03c6\u03c0^+\u03c0^-$ and evidence for $B^0 \\to \u03c6\u03c0^+\u03c0^-$ are reported, using $pp$ collision data recorded by the LHCb detector at centre-of-mass energies $\\sqrt{s} = 7$ and 8~TeV, corresponding to an integrated luminosity of $3{\\mbox{\\,fb}^{-1}}$. The branching fractions in the $\u03c0^+\u03c0^-$ invariant mass range $400<m(\u03c0^+\u03c0^-)<1600{\\mathrm{\\,Me\\kern -0.1em V\\!/}c^2}$ are $[3.48\\pm 0.23\\pm 0.17\\pm 0.35]\\times 10^{-6}$ and $[1.82\\pm 0.25\\pm 0.41\\pm 0.14]\\times 10^{-7}$ for $B^0_s \\to \u03c6\u03c0^+\u03c0^-$ and $B^0 \\to \u03c6\u03c0^+\u03c0^-$ respectively, where the uncertainties are statistical, systematic and from the normalisation mode $B^0_s \\to \u03c6\u03c6$. A combined analysis of the $\u03c0^+\u03c0^-$ mass spectrum and the decay angles of the final-state particles identifies the exclusive decays $B^0_s \\to \u03c6f_0(980) $, $B_s^0 \\to \u03c6f_2(1270) $, and $B^0_s \\to \u03c6\u03c1^0$ with branching fractions of $[1.12\\pm 0.16^{+0.09}_{-0.08}\\pm 0.11]\\times 10^{-6}$, $[0.61\\pm 0.13^{+0.12}_{-0.05}\\pm 0.06]\\times 10^{-6}$ and $[2.7\\pm 0.7\\pm 0.2\\pm 0.2]\\times 10^{-7}$, respectively.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The production of $W$ and $Z$ bosons in association with jets is studied in the forward region of proton-proton collisions collected at a centre-of-mass energy of 8 TeV by the LHCb experiment, corresponding to an integrated luminosity of 1.98 $\\pm$ 0.02 fb$^{-1}$. The $W$ boson is identified using its decay to a muon and a neutrino, while the $Z$ boson is identified through its decay to a muon pair. Total cross-sections are measured and combined into charge ratios, asymmetries, and ratios of $W+$jet and $Z$+jet production cross-sections. Differential measurements are also performed as a function of both boson and jet kinematic variables. All results are in agreement with Standard Model predictions.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "We introduce a meshless method for solving both continuous and discrete variational formulations of a volume constrained, nonlocal diffusion problem. We use the discrete solution to approximate the continuous solution. Our method is nonconforming and uses a localized Lagrange basis that is constructed out of radial basis functions. By verifying that certain inf-sup conditions hold, we demonstrate that both the continuous and discrete problems are well-posed, and also present numerical and theoretical results for the convergence behavior of the method. The stiffness matrix is assembled by a special quadrature routine unique to the localized basis. Combining the quadrature method with the localized basis produces a well-conditioned, symmetric matrix. This then is used to find the discretized solution.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The evolution of the electronic properties of electron-doped (Sr{1-x}La{x})2IrO4 is experimentally explored as the doping limit of La is approached. As electrons are introduced, the electronic ground state transitions from a spin-orbit Mott phase into an electronically phase separated state, where long-range magnetic order vanishes beyond x = 0.02 and charge transport remains percolative up to the limit of La substitution (x~0.06). In particular, the electronic ground state remains inhomogeneous even beyond the collapse of the parent state's long-range antiferromagnetic order, while persistent short-range magnetism survives up to the highest La-substitution levels. Furthermore, as electrons are doped into Sr2IrO4, we observe the appearance of a low temperature magnetic glass-like state intermediate to the complete suppression of antiferromagnetic order. Universalities and differences in the electron-doped phase diagrams of single layer and bilayer Ruddlesden-Popper strontium iridates are discussed.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The electronic phase diagram of the weak spin-orbit Mott insulator (Sr(1-x)Lax)3Ir2O7 is determined via an exhaustive experimental study. Upon doping electrons via La substitution, an immediate collapse in resistivity occurs along with a narrow regime of nanoscale phase separation comprised of antiferromagnetic, insulating regions and paramagnetic, metallic puddles persisting until x~0.04. Continued electron doping results in an abrupt, first-order phase boundary where the Neel state is suppressed and a homogenous, correlated, metallic state appears with an enhanced spin susceptibility and local moments. As the metallic state is stabilized, a weak structural distortion develops and suggests a competing instability with the parent spin-orbit Mott state.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The results of experimental tests of a novel method for moving large (pyramid construction size) stone blocks by rolling them are presented. The method is implemented by tying 12 identical rods of appropriately chosen radius to the faces of the block forming a rough dodecagon prism. Experiments using a 1,000 kg block show that it can be moved across level open ground with a dynamic coefficient of friction of less than 0.06. This value is a factor of five lower than that obtained for dragging the block, and the best values reported for dragging by others, at 0.3. the results are more dramatic than those obtained on smaller scale experiments on a 29.6 kg block, also reported here. For full scale pyramid blocks, the wooden \"rods\" woudl need to be posts of order 30 cm in diameter, similar in size to those used as masts on ships in the Nile.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "We present a novel Galerkin method for solving partial differential equations on the sphere. The problem is discretized by a highly localized basis which is easily constructed. The stiffness matrix entries are computed by a recently developed quadrature formula unique to the localized bases we consider. We present error estimates and investigate the stability of the discrete stiffness matrix. Implementation and numerical experiments are discussed.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "We present the results of a search for gravitational waves associated with 223 gamma-ray bursts (GRBs) detected by the InterPlanetary Network (IPN) in 2005-2010 during LIGO's fifth and sixth science runs and Virgo's first, second and third science runs. The IPN satellites provide accurate times of the bursts and sky localizations that vary significantly from degree scale to hundreds of square degrees. We search for both a well-modeled binary coalescence signal, the favored progenitor model for short GRBs, and for generic, unmodeled gravitational wave bursts. Both searches use the event time and sky localization to improve the gravitational-wave search sensitivity as compared to corresponding all-time, all-sky searches. We find no evidence of a gravitational-wave signal associated with any of the IPN GRBs in the sample, nor do we find evidence for a population of weak gravitational-wave signals associated with the GRBs. For all IPN-detected GRBs, for which a sufficient duration of quality gravitational-wave data is available, we place lower bounds on the distance to the source in accordance with an optimistic assumption of gravitational-wave emission energy of $10^{-2}M_{\\odot}c^2$ at 150 Hz, and find a median of 13 Mpc. For the 27 short-hard GRBs we place 90% confidence exclusion distances to two source models: a binary neutron star coalescence, with a median distance of 12Mpc, or the coalescence of a neutron star and black hole, with a median distance of 22 Mpc. Finally, we combine this search with previously published results to provide a population statement for GRB searches in first-generation LIGO and Virgo gravitational-wave detectors, and a resulting examination of prospects for the advanced gravitational-wave detectors.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "We present possible observing scenarios for the Advanced LIGO, Advanced Virgo and KAGRA gravitational-wave detectors over the next decade, with the intention of providing information to the astronomy community to facilitate planning for multi-messenger astronomy with gravitational waves. We estimate the sensitivity of the network to transient gravitational-wave signals, and study the capability of the network to determine the sky location of the source. We report our findings for gravitational-wave transients, with particular focus on gravitational-wave signals from the inspiral of binary neutron star systems, which are the most promising targets for multi-messenger astronomy. The ability to localize the sources of the detected signals depends on the geographical distribution of the detectors and their relative sensitivity, and 90% credible regions can be as large as thousands of square degrees when only two sensitive detectors are operational. Determining the sky position of a significant fraction of detected signals to areas of 5-20 square degrees requires at least three detectors of sensitivity within a factor of ~2 of each other and with a broad frequency bandwidth. When all detectors, including KAGRA and the third LIGO detector in India, reach design sensitivity, a significant fraction of gravitational-wave signals will be localized to a few square degrees by gravitational-wave observations alone.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "We summarize the sensitivity achieved by the LIGO and Virgo gravitational wave detectors for compact binary coalescence (CBC) searches during LIGO's fifth science run and Virgo's first science run. We present noise spectral density curves for each of the four detectors that operated during these science runs which are representative of the typical performance achieved by the detectors for CBC searches. These spectra are intended for release to the public as a summary of detector performance for CBC searches during these science runs.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The TEDI (TripleSpec - Exoplanet Discovery Instrument) will be the first instrument fielded specifically for finding low-mass stellar companions. The instrument is a near infra-red interferometric spectrometer used as a radial velocimeter. TEDI joins Externally Dispersed Interferometery (EDI) with an efficient, medium-resolution, near IR (0.9 - 2.4 micron) echelle spectrometer, TripleSpec, at the Palomar 200\" telescope. We describe the instrument and its radial velocimetry demonstration program to observe cool stars.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "We report on the optical response of a suspended-mass detuned resonant sideband extraction (RSE) interferometer with power recycling. The purpose of the detuned RSE configuration is to manipulate and optimize the optical response of the interferometer to differential displacements (induced by gravitational waves) as a function of frequency, independently of other parameters of the interferometer. The design of our interferometer results in an optical gain with two peaks: an RSE optical resonance at around 4 kHz and a radiation pressure induced optical spring at around 41 Hz. We have developed a reliable procedure for acquiring lock and establishing the desired optical configuration. In this configuration, we have measured the optical response to differential displacement and found good agreement with predictions at both resonances and all other relevant frequencies. These results build confidence in both the theory and practical implementation of the more complex optical configuration being planned for Advanced LIGO.\n        \u25b3 Less", "author": "Stephen Ward"}, {"abstract": "The Cosmology Large Angular Scale Surveyor consists of four instruments performing a CMB polarization survey. Currently, the 40 GHz and first 90 GHz instruments are deployed and observing, with the second 90 GHz and a multichroic 150/220 GHz instrument to follow. The receiver is a central component of each instrument's design and functionality. This paper describes the CLASS receiver design, using the first 90 GHz receiver as a primary reference. Cryogenic cooling and filters maintain a cold, low-noise environment for the detectors. We have achieved receiver detector temperatures below 50 mK in the 40 GHz instrument for 85% of the initial 1.5 years of operation, and observed in-band efficiency that is consistent with pre-deployment estimates. At 90 GHz, less than 26% of in-band power is lost to the filters and lenses in the receiver, allowing for high optical efficiency. We discuss the mounting scheme for the filters and lenses, the alignment of the cold optics and detectors, stray light control, and magnetic shielding.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "The Cosmology Large Angular Scale Surveyor (CLASS) aims to detect and characterize the primordial B-mode signal and make a sample-variance-limited measurement of the optical depth to reionization. CLASS is a ground-based, multi-frequency microwave polarimeter that surveys 70% of the microwave sky every day from the Atacama Desert. The focal plane detector arrays of all CLASS telescopes contain smooth-walled feedhorns that couple to transition-edge sensor (TES) bolometers through symmetric planar orthomode transducer (OMT) antennas. These low noise polarization-sensitive detector arrays are fabricated on mono-crystalline silicon wafers to maintain TES uniformity and optimize optical efficiency throughout the wafer. In this paper, we discuss the design and characterization of the first CLASS 93 GHz detector array. We measure the dark parameters, bandpass, and noise spectra of the detectors and report that the detectors are photon-noise limited. With current array yield of 82%, we estimate the total array noise-equivalent power (NEP) to be 2.1 aW$\\sqrt[]{\\mathrm{s}}$.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "The search for inflationary primordial gravitational waves and the measurement of the optical depth to reionization, both through their imprint on the large angular scale correlations in the polarization of the cosmic microwave background (CMB), has created the need for high sensitivity measurements of polarization across large fractions of the sky at millimeter wavelengths. These measurements are subject to instrumental and atmospheric $1/f$ noise, which has motivated the development of polarization modulators to facilitate the rejection of these large systematic effects.\n  Variable-delay polarization modulators (VPMs) are used in the Cosmology Large Angular Scale Surveyor (CLASS) telescopes as the first element in the optical chain to rapidly modulate the incoming polarization. VPMs consist of a linearly polarizing wire grid in front of a movable flat mirror. Varying the distance between the grid and the mirror produces a changing phase shift between polarization states parallel and perpendicular to the grid which modulates Stokes U (linear polarization at $45^\\circ$) and Stokes V (circular polarization). The CLASS telescopes have VPMs as the first optical element from the sky; this simultaneously allows a lock-in style polarization measurement and the separation of sky polarization from any instrumental polarization further along in the optical path. The Q-band CLASS VPM was the first VPM to begin observing the CMB full time, starting in the Spring of 2016. The first W-band CLASS VPM was installed in the Spring of 2018.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "Machine learning is an important research area in particle physics, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas in machine learning in particle physics with a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "We describe the Spectroscopic Time-Resolving Observatory for Broadband Energy X-rays (STROBE-X), a probe-class mission concept that will provide an unprecedented view of the X-ray sky, performing timing and spectroscopy over both a broad energy band (0.2-30 keV) and a wide range of timescales from microseconds to years. STROBE-X comprises two narrow-field instruments and a wide field monitor. The soft or low-energy band (0.2-12 keV) is covered by an array of lightweight optics (3-m focal length) that concentrate incident photons onto small solid-state detectors with CCD-level (85-175 eV) energy resolution, 100 ns time resolution, and low background rates. This technology has been fully developed for NICER and will be scaled up to take advantage of the longer focal length of STROBE-X. The higher-energy band (2-30 keV) is covered by large-area, collimated silicon drift detectors that were developed for the European LOFT mission concept. Each instrument will provide an order of magnitude improvement in effective area over its predecessor (NICER in the soft band and RXTE in the hard band). Finally, STROBE-X offers a sensitive wide-field monitor (WFM), both to act as a trigger for pointed observations of X-ray transients and also to provide high duty-cycle, high time-resolution, and high spectral-resolution monitoring of the variable X-ray sky. The WFM will boast approximately 20 times the sensitivity of the RXTE All-Sky Monitor, enabling multi-wavelength and multi-messenger investigations with a large instantaneous field of view. This mission concept will be presented to the 2020 Decadal Survey for consideration.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "At the heart of experimental high energy physics (HEP) is the development of facilities and instrumentation that provide sensitivity to new phenomena. Our understanding of nature at its most fundamental level is advanced through the analysis and interpretation of data from sophisticated detectors in HEP experiments. The goal of data analysis systems is to realize the maximum possible scientific potential of the data within the constraints of computing and human resources in the least time. To achieve this goal, future analysis systems should empower physicists to access the data with a high level of interactivity, reproducibility and throughput capability. As part of the HEP Software Foundation Community White Paper process, a working group on Data Analysis and Interpretation was formed to assess the challenges and opportunities in HEP data analysis and develop a roadmap for activities in this area over the next decade. In this report, the key findings and recommendations of the Data Analysis and Interpretation Working Group are presented.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "During the shutdown of the CERN Large Hadron Collider in 2013-2014, an additional pixel layer was installed between the existing Pixel detector of the ATLAS experiment and a new, smaller radius beam pipe. The motivation for this new pixel layer, the Insertable B-Layer (IBL), was to maintain or improve the robustness and performance of the ATLAS tracking system, given the higher instantaneous and integrated luminosities realised following the shutdown. Because of the extreme radiation and collision rate environment, several new radiation-tolerant sensor and electronic technologies were utilised for this layer. This paper reports on the IBL construction and integration prior to its operation in the ATLAS detector.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "Particle physics has an ambitious and broad experimental programme for the coming decades. This programme requires large investments in detector hardware, either to build new facilities and experiments, or to upgrade existing ones. Similarly, it requires commensurate investment in the R&D of software to acquire, manage, process, and analyse the shear amounts of data to be recorded. In planning for the HL-LHC in particular, it is critical that all of the collaborating stakeholders agree on the software goals and priorities, and that the efforts complement each other. In this spirit, this white paper describes the R&D activities required to prepare for this software upgrade.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "In this paper, it is shown that the underlying differential structure of an orbit space by an effective linear circle action contains enough invariants so that it can tell different such actions apart. As a consequence, it is shown that the same result holds for general smooth circle actions on manifolds, up to an integer label of 0 or 1 on codimension-1 strata.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "The change in electrical resistance associated with the application of an external magnetic field is known as the magnetoresistance (MR). The measured MR is quite complex in the class of connected networks of single-domain ferromagnetic nanowires, known as \"artificial spin ice\", due to the geometrically-induced collective behavior of the nanowire moments. We have conducted a thorough experimental study of the MR of a connected honeycomb artificial spin ice, and we present a simulation methodology for understanding the detailed behavior of this complex correlated magnetic system. Our results demonstrate that the behavior, even at low magnetic fields, can be well-described only by including significant contributions from the vertices at which the legs meet, opening the door to new geometrically-induced MR phenomena.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "The reactions $\u03b3p\\to \u03b7p$ and $\u03b3p\\to \u03b7' p$ have been measured from their thresholds up to the center-of-mass energy $W=1.96$GeV with the tagged-photon facilities at the Mainz Microtron, MAMI. Differential cross sections were obtained with unprecedented accuracy, providing fine energy binning and full production-angle coverage. A strong cusp is observed in the total cross section and excitation functions for $\u03b7$ photoproduction at the energies in vicinity of the $\u03b7'$ threshold, $W=1896$MeV ($E_\u03b3=1447$MeV). This behavior is explained in a revised $\u03b7$MAID isobar model by a significant branching of the $N(1895)1/2^-$ nucleon resonance to both, $\u03b7p$ and $\u03b7' p$, confirming the existence and constraining the properties of this poorly known state.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "The Cosmology Large Angular Scale Surveyor (CLASS) is a four telescope array designed to characterize relic primordial gravitational waves from inflation and the optical depth to reionization through a measurement of the polarized cosmic microwave background (CMB) on the largest angular scales. The frequencies of the four CLASS telescopes, one at 38 GHz, two at 93 GHz, and one dichroic system at 145/217 GHz, are chosen to avoid spectral regions of high atmospheric emission and span the minimum of the polarized Galactic foregrounds: synchrotron emission at lower frequencies and dust emission at higher frequencies. Low-noise transition edge sensor detectors and a rapid front-end polarization modulator provide a unique combination of high sensitivity, stability, and control of systematics. The CLASS site, at 5200 m in the Chilean Atacama desert, allows for daily mapping of up to 70\\% of the sky and enables the characterization of CMB polarization at the largest angular scales. Using this combination of a broad frequency range, large sky coverage, control over systematics, and high sensitivity, CLASS will observe the reionization and recombination peaks of the CMB E- and B-mode power spectra. CLASS will make a cosmic variance limited measurement of the optical depth to reionization and will measure or place upper limits on the tensor-to-scalar ratio, $r$, down to a level of 0.01 (95\\% C.L.).\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "LOFT-P is a concept for a NASA Astrophysics Probe-Class (<$1B) X-ray timing mission, based on the LOFT concept originally proposed to ESAs M3 and M4 calls. LOFT-P requires very large collecting area (>6 m^2, >10x RXTE), high time resolution, good spectral resolution, broad-band spectral coverage (2-30 keV), highly flexible scheduling, and an ability to detect and respond promptly to time-critical targets of opportunity. It addresses science questions such as: What is the equation of state of ultra dense matter? What are the effects of strong gravity on matter spiraling into black holes? It would be optimized for sub-millisecond timing to study phenomena at the natural timescales of neutron star surfaces and black hole event horizons and to measure mass and spin of black holes. These measurements are synergistic to imaging and high-resolution spectroscopy instruments, addressing much smaller distance scales than are possible without very long baseline X-ray interferometry, and using complementary techniques to address the geometry and dynamics of emission regions. A sky monitor (2-50 keV) acts as a trigger for pointed observations, providing high duty cycle, high time resolution monitoring of the X-ray sky with ~20 times the sensitivity of the RXTE All-Sky Monitor, enabling multi-wavelength and multi-messenger studies. A probe-class mission concept would employ lightweight collimator technology and large-area solid-state detectors, technologies which have been recently greatly advanced during the ESA M3 study. Given the large community interested in LOFT (>800 supporters, the scientific productivity of this mission is expected to be very high, similar to or greater than RXTE (~2000 refereed publications). We describe the results of a study, recently completed by the MSFC Advanced Concepts Office, that demonstrates that LOFT-P is feasible within a NASA probe-class mission budget.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "False discovery rate (FDR) procedures provide misleading inference when testing multiple null hypotheses with heterogeneous multinomial data. For example, in the motivating study the goal is to identify species of bacteria near the roots of wheat plants (rhizobacteria) that are associated with productivity, but standard procedures discover the most abundant species even when the association is weak or negligible, and fail to discover strong associations when species are not abundant. Consequently, a list of abundant species is produced by the multiple testing procedure even though the goal was to provide a list of producitivity-associated species. This paper provides an FDR method based on a mixture of multinomial distributions and shows that it tends to discover more non-negligible effects and fewer negligible effects when the data are heterogeneous across tests. The proposed method and competing methods are applied to the motivating data. The new method identifies more species that are strongly associated with productivity and identifies fewer species that are weakly associated with productivity.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "We present a search for ten baryon-number violating decay modes of $\u039b$ hyperons using the CLAS detector at Jefferson Laboratory. Nine of these decay modes result in a single meson and single lepton in the final state ($\u039b\\rightarrow m \\ell$) and conserve either the sum or the difference of baryon and lepton number ($B \\pm L$). The tenth decay mode ($\u039b\\rightarrow \\bar{p}\u03c0^+$) represents a difference in baryon number of two units and no difference in lepton number. We observe no significant signal and set upper limits on the branching fractions of these reactions in the range $(4-200)\\times 10^{-7}$ at the $90\\%$ confidence level.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "Optimal control theory is a powerful tool for improving figures of merit in quantum information tasks. Finding the solution to any optimal control problem via numerical optimization depends crucially on the choice of the optimization functional. Here, we derive a functional that targets the full set of two-qubit perfect entanglers, gates capable of creating a maximally-entangled state out of some initial product state. The functional depends on easily-computable local invariants and uniquely determines when a gate evolves into a perfect entangler. Optimization with our functional is most useful if the two-qubit dynamics allows for the implementation of more than one perfect entangler. We discuss the reachable set of perfect entanglers for a generic Hamiltonian that corresponds to several quantum information platforms of current interest.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "Light carrying orbital angular momentum (OAM) has potential to impact a wide variety of applications ranging from optical communications to quantum information and optical forces for the excitation and manipulation of atoms, molecules, and micro-particles. The unique advantage of utilizing OAM in these applications relies, to a large extent, on the use of multiple different OAM states. Therefore, it is desirable to have a device that is able to gen- erate light with freely adjustable OAM states in an integrated form for large- scale integration. We propose and demonstrate a compact silicon photonic integrated circuit to generate a free-space optical beam with OAM state con- tinuously tuned from a single electrical input signal, realizing both integer and non-integer OAM states. The compactness and flexibility of the device and its compatibility with complementary metal-oxide-semiconductor (CMOS) pro- cessing hold promise for integration with other silicon photonic components for wide-ranging applications.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "Silicon photonics has emerged as the leading candidate for implementing ultralow power wavelength division multiplexed communication networks in high-performance computers, yet current components (lasers, modulators, filters, and detectors) consume too much power for the femtojouleclass links that will ultimately be required. Here, we propose, demonstrate, and characterize the first modulator to achieve simultaneous high-speed (25-Gb/s), low voltage (0.5VPP) and efficient 1-fJ/bit error-free operation while maintaining athermal operation. Both the low energy and athermal operation were enabled by a record free-carrier accumulation/depletion response obtained in a vertical p-n junction device that at 250-pm/V (30-GHz/V) is up to ten times larger than prior demonstrations. Over a 7.5\u00b0C temperature range, the massive electro-optic response was used to compensate for thermal drift without increasing energy consumption and over a 10\u00b0C temperature range, increasing energy consumption by only 2-fJ/bit. The results represent a new paradigm in modulator development, one where thermal compensation is achieved electro-optically.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "We report the first extraction of the pion-nucleon multipoles near the production threshold for the $n\u03c0^+$ channel at relatively high momentum transfer ($Q^2$ up to 4.2 $\\rm{GeV^2}$). The dominance of the s-wave transverse multipole ($E_{0+}$), expected in this region, allowed us to access the generalized form factor $G_1$ within the light-cone sum rule (LCSR) framework as well as the axial form factor $G_A$. The data analyzed in this work were collected by the nearly $4\u03c0$ CEBAF Large Acceptance Spectrometer (CLAS) using a 5.754 $\\rm{GeV}$ electron beam on a proton target. The differential cross section and the $\u03c0-N$-multipole $E_{0+}/G_D$ were measured using two different methods, the LCSR and a direct multipole fit. The results from the two methods are found to be consistent and almost $Q^2$ independent.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "We report on the first measurement of the F2 structure function of the neutron from semi-inclusive scattering of electrons from deuterium, with low-momentum protons detected in the backward hemisphere. Restricting the momentum of the spectator protons to < 100 MeV and their angles to < 100 degrees relative to the momentum transfer allows an interpretation of the process in terms of scattering from nearly on-shell neutrons. The F2n data collected cover the nucleon resonance and deep-inelastic regions over a wide range of Bjorken x for 0.65 < Q2 < 4.52 GeV2, with uncertainties from nuclear corrections estimated to be less than a few percent. These measurements provide the first determination of the neutron to proton structure function ratio F2n/F2p at 0.2 < x < 0.8 with little uncertainty due to nuclear effects.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "Swift/BAT detected the first burst from 1E 1841-045 in May 2010 with intermittent burst activity recorded through at least July 2011. Here we present Swift and Fermi/GBM observations of this burst activity and search for correlated changes to the persistent X-ray emission of the source. The T90 durations of the bursts range between 18-140 ms, comparable to other magnetar burst durations, while the energy released in each burst ranges between (0.8 - 25)E38 erg, which is in the low side of SGR bursts. We find that the bursting activity did not have a significant effect on the persistent flux level of the source. We argue that the mechanism leading to this sporadic burst activity in 1E 1841-045 might not involve large scale restructuring (either crustal or magnetospheric) as seen in other magnetar sources.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "Cross sections for the ^{3}He(e,e'pn)p reaction were measured for the first time at energy transfers of 220 and 270 MeV for several momentum transfers ranging from 300 to 450 MeV/c. Cross sections are presented as a function of the momentum of the recoil proton and the momentum transfer. Continuum Faddeev calculations using the Argonne V18 and Bonn-B nucleon-nucleon potentials overestimate the measured cross sections by a factor 5 at low recoil proton momentum with the discrepancy becoming much smaller at higher recoil momentum.\n        \u25b3 Less", "author": "Michael Watts"}, {"abstract": "End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "In this paper, we present a novel system that separates the voice of a target speaker from multi-speaker signals, by making use of a reference signal from the target speaker. We achieve this by training two separate neural networks: (1) A speaker recognition network that produces speaker-discriminative embeddings; (2) A spectrogram masking network that takes both noisy spectrogram and speaker embedding as input, and produces a mask. Our system significantly reduces the speech recognition WER on multi-speaker signals, with minimal WER degradation on single-speaker signals.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Signal Temporal Logic (STL) is a formal language for describing a broad range of real-valued, temporal properties in cyber-physical systems. While there has been extensive research on verification and control synthesis from STL requirements, there is no formal framework for comparing two STL formulae. In this paper, we show that under mild assumptions, STL formulae admit a metric space. We propose two metrics over this space based on i) the Pompeiu-Hausdorff distance and ii) the symmetric difference measure, and present algorithms to compute them. Alongside illustrative examples, we present applications of these metrics for two fundamental problems: a) design quality measures: to compare all the temporal behaviors of a designed system, such as a synthetic genetic circuit, with the \"desired\" specification, and b) loss functions: to quantify errors in Temporal Logic Inference (TLI) as a first step to establish formal performance guarantees of TLI algorithms.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Texture synthesis techniques based on matching the Gram matrix of feature activations in neural networks have achieved spectacular success in the image domain. In this paper we extend these techniques to the audio domain. We demonstrate that synthesizing diverse audio textures is challenging, and argue that this is because audio data is relatively low-dimensional. We therefore introduce two new terms to the original Grammian loss: an autocorrelation term that preserves rhythm, and a diversity term that encourages the optimization procedure to synthesize unique textures. We quantitatively study the impact of our design choices on the quality of the synthesized audio by introducing an audio analogue to the Inception loss which we term the VGGish loss. We show that there is a trade-off between the diversity and quality of the synthesized audio using this technique. We additionally perform a number of experiments to qualitatively study how these design choices impact the quality of the synthesized audio. Finally we describe the implications of these results for the problem of audio style transfer.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Properly designed colloidal semiconductor quantum dots (QDs) have already been shown to exhibit high sensitivity to external electric fields via the quantum confined Stark effect (QCSE). Yet, detection of the characteristic spectral shifts associated with the effect of QCSE has traditionally been painstakingly slow, dramatically limiting the sensitivity of these QD sensors to fast transients. We experimentally demonstrate a new detection scheme designed at achieving shot-noise limited sensitivity to emission wavelength shifts in QDs, showing feasibility for their use as local electric field sensors on the millisecond time scale. This regime of operation is already potentially suitable for detection of single action potentials in neurons at a high spatial resolution.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Inspired by recent work on neural network image generation which rely on backpropagation towards the network inputs, we present a proof-of-concept system for speech texture synthesis and voice conversion based on two mechanisms: approximate inversion of the representation learned by a speech recognition neural network, and on matching statistics of neuron activations between different source and target utterances. Similar to image texture synthesis and neural style transfer, the system works by optimizing a cost function with respect to the input waveform samples. To this end we use a differentiable mel-filterbank feature extraction pipeline and train a convolutional CTC speech recognition network. Our system is able to extract speaker characteristics from very limited amounts of target speaker data, as little as a few seconds, and can be used to generate realistic speech babble or reconstruct an utterance in a different voice.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to 5% for the conventional system.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Supervised (linear) embedding models like Wsabie and PSI have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration's features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn to model user's preferences over items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query x user x item tensor for training instead of the more traditional user x item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user's profile. We introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user. We report empirical results where it outperforms several baselines.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "In the last ten years, the field of proteomics has expanded at a rapid rate. A range of exciting new technology has been developed and enthusiastically applied to an enormous variety of biological questions. However, the degree of stringency required in proteomic data generation and analysis appears to have been underestimated. As a result, there are likely to be numerous published findings that are of questionable quality, requiring further confirmation and/or validation. This manuscript outlines a number of key issues in proteomic research, including those associated with experimental design, differential display and biomarker discovery, protein identification and analytical incompleteness. In an effort to set a standard that reflects current thinking on the necessary and desirable characteristics of publishable manuscripts in the field, a minimal set of guidelines for proteomics research is then described. These guidelines will serve as a set of criteria which editors of PROTEOMICS will use for assessment of future submissions to the Journal.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "Using the Australia Telescope Compact Array we have detected CO(1-0) and CO(5-4) from TNJ0924-2201 at z=5.2, the most distant radio galaxy known to date. This is the second highest redshift detection of CO published so far. The CO(1-0) is 250-400 km/sec wide with a peak flux density of 520 +- 115 microJy/beam whilst the CO(5-4) line emission is 200-300 km/sec wide with a peak flux density of 7.8 +- 2.7 mJy/beam. Both transitions are spatially unresolved but there is marginal evidence for spatial offsets between the CO and the host galaxy; the CO(1-0) is located 28 +- 11 kpc (4.5 +- 1.7 arcsec) north of the radio galaxy whilst the CO(5-4) is located 18 +- 8 kpc (2.8 +- 1.2 arcsec) south of the radio galaxy. Higher spatial resolution observations are required to determine the reality of these offsets. Our result is the second detection of CO in a high redshift galaxy without pre-selection based on a massive dust content.\n        \u25b3 Less", "author": "Ron Weiss"}, {"abstract": "The recent discovery by Advanced LIGO and Advanced Virgo of a gravitational wave signal from a binary neutron star inspiral has enabled tests of general relativity (GR) with this new type of source. This source, for the first time, permits tests of strong-field dynamics of compact binaries in presence of matter. In this paper, we place constraints on the dipole radiation and possible deviations from GR in the post-Newtonian coefficients that govern the inspiral regime. Bounds on modified dispersion of gravitational waves are obtained; in combination with information from the observed electromagnetic counterpart we can also constrain effects due to large extra dimensions. Finally, the polarization content of the gravitational wave signal is studied. The results of all tests performed here show good agreement with GR.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Astrophysical sources of gravitational waves, such as binary neutron star and black hole mergers or core-collapse supernovae, can drive relativistic outflows, giving rise to non-thermal high-energy emission. High-energy neutrinos are signatures of such outflows. The detection of gravitational waves and high-energy neutrinos from common sources could help establish the connection between the dynamics of the progenitor and the properties of the outflow. We searched for associated emission of gravitational waves and high-energy neutrinos from astrophysical transients with minimal assumptions using data from Advanced LIGO from its first observing run O1, and data from the ANTARES and IceCube neutrino observatories from the same time period. We focused on candidate events whose astrophysical origin could not be determined from a single messenger. We found no significant coincident candidate, which we used to constrain the rate density of astrophysical sources dependent on their gravitational wave and neutrino emission processes.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present a search for prompt gamma-ray counterparts to compact binary coalescence gravitational wave (GW) candidates from Advanced LIGO's first observing run (O1). As demonstrated by the multimessenger observations of GW170817/GRB 170817A, electromagnetic and GW observations provide complementary information about the astrophysical source and, in the case of weaker candidates, may strengthen the case for an astrophysical origin. Here we investigate low-significance GW candidates from the O1 compact-binary coalescence searches using the Fermi Gamma-ray Burst Monitor (GBM), leveraging its all-sky and broad energy coverage. Candidates are ranked and compared to background to measure significance. Those with false alarm rates of less than 10^-5 Hz (about one per day) are used as the search sample for gamma-ray follow-up. No GW candidates were found to be coincident with gamma-ray transients independently identified by blind searches of the GBM data. In addition, GW candidate event times were followed up by a separate targeted search of GBM data. Among the resulting GBM events, the two with lowest false alarm rates were the gamma-ray transient GW150914-GBM presented in Connaughton et al. (2016) and a solar flare in chance coincidence with a GW candidate.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "One unanswered question about the binary neutron star coalescence GW170817 is the nature of its post-merger remnant. A previous search for post-merger gravitational waves targeted high-frequency signals from a possible neutron star remnant with a maximum signal duration of 500 s. Here we revisit the neutron star remnant scenario with a focus on longer signal durations up until the end of the Second Advanced LIGO-Virgo Observing run, 8.5 days after the coalescence of GW170817. The main physical scenario for such emission is the power-law spindown of a massive magnetar-like remnant. We use four independent search algorithms with varying degrees of restrictiveness on the signal waveformand different ways of dealing with noise artefacts. In agreement with theoretical estimates, we find no significant signal candidates. Through simulated signals, we quantify that with the current detector sensitivity, nowhere in the studied parameter space are we sensitive to a signal from more than 1 Mpc away, compared to the actual distance of 40 Mpc. This study however serves as a prototype for post-merger analyses in future observing runs with expected higher sensitivity.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We analyze the impact of a proposed tidal instability coupling $p$-modes and $g$-modes within neutron stars on GW170817. This non-resonant instability transfers energy from the orbit of the binary to internal modes of the stars, accelerating the gravitational-wave driven inspiral. We model the impact of this instability on the phasing of the gravitational wave signal using three parameters per star: an overall amplitude, a saturation frequency, and a spectral index. Incorporating these additional parameters, we compute the Bayes Factor ($\\ln B^{pg}_{!pg}$) comparing our $p$-$g$ model to a standard one. We find that the observed signal is consistent with waveform models that neglect $p$-$g$ effects, with $\\ln B^{pg}_{!pg} = 0.03^{+0.70}_{-0.58}$ (maximum a posteriori and 90% credible region). By injecting simulated signals that do not include $p$-$g$ effects and recovering them with the $p$-$g$ model, we show that there is a $\\simeq 50\\%$ probability of obtaining similar $\\ln B^{pg}_{!pg}$ even when $p$-$g$ effects are absent. We find that the $p$-$g$ amplitude for 1.4 $M_\\odot$ neutron stars is constrained to $\\lesssim \\text{few}\\times10^{-7}$, with maxima a posteriori near $\\sim 10^{-7}$ and $p$-$g$ saturation frequency $\\sim 70\\, \\mathrm{Hz}$. This suggests that there are less than a few hundred excited modes, assuming they all saturate by wave breaking. For comparison, theoretical upper bounds suggest a $p$-$g$ amplitude $\\lesssim 10^{-6}$ and $\\lesssim 10^{3}$ modes saturating by wave breaking. Thus, the measured constraints only rule out extreme values of the $p$-$g$ parameters. They also imply that the instability dissipates $\\lesssim 10^{51}\\, \\mathrm{ergs}$ over the entire inspiral, i.e., less than a few percent of the energy radiated as gravitational waves.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present the first Advanced LIGO and Advanced Virgo search for ultracompact binary systems with component masses between 0.2 $M_\\odot$ - 1.0 $M_\\odot$ using data taken between September 12, 2015 and January 19, 2016. We find no viable gravitational wave candidates. Our null result constrains the coalescence rate of monochromatic (delta function) distributions of non-spinning (0.2 $M_\\odot$, 0.2 $M_\\odot$) ultracompact binaries to be less than $1.0 \\times 10^6 \\text{Gpc}^{-3} \\text{yr}^{-1}$ and the coalescence rate of a similar distribution of (1.0 $M_\\odot$, 1.0 $M_\\odot$) ultracompact binaries to be less than $1.9 \\times 10^4 \\text{Gpc}^{-3} \\text{yr}^{-1}$ (at 90 percent confidence). Neither black holes nor neutron stars are expected to form below ~ 1 solar mass through conventional stellar evolution, though it has been proposed that similarly low mass black holes could be formed primordially through density fluctuations in the early universe. Under a particular primordial black hole binary formation scenario, we constrain monochromatic primordial black hole populations of 0.2 $M_\\odot$ to be less than $33\\%$ of the total dark matter density and monochromatic populations of 1.0 $M_\\odot$ to be less than $5\\%$ of the dark matter density. The latter strengthens the presently placed bounds from micro-lensing surveys of MAssive Compact Halo Objects (MACHOs) provided by the MACHO and EROS collaborations.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We report on the measurement of the $^{7}$Be($n, p$)$^{7}$Li cross section from thermal to approximately 325 keV neutron energy, performed in the high-flux experimental area (EAR2) of the n\\_TOF facility at CERN. This reaction plays a key role in the lithium yield of the Big Bang Nucleosynthesis (BBN) for standard cosmology. The only two previous time-of-flight measurements performed on this reaction did not cover the energy window of interest for BBN, and showed a large discrepancy between each other. The measurement was performed with a Si-telescope, and a high-purity sample produced by implantation of a $^{7}$Be ion beam at the ISOLDE facility at CERN. While a significantly higher cross section is found at low-energy, relative to current evaluations, in the region of BBN interest the present results are consistent with the values inferred from the time-reversal $^{7}$Li($p, n$)$^{7}$Be reaction, thus yielding only a relatively minor improvement on the so-called Cosmological Lithium Problem (CLiP). The relevance of these results on the near-threshold neutron production in the p+$^{7}$Li reaction is also discussed.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The Advanced LIGO detectors have recently completed their second observation run successfully. The run lasted for approximately 10 months and lead to multiple new discoveries. The sensitivity to gravitational waves was partially limited by correlated noise. Here, we utilize auxiliary sensors that witness these correlated noise sources, and use them for noise subtraction in the time domain data. This noise and line removal is particularly significant for the LIGO Hanford Observatory, where the improvement in sensitivity is greater than 20%. Consequently, we were also able to improve the astrophysical estimation for the location, masses, spins and orbital parameters of the gravitational wave progenitors.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "On 17 August 2017, the LIGO and Virgo observatories made the first direct detection of gravitational waves from the coalescence of a neutron star binary system. The detection of this gravitational-wave signal, GW170817, offers a novel opportunity to directly probe the properties of matter at the extreme conditions found in the interior of these stars. The initial, minimal-assumption analysis of the LIGO and Virgo data placed constraints on the tidal effects of the coalescing bodies, which were then translated to constraints on neutron star radii. Here, we expand upon previous analyses by working under the hypothesis that both bodies were neutron stars that are described by the same equation of state and have spins within the range observed in Galactic binary neutron stars. Our analysis employs two methods: the use of equation-of-state-insensitive relations between various macroscopic properties of the neutron stars and the use of an efficient parametrization of the defining function $p(\u03c1)$ of the equation of state itself. From the LIGO and Virgo data alone and the first method, we measure the two neutron star radii as $R_1=10.8^{+2.0}_{-1.7}$ km for the heavier star and $R_2= 10.7^{+2.1}_{-1.5}$ km for the lighter star at the 90% credible level. If we additionally require that the equation of state supports neutron stars with masses larger than $1.97 \\,M_\\odot$ as required from electromagnetic observations and employ the equation-of-state parametrization, we further constrain $R_1= 11.9^{+1.4}_{-1.4}$ km and $R_2= 11.9^{+1.4}_{-1.4}$ km at the 90% credible level. Finally, we obtain constraints on $p(\u03c1)$ at supranuclear densities, with pressure at twice nuclear saturation density measured at $3.5^{+2.7}_{-1.7}\\times 10^{34} \\,\\mathrm{dyn}/\\mathrm{cm}^{2}$ at the 90% level.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "On August 17, 2017, the Advanced LIGO and Advanced Virgo gravitational-wave detectors observed a low-mass compact binary inspiral. The initial sky localization of the source of the gravitational-wave signal, GW170817, allowed electromagnetic observatories to identify NGC 4993 as the host galaxy. In this work we improve initial estimates of the binary's properties, including component masses, spins, and tidal parameters, using the known source location, improved modeling, and re-calibrated Virgo data. We extend the range of gravitational-wave frequencies considered down to 23 Hz, compared to 30 Hz in the initial analysis. We also compare results inferred using several signal models, which are more accurate and incorporate additional physical effects as compared to the initial analysis. We improve the localization of the gravitational-wave source to a 90% credible region of $16~\\mathrm{deg}^2$. We find tighter constraints on the masses, spins, and tidal parameters, and continue to find no evidence for non-zero component spins. The component masses are inferred to lie between 1.00 and 1.89 $M_\\odot$ when allowing for large component spins, and to lie between 1.16 and 1.60 $M_\\odot$ (with a total mass $2.73^{+0.04}_{-0.01} \\, M_\\odot$) when the spins are restricted to be within the range observed in Galactic binary neutron stars. Under minimal assumptions about the nature of the compact objects, our constraints for the tidal deformability parameter $\\tilde \u039b$ are $(0,630)$ when we allow for large component spins, and $300^{+420}_{-230}$ (using a 90% highest posterior density interval) when restricting the magnitude of the component spins, ruling out several equation of state models at the 90% credible level. Finally, with LIGO and GEO600 data, we use a Bayesian analysis to place upper limits on the amplitude and spectral energy density of a possible post-merger signal. (Abridged)\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Neutron capture measurements on $^{155}$Gd and $^{157}$Gd were performed using the time-of-flight technique at the n\\_TOF facility at CERN. Four samples in form of self-sustaining metallic discs isotopically enriched in $^{155}$Gd and $^{157}$Gd were used. The measurements were carried out at the experimental area (EAR1) at 185 m from the neutron source, with an array of 4 C$_6$D$_6$ liquid scintillation detectors.\n  The capture cross sections of $^{155}$Gd and $^{157}$Gd at neutron kinetic energy of 0.0253 eV have been estimated to be 62.2(2.2) kb and 239.8(9.3) kb, respectively, thus up to 6\\% different relative to the ones reported in the nuclear data libraries. A resonance shape analysis has been performed in the resolved resonance region up to 180 eV and 300 eV, respectively, in average resonance parameters have been found in good agreement with evaluations. Above these energies the observed resonance-like structures in the cross section have been tentatively characterised in terms of resonance energy and area up to 1 keV.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The detection of gravitational waves with Advanced LIGO and Advanced Virgo has enabled novel tests of general relativity, including direct study of the polarization of gravitational waves. While general relativity allows for only two tensor gravitational-wave polarizations, general metric theories can additionally predict two vector and two scalar polarizations. The polarization of gravitational waves is encoded in the spectral shape of the stochastic gravitational-wave background, formed by the superposition of cosmological and individually-unresolved astrophysical sources. Using data recorded by Advanced LIGO during its first observing run, we search for a stochastic background of generically-polarized gravitational waves. We find no evidence for a background of any polarization, and place the first direct bounds on the contributions of vector and scalar polarizations to the stochastic background. Under log-uniform priors for the energy in each polarization, we limit the energy-densities of tensor, vector, and scalar modes at 95% credibility to $\u03a9^T_0 < 5.6 \\times 10^{-8}$, $\u03a9^V_0 < 6.4\\times 10^{-8}$, and $\u03a9^S_0 < 1.1\\times 10^{-7}$ at a reference frequency $f_0 = 25$ Hz.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Machine learning with artificial neural networks is revolutionizing science. The most advanced challenges require discovering answers autonomously. This is the domain of reinforcement learning, where control strategies are improved according to a reward function. The power of neural-network-based reinforcement learning has been highlighted by spectacular recent successes, such as playing Go, but its benefits for physics are yet to be demonstrated. Here, we show how a network-based \"agent\" can discover complete quantum-error-correction strategies, protecting a collection of qubits against noise. These strategies require feedback adapted to measurement outcomes. Finding them from scratch, without human guidance, tailored to different hardware resources, is a formidable challenge due to the combinatorially large search space. To solve this, we develop two ideas: two-stage learning with teacher/student networks and a reward quantifying the capability to recover the quantum information stored in a multi-qubit system. Beyond its immediate impact on quantum computation, our work more generally demonstrates the promise of neural-network-based reinforcement learning in physics.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We report on a new all-sky search for periodic gravitational waves in the frequency band 475-2000 Hz and with a frequency time derivative in the range of [-1.0e-8, +1e-9] Hz/s. Potential signals could be produced by a nearby spinning and slightly non-axisymmetric isolated neutron star in our galaxy.\n  This search uses the data from Advanced LIGO's first observational run O1. No gravitational wave signals were observed, and upper limits were placed on their strengths. For completeness, results from the separately published low frequency search 20-475 Hz are included as well.\n  Our lowest upper limit on worst-case (linearly polarized) strain amplitude h_0 is 4e-25 near 170 Hz, while at the high end of our frequency range we achieve a worst-case upper limit of 1.3e-24. For a circularly polarized source (most favorable orientation), the smallest upper limit obtained is ~1.5e-25.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present optical continuum lags for two Seyfert 1 galaxies, MCG+08-11-011 and NGC 2617, using monitoring data from a reverberation mapping campaign carried out in 2014. Our light curves span the ugriz filters over four months, with median cadences of 1.0 and 0.6 days for MCG+08-11-011 and NGC\\,2617, respectively, combined with roughly daily X-ray and near-UV data from Swift for NGC 2617. We find lags consistent with geometrically thin accretion-disk models that predict a lag-wavelength relation of $\u03c4\\propto \u03bb^{4/3}$. However, the observed lags are larger than predictions based on standard thin-disk theory by factors of 3.3 for MCG+08-11-011 and 2.3 for NGC\\,2617. These differences can be explained if the mass accretion rates are larger than inferred from the optical luminosity by a factor of 4.3 in MCG+08-11-011 and a factor of 1.3 in NGC\\,2617, although uncertainty in the SMBH masses determines the significance of this result. While the X-ray variability in NGC\\,2617 precedes the UV/optical variability, the long 2.6 day lag is problematic for coronal reprocessing models.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We review exclusive top pair production including decays at a future high-energy lepton collider, both in the threshold region and for higher energies. For the continuum process, we take complete QCD next-to-leading order matrix elements for the $2\\to 6$ process with leptonic W decays into account. At threshold, we match the fixed-order relativistic QCD-NLO cross section to a nonrelativistic cross section with next-to-leading logarithmic (NLL) threshold resummation implemented via a form factor.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Searches are under way in Advanced LIGO and Virgo data for persistent gravitational waves from continuous sources, e.g. rapidly rotating galactic neutron stars, and stochastic sources, e.g. relic gravitational waves from the Big Bang or superposition of distant astrophysical events such as mergers of black holes or neutron stars. These searches can be degraded by the presence of narrow spectral artifacts (lines) due to instrumental or environmental disturbances. We describe a variety of methods used for finding, identifying and mitigating these artifacts, illustrated with particular examples. Results are provided in the form of lists of line artifacts that can safely be treated as non-astrophysical. Such lists are used to improve the efficiencies and sensitivities of continuous and stochastic gravitational wave searches by allowing vetoes of false outliers and permitting data cleaning.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "$\u03b7$ Car is a massive, eccentric binary with a rich observational history. We obtained the first high-cadence, high-precision light curves with the BRITE-Constellation nanosatellites over 6 months in 2016 and 6 months in 2017. The light curve is contaminated by several sources including the Homunculus nebula and neighboring stars, including the eclipsing binary CPD$-$59$^\\circ$2628. However, we found two coherent oscillations in the light curve. These may represent pulsations that are not yet understood but we postulate that they are related to tidally excited oscillations of $\u03b7$ Car's primary star, and would be similar to those detected in lower-mass eccentric binaries. In particular, one frequency was previously detected by van Genderen et al. and Sterken et al. through the time period of 1974 to 1995 through timing measurements of photometric maxima. Thus, this frequency seems to have been detected for nearly four decades, indicating that it has been stable in frequency over this time span. These pulsations could help provide the first direct constraints on the fundamental parameters of the primary star if confirmed and refined with future observations.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present an approach to predict exclusive $W^+bW^-\\bar{b}$ production at lepton colliders that correctly describes the top-anti-top threshold as well as the continuum region. We incorporate $t\\bar{t}$ form factors for the NLL threshold resummation derived in NRQCD into a factorized relativistic cross section using an extended double-pole approximation, which accounts for fixed-order QCD corrections to the top decays at NLO. This is combined with the full fixed-order QCD result at NLO for $W^+bW^-\\bar{b}$ production to obtain predictions that are not only valid at threshold but smoothly transition to the continuum region. Our implementation is based on the Monte Carlo event generator WHIZARD and the code TOPPIK and allows to compute fully-differential threshold-resummed cross sections including the interference with non-resonant background processes. For the first time it is now possible to systematically study general differential observables at future lepton colliders involving the decay products of the top quarks at energies close to the pair production threshold and beyond.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Cosmic strings are topological defects which can be formed in GUT-scale phase transitions in the early universe. They are also predicted to form in the context of string theory. The main mechanism for a network of Nambu-Goto cosmic strings to lose energy is through the production of loops and the subsequent emission of gravitational waves, thus offering an experimental signature for the existence of cosmic strings. Here we report on the analysis conducted to specifically search for gravitational-wave bursts from cosmic string loops in the data of Advanced LIGO 2015-2016 observing run (O1). No evidence of such signals was found in the data, and as a result we set upper limits on the cosmic string parameters for three recent loop distribution models. In this paper, we initially derive constraints on the string tension $G\u03bc$ and the intercommutation probability, using not only the burst analysis performed on the O1 data set, but also results from the previously published LIGO stochastic O1 analysis, pulsar timing arrays, cosmic microwave background and Big-Bang nucleosynthesis experiments. We show that these data sets are complementary in that they probe gravitational waves produced by cosmic string loops during very different epochs. Finally, we show that the data sets exclude large parts of the parameter space of the three loop distribution models we consider.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present the results of a search for long-duration gravitational wave transients in the data of the LIGO Hanford and LIGO Livingston second generation detectors between September 2015 and January 2016, with a total observational time of 49 days. The search targets gravitational wave transients of \\unit[10 -- 500]{s} duration in a frequency band of \\unit[24 -- 2048]{Hz}, with minimal assumptions about the signal waveform, polarization, source direction, or time of occurrence. No significant events were observed. %All candidate triggers were consistent with the expected background, As a result we set 90\\% confidence upper limits on the rate of long-duration gravitational wave transients for different types of gravitational wave signals. We also show that the search is sensitive to sources in the Galaxy emitting at least $\\sim$ \\unit[$10^{-8}$]{$\\mathrm{M_{\\odot} c^2}$} in gravitational waves.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "On June 8, 2017 at 02:01:16.49 UTC, a gravitational-wave signal from the merger of two stellar-mass black holes was observed by the two Advanced LIGO detectors with a network signal-to-noise ratio of 13. This system is the lightest black hole binary so far observed, with component masses $12^{+7}_{-2}\\,M_\\odot$ and $7^{+2}_{-2}\\,M_\\odot$ (90% credible intervals). These lie in the range of measured black hole masses in low-mass X-ray binaries, thus allowing us to compare black holes detected through gravitational waves with electromagnetic observations. The source's luminosity distance is $340^{+140}_{-140}$ Mpc, corresponding to redshift $0.07^{+0.03}_{-0.03}$. We verify that the signal waveform is consistent with the predictions of general relativity.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The first observation of a binary neutron star coalescence by the Advanced LIGO and Advanced Virgo gravitational-wave detectors offers an unprecedented opportunity to study matter under the most extreme conditions. After such a merger, a compact remnant is left over whose nature depends primarily on the masses of the inspiralling objects and on the equation of state of nuclear matter. This could be either a black hole or a neutron star (NS), with the latter being either long-lived or too massive for stability implying delayed collapse to a black hole. Here, we present a search for gravitational waves from the remnant of the binary neutron star merger GW170817 using data from Advanced LIGO and Advanced Virgo. We search for short ($\\lesssim1$ s) and intermediate-duration ($\\lesssim 500$ s) signals, which includes gravitational-wave emission from a hypermassive NS or supramassive NS, respectively. We find no signal from the post-merger remnant. Our derived strain upper limits are more than an order of magnitude larger than those predicted by most models. For short signals, our best upper limit on the root-sum-square of the gravitational-wave strain emitted from 1--4 kHz is $h_{\\rm rss}^{50\\%}=2.1\\times 10^{-22}$ Hz$^{-1/2}$ at 50% detection efficiency. For intermediate-duration signals, our best upper limit at 50% detection efficiency is $h_{\\rm rss}^{50\\%}=8.4\\times 10^{-22}$ Hz$^{-1/2}$ for a millisecond magnetar model, and $h_{\\rm rss}^{50\\%}=5.9\\times 10^{-22}$ Hz$^{-1/2}$ for a bar-mode model. These results indicate that post-merger emission from a similar event may be detectable when advanced detectors reach design sensitivity or with next-generation detectors.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "From $5.5$ months of dual-band optical photometric monitoring at the $1$ mmag level, BRITE-Constellation has revealed two simultaneous types of variability in the O4I(n)fp star $\u03b6$ Puppis: one single periodic non-sinusoidal component superimposed on a stochastic component. The monoperiodic component is the $1.78$ d signal previously detected by Coriolis/SMEI, but this time along with a prominent first harmonic. The shape of this signal changes over time, a behaviour that is incompatible with stellar oscillations but consistent with rotational modulation arising from evolving bright surface inhomogeneities. By means of a constrained non-linear light curve inversion algorithm we mapped the locations of the bright surface spots and traced their evolution. Our simultaneous ground-based multi-site spectroscopic monitoring of the star unveiled cyclical modulation of its He II $\\lambda4686$ wind emission line with the $1.78$-day rotation period, showing signatures of Corotating Interaction Regions (CIRs) that turn out to be driven by the bright photospheric spots observed by BRITE. Traces of wind clumps are also observed in the He II $\\lambda4686$ line and are correlated with the amplitudes of the stochastic component of the light variations probed by BRITE at the photosphere, suggesting that the BRITE observations additionally unveiled the photospheric drivers of wind clumps in $\u03b6$ Pup and that the clumping phenomenon starts at the very base of the wind. The origins of both the bright surface inhomogeneities and the stochastic light variations remain unknown, but a subsurface convective zone might play an important role in the generation of these two types of photospheric variability.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The Advanced LIGO and Advanced Virgo observatories recently discovered gravitational waves from a binary neutron star inspiral. A short gamma-ray burst (GRB) that followed the merger of this binary was also recorded by the Fermi Gamma-ray Burst Monitor (Fermi-GBM), and the Anticoincidence Shield for the Spectrometer for the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), indicating particle acceleration by the source. The precise location of the event was determined by optical detections of emission following the merger. We searched for high-energy neutrinos from the merger in the GeV--EeV energy range using the ANTARES, IceCube, and Pierre Auger Observatories. No neutrinos directionally coincident with the source were detected within $\\pm500$ s around the merger time. Additionally, no MeV neutrino burst signal was detected coincident with the merger. We further carried out an extended search in the direction of the source for high-energy neutrinos within the 14-day period following the merger, but found no evidence of emission. We used these results to probe dissipation mechanisms in relativistic outflows driven by the binary neutron star merger. The non-detection is consistent with model predictions of short GRBs observed at a large off-axis angle.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "On 2017 August 17 the merger of two compact objects with masses consistent with two neutron stars was discovered through gravitational-wave (GW170817), gamma-ray (GRB 170817A), and optical (SSS17a/AT 2017gfo) observations. The optical source was associated with the early-type galaxy NGC 4993 at a distance of just $\\sim$40 Mpc, consistent with the gravitational-wave measurement, and the merger was localized to be at a projected distance of $\\sim$2 kpc away from the galaxy's center. We use this minimal set of facts and the mass posteriors of the two neutron stars to derive the first constraints on the progenitor of GW170817 at the time of the second supernova (SN). We generate simulated progenitor populations and follow the three-dimensional kinematic evolution from the binary neutron star (BNS) birth to the merger time, accounting for pre-SN galactic motion, for considerably different input distributions of the progenitor mass, pre-SN semimajor axis, and SN-kick velocity. Though not considerably tight, we find these constraints to be comparable to those for Galactic BNS progenitors. The derived constraints are very strongly influenced by the requirement of keeping the binary bound after the second SN and having the merger occur relatively close to the center of the galaxy. These constraints are insensitive to the galaxy's star formation history, provided the stellar populations are older than 1 Gyr.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The LIGO Scientific and Virgo Collaborations have announced the first detection of gravitational waves from the coalescence of two neutron stars. The merger rate of binary neutron stars estimated from this event suggests that distant, unresolvable binary neutron stars create a significant astrophysical stochastic gravitational-wave background. The binary neutron star background will add to the background from binary black holes, increasing the amplitude of the total astrophysical background relative to previous expectations. In the Advanced LIGO-Virgo frequency band most sensitive to stochastic backgrounds (near 25 Hz), we predict a total astrophysical background with amplitude $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.8_{-1.3}^{+2.7} \\times 10^{-9}$ with $90\\%$ confidence, compared with $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.1_{-0.7}^{+1.2} \\times 10^{-9}$ from binary black holes alone. Assuming the most probable rate for compact binary mergers, we find that the total background may be detectable with a signal-to-noise-ratio of 3 after 40 months of total observation time, based on the expected timeline for Advanced LIGO and Virgo to reach their design sensitivity.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The source of the gravitational-wave signal GW170817, very likely a binary neutron star merger, was also observed electromagnetically, providing the first multi-messenger observations of this type. The two week long electromagnetic counterpart had a signature indicative of an r-process-induced optical transient known as a kilonova. This Letter examines how the mass of the dynamical ejecta can be estimated without a direct electromagnetic observation of the kilonova, using gravitational-wave measurements and a phenomenological model calibrated to numerical simulations of mergers with dynamical ejecta. Specifically, we apply the model to the binary masses inferred from the gravitational-wave measurements, and use the resulting mass of the dynamical ejecta to estimate its contribution (without the effects of wind ejecta) to the corresponding kilonova light curves from various models. The distributions of dynamical ejecta mass range between $M_{ej} = 10^{-3} - 10^{-2} M_{\\odot}$ for various equations of state, assuming the neutron stars are rotating slowly. In addition, we use our estimates of the dynamical ejecta mass and the neutron star merger rates inferred from GW170817 to constrain the contribution of events like this to the r-process element abundance in the Galaxy when ejecta mass from post-merger winds is neglected. We find that if $\\gtrsim10\\%$ of the matter dynamically ejected from BNS mergers is converted to r-process elements, GW170817-like BNS mergers could fully account for the amount of r-process material observed in the Milky Way.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The detection of GW170817 in both gravitational waves and electromagnetic waves heralds the age of gravitational-wave multi-messenger astronomy. On 17 August 2017 the Advanced LIGO and Virgo detectors observed GW170817, a strong signal from the merger of a binary neutron-star system. Less than 2 seconds after the merger, a gamma-ray burst (GRB 170817A) was detected within a region of the sky consistent with the LIGO-Virgo-derived location of the gravitational-wave source. This sky region was subsequently observed by optical astronomy facilities, resulting in the identification of an optical transient signal within $\\sim 10$ arcsec of the galaxy NGC 4993. These multi-messenger observations allow us to use GW170817 as a standard siren, the gravitational-wave analog of an astronomical standard candle, to measure the Hubble constant. This quantity, which represents the local expansion rate of the Universe, sets the overall scale of the Universe and is of fundamental importance to cosmology. Our measurement combines the distance to the source inferred purely from the gravitational-wave signal with the recession velocity inferred from measurements of the redshift using electromagnetic data. This approach does not require any form of cosmic \"distance ladder;\" the gravitational wave analysis can be used to estimate the luminosity distance out to cosmological scales directly, without the use of intermediate astronomical distance measurements. We determine the Hubble constant to be $70.0^{+12.0}_{-8.0} \\, \\mathrm{km} \\, \\mathrm{s}^{-1} \\, \\mathrm{Mpc}^{-1}$ (maximum a posteriori and 68% credible interval). This is consistent with existing measurements, while being completely independent of them. Additional standard-siren measurements from future gravitational-wave sources will provide precision constraints of this important cosmological parameter.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Spinning neutron stars asymmetric with respect to their rotation axis are potential sources of continuous gravitational waves for ground-based interferometric detectors. In the case of known pulsars a fully coherent search, based on matched filtering, which uses the position and rotational parameters obtained from electromagnetic observations, can be carried out. Matched filtering maximizes the signal-to-noise (SNR) ratio, but a large sensitivity loss is expected in case of even a very small mismatch between the assumed and the true signal parameters. For this reason, {\\it narrow-band} analyses methods have been developed, allowing a fully coherent search for gravitational waves from known pulsars over a fraction of a hertz and several spin-down values. In this paper we describe a narrow-band search of eleven pulsars using data from Advanced LIGO's first observing run. Although we have found several initial outliers, further studies show no significant evidence for the presence of a gravitational wave signal. Finally, we have placed upper limits on the signal strain amplitude lower than the spin-down limit for 5 of the 11 targets over the bands searched: in the case of J1813-1749 the spin-down limit has been beaten for the first time. For an additional 3 targets, the median upper limit across the search bands is below the spin-down limit. This is the most sensitive narrow-band search for continuous gravitational waves carried out so far.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The first observing run of Advanced LIGO spanned 4 months, from September 12, 2015 to January 19, 2016, during which gravitational waves were directly detected from two binary black hole systems, namely GW150914 and GW151226. Confident detection of gravitational waves requires an understanding of instrumental transients and artifacts that can reduce the sensitivity of a search. Studies of the quality of the detector data yield insights into the cause of instrumental artifacts and data quality vetoes specific to a search are produced to mitigate the effects of problematic data. In this paper, the systematic removal of noisy data from analysis time is shown to improve the sensitivity of searches for compact binary coalescences. The output of the PyCBC pipeline, which is a python-based code package used to search for gravitational wave signals from compact binary coalescences, is used as a metric for improvement. GW150914 was a loud enough signal that removing noisy data did not improve its significance. However, the removal of data with excess noise decreased the false alarm rate of GW151226 by more than two orders of magnitude, from 1 in 770 years to less than 1 in 186000 years.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "On August 14, 2017 at 10:30:43 UTC, the Advanced Virgo detector and the two Advanced LIGO detectors coherently observed a transient gravitational-wave signal produced by the coalescence of two stellar mass black holes, with a false-alarm-rate of $\\lesssim$ 1 in 27000 years. The signal was observed with a three-detector network matched-filter signal-to-noise ratio of 18. The inferred masses of the initial black holes are $30.5_{-3.0}^{+5.7}$ Msun and $25.3_{-4.2}^{+2.8}$ Msun (at the 90% credible level). The luminosity distance of the source is $540_{-210}^{+130}~\\mathrm{Mpc}$, corresponding to a redshift of $z=0.11_{-0.04}^{+0.03}$. A network of three detectors improves the sky localization of the source, reducing the area of the 90% credible region from 1160 deg$^2$ using only the two LIGO detectors to 60 deg$^2$ using all three detectors. For the first time, we can test the nature of gravitational wave polarizations from the antenna response of the LIGO-Virgo network, thus enabling a new class of phenomenological tests of gravity.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present results from the first directed search for nontensorial gravitational waves. While general relativity allows for tensorial (plus and cross) modes only, a generic metric theory may, in principle, predict waves with up to six different polarizations. This analysis is sensitive to continuous signals of scalar, vector or tensor polarizations, and does not rely on any specific theory of gravity. After searching data from the first observation run of the advanced LIGO detectors for signals at twice the rotational frequency of 200 known pulsars, we find no evidence of gravitational waves of any polarization. We report the first upper limits for scalar and vector strains, finding values comparable in magnitude to previously-published limits for tensor strain. Our results may be translated into constraints on specific alternative theories of gravity.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Following the completion of the second neutron beam line and the related experimental area (EAR2) at the n_TOF spallation neutron source at CERN, several experiments were planned and performed. The high instantaneous neutron flux available in EAR2 allows to investigate neutron indiced reactions with charged particles in the exit channel even employing targets made out of small amounts of short-lived radioactive isotopes. After the successful measurement of the 7Be(n,\u03b1)\u03b1 cross section, the 7Be(n,p)7Li reaction was studied in order to provide still missing cross section data of relevance for Big Bang Nucleosynthesis (BBN), in an attempt to find a solution to the cosmological Lithium abundance problem. This paper describes the experimental setup employed in such a measurement and its characterization.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We report on the first multi-color precision light curve of the bright Wolf-Rayet binary $\u03b3^2$ Velorum, obtained over six months with the nanosatellites in the BRITE- Constellation fleet. In parallel, we obtained 488 high-resolution optical spectra of the system. In this first report on the datasets, we revise the spectroscopic orbit and report on the bulk properties of the colliding winds. We find a dependence of both the light curve and excess emission properties that scales with the inverse of the binary separation. When analyzing the spectroscopic properties in combination with the photometry, we find that the phase dependence is caused only by excess emission in the lines, and not from a changing continuum. We also detect a narrow, high-velocity absorption component from the He I $\u03bb$5876 transition, which appears twice in the orbit. We calculate smoothed-particle hydrodynamical simulations of the colliding winds and can accurately associate the absorption from He I to the leading and trailing arms of the wind shock cone passing tangentially through our line of sight. The simulations also explain the general strength and kinematics of the emission excess observed in wind lines such as C III $\u03bb$5696 of the system. These results represent the first in a series of investigations into the winds and properties of $\u03b3^2$ Velorum through multi-technique and multi-wavelength observational campaigns.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We report results of a deep all-sky search for periodic gravitational waves from isolated neutron stars in data from the first Advanced LIGO observing run. This search investigates the low frequency range of Advanced LIGO data, between 20 and 100 Hz, much of which was not explored in initial LIGO. The search was made possible by the computing power provided by the volunteers of the Einstein@Home project. We find no significant signal candidate and set the most stringent upper limits to date on the amplitude of gravitational wave signals from the target population, corresponding to a sensitivity depth of 48.7 [1/$\\sqrt{\\textrm{Hz}}$]. At the frequency of best strain sensitivity, near 100 Hz, we set 90% confidence upper limits of $1.8 \\times 10^{-25}$. At the low end of our frequency range, 20 Hz, we achieve upper limits of $3.9 \\times 10^{-24}$. At 55 Hz we can exclude sources with ellipticities greater than $10^{-5}$ within 100 pc of Earth with fiducial value of the principal moment of inertia of $10^{38} \\textrm{kg m}^2$.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We report on an all-sky search for periodic gravitational waves in the frequency band 20-475 Hz and with a frequency time derivative in the range of [-1.0, +0.1]e-8 Hz/s. Such a signal could be produced by a nearby spinning and slightly non-axisymmetric isolated neutron star in our galaxy. This search uses the data from Advanced LIGO's first observational run, O1. No periodic gravitational wave signals were observed, and upper limits were placed on their strengths. The lowest upper limits on worst-case (linearly polarized) strain amplitude h0 are 4e-25 near 170 Hz. For a circularly polarized source (most favorable orientation), the smallest upper limits obtained are 1.5e-25. These upper limits refer to all sky locations and the entire range of frequency derivative values. For a population-averaged ensemble of sky locations and stellar orientations, the lowest upper limits obtained for the strain amplitude are 2.5e-25.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "A large spin-dependent and electric field-tunable magnetoresistance of a two-dimensional electron system (2DES) is a key ingredient for the realization of many novel concepts for spin-based electronic devices. The low magnetoresistance observed during the last decades in devices with lateral semiconducting (SC) transport channels between ferromagnetic (FM) source (S) and drain (D) contacts has been the main obstacle for realizing spin field effect transistor proposals. Here, we show both, a large two terminal magnetoresistance in lateral 2DES-based spin valve geometry, with up to 80% resistance change, and tunability of the magnetoresistance by an electric gate. The large magnetoresistance is due to finite electric field effects at the FM/SC interface, which boost spin-to-charge conversion. The gating scheme we use is based on switching between uni- and bi-directional spin diffusion, without resorting to the spin-orbit coupling.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present the results of a semicoherent search for continuous gravitational waves from the low-mass X-ray binary Scorpius X-1, using data from the first Advanced LIGO observing run. The search method uses details of the modelled, parametrized continuous signal to combine coherently data separated by less than a specified coherence time, which can be adjusted to trade off sensitivity against computational cost. A search was conducted over the frequency range from 25 Hz to 2000 Hz, spanning the current observationally-constrained range of the binary orbital parameters. No significant detection candidates were found, and frequency-dependent upper limits were set using a combination of sensitivity estimates and simulated signal injections. The most stringent upper limit was set at 175 Hz, with comparable limits set across the most sensitive frequency range from 100 Hz to 200 Hz. At this frequency, the 95 pct upper limit on signal amplitude h0 is 2.3e-25 marginalized over the unknown inclination angle of the neutron star's spin, and 8.03e-26 assuming the best orientation (which results in circularly polarized gravitational waves). These limits are a factor of 3-4 stronger than those set by other analyses of the same data, and a factor of about 7 stronger than the best upper limits set using initial LIGO data. In the vicinity of 100 Hz, the limits are a factor of between 1.2 and 3.5 above the predictions of the torque balance model, depending on inclination angle, if the most likely inclination angle of 44 degrees is assumed, they are within a factor of 1.7.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "HD 3167 is a bright (V = 8.9), nearby K0 star observed by the NASA K2 mission (EPIC 220383386), hosting two small, short-period transiting planets. Here we present the results of a multi-site, multi-instrument radial velocity campaign to characterize the HD 3167 system. The masses of the transiting planets are 5.02+/-0.38 MEarth for HD 3167 b, a hot super-Earth with a likely rocky composition (rho_b = 5.60+2.15-1.43 g/cm^3), and 9.80+1.30-1.24 MEarth for HD 3167 c, a warm sub-Neptune with a likely substantial volatile complement (rho_c = 1.97+0.94-0.59 g/cm^3). We explore the possibility of atmospheric composition analysis and determine that planet c is amenable to transmission spectroscopy measurements, and planet b is a potential thermal emission target. We detect a third, non-transiting planet, HD 3167 d, with a period of 8.509+/-0.045 d (between planets b and c) and a minimum mass of 6.90+/-0.71 MEarth. We are able to constrain the mutual inclination of planet d with planets b and c: we rule out mutual inclinations below 1.3 degrees as we do not observe transits of planet d. From 1.3-40 degrees, there are viewing geometries invoking special nodal configurations which result in planet d not transiting some fraction of the time. From 40-60 degrees, Kozai-Lidov oscillations increase the system's instability, but it can remain stable for up to 100Myr. Above 60 degrees, the system is unstable. HD 3167 promises to be a fruitful system for further study and a preview of the many exciting systems expected from the upcoming NASA TESS mission.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We describe the observation of GW170104, a gravitational-wave signal produced by the coalescence of a pair of stellar-mass black holes. The signal was measured on January 4, 2017 at 10:11:58.6 UTC by the twin advanced detectors of the Laser Interferometer Gravitational-Wave Observatory during their second observing run, with a network signal-to-noise ratio of 13 and a false alarm rate less than 1 in 70,000 years. The inferred component black hole masses are $31.2^{+8.4}_{-6.0}\\,M_\\odot$ and $19.4^{+5.3}_{-5.9}\\,M_\\odot$ (at the 90% credible level). The black hole spins are best constrained through measurement of the effective inspiral spin parameter, a mass-weighted combination of the spin components perpendicular to the orbital plane, $\u03c7_\\mathrm{eff} = -0.12^{+0.21}_{-0.30}.$ This result implies that spin configurations with both component spins positively aligned with the orbital angular momentum are disfavored. The source luminosity distance is $880^{+450}_{-390}~\\mathrm{Mpc}$ corresponding to a redshift of $z = 0.18^{+0.08}_{-0.07}$. We constrain the magnitude of modifications to the gravitational-wave dispersion relation and perform null tests of general relativity. Assuming that gravitons are dispersed in vacuum like massive particles, we bound the graviton mass to $m_g \\le 7.7 \\times 10^{-23}~\\mathrm{eV}/c^2$. In all cases, we find that GW170104 is consistent with general relativity.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "During their first observational run, the two Advanced LIGO detectors attained an unprecedented sensitivity, resulting in the first direct detections of gravitational-wave signals and GW151226, produced by stellar-mass binary black hole systems. This paper reports on an all-sky search for gravitational waves (GWs) from merging intermediate mass black hole binaries (IMBHBs). The combined results from two independent search techniques were used in this study: the first employs a matched-filter algorithm that uses a bank of filters covering the GW signal parameter space, while the second is a generic search for GW transients (bursts). No GWs from IMBHBs were detected, therefore, we constrain the rate of several classes of IMBHB mergers. The most stringent limit is obtained for black holes of individual mass $100\\,M_\\odot$, with spins aligned with the binary orbital angular momentum. For such systems, the merger rate is constrained to be less than $0.93~\\mathrm{Gpc^{-3}\\,yr}^{-1}$ in comoving units at the $90\\%$ confidence level, an improvement of nearly 2 orders of magnitude over previous upper limits.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Results are presented from a semi-coherent search for continuous gravitational waves from the brightest low-mass X-ray binary, Scorpius X-1, using data collected during the first Advanced LIGO observing run (O1). The search combines a frequency domain matched filter (Bessel-weighted $\\mathcal{F}$-statistic) with a hidden Markov model to track wandering of the neutron star spin frequency. No evidence of gravitational waves is found in the frequency range 60-650 Hz. Frequentist 95% confidence strain upper limits, $h_0^{95\\%} = 4.0\\times10^{-25}$, $8.3\\times10^{-25}$, and $3.0\\times10^{-25}$ for electromagnetically restricted source orientation, unknown polarization, and circular polarization, respectively, are reported at 106 Hz. They are $\\leq 10$ times higher than the theoretical torque-balance limit at 106 Hz.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Stellar rotation affects the transport of chemical elements and angular momentum and is therefore a key process during stellar evolution, which is still not fully understood. This is especially true for massive stars, which are important for the chemical enrichment of the universe. It is therefore important to constrain their physical parameters and internal angular momentum distribution to calibrate stellar structure and evolution models. Stellar internal rotation can be probed through asteroseismic studies of rotationally split oscillations but such results are still quite rare, especially for stars more massive than the Sun. The SPB star HD201433 is known to be part of a single-lined spectroscopic triple system, with two low-mass companions orbiting with periods of about 3.3 and 154 d. Our results are based on photometric observations made by BRITE - Constellation and the SMEI on board the Coriolis satellite, high-resolution spectroscopy, and more than 96 years of radial velocity measurements. We identify a sequence of 9 rotationally split dipole modes in the photometric time series and establish that HD201433 is in principle a solid-body rotator with a very slow rotation period of 297+/-76 d. Tidal interaction with the inner companion has, however, significantly accelerated the spin of the surface layers by a factor of approximately one hundred. The angular momentum transfer onto the surface of HD201433 is also reflected by the statistically significant decrease of the orbital period of about 0.9 s during the last 96 years. Combining the asteroseismic inferences with the spectroscopic measurements and the orbital analysis of the inner binary system, we conclude that tidal interactions between the central SPB star and its inner companion have almost circularised the orbit but not yet aligned all spins of the system and have just begun to synchronise rotation.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "The Advanced LIGO observatories detected gravitational waves from two binary black hole mergers during their first observation run (O1). We present a high-energy neutrino follow-up search for the second gravitational wave event, GW151226, as well as for gravitational wave candidate LVT151012. We find 2 and 4 neutrino candidates detected by IceCube, and 1 and 0 detected by ANTARES, within $\\pm500$ s around the respective gravitational wave signals, consistent with the expected background rate. None of these neutrino candidates are found to be directionally coincident with GW151226 or LVT151012. We use non-detection to constrain isotropic-equivalent high-energy neutrino emission from GW151226 adopting the GW event's 3D localization, to less than $2\\times 10^{51}-2\\times10^{54}$ erg.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "This paper presents an analysis of the transient behavior of the Advanced LIGO suspensions used to seismically isolate the optics. We have characterized the transients in the longitudinal motion of the quadruple suspensions during Advanced LIGO's first observing run. Propagation of transients between stages is consistent with modelled transfer functions, such that transient motion originating at the top of the suspension chain is significantly reduced in amplitude at the test mass. We find that there are transients seen by the longitudinal motion monitors of quadruple suspensions, but they are not significantly correlated with transient motion above the noise floor in the gravitational wave strain data, and therefore do not present a dominant source of background noise in the searches for transient gravitational wave signals.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Quantum fluctuations in the phase and amplitude quadratures of light set limitations on the sensitivity of modern optical instruments. The sensitivity of the interferometric gravitational wave detectors, such as the Advanced Laser Interferometer Gravitational wave Observatory (LIGO), is limited by quantum shot noise, quantum radiation pressure noise, and a set of classical noises. We show how the quantum properties of light can be used to distinguish these noises using correlation techniques. Particularly, in the first part of the paper we show estimations of the coating thermal noise and gas phase noise, hidden below the quantum shot noise in the Advanced LIGO sensitivity curve. We also make projections on the observatory sensitivity during the next science runs. In the second part of the paper we discuss the correlation technique that reveals the quantum radiation pressure noise from the background of classical noises and shot noise. We apply this technique to the Advanced LIGO data, collected during the first science run, and experimentally estimate the quantum correlations and quantum radiation pressure noise in the interferometer for the first time.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We report the discovery of KELT-18b, a transiting hot Jupiter in a 2.87d orbit around the bright (V=10.1), hot, F4V star BD+60 1538 (TYC 3865-1173-1). We present follow-up photometry, spectroscopy, and adaptive optics imaging that allow a detailed characterization of the system. Our preferred model fits yield a host stellar temperature of 6670+/-120 K and a mass of 1.524+/-0.069 Msun, situating it as one of only a handful of known transiting planets with hosts that are as hot, massive, and bright. The planet has a mass of 1.18+/-0.11 Mjup, a radius of 1.57+/-0.04 Rjup, and a density of 0.377+/-0.040 g/cm^3, making it one of the most inflated planets known around a hot star. We argue that KELT-18b's high temperature and low surface gravity, which yield an estimated ~600 km atmospheric scale height, combined with its hot, bright host make it an excellent candidate for observations aimed at atmospheric characterization. We also present evidence for a bound stellar companion at a projected separation of ~1100 AU, and speculate that it may have contributed to the strong misalignment we suspect between KELT-18's spin axis and its planet's orbital axis. The inferior conjunction time is 2457542.524998 +/-0.000416 (BJD_TDB) and the orbital period is 2.8717510 +/- 0.0000029 days. We encourage Rossiter-McLaughlin measurements in the near future to confirm the suspected spin-orbit misalignment of this system.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present the results of an optical spectroscopic monitoring program targeting NGC 5548 as part of a larger multi-wavelength reverberation mapping campaign. The campaign spanned six months and achieved an almost daily cadence with observations from five ground-based telescopes. The H$\u03b2$ and He II $\u03bb$4686 broad emission-line light curves lag that of the 5100 $\u00c5$ optical continuum by $4.17^{+0.36}_{-0.36}$ days and $0.79^{+0.35}_{-0.34}$ days, respectively. The H$\u03b2$ lag relative to the 1158 $\u00c5$ ultraviolet continuum light curve measured by the Hubble Space Telescope is roughly $\\sim$50% longer than that measured against the optical continuum, and the lag difference is consistent with the observed lag between the optical and ultraviolet continua. This suggests that the characteristic radius of the broad-line region is $\\sim$50% larger than the value inferred from optical data alone. We also measured velocity-resolved emission-line lags for H$\u03b2$ and found a complex velocity-lag structure with shorter lags in the line wings, indicative of a broad-line region dominated by Keplerian motion. The responses of both the H$\u03b2$ and He II $\u03bb$4686 emission lines to the driving continuum changed significantly halfway through the campaign, a phenomenon also observed for C IV, Ly $\u03b1$, He II(+O III]), and Si IV(+O IV]) during the same monitoring period. Finally, given the optical luminosity of NGC 5548 during our campaign, the measured H$\u03b2$ lag is a factor of five shorter than the expected value implied by the $R_\\mathrm{BLR} - L_\\mathrm{AGN}$ relation based on the past behavior of NGC 5548.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "We present the result of searches for gravitational waves from 200 pulsars using data from the first observing run of the Advanced LIGO detectors. We find no significant evidence for a gravitational-wave signal from any of these pulsars, but we are able to set the most constraining upper limits yet on their gravitational-wave amplitudes and ellipticities. For eight of these pulsars, our upper limits give bounds that are improvements over the indirect spin-down limit values. For another 32, we are within a factor of 10 of the spin-down limit, and it is likely that some of these will be reachable in future runs of the advanced detector. Taken as a whole, these new results improve on previous limits by more than a factor of two.\n        \u25b3 Less", "author": "Thomas F. Weiss"}, {"abstract": "Episodic accretion may be a common occurrence in the evolution of young pre-main sequence stars and has important implications for our understanding of star and planet formation. Many fundamental aspects of what drives the accretion physics, however, are still unknown. The ngVLA will be a key tool in understanding the nature of these events. The high spatial resolution, broad spectral coverage, and unprecedented sensitivity will allow for the detailed analysis of outburst systems. The proposed frequency range of the ngVLA allows for observations of the gas, dust, and non-thermal emission from the star and disk.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "Our understanding of stellar atmospheres and our ability to infer architectures of extrasolar planetary systems rely on understanding the emission of stars at submillimeter to centimeter wavelengths. In this chapter we describe how unconstrained stellar emission can interfere with the accurate characterization of circumstellar debris. The ngVLA is the only facility with the sensitivity that allows for the observations of a broad range of stellar spectral types in a feasible amount of time. The observations will enable the building and testing of accurate models of stellar emission, which in turn are required for evaluating both the occurrence and abundance of debris over the proposed wavelength range of the ngVLA\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "In the early stages of planet formation, small dust grains grow to become mm sized particles in debris disks around stars. These disks can in principle be characterized by their emission at submillimeter and millimeter wavelengths. Determining both the occurrence and abundance of debris in unresolved circumstellar disks of A-type main-sequence stars requires that the stellar photospheric emission be accurately modeled. To better constrain the photospheric emission for such systems, we present observations of Sirius A, an A-type star with no known debris, from the JCMT, SMA, and VLA at 0.45, 0.85, 0.88, 1.3, 6.7, and 9.0 mm. We use these observations to inform a PHOENIX model of Sirius A's atmosphere. We find the model provides a good match to these data and can be used as a template for the submm/mm emission of other early A-type stars where unresolved debris may be present. The observations are part of an ongoing observational campaign entitled Measuring the Emission of Stellar Atmospheres at Submm/mm wavelengths (MESAS)\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We present archival ALMA observations of the HD 141569 circumstellar disk at 345, 230, and 100 GHz. These data detect extended millimeter emission that is exterior to the inner disk. We find through simultaneous visibility modeling of all three data sets that the system's morphology is described well by a two-component disk model. The inner disk ranges from approximately 16 to 45 au with a spectral index of 1.81 (q = 2.95) and the outer disk ranges from 95 to 300 au with a spectral index of 2.28 (q = 3.21). Azimuthally averaged radial emission profiles derived from the continuum images at each frequency show potential emission that is consistent with the visibility modeling. The analysis presented here shows that at ~5 Myr HD 141569's grain size distribution is steeper, and therefore more evolved, in the outer disk than in the inner disk.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "This paper describes the implementation and performance of adiabatic absorbing layers in an FFT-accelerated volume integral equation (VIE) method for simulating truncated nanophotonics structures. At the truncation sites, we place absorbing regions in which the conductivity is increased gradually in order to minimize reflections. In the continuous setting, such adiabatic absorbers have been shown via coupled-mode theory to produce reflections that diminish at a rate related to the smoothness of the absorption profile function. The VIE formulation we employ relies on uniform discretizations of the geometry over which the continuously varying fields and material properties are represented by piecewise constant functions. Such a discretization enables the acceleration of the method via the FFT and, furthermore, the introduction of varying absorption can be performed in a straightforward manner without compromising this speedup. We demonstrate that, in spite of the crude discrete approximation to the smooth absorption profiles, our approach recovers the theoretically predicted reflection behavior of adiabatic absorbers. We thereby show that the FFT-accelerated VIE method is an effective and fast simulation tool for nanophotonics simulations.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We introduce megagreedoids, which generalize polymatroids, megamatroids, and greedoids. We define a quasisymmetric function invariant for a megagreedoid, and show that it has a positive expansion in the basis of fundamental quasisymmetric functions. Our proof involves lexicographic shellability. We also show that megagreedoids form a Hopf monoid. A running example is a megagreedoid associated to a rooted connected graph, and the resulting generalization of the chromatic symmetric function.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We present VLA 9 mm (33 GHz) observations of the HD 141569 system from semester 16A. The observations achieve a resolution of 0.25 arcsec ($\\sim28$ au) and a sensitivity of $4.7~\u03bc\\rm Jy~beam^{-1}$. We find (1) a $52\\pm 5~\u03bc$Jy point source at the location of HD 141569A that shows potential variability, (2) the detected flux is contained within the SED-inferred central clearing of the disc meaning the spectral index of the dust disc is steeper than previously inferred, and (3) the M dwarf companions are also detected and variable. Previous lower-resolution VLA observations (semester 14A) found a higher flux density, interpreted as solely dust emission. When combined with ALMA observations, the VLA 14A observations suggested the spectral index and grain size distribution of HD 141569's disc was shallow and an outlier among debris systems. Using archival ALMA observations of HD 141569 at 0.87 mm and 2.9 mm we find a dust spectral index of $\u03b1_{\\rm mm} = 1.81\\pm 0.20$. The VLA 16A flux corresponds to a brightness temperature of $\\sim5\\times10^{6}$ K, suggesting strong non-disc emission is affecting the inferred grain properties. The VLA 16A flux density of the M2V companion HD 141569B is $149\\pm9~\u03bc$Jy, corresponding to a brightness temperature of $\\sim2\\times10^{8}$ K and suggesting significant stellar variability when compared to the VLA14A observations, which are smaller by a factor of $\\sim6$.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "In this study we investigate jet interaction at a microscopic level in a cosmological environment, which responds to a key open question in the study of relativistic jets. Using small simulation systems during prior research, we initially studied the evolution of both electron-proton and electron-positron relativistic jets containing helical magnetic fields, by focusing on their interactions with an ambient plasma. Here, using larger jet radii, we have performed simulations of global jets containing helical magnetic fields in order to examine how helical magnetic fields affect kinetic instabilities such as the Weibel instability, the kinetic Kelvin-Helmholtz instability (kKHI) and the Mushroom instability (MI). We found that the evolution of global jets strongly depends on the size of the jet radius. For example, phase bunching of jet electrons, in particular in the electron-proton jet, is mixed with larger jet radius due to the more complicated structures of magnetic fields with excited kinetic instabilities. In our simulation study these kinetic instabilities lead to new types of instabilities in global jets. In the electron-proton jet simulation a modified recollimation occurs and jet electrons are strongly perturbed. In the electron-positron jet simulation mixed kinetic instabilities occur at early times followed by a turbulence-like structure. Simulations using much larger (and longer) systems are further required in order to thoroughly investigate the evolution of global jets containing helical magnetic fields.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We provide a one-to-one map between the spin correlations and certain three-dimensional shapes, analogous to the map between single spins and Bloch vectors, and demonstrate its utility. Much as one can reason geometrically about dynamics using a Bloch vector -- e.g. a magnetic field causes it to precess and dissipation causes it to shrink -- one can reason similarly about the shapes we use to visualize correlations. This visualization demonstrates its usefulness by unveiling the hidden structure in the correlations. For example, seemingly complex correlation dynamics can be described as simple motions of the shapes. We demonstrate the simplicity of the dynamics, which is obscured in conventional analyses, by analyzing several physical systems of relevance to cold atoms.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We present ALMA Band 6 observations (1.3 mm/233 GHz) of Fomalhaut and its debris disc. The observations achieve a sensitivity of 17 $\u03bc$Jy and a resolution of 0.28 arcsec (2.1 au at a distance of 7.66 pc), which are the highest resolution observations to date of the millimetre grains in Fomalhaut's main debris ring. The ring is tightly constrained to $139^{+2}_{-3}$ au with a FWHM of $13\\pm3$ au, following a Gaussian profile. The millimetre spectral index is constrained to $\u03b1_{mm} = -2.62\\pm0.12$. We explore fitting debris disc models in the image plane, as well as fitting models using visibility data directly. The results are compared and the potential advantages/disadvantages of each approach are discussed.\n  The detected central emission is indistinguishable from a point source, with a most probable flux of $0.90\\pm 0.12$ mJy (including calibration uncertainties). This implies that any inner debris structure, as was inferred from far-Infrared observations, must contribute little to the total central emission. Moreover, the stellar flux is less than 70\\% of that predicted by extrapolating a black body from the constrained stellar photosphere temperature. This result emphasizes that unresolved inner debris components cannot be fully characterized until the behaviour of the host star's intrinsic stellar emission at millimetre wavelengths is properly understood.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We study coloring problems, which are induced subposets P of a Boolean lattice, paired with an order ideal I from the poset of intervals, ordered by inclusion. We study a quasisymmetric function associated to coloring problems, called the chromatic quasisymmetric function, generalizing Stanley's chromatic symmetric function of a graph. We show that the chromatic quasisymmetric function is an Ehrhart quasisymmetric function, and that a transformation of the chromatic quasisymmetric function gives a Hilbert polynomial. Finally, we introduce combinatorial Hopf monoids in pointed set species, and show that coloring problems form the terminal object in the category of combinatorial Hopf monoids.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "Abell 2146 consists of two galaxy clusters that have recently collided close to the plane of the sky, and it is unique in showing two large shocks on $\\textit{Chandra X-ray Observatory}$ images. With an early stage merger, shortly after first core passage, one would expect the cluster galaxies and the dark matter to be leading the X-ray emitting plasma. In this regard, the cluster Abell 2146-A is very unusual in that the X-ray cool core appears to lead, rather than lag, the Brightest Cluster Galaxy (BCG) in their trajectories. Here we present a strong lensing analysis of multiple image systems identified on $\\textit{Hubble Space Telescope}$ images. In particular, we focus on the distribution of mass in Abell 2146-A in order to determine the centroid of the dark matter halo. We use object colours and morphologies to identify multiple image systems; very conservatively, four of these systems are used as constraints on a lens mass model. We find that the centroid of the dark matter halo, constrained using the strongly lensed features, is coincident with the BCG, with an offset of $\\approx$ 2 kpc between the centres of the dark matter halo and the BCG. Thus from the strong lensing model, the X-ray cool core also leads the centroid of the dark matter in Abell 2146-A, with an offset of $\\approx$ 30 kpc.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "Abell 2146 ($z$ = 0.232) consists of two galaxy clusters undergoing a major merger. The system was discovered in previous work, where two large shock fronts were detected using the $\\textit{Chandra X-ray Observatory}$, consistent with a merger close to the plane of the sky, caught soon after first core passage. A weak gravitational lensing analysis of the total gravitating mass in the system, using the distorted shapes of distant galaxies seen with ACS-WFC on $\\textit{Hubble Space Telescope}$, is presented. The highest peak in the reconstruction of the projected mass is centred on the Brightest Cluster Galaxy (BCG) in Abell 2146-A. The mass associated with Abell 2146-B is more extended. Bootstrapped noise mass reconstructions show the mass peak in Abell 2146-A to be consistently centred on the BCG. Previous work showed that BCG-A appears to lag behind an X-ray cool core; although the peak of the mass reconstruction is centred on the BCG, it is also consistent with the X-ray peak given the resolution of the weak lensing mass map. The best-fit mass model with two components centred on the BCGs yields $M_{200}$ = 1.1$^{+0.3}_{-0.4}$$\\times$10$^{15}$M$_{\\odot}$ and 3$^{+1}_{-2}$$\\times$10$^{14}$M$_{\\odot}$ for Abell 2146-A and Abell 2146-B respectively, assuming a mass concentration parameter of $c=3.5$ for each cluster. From the weak lensing analysis, Abell 2146-A is the primary halo component, and the origin of the apparent discrepancy with the X-ray analysis where Abell 2146-B is the primary halo is being assessed using simulations of the merger.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We present ALMA band 7 (345 GHz) continuum and $^{12}$CO(J = 3-2) observations of the circumstellar disk surrounding HD141569. At an age of about 5 Myr, the disk has a complex morphology that may be best interpreted as a nascent debris system with gas. Our $870\\rm~\u03bcm$ ALMA continuum observations resolve a dust disk out to approximately $ 56 ~\\rm au$ from the star (assuming a distance of 116 pc) with $0.\"38$ resolution and $0.07 ~ \\rm mJy~beam^{-1}$ sensitivity. We measure a continuum flux density for this inner material of $3.8 \\pm 0.4 ~ \\rm mJy$ (including calibration uncertainties). The $^{12}$CO(3-2) gas is resolved kinematically and spatially from about 30 to 210 au. The integrated $^{12}$CO(3-2) line flux density is $15.7 \\pm 1.6~\\rm Jy~km~s^{-1}$. We estimate the mass of the millimeter debris and $^{12}$CO(3-2) gas to be $\\gtrsim0.04~\\rm M_{\\oplus}$ and $\\sim2\\times 10^{-3}~\\rm M_{\\oplus}$, respectively. If the millimeter grains are part of a collisional cascade, then we infer that the inner disk ($<50$ au) has $\\sim 160~\\rm M_{\\oplus}$ contained within objects less than 50 km in radius, depending on the planetesimal size distribution and density assumptions. MCMC modeling of the system reveals a disk morphology with an inclination of $53.4^{\\circ}$ centered around a $\\rm M=2.39~ M_{\\odot}$ host star ($\\rm Msin(i)=1.92~ M_{\\odot}$). We discuss whether the gas in HD141569's disk may be second generation. If it is, the system can be used to study the clearing stages of planet formation.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We investigate quasisymmetric functions coming from combinatorial Hopf monoids. We show that these invariants arise naturally in Ehrhart theory, and that some of their specializations are Hilbert functions for relative simplicial complexes. This class of complexes, called forbidden composition complexes, also forms a Hopf monoid, thus demonstrating a link between Hopf algebras, Ehrhart theory, and commutative algebra. We also study various specializations of quasisymmetric functions.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We consider the solution of linear saddle-point problems, using the alternating direction method-of-multipliers (ADMM) as a preconditioner for the generalized minimum residual method (GMRES). We show, using theoretical bounds and empirical results, that ADMM is made remarkably insensitive to the parameter choice with Krylov subspace acceleration. We prove that ADMM-GMRES can consistently converge, irrespective of the exact parameter choice, to an $\u03b5$-accurate solution of a $\u03ba$-conditioned problem in $O(\u03ba^{2/3}\\log\u03b5^{-1})$ iterations. The accelerated method is applied to randomly generated problems, as well as the Newton direction computation for the interior-point solution of semidefinite programs in the SDPLIB test suite. The empirical results confirm this parameter insensitivity, and suggest a slightly improved iteration bound of $O(\\sqrt\u03ba\\log\u03b5^{-1})$.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We consider the sequence acceleration problem for the alternating direction method-of-multipliers (ADMM) applied to a class of equality-constrained problems with strongly convex quadratic objectives, which frequently arise as the Newton subproblem of interior-point methods. Within this context, the ADMM update equations are linear, the iterates are confined within a Krylov subspace, and the General Minimum RESidual (GMRES) algorithm is optimal in its ability to accelerate convergence. The basic ADMM method solves a $\u03ba$-conditioned problem in $O(\\sqrt\u03ba)$ iterations. We give theoretical justification and numerical evidence that the GMRES-accelerated variant consistently solves the same problem in $O(\u03ba^{1/4})$ iterations for an order-of-magnitude reduction in iterations, despite a worst-case bound of $O(\\sqrt\u03ba)$ iterations. The method is shown to be competitive against standard preconditioned Krylov subspace methods for saddle-point problems. The method is embedded within SeDuMi, a popular open-source solver for conic optimization written in MATLAB, and used to solve many large-scale semidefinite programs with error that decreases like $O(1/k^{2})$, instead of $O(1/k)$, where $k$ is the iteration index.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "The disk around HD 141569 is one of a handful of systems whose weak infrared emission is consistent with a debris disk, but still has a significant reservoir of gas. Here we report spatially resolved mm observations of the CO(3-2) and CO(1-0) emission as seen with the SMA and CARMA. We find that the excitation temperature for CO is lower than expected from cospatial blackbody grains, similar to previous observations of analogous systems, and derive a gas mass that lies between that of gas-rich primordial disks and gas-poor debris disks. The data also indicate a large inner hole in the CO gas distribution and an outer radius that lies interior to the outer scattered light rings. This spatial distribution, with the dust rings just outside the gaseous disk, is consistent with the expected interactions between gas and dust in an optically thin disk. This indicates that gas can have a significant effect on the location of the dust within debris disks.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We present a dynamical analysis of the merging galaxy cluster system Abell 2146 using spectroscopy obtained with the Gemini Multi-Object Spectrograph on the Gemini North telescope. As revealed by the Chandra X-ray Observatory, the system is undergoing a major merger and has a gas structure indicative of a recent first core passage. The system presents two large shock fronts, making it unique amongst these rare systems. The hot gas structure indicates that the merger axis must be close to the plane of the sky and that the two merging clusters are relatively close in mass, from the observation of two shock fronts. Using 63 spectroscopically determined cluster members, we apply various statistical tests to establish the presence of two distinct massive structures. With the caveat that the system has recently undergone a major merger, the virial mass estimate is M_vir = 8.5 +4.3 -4.7 x 10 ^14 M_sol for the whole system, consistent with the mass determination in a previous study using the Sunyaev-Zeldovich signal. The newly calculated redshift for the system is z = 0.2323. A two-body dynamical model gives an angle of 13-19 degrees between the merger axis and the plane of the sky, and a timescale after first core passage of 0.24-0.28 Gyr.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We verify a conjecture of Nowakowski and Ottaway that closed $1 \\times n$ Dots-and-Triangles is a first-player win when $n \\neq 2$. We also prove that in both the open and closed $1 \\times n$ Dots-and-Boxes games where $n$ is even, the first player can guarantee a tie.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We describe a fluctuating volume--current formulation of electromagnetic fluctuations that extends our recent work on heat exchange and Casimir interactions between arbitrarily shaped homogeneous bodies [Phys. Rev. B. 88, 054305] to situations involving incandescence and luminescence problems, including thermal radiation, heat transfer, Casimir forces, spontaneous emission, fluorescence, and Raman scattering, in inhomogeneous media. Unlike previous scattering formulations based on field and/or surface unknowns, our work exploits powerful techniques from the volume--integral equation (VIE) method, in which electromagnetic scattering is described in terms of volumetric, current unknowns throughout the bodies. The resulting trace formulas (boxed equations) involve products of well-studied VIE matrices and describe power and momentum transfer between objects with spatially varying material properties and fluctuation characteristics. We demonstrate that thanks to the low-rank properties of the associatedmatrices, these formulas are susceptible to fast-trace computations based on iterative methods, making practical calculations tractable. We apply our techniques to study thermal radiation, heat transfer, and fluorescence in complicated geometries, checking our method against established techniques best suited for homogeneous bodies as well as applying it to obtain predictions of radiation from complex bodies with spatially varying permittivities and/or temperature profiles.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "The degradation of signal in silicon sensors is studied under conditions expected at the CERN High-Luminosity LHC. 200 $\u03bc$m thick n-type silicon sensors are irradiated with protons of different energies to fluences of up to $3 \\cdot 10^{15}$ neq/cm$^2$. Pulsed red laser light with a wavelength of 672 nm is used to generate electron-hole pairs in the sensors. The induced signals are used to determine the charge collection efficiencies separately for electrons and holes drifting through the sensor. The effective trapping rates are extracted by comparing the results to simulation. The electric field is simulated using Synopsys device simulation assuming two effective defects. The generation and drift of charge carriers are simulated in an independent simulation based on PixelAV. The effective trapping rates are determined from the measured charge collection efficiencies and the simulated and measured time-resolved current pulses are compared. The effective trapping rates determined for both electrons and holes are about 50% smaller than those obtained using standard extrapolations of studies at low fluences and suggests an improved tracker performance over initial expectations.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We investigate double transitivity of Galois groups in the classical Schubert calculus on Grassmannians. We show that all Schubert problems on Grassmannians of 2- and 3-planes have doubly transitive Galois groups, as do all Schubert problems involving only special Schubert conditions. We use these results to give a new proof that Schubert problems on Grassmannians of 2-planes have Galois groups that contain the alternating group. We also investigate the Galois group of every Schubert problem on Gr(4,8), finding that each Galois group either contains the alternating group or is an imprimitive permutation group and therefore fails to be doubly transitive. These imprimitive examples show that our results are the best possible general results on double transitivity of Schubert problems.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We present a generic technique, automated by computer-algebra systems and available as open-source software \\cite{scuff-em}, for efficient numerical evaluation of a large family of singular and nonsingular 4-dimensional integrals over triangle-product domains, such as those arising in the boundary-element method (BEM) of computational electromagnetism. To date, practical implementation of BEM solvers has often required the aggregation of multiple disparate integral-evaluation schemes to treat all of the distinct types of integrals needed for a given BEM formulation; in contrast, our technique allows many different types of integrals to be handled by the \\emph{same} algorithm and the same code implementation. Our method is a significant generalization of the Taylor--Duffy approach \\cite{Taylor2003,Duffy1982}, which was originally presented for just a single type of integrand; in addition to generalizing this technique to a broad class of integrands, we also achieve a significant improvement in its efficiency by showing how the \\emph{dimension} of the final numerical integral may often be reduced by one. In particular, if $n$ is the number of common vertices between the two triangles, in many cases we can reduce the dimension of the integral from $4-n$ to $3-n$, obtaining a closed-form analytical result for $n=3$ (the common-triangle case).\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "In this paper we define and study a notion of discrete homology theory for metric spaces. Instead of working with simplicial homology, our chain complexes are given by Lipschitz maps from an $n$-dimensional cube to a fixed metric space. We prove that the resulting homology theory verifies a discrete analogue of the Eilenberg-Steenrod axioms, and prove a discrete analogue of the Mayer-Vietoris exact sequence. Moreover, this discrete homology theory is related to the discrete homotopy theory of a metric space through a discrete analogue of the Hurewicz theorem. We study the class of groups that can arise as discrete homology groups and, in this setting, we prove that the fundamental group of a smooth, connected, metrizable, compact manifold is isomorphic to the discrete fundamental group of a `fine enough' rectangulation of the manifold. Finally, we show that this discrete homology theory can be coarsened, leading to a new non-trivial coarse invariant of a metric space.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We define decision trees for monotone functions on a simplicial complex. We define homology decidability of monotone functions, and show that various monotone functions related to semimatroids are homology decidable. Homology decidability is a generalization of semi-nonevasiveness, a notion due to Jonsson. The motivating example is the complex of bipartite graphs, whose Betti numbers are unknown in general.\n  We show that these monotone functions have optimum decision trees, from which we can compute relative Betti numbers of related pairs of simplicial complexes. Moreover, these relative Betti numbers are coefficients of evaluations of the Tutte polynomial, and every semimatroid collapses onto its broken circuit complex.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "This paper presents a new method for the efficient numerical computation of Casimir interactions between objects of arbitrary geometries, composed of materials with arbitrary frequency-dependent electrical properties. Our method formulates the Casimir effect as an interaction between effective electric and magnetic current distributions on the surfaces of material bodies, and obtains Casimir energies, forces, and torques from the spectral properties of a matrix that quantifies the interactions of these surface currents. The method can be formulated and understood in two distinct ways: \\textbf{(1)} as a consequence of the familiar \\textit{stress-tensor} approach to Casimir physics, or, alternatively, \\textbf{(2)} as a particular case of the \\textit{path-integral} approach to Casimir physics, and we present both formulations in full detail. In addition to providing an algorithm for computing Casimir interactions in geometries that could not be efficiently handled by any other method, the framework proposed here thus achieves an explicit unification of two seemingly disparate approaches to computational Casimir physics.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "A computational scheme for solving 2D Laplace boundary-value problems using rational functions as the basis functions is described. The scheme belongs to the class of desingularized methods, for which the location of singularities and testing points is a major issue that is addressed by the proposed scheme, in the context of the 2D Laplace equation. Well-established rational-function fitting techniques are used to set the poles, while residues are determined by enforcing the boundary conditions in the least-squares sense at the nodes of rational Gauss-Chebyshev quadrature rules. Numerical results show that errors approaching the machine epsilon can be obtained for sharp and almost sharp corners, nearly-touching boundaries, and almost-singular boundary data. We show various examples of these cases in which the method yields compact solutions, requiring fewer basis functions than the Nystr\u00f6m method, for the same accuracy. A scheme for solving fairly large-scale problems is also presented.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "Combinatorial Hopf algebras arise in a variety of applications. Recently, Aguiar and Mahajan showed how many well-studied Hopf algebras are closely related to Hopf monoids in species.\n  In this paper, we study Hopf monoids in graphical species, giving a `graph-theoretic' analogue to the work of Aguiar and Mahajan. In particular, several examples of Hopf monoids in graphical species are detailed, most of which are related to graph coloring, or hyperplane arrangements associated to graphs.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "Motivation: Modeling biological signaling networks using ordinary differential equations (ODEs) has proven to be a powerful technique for generating insight into cellular dynamics, but it typically requires estimating rate parameters based on experimentally observed concentrations. New measurement methods can measure concentrations for all molecular species in a pathway, which creates a new opportunity to decompose the optimization of rate parameters. Results: In contrast with conventional methods that minimize the disagreement between simulated and observed concentrations, the BPPE method fits a spline curve through the observed concentration points, and then matches the derivatives of the spline-curve to the production and consumption of each species. Whereas traditional methods follow the ODEs exactly and then attempt to match the data, BPPE follows the data exactly and then attempts to match the ODEs. The new objective function is an extreme decomposition of the problem because each factor in the function is enforcing the equality of one ODE at one timeslice. A \"loopy belief propagation\" algorithm solves this factorized approximation of the parameter estimation problem providing systematic coverage of the search space and unique asymptotic behavior; the run time is polynomial in the number of molecules and timepoints, but exponential in the degree of the biochemical network. The implementation is a global-local hybrid optimization, and we compare with the performance of local, global, and hybrid methods. BPPE is demonstrated for a novel model of Akt activation dynamics including redox-mediated inactivation of PTEN. Availability: Software and supplementary information are available at http://webbppe.nus.edu.sg:8080/opal2/WebBPPE . Contact: LisaTK@nus.edu.sg . Keywords: probabilistic graphical models, physico-chemical modeling, systems biology, signal transduction.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "Building on the legacy of the Sloan Digital Sky Survey (SDSS-I and II), SDSS-III is a program of four spectroscopic surveys on three scientific themes: dark energy and cosmological parameters, the history and structure of the Milky Way, and the population of giant planets around other stars. In keeping with SDSS tradition, SDSS-III will provide regular public releases of all its data, beginning with SDSS DR8 (which occurred in Jan 2011). This paper presents an overview of the four SDSS-III surveys. BOSS will measure redshifts of 1.5 million massive galaxies and Lya forest spectra of 150,000 quasars, using the BAO feature of large scale structure to obtain percent-level determinations of the distance scale and Hubble expansion rate at z<0.7 and at z~2.5. SEGUE-2, which is now completed, measured medium-resolution (R=1800) optical spectra of 118,000 stars in a variety of target categories, probing chemical evolution, stellar kinematics and substructure, and the mass profile of the dark matter halo from the solar neighborhood to distances of 100 kpc. APOGEE will obtain high-resolution (R~30,000), high signal-to-noise (S/N>100 per resolution element), H-band (1.51-1.70 micron) spectra of 10^5 evolved, late-type stars, measuring separate abundances for ~15 elements per star and creating the first high-precision spectroscopic survey of all Galactic stellar populations (bulge, bar, disks, halo) with a uniform set of stellar tracers and spectral diagnostics. MARVELS will monitor radial velocities of more than 8000 FGK stars with the sensitivity and cadence (10-40 m/s, ~24 visits per star) needed to detect giant planets with periods up to two years, providing an unprecedented data set for understanding the formation and dynamical evolution of giant planet systems. (Abridged)\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "In this paper, we consider multivariate hyperedge elimination polynomials and multivariate chromatic polynomials for hypergraphs. The first set of polynomials is defined in terms of a deletion-contraction-extraction recurrence, previously investigated for graphs by Averbouch, Godlin, and Makowsky. The multivariate chromatic polynomial is an equivalent polynomial defined in terms of colorings, and generalizes the coboundary polynomial of Crapo, and the bivariate chromatic polynomial of Dohmen, P\u00f6nitz and Tittman. We show that specializations of these new polynomials recover polynomials which enumerate hyperedge coverings, matchings, transversals, and section hypergraphs. We also prove that the polynomials can be defined in terms of M\u00f6bius inversion on the bond lattice of a hypergraph, as well as compute these polynomials for various classes of hypergraphs.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "In this paper, we study $k$-parabolic arrangements, a generalization of the $k$-equal arrangement for any finite real reflection group. When $k=2$, these arrangements correspond to the well-studied Coxeter arrangements. We construct a cell complex $Perm_k(W)$ that is homotopy equivalent to the complement. We then apply discrete Morse theory to obtain a minimal cell complex for the complement. As a result, we give combinatorial interpretations for the Betti numbers, and show that the homology groups are torsion free. We also study a generalization of the Independence Complex of a graph, and show that this generalization is shellable when the graph is a forest. This result is used in studying $Perm_k(W)$ using discrete Morse theory.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "The associahedron is an object that has been well studied and has numerous applications, particularly in the theory of operads, the study of non-crossing partitions, lattice theory and more recently in the study of cluster algebras. We approach the associahedron from the point of view of discrete homotopy theory. We study the abelianization of the discrete fundamental group, and show that it is free abelian of rank $\\binom{n+2}{4}$. We also find a combinatorial description for a basis of this rank. We also introduce the exchange module of the type $A_n$ cluster algebra, used to model the relations in the cluster algebra. We use the discrete fundamental group to the study of exchange module, and show that it is also free abelian of rank $\\binom{n+2}{3}$.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We show how to compute Casimir forces at nonzero temperatures with time-domain electromagnetic simulations, for example using a finite-difference time-domain (FDTD) method. Compared to our previous zero-temperature time-domain method, only a small modification is required, but we explain that some care is required to properly capture the zero-frequency contribution. We validate the method against analytical and numerical frequency-domain calculations, and show a surprising high-temperature disappearance of a non-monotonic behavior previously demonstrated in a piston-like geometry.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We extend a recently introduced method for computing Casimir forces between arbitrarily--shaped metallic objects [M. T. H. Reid et al., Phys. Rev. Lett._103_ 040401 (2009)] to allow treatment of objects with arbitrary material properties, including imperfect conductors, dielectrics, and magnetic materials. Our original method considered electric currents on the surfaces of the interacting objects; the extended method considers both electric and magnetic surface current distributions, and obtains the Casimir energy of a configuration of objects in terms of the interactions of these effective surface currents. Using this new technique, we present the first predictions of Casimir interactions in several experimentally relevant geometries that would be difficult to treat with any existing method. In particular, we investigate Casimir interactions between dielectric nanodisks embedded in a dielectric fluid; we identify the threshold surface--surface separation at which finite--size effects become relevant, and we map the rotational energy landscape of bound nanoparticle diclusters.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "Fast surface integral equation (SIE) solvers seem to be ideal approaches for simulating 3-D nanophotonic devices, as these devices generate fields both in an interior channel and in the infinite exterior domain. However, many devices of interest, such as optical couplers, have channels that can not be terminated without generating reflections.  Generating absorbers for these channels is a new problem for SIE methods, as the methods were initially developed for problems with finite surfaces. In this paper we show that the obvious approach for eliminating reflections, making the channel mildly conductive outside the domain of interest, is inaccurate. We describe a new method, in which the absorber has a gradually increasing surface conductivity; such an absorber can be easily incorporated in fast integral equation solvers. Numerical experiments from a surface-conductivity modified FFT-accelerated PMCHW-based solver are correlated with analytic results, demonstrating that this new method is orders of magnitude more effective than a volume absorber, and that the smoothness of the surface conductivity function determines the performance of the absorber. In particular, we show that the magnitude of the transition reflection is proportional to 1/L^(2d+2), where L is the absorber length and d is the order of the differentiability of the surface conductivity function.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "In this paper, we study $k$-parabolic arrangements, a generalization of $k$-equal arrangements for finite real reflection groups. When $k=2$, these arrangements correspond to the well-studied Coxeter arrangements. Brieskorn (1971) showed that the fundamental group of the complement, over $\\mathbb{C}$, of the type $W$ Coxeter arrangement is isomorphic to the pure Artin group of type $W$. Khovanov (1996) gave an algebraic description for the fundamental group of the complement, over $\\mathbb{R}$, of the 3-equal arrangement. We generalize Khovanov's result to obtain an algebraic description of the fundamental groups of the complements of 3-parabolic arrangements for arbitrary finite reflection groups. Our description is a real analogue to Brieskorn's description.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "The fundamental problem faced in quantum chemistry is the calculation of molecular properties, which are of practical importance in fields ranging from materials science to biochemistry. Within chemical precision, the total energy of a molecule as well as most other properties, can be calculated by solving the Schrodinger equation. However, the computational resources required to obtain exact solutions on a conventional computer generally increase exponentially with the number of atoms involved. This renders such calculations intractable for all but the smallest of systems. Recently, an efficient algorithm has been proposed enabling a quantum computer to overcome this problem by achieving only a polynomial resource scaling with system size. Such a tool would therefore provide an extremely powerful tool for new science and technology. Here we present a photonic implementation for the smallest problem: obtaining the energies of H2, the hydrogen molecule in a minimal basis. We perform a key algorithmic step - the iterative phase estimation algorithm - in full, achieving a high level of precision and robustness to error. We implement other algorithmic steps with assistance from a classical computer and explain how this non-scalable approach could be avoided. Finally, we provide new theoretical results which lay the foundations for the next generation of simulation experiments using quantum computers. We have made early experimental progress towards the long-term goal of exploiting quantum information to speed up quantum chemistry calculations.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We introduce an efficient technique for computing Casimir energies and forces between objects of arbitrarily complex 3D geometries. In contrast to other recently developed methods, our technique easily handles non-spheroidal, non-axisymmetric objects and objects with sharp corners. Using our new technique, we obtain the first predictions of Casimir interactions in a number of experimentally relevant geometries, including crossed cylinders and tetrahedral nanoparticles.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We report detailed measurements of the low temperature magnetic phase diagram of Er$_2$Ti$_2$O$_7$. Heat capacity and time-of-flight neutron scattering studies of single crystals, subject to magnetic fields applied along the crystallographic [110] direction, reveal unconventional low energy states. Er$^{3+}$ magnetic ions reside on a pyrochlore lattice in Er$_2$Ti$_2$O$_7$, where local XY anisotropy and antiferromagnetic interactions give rise to a unique frustrated system. In zero field, the ground state exhibits coexisting short and long range order, accompanied by soft collective spin excitations previously believed to be absent. The application of finite magnetic fields tunes the ground state continuously through a landscape of non-collinear phases, divided by a zero temperature phase transition at $\u03bc_0 H_c \\sim$ 1.5 T. The characteristic energy scale for spin fluctuations is seen to vanish at the critical point, as expected for a second order quantum phase transition driven by quantum fluctuations.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We present the results for the predicted density and peculiar velocity fields and the dipole from the PSCz survey of 15,000 IRAS galaxies over 84% of the sky. We find a significant component to the dipole arising between 6000 and 15,000 km/s, but no significant component from greater distances. The misalignment with the CMB is 20 degrees. The most remarkable feature of the PSCz model velocity field is a coherent large-scale flow along the baseline connecting Perseus-Pisces, the Local Supercluster, Great Attractor and the Shapley Concentration. We have measured the parameter beta using the amplitude of the dipole, bulk flow and point by point comparisons between the individual velocities of galaxies in the MarkIII and SFI datasets, and the large-scale clustering distortion in redshift space.All our results are consistent with beta = 0.6 +- 0.1.\n        \u25b3 Less", "author": "Jacob White"}, {"abstract": "We study the known techniques for designing Matrix Multiplication algorithms. The two main approaches are the Laser method of Strassen, and the Group theoretic approach of Cohn and Umans. We define a generalization based on zeroing outs which subsumes these two approaches, which we call the Solar method, and an even more general method based on monomial degenerations, which we call the Galactic method.\n  We then design a suite of techniques for proving lower bounds on the value of $\u03c9$, the exponent of matrix multiplication, which can be achieved by algorithms using many tensors $T$ and the Galactic method. Some of our techniques exploit `local' properties of $T$, like finding a sub-tensor of $T$ which is so `weak' that $T$ itself couldn't be used to achieve a good bound on $\u03c9$, while others exploit `global' properties, like $T$ being a monomial degeneration of the structural tensor of a group algebra.\n  Our main result is that there is a universal constant $\\ell>2$ such that a large class of tensors generalizing the Coppersmith-Winograd tensor $CW_q$ cannot be used within the Galactic method to show a bound on $\u03c9$ better than $\\ell$, for any $q$. We give evidence that previous lower-bounding techniques were not strong enough to show this. We also prove a number of complementary results along the way, including that for any group $G$, the structural tensor of $\\mathbb{C}[G]$ can be used to recover the best bound on $\u03c9$ which the Coppersmith-Winograd approach gets using $CW_{|G|-2}$ as long as the asymptotic rank of the structural tensor is not too large.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Among the most important graph parameters is the Diameter, the largest distance between any two vertices. There are no known very efficient algorithms for computing the Diameter exactly. Thus, much research has been devoted to how fast this parameter can be approximated. Chechik et al. showed that the diameter can be approximated within a multiplicative factor of $3/2$ in $\\tilde{O}(m^{3/2})$ time. Furthermore, Roditty and Vassilevska W. showed that unless the Strong Exponential Time Hypothesis (SETH) fails, no $O(n^{2-\u03b5})$ time algorithm can achieve an approximation factor better than $3/2$ in sparse graphs. Thus the above algorithm is essentially optimal for sparse graphs for approximation factors less than $3/2$. It was, however, completely plausible that a $3/2$-approximation is possible in linear time. In this work we conditionally rule out such a possibility by showing that unless SETH fails no $O(m^{3/2-\u03b5})$ time algorithm can achieve an approximation factor better than $5/3$.\n  Another fundamental set of graph parameters are the Eccentricities. The Eccentricity of a vertex $v$ is the distance between $v$ and the farthest vertex from $v$. Chechik et al. showed that the Eccentricities of all vertices can be approximated within a factor of $5/3$ in $\\tilde{O}(m^{3/2})$ time and Abboud et al. showed that no $O(n^{2-\u03b5})$ algorithm can achieve better than $5/3$ approximation in sparse graphs. We show that the runtime of the $5/3$ approximation algorithm is also optimal under SETH. We also show that no near-linear time algorithm can achieve a better than $2$ approximation for the Eccentricities and that this is essentially tight: we give an algorithm that approximates Eccentricities within a $2+\u03b4$ factor in $\\tilde{O}(m/\u03b4)$ time for any $0<\u03b4<1$. This beats all Eccentricity algorithms in Cairo et al.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Since the introduction of retroactive data structures at SODA 2004, a major unsolved problem has been to bound the gap between the best partially retroactive data structure (where changes can be made to the past, but only the present can be queried) and the best fully retroactive data structure (where the past can also be queried) for any problem. It was proved in 2004 that any partially retroactive data structure with operation time $T(n,m)$ can be transformed into a fully retroactive data structure with operation time $O(\\sqrt{m} \\cdot T(n,m))$, where $n$ is the size of the data structure and $m$ is the number of operations in the timeline [Demaine 2004], but it has been open for 14 years whether such a gap is necessary.\n  In this paper, we prove nearly matching upper and lower bounds on this gap for all $n$ and $m$. We improve the upper bound for $n \\ll \\sqrt m$ by showing a new transformation with multiplicative overhead $n \\log m$. We then prove a lower bound of $\\min\\{n \\log m, \\sqrt m\\}^{1-o(1)}$ assuming any of the following conjectures:\n  - Conjecture I: Circuit SAT requires $2^{n - o(n)}$ time on $n$-input circuits of size $2^{o(n)}$. (Far weaker than the well-believed SETH conjecture, which asserts that CNF SAT with $n$ variables and $O(n)$ clauses already requires $2^{n-o(n)}$ time.)\n  - Conjecture II: Online $(\\min,+)$ product between an integer $n\\times n$ matrix and $n$ vectors requires $n^{3 - o(1)}$ time.\n  - Conjecture III (3-SUM Conjecture): Given three sets $A,B,C$ of integers, each of size $n$, deciding whether there exist $a \\in A, b \\in B, c \\in C$ such that $a + b + c = 0$ requires $n^{2 - o(1)}$ time.\n  Our lower bound construction illustrates an interesting power of fully retroactive queries: they can be used to quickly solve batched pair evaluation. We believe this technique can prove useful for other data structure lower bounds, especially dynamic ones.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Fine-grained reductions have established equivalences between many core problems with $\\tilde{O}(n^3)$-time algorithms on $n$-node weighted graphs, such as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement Paths, Second Shortest Paths, and so on. These problems also have $\\tilde{O}(mn)$-time algorithms on $m$-edge $n$-node weighted graphs, and such algorithms have wider applicability. Are these $mn$ bounds optimal when $m \\ll n^2$?\n  Starting from the hypothesis that the minimum weight $(2\\ell+1)$-Clique problem in edge weighted graphs requires $n^{2\\ell+1-o(1)}$ time, we prove that for all sparsities of the form $m = \u0398(n^{1+1/\\ell})$, there is no $O(n^2 + mn^{1-\u03b5})$ time algorithm for $\u03b5>0$ for \\emph{any} of the below problems:\n  Minimum Weight $(2\\ell+1)$-Cycle in a directed weighted graph,\n  Shortest Cycle in a directed weighted graph,\n  APSP in a directed or undirected weighted graph,\n  Radius (or Eccentricities) in a directed or undirected weighted graph,\n  Wiener index of a directed or undirected weighted graph,\n  Replacement Paths in a directed weighted graph,\n  Second Shortest Path in a directed weighted graph,\n  Betweenness Centrality of a given node in a directed weighted graph.\n  That is, we prove hardness for a variety of sparse graph problems from the hardness of a dense graph problem. Our results also lead to new conditional lower bounds from several related hypothesis for unweighted sparse graph problems including $k$-cycle, shortest cycle, Radius, Wiener index and APSP.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "We consider the techniques behind the current best algorithms for matrix multiplication. Our results are threefold.\n  (1) We provide a unifying framework, showing that all known matrix multiplication running times since 1986 can be achieved from a single very natural tensor - the structural tensor $T_q$ of addition modulo an integer $q$.\n  (2) We show that if one applies a generalization of the known techniques (arbitrary zeroing out of tensor powers to obtain independent matrix products in order to use the asymptotic sum inequality of Sch\u00f6nhage) to an arbitrary monomial degeneration of $T_q$, then there is an explicit lower bound, depending on $q$, on the bound on the matrix multiplication exponent $\u03c9$ that one can achieve. We also show upper bounds on the value $\u03b1$ that one can achieve, where $\u03b1$ is such that $n\\times n^\u03b1\\times n$ matrix multiplication can be computed in $n^{2+o(1)}$ time.\n  (3) We show that our lower bound on $\u03c9$ approaches $2$ as $q$ goes to infinity. This suggests a promising approach to improving the bound on $\u03c9$: for variable $q$, find a monomial degeneration of $T_q$ which, using the known techniques, produces an upper bound on $\u03c9$ as a function of $q$. Then, take $q$ to infinity. It is not ruled out, and hence possible, that one can obtain $\u03c9=2$ in this way.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "This paper initiates the study of I/O algorithms (minimizing cache misses) from the perspective of fine-grained complexity (conditional polynomial lower bounds). Specifically, we aim to answer why sparse graph problems are so hard, and why the Longest Common Subsequence problem gets a savings of a factor of the size of cache times the length of a cache line, but no more. We take the reductions and techniques from complexity and fine-grained complexity and apply them to the I/O model to generate new (conditional) lower bounds as well as faster algorithms. We also prove the existence of a time hierarchy for the I/O model, which motivates the fine-grained reductions.\n  Using fine-grained reductions, we give an algorithm for distinguishing 2 vs. 3 diameter and radius that runs in $O(|E|^2/(MB))$ cache misses, which for sparse graphs improves over the previous $O(|V|^2/B)$ running time. We give new reductions from radius and diameter to Wiener index and median. We show meaningful reductions between problems that have linear-time solutions in the RAM model. The reductions use low I/O complexity (typically $O(n/B)$), and thus help to finely capture the relationship between \"I/O linear time\" $\u0398(n/B)$ and RAM linear time $\u0398(n)$. We generate new I/O assumptions based on the difficulty of improving sparse graph problem running times in the I/O model. We create conjectures that the current best known algorithms for Single Source Shortest Paths (SSSP), diameter, and radius are optimal. From these I/O-model assumptions, we show that many of the known reductions in the word-RAM model can naturally extend to hold in the I/O model as well (e.g., a lower bound on the I/O complexity of Longest Common Subsequence that matches the best known running time). Finally, we prove an analog of the Time Hierarchy Theorem in the I/O model.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "A $k$-spanner of a graph $G$ is a sparse subgraph $H$ whose shortest path distances match those of $G$ up to a multiplicative error $k$. In this paper we study spanners that are resistant to faults. A subgraph $H \\subseteq G$ is an $f$ vertex fault tolerant (VFT) $k$-spanner if $H \\setminus F$ is a $k$-spanner of $G \\setminus F$ for any small set $F$ of $f$ vertices that might \"fail.\" One of the main questions in the area is: what is the minimum size of an $f$ fault tolerant $k$-spanner that holds for all $n$ node graphs (as a function of $f$, $k$ and $n$)? This question was first studied in the context of geometric graphs [Levcopoulos et al. STOC '98, Czumaj and Zhao SoCG '03] and has more recently been considered in general undirected graphs [Chechik et al. STOC '09, Dinitz and Krauthgamer PODC '11].\n  In this paper, we settle the question of the optimal size of a VFT spanner, in the setting where the stretch factor $k$ is fixed. Specifically, we prove that every (undirected, possibly weighted) $n$-node graph $G$ has a $(2k-1)$-spanner resilient to $f$ vertex faults with $O_k(f^{1 - 1/k} n^{1 + 1/k})$ edges, and this is fully optimal (unless the famous Erdos Girth Conjecture is false). Our lower bound even generalizes to imply that no data structure capable of approximating $dist_{G \\setminus F}(s, t)$ similarly can beat the space usage of our spanner in the worst case. We also consider the edge fault tolerant (EFT) model, defined analogously with edge failures rather than vertex failures. We show that the same spanner upper bound applies in this setting. Our data structure lower bound extends to the case $k=2$ (and hence we close the EFT problem for $3$-approximations), but it falls to $\u03a9(f^{1/2 - 1/(2k)} \\cdot n^{1 + 1/k})$ for $k \\ge 3$. We leave it as an open problem to close this gap.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "It is a major open problem whether the $(\\min,+)$-product of two $n\\times n$ matrices has a truly sub-cubic (i.e. $O(n^{3-\u03b5})$ for $\u03b5>0$) time algorithm, in particular since it is equivalent to the famous All-Pairs-Shortest-Paths problem (APSP) in $n$-vertex graphs. Some restrictions of the $(\\min,+)$-product to special types of matrices are known to admit truly sub-cubic algorithms, each giving rise to a special case of APSP that can be solved faster. In this paper we consider a new, different and powerful restriction in which all matrix entries are integers and one matrix can be arbitrary, as long as the other matrix has \"bounded differences\" in either its columns or rows, i.e. any two consecutive entries differ by only a small amount. We obtain the first truly sub-cubic algorithm for this bounded-difference $(\\min,+)$-product (answering an open problem of Chan and Lewenstein).\n  Our new algorithm, combined with a strengthening of an approach of L.~Valiant for solving context-free grammar parsing with matrix multiplication, yields the first truly sub-cubic algorithms for the following problems: Language Edit Distance (a major problem in the parsing community), RNA-folding (a major problem in bioinformatics) and Optimum Stack Generation (answering an open problem of Tarjan).\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Fixed-parameter algorithms and kernelization are two powerful methods to solve $\\mathsf{NP}$-hard problems. Yet, so far those algorithms have been largely restricted to static inputs.\n  In this paper we provide fixed-parameter algorithms and kernelizations for fundamental $\\mathsf{NP}$-hard problems with dynamic inputs. We consider a variety of parameterized graph and hitting set problems which are known to have $f(k)n^{1+o(1)}$ time algorithms on inputs of size $n$, and we consider the question of whether there is a data structure that supports small updates (such as edge/vertex/set/element insertions and deletions) with an update time of $g(k)n^{o(1)}$; such an update time would be essentially optimal. Update and query times independent of $n$ are particularly desirable. Among many other results, we show that Feedback Vertex Set and $k$-Path admit dynamic algorithms with $f(k)\\log^{O(1)}n$ update and query times for some function $f$ depending on the solution size $k$ only.\n  We complement our positive results by several conditional and unconditional lower bounds. For example, we show that unlike their undirected counterparts, Directed Feedback Vertex Set and Directed $k$-Path do not admit dynamic algorithms with $n^{o(1)}$ update and query times even for constant solution sizes $k\\leq 3$, assuming popular hardness hypotheses. We also show that unconditionally, in the cell probe model, Directed Feedback Vertex Set cannot be solved with update time that is purely a function of $k$.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Preservers and additive spanners are sparse (hence cheap to store) subgraphs that preserve the distances between given pairs of nodes exactly or with some small additive error, respectively. Since real-world networks are prone to failures, it makes sense to study fault-tolerant versions of the above structures. This turns out to be a surprisingly difficult task. For every small but arbitrary set of edge or vertex failures, the preservers and spanners need to contain {\\em replacement paths} around the faulted set. In this paper we make substantial progress on fault tolerant preservers and additive spanners:\n  (1) We present the first truly sub-quadratic size single-pair preservers in unweighted (possibly directed) graphs for \\emph{any} fixed number $f$ of faults. Our result indeed generalizes to the single-source case, and can be used to build new fault-tolerant additive spanners (for all pairs).\n  (2) The size of the above single-pair preservers is $O(n^{2-g(f)})$ for some positive function $g$, and grows to $O(n^2)$ for increasing $f$. We show that this is necessary even in undirected unweighted graphs, and even if you allow for a small additive error: If you aim at size $O(n^{2-\u03b5})$ for $\u03b5>0$, then the additive error has to be $\u03a9(\\eps f)$. This surprisingly matches known upper bounds in the literature.\n  (3) For weighted graphs, we provide matching upper and lower bounds for the single pair case. Namely, the size of the preserver is $\u0398(n^2)$ for $f\\geq 2$ in both directed and undirected graphs, while for $f=1$ the size is $\u0398(n)$ in undirected graphs. For directed graphs, we have a superlinear upper bound and a matching lower bound.\n  Most of our lower bounds extend to the distance oracle setting, where rather than a subgraph we ask for any compact data structure.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Turbulence models attempt to account for unresolved dynamics and diffusion in hydrodynamical simulations. We develop a common framework for two-equation Reynolds-Averaged Navier-Stokes (RANS) turbulence models, and we implement six models in the Athena code. We verify each implementation with the standard subsonic mixing layer, although the level of agreement depends on the definition of the mixing layer width. We then test the validity of each model into the supersonic regime, showing that compressibility corrections can improve agreement with experiment. For models with buoyancy effects, we also verify our implementation via the growth of the Rayleigh-Taylor instability in a stratified medium. The models are then applied to the ubiquitous astrophysical shock-cloud interaction in three dimensions. We focus on the mixing of shock and cloud material, comparing results from turbulence models to high-resolution simulations (up to 200 cells per cloud radius) and ensemble-averaged simulations. We find that the turbulence models lead to increased spreading and mixing of the cloud, although no two models predict the same result. Increased mixing is also observed in inviscid simulations at resolutions greater than 100 cells per radius, which suggests that the turbulent mixing begins to be resolved.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "In recent years it has become popular to study dynamic problems in a sensitivity setting: Instead of allowing for an arbitrary sequence of updates, the sensitivity model only allows to apply batch updates of small size to the original input data. The sensitivity model is particularly appealing since recent strong conditional lower bounds ruled out fast algorithms for many dynamic problems, such as shortest paths, reachability, or subgraph connectivity.\n  In this paper we prove conditional lower bounds for sensitivity problems. For example, we show that under the Boolean Matrix Multiplication (BMM) conjecture combinatorial algorithms cannot compute the (4/3 - \u03b5)-approximate diameter of an undirected unweighted dense graph with truly subcubic preprocessing time and truly subquadratic update/query time. This result is surprising since in the static setting it is not clear whether a reduction from BMM to diameter is possible. We further show under the BMM conjecture that many problems, such as reachability or approximate shortest paths, cannot be solved faster than by recomputation from scratch even after only one or two edge insertions. We give more lower bounds under the Strong Exponential Time Hypothesis and the All Pairs Shortest Paths Conjecture. Many of our lower bounds also hold for static oracle data structures where no sensitivity is required. Finally, we give the first algorithm for the (1 + \u03b5)-approximate radius, diameter, and eccentricity problems in directed or undirected unweighted graphs in case of single edges failures. The algorithm has a truly subcubic running time for graphs with a truly subquadratic number of edges; it is tight w.r.t. the conditional lower bounds we obtain.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "The girth of a graph, i.e. the length of its shortest cycle, is a fundamental graph parameter. Unfortunately all known algorithms for computing, even approximately, the girth and girth-related structures in directed weighted $m$-edge and $n$-node graphs require $\u03a9(\\min\\{n^\u03c9, mn\\})$ time (for $2\\leq\u03c9<2.373$). In this paper, we drastically improve these runtimes as follows:\n  * Multiplicative Approximations in Nearly Linear Time: We give an algorithm that in $\\widetilde{O}(m)$ time computes an $\\widetilde{O}(1)$-multiplicative approximation of the girth as well as an $\\widetilde{O}(1)$-multiplicative roundtrip spanner with $\\widetilde{O}(n)$ edges with high probability (w.h.p).\n  * Nearly Tight Additive Approximations: For unweighted graphs and any $\u03b1\\in (0,1)$ we give an algorithm that in $\\widetilde{O}(mn^{1 - \u03b1})$ time computes an $O(n^\u03b1)$-additive approximation of the girth w.h.p, and partially derandomize it. We show that the runtime of our algorithm cannot be significantly improved without a breakthrough in combinatorial Boolean matrix multiplication.\n  Our main technical contribution to achieve these results is the first nearly linear time algorithm for computing roundtrip covers, a directed graph decomposition concept key to previous roundtrip spanner constructions. Previously it was not known how to compute these significantly faster than $\u03a9(\\min\\{n^\u03c9, mn\\})$ time. Given the traditional difficulty in efficiently processing directed graphs, we hope our techniques may find further applications.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Given a set of numbers, the $k$-SUM problem asks for a subset of $k$ numbers that sums to zero. When the numbers are integers, the time and space complexity of $k$-SUM is generally studied in the word-RAM model; when the numbers are reals, the complexity is studied in the real-RAM model, and space is measured by the number of reals held in memory at any point.\n  We present a time and space efficient deterministic self-reduction for the $k$-SUM problem which holds for both models, and has many interesting consequences. To illustrate:\n  * $3$-SUM is in deterministic time $O(n^2 \\lg\\lg(n)/\\lg(n))$ and space $O\\left(\\sqrt{\\frac{n \\lg(n)}{\\lg\\lg(n)}}\\right)$. In general, any polylogarithmic-time improvement over quadratic time for $3$-SUM can be converted into an algorithm with an identical time improvement but low space complexity as well. * $3$-SUM is in deterministic time $O(n^2)$ and space $O(\\sqrt n)$, derandomizing an algorithm of Wang.\n  * A popular conjecture states that 3-SUM requires $n^{2-o(1)}$ time on the word-RAM. We show that the 3-SUM Conjecture is in fact equivalent to the (seemingly weaker) conjecture that every $O(n^{.51})$-space algorithm for $3$-SUM requires at least $n^{2-o(1)}$ time on the word-RAM.\n  * For $k \\ge 4$, $k$-SUM is in deterministic $O(n^{k - 2 + 2/k})$ time and $O(\\sqrt{n})$ space.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "A single-elimination (SE) tournament is a popular way to select a winner in both sports competitions and in elections. A natural and well-studied question is the tournament fixing problem (TFP): given the set of all pairwise match outcomes, can a tournament organizer rig an SE tournament by adjusting the initial seeding so that their favorite player wins? We prove new sufficient conditions on the pairwise match outcome information and the favorite player, under which there is guaranteed to be a seeding where the player wins the tournament. Our results greatly generalize previous results. We also investigate the relationship between the set of players that can win an SE tournament under some seeding (so called SE winners) and other traditional tournament solutions. In addition, we generalize and strengthen prior work on probabilistic models for generating tournaments. For instance, we show that \\emph{every} player in an $n$ player tournament generated by the Condorcet Random Model will be an SE winner even when the noise is as small as possible, $p=\u0398(\\ln n/n)$; prior work only had such results for $p\\geq \u03a9(\\sqrt{\\ln n/n})$. We also establish new results for significantly more general generative models.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "A recent and active line of work achieves tight lower bounds for fundamental problems under the Strong Exponential Time Hypothesis (SETH). A celebrated result of Backurs and Indyk (STOC'15) proves that the Edit Distance of two sequences of length n cannot be computed in strongly subquadratic time under SETH. The result was extended by follow-up works to simpler looking problems like finding the Longest Common Subsequence (LCS).\n  SETH is a very strong assumption, asserting that even linear size CNF formulas cannot be analyzed for satisfiability with an exponential speedup over exhaustive search. We consider much safer assumptions, e.g. that such a speedup is impossible for SAT on much more expressive representations, like NC circuits. Intuitively, this seems much more plausible: NC circuits can implement complex cryptographic primitives, while CNFs cannot even approximately compute an XOR of bits.\n  Our main result is a surprising reduction from SAT on Branching Programs to fundamental problems in P like Edit Distance, LCS, and many others. Truly subquadratic algorithms for these problems therefore have consequences that we consider to be far more remarkable than merely faster CNF SAT algorithms. For example, SAT on arbitrary o(n)-depth bounded fan-in circuits (and therefore also NC-Circuit-SAT) can be solved in (2-eps)^n time.\n  A very interesting feature of our work is that we can prove major consequences even from mildly subquadratic algorithms for Edit Distance or LCS. For example, we show that if we can shave an arbitrarily large polylog factor from n^2 for Edit Distance then NEXP does not have non-uniform NC^1 circuits. A more fine-grained examination shows that even shaving a $\\log^c{n}$ factor, for a specific constant $c \\approx 10^3$, already implies new circuit lower bounds.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "We consider the minimum-weight feedback vertex set problem in tournaments: given a tournament with non-negative vertex weights, remove a minimum-weight set of vertices that intersects all cycles. This problem is $\\mathsf{NP}$-hard to solve exactly, and Unique Games-hard to approximate by a factor better than 2. We present the first $7/3$ approximation algorithm for this problem, improving on the previously best known ratio $5/2$ given by Cai et al. [FOCS 1998, SICOMP 2001].\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "The Subtree Isomorphism problem asks whether a given tree is contained in another given tree. The problem is of fundamental importance and has been studied since the 1960s. For some variants, e.g., ordered trees, near-linear time algorithms are known, but for the general case truly subquadratic algorithms remain elusive.\n  Our first result is a reduction from the Orthogonal Vectors problem to Subtree Isomorphism, showing that a truly subquadratic algorithm for the latter refutes the Strong Exponential Time Hypothesis (SETH).\n  In light of this conditional lower bound, we focus on natural special cases for which no truly subquadratic algorithms are known. We classify these cases against the quadratic barrier, showing in particular that:\n  -- Even for binary, rooted trees, a truly subquadratic algorithm refutes SETH.\n  -- Even for rooted trees of depth $O(\\log\\log{n})$, where $n$ is the total number of vertices, a truly subquadratic algorithm refutes SETH.\n  -- For every constant $d$, there is a constant $\u03b5_d>0$ and a randomized, truly subquadratic algorithm for degree-$d$ rooted trees of depth at most $(1+ \u03b5_d) \\log_{d}{n}$. In particular, there is an $O(\\min\\{ 2.85^h ,n^2 \\})$ algorithm for binary trees of depth $h$.\n  Our reductions utilize new \"tree gadgets\" that are likely useful for future SETH-based lower bounds for problems on trees. Our upper bounds apply a folklore result from randomized decision tree complexity.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "The radius and diameter are fundamental graph parameters. They are defined as the minimum and maximum of the eccentricities in a graph, respectively, where the eccentricity of a vertex is the largest distance from the vertex to another node. In directed graphs, there are several versions of these problems. For instance, one may choose to define the eccentricity of a node in terms of the largest distance into the node, out of the node, the sum of the two directions (i.e. roundtrip) and so on. All versions of diameter and radius can be solved via solving all-pairs shortest paths (APSP), followed by a fast postprocessing step. Solving APSP, however, on $n$-node graphs requires $\u03a9(n^2)$ time even in sparse graphs, as one needs to output $n^2$ distances.\n  Motivated by known and new negative results on the impossibility of computing these measures exactly in general graphs in truly subquadratic time, under plausible assumptions, we search for \\emph{approximation} and \\emph{fixed parameter subquadratic} algorithms, and for reasons why they do not exist.\n  Our results include: - Truly subquadratic approximation algorithms for most of the versions of Diameter and Radius with \\emph{optimal} approximation guarantees (given truly subquadratic time), under plausible assumptions. In particular, there is a $2$-approximation algorithm for directed Radius with one-way distances that runs in $\\tilde{O}(m\\sqrt{n})$ time, while a $(2-\u03b4)$-approximation algorithm in $O(n^{2-\u03b5})$ time is unlikely. - On graphs with treewidth $k$, we can solve the problems in $2^{O(k\\log{k})}n^{1+o(1)}$ time. We show that these algorithms are near optimal since even a $(3/2-\u03b4)$-approximation algorithm that runs in time $2^{o(k)}n^{2-\u03b5}$ would refute the plausible assumptions.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "We obtain new upper bounds on the additive distortion for graph emulators and spanners on relatively few edges. We introduce a new subroutine called \"strip creation,\" and we combine this subroutine with several other ideas to obtain the following results:\n  \\item Every graph has a spanner on $O(n^{1+\u03b5})$ edges with $\\tilde{O}(n^{1/2 - \u03b5/2})$ additive distortion, for arbitrary $\u03b5\\in [0,1]$. \\item Every graph has an emulator on $\\tilde{O}(n^{1 + \u03b5})$ edges with $\\tilde{O}(n^{1/3 - 2\u03b5/3})$ additive distortion whenever $\u03b5\\in [0, \\frac{1}{5}]$. \\item Every graph has a spanner on $\\tilde{O}(n^{1 + \u03b5})$ edges with $\\tilde{O}(n^{2/3 - 5\u03b5/3})$ additive distortion whenever $\u03b5\\in [0, \\frac{1}{4}]$.\n  Our first spanner has the new best known asymptotic edge-error tradeoff for additive spanners whenever $\u03b5\\in [0, \\frac{1}{7}]$. Our second spanner has the new best tradeoff whenever $\u03b5\\in [\\frac{1}{7}, \\frac{3}{17}]$. Our emulator has the new best asymptotic edge-error tradeoff whenever $\u03b5\\in [0, \\frac{1}{5}]$.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "We make improvements to the upper bounds on several popular types of distance preserving graph sketches. These sketches are all various restrictions of the {\\em additive pairwise spanner} problem, in which one is given an undirected unweighted graph $G$, a set of node pairs $P$, and an error allowance $+\u03b2$, and one must construct a sparse subgraph $H$ satisfying $\u03b4_H(u, v) \\le \u03b4_G(u, v) + \u03b2$ for all $(u, v) \\in P$.\n  The first part of our paper concerns {\\em pairwise distance preservers}, which make the restriction $\u03b2=0$ (i.e. distances must be preserved {\\em exactly}). Our main result here is an upper bound of $|H| = O(n^{2/3}|P|^{2/3} + n|P|^{1/3})$ when $G$ is undirected and unweighted. This improves on existing bounds whenever $|P| = \u03c9(n^{3/4})$, and it is the first such improvement in the last ten years.\n  We then devise a new application of distance preservers to graph clustering algorithms, and we apply this algorithm to {\\em subset spanners}, which require $P = S \\times S$ for some node subset $S$, and {\\em (standard) spanners}, which require $P = V \\times V$. For both of these objects, our construction generalizes the best known bounds when the error allowance is constant, and we obtain the strongest polynomial error/sparsity tradeoff that has yet been reported (in fact, for subset spanners, ours is the {\\em first} nontrivial construction that enjoys improved sparsity from a polynomial error allowance).\n  We leave open a conjecture that $O(n^{2/3}|P|^{2/3} + n)$ pairwise distance preservers are possible for undirected unweighted graphs. Resolving this conjecture in the affirmative would improve and simplify our upper bounds for all the graph sketches mentioned above.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "The CFG recognition problem is: given a context-free grammar $\\mathcal{G}$ and a string $w$ of length $n$, decide if $w$ can be obtained from $\\mathcal{G}$. This is the most basic parsing question and is a core computer science problem. Valiant's parser from 1975 solves the problem in $O(n^\u03c9)$ time, where $\u03c9<2.373$ is the matrix multiplication exponent. Dozens of parsing algorithms have been proposed over the years, yet Valiant's upper bound remains unbeaten. The best combinatorial algorithms have mildly subcubic $O(n^3/\\log^3{n})$ complexity.\n  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for CFG parsing, and that very efficient and practical algorithms might be hard or even impossible to obtain. Lee showed that any algorithm for a more general parsing problem with running time $O(|\\mathcal{G}|\\cdot n^{3-\\varepsilon})$ can be converted into a surprising subcubic algorithm for Boolean Matrix Multiplication. Unfortunately, Lee's hardness result required that the grammar size be $|\\mathcal{G}|=\u03a9(n^6)$. Nothing was known for the more relevant case of constant size grammars.\n  In this work, we prove that any improvement on Valiant's algorithm, even for constant size grammars, either in terms of runtime or by avoiding the inefficiencies of fast matrix multiplication, would imply a breakthrough algorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if there are $k$ that form a clique.\n  Besides classifying the complexity of a fundamental problem, our reduction has led us to similar lower bounds for more modern and well-studied cubic time problems for which faster algorithms are highly desirable in practice: RNA Folding, a central problem in computational biology, and Dyck Language Edit Distance, answering an open question of Saha (FOCS'14).\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "Two important similarity measures between sequences are the longest common subsequence (LCS) and the dynamic time warping distance (DTWD). The computations of these measures for two given sequences are central tasks in a variety of applications. Simple dynamic programming algorithms solve these tasks in $O(n^2)$ time, and despite an extensive amount of research, no algorithms with significantly better worst case upper bounds are known.\n  In this paper, we show that an $O(n^{2-\u03b5})$ time algorithm, for some $\u03b5>0$, for computing the LCS or the DTWD of two sequences of length $n$ over a constant size alphabet, refutes the popular Strong Exponential Time Hypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over an alphabet of size $O(k)$ cannot be done in $O(n^{k-\u03b5})$ time, for any $\u03b5>0$, under SETH. Finally, we also address the time complexity of approximating the DTWD of two strings in truly subquadratic time.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "We consider the quantum time complexity of the all pairs shortest paths (APSP) problem and some of its variants. The trivial classical algorithm for APSP and most all pairs path problems runs in $O(n^3)$ time, while the trivial algorithm in the quantum setting runs in $\\tilde{O}(n^{2.5})$ time, using Grover search. A major open problem in classical algorithms is to obtain a truly subcubic time algorithm for APSP, i.e. an algorithm running in $O(n^{3-\\varepsilon})$ time for constant $\\varepsilon>0$. To approach this problem, many truly subcubic time classical algorithms have been devised for APSP and its variants for structured inputs. Some examples of such problems are APSP in geometrically weighted graphs, graphs with small integer edge weights or a small number of weights incident to each vertex, and the all pairs earliest arrivals problem. In this paper we revisit these problems in the quantum setting and obtain the first nontrivial (i.e. $O(n^{2.5-\\varepsilon})$ time) quantum algorithms for the problems.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "We consider several well-studied problems in dynamic algorithms and prove that sufficient progress on any of them would imply a breakthrough on one of five major open problems in the theory of algorithms:\n  1. Is the 3SUM problem on $n$ numbers in $O(n^{2-\u03b5})$ time for some $\u03b5>0$?\n  2. Can one determine the satisfiability of a CNF formula on $n$ variables in $O((2-\u03b5)^n poly n)$ time for some $\u03b5>0$?\n  3. Is the All Pairs Shortest Paths problem for graphs on $n$ vertices in $O(n^{3-\u03b5})$ time for some $\u03b5>0$?\n  4. Is there a linear time algorithm that detects whether a given graph contains a triangle?\n  5. Is there an $O(n^{3-\u03b5})$ time combinatorial algorithm for $n\\times n$ Boolean matrix multiplication?\n  The problems we consider include dynamic versions of bipartite perfect matching, bipartite maximum weight matching, single source reachability, single source shortest paths, strong connectivity, subgraph connectivity, diameter approximation and some nongraph problems such as Pagh's problem defined in a recent paper by Patrascu [STOC 2010].\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "A rare group of high mass X-ray binaries (HMXBs) are known that also exhibit MeV, GeV, and/or TeV emission (\"gamma-ray binaries\"). Expanding the sample of gamma-ray binaries and identifying unknown Fermi sources are currently of great interest to the community. Based upon their positional coincidence with the unidentified Fermi sources 1FGL J1127.7-6244c and 1FGL J1808.5-1954c, the Be stars HD 99771 and HD 165783 have been proposed as gamma-ray binary candidates. During Fermi Cycle 4, we have performed multiwavelength observations of these sources using XMM-Newton and the CTIO 1.5m telescope. We do not confirm high energy emission from the Be stars. Here we examine other X-ray sources in the field of view that are potential counterparts to the Fermi sources.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "In this paper we consider the fundamental problem of approximating the diameter $D$ of directed or undirected graphs. In a seminal paper, Aingworth, Chekuri, Indyk and Motwani [SIAM J. Comput. 1999] presented an algorithm that computes in $\\Ot(m\\sqrt n + n^2)$ time an estimate $\\hat{D}$ for the diameter of an $n$-node, $m$-edge graph, such that $\\lfloor 2/3 D \\rfloor \\leq \\hat{D} \\leq D$. In this paper we present an algorithm that produces the same estimate in $\\Ot(m\\sqrt n)$ expected running time. We then provide strong evidence that a better approximation may be hard to obtain if we insist on an $O(m^{2-\\eps})$ running time. In particular, we show that if there is some constant $\\eps>0$ so that there is an algorithm for undirected unweighted graphs that runs in $O(m^{2-\\eps})$ time and produces an approximation $\\hat{D}$ such that $ (2/3+\\eps) D \\leq \\hat{D} \\leq D$, then SAT for CNF formulas on $n$ variables can be solved in $O^{*}((2-\u03b4)^{n})$ time for some constant $\u03b4>0$, and the strong exponential time hypothesis of [Impagliazzo, Paturi, Zane JCSS'01] is false.\n  Motivated by this somewhat negative result, we study whether it is possible to obtain a better approximation for specific cases. For unweighted directed or undirected graphs, we show that if $D=3h+z$, where $h\\geq 0$ and $z\\in {0,1,2}$, then it is possible to report in $\\tilde{O}(\\min{m^{2/3} n^{4/3},m^{2-1/(2h+3)}})$ time an estimate $\\hat{D}$ such that $2h+z \\leq \\hat{D}\\leq D$, thus giving a better than 3/2 approximation whenever $z\\neq 0$. This is significant for constant values of $D$ which is exactly when the diameter approximation problem is hardest to solve. For the case of unweighted undirected graphs we present an $\\tilde{O}(m^{2/3} n^{4/3})$ time algorithm that reports an estimate $\\hat{D}$ such that $\\lfloor 4D/5\\rfloor \\leq \\hat{D}\\leq D$.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "The star HD 193322 is a remarkable multiple system of massive stars that lies at the heart of the cluster Collinder 419. Here we report on new spectroscopic observations and radial velocities of the narrow-lined component Ab1 that we use to determine its orbital motion around a close companion Ab2 ($P = 312$ d) and around a distant third star Aa ($P = 35$ y).We have also obtained long baseline interferometry of the target in the $K^\\prime$-band with the CHARA Array that we use in two ways. First, we combine published speckle interferometric measurements with CHARA separated fringe packet measurements to improve the visual orbit for the wide Aa,Ab binary. Second, we use measurements of the fringe packet from Aa to calibrate the visibility of the fringes of the Ab1,Ab2 binary, and we analyze these fringe visibilities to determine the visual orbit of the close system. The two most massive stars, Aa and Ab1, have masses of approximately 21 and $23 M_\\odot$, respectively, and their spectral line broadening indicates that they represent extremes of fast and slow projected rotational velocity, respectively.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "We consider the fundamental algorithmic problem of finding a cycle of minimum weight in a weighted graph. In particular, we show that the minimum weight cycle problem in an undirected n-node graph with edge weights in {1,...,M} or in a directed n-node graph with edge weights in {-M,..., M} and no negative cycles can be efficiently reduced to finding a minimum weight triangle in an Theta(n)-node undirected graph with weights in {1,...,O(M)}. Roughly speaking, our reductions imply the following surprising phenomenon: a minimum cycle with an arbitrary number of weighted edges can be \"encoded\" using only three edges within roughly the same weight interval! This resolves a longstanding open problem posed by Itai and Rodeh [SIAM J. Computing 1978 and STOC'77].\n  A direct consequence of our efficient reductions are O (Mn^{omega})-time algorithms using fast matrix multiplication (FMM) for finding a minimum weight cycle in both undirected graphs with integral weights from the interval [1,M] and directed graphs with integral weights from the interval [-M,M]. The latter seems to reveal a strong separation between the all pairs shortest paths (APSP) problem and the minimum weight cycle problem in directed graphs as the fastest known APSP algorithm has a running time of O(M^{0.681}n^{2.575}) by Zwick [J. ACM 2002].\n  In contrast, when only combinatorial algorithms are allowed (that is, without FMM) the only known solution to minimum weight cycle is by computing APSP. Interestingly, any separation between the two problems in this case would be an amazing breakthrough as by a recent paper by Vassilevska W. and Williams [FOCS'10], any O(n^{3-eps})-time algorithm (eps>0) for minimum weight cycle immediately implies a O(n^{3-delta})-time algorithm (delta>0) for APSP.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "The replacement paths problem for directed graphs is to find for given nodes s and t and every edge e on the shortest path between them, the shortest path between s and t which avoids e. For unweighted directed graphs on n vertices, the best known algorithm runtime was \\tilde{O}(n^{2.5}) by Roditty and Zwick. For graphs with integer weights in {-M,...,M}, Weimann and Yuster recently showed that one can use fast matrix multiplication and solve the problem in O(Mn^{2.584}) time, a runtime which would be O(Mn^{2.33}) if the exponent \u03c9of matrix multiplication is 2.\n  We improve both of these algorithms. Our new algorithm also relies on fast matrix multiplication and runs in O(M n^\u03c9 polylog(n)) time if \u03c9>2 and O(n^{2+\\eps}) for any \\eps>0 if \u03c9=2. Our result shows that, at least for small integer weights, the replacement paths problem in directed graphs may be easier than the related all pairs shortest paths problem in directed graphs, as the current best runtime for the latter is \u03a9(n^{2.5}) time even if \u03c9=2.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "A survey that can cover the sky in optical bands over wide fields to faint magnitudes with a fast cadence will enable many of the exciting science opportunities of the next decade. The Large Synoptic Survey Telescope (LSST) will have an effective aperture of 6.7 meters and an imaging camera with field of view of 9.6 deg^2, and will be devoted to a ten-year imaging survey over 20,000 deg^2 south of +15 deg. Each pointing will be imaged 2000 times with fifteen second exposures in six broad bands from 0.35 to 1.1 microns, to a total point-source depth of r~27.5. The LSST Science Book describes the basic parameters of the LSST hardware, software, and observing plans. The book discusses educational and outreach opportunities, then goes on to describe a broad range of science that LSST will revolutionize: mapping the inner and outer Solar System, stellar populations in the Milky Way and nearby galaxies, the structure of the Milky Way disk and halo and other objects in the Local Volume, transient and variable objects both at low and high redshift, and the properties of normal and active galaxies at low and high redshift. It then turns to far-field cosmological topics, exploring properties of supernovae to z~1, strong and weak lensing, the large-scale distribution of galaxies and baryon oscillations, and how these different probes may be combined to constrain cosmological models and the physics of dark energy.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "For a graph G with real weights assigned to the vertices (edges), the MAX H-SUBGRAPH problem is to find an H-subgraph of G with maximum total weight, if one exists. The all-pairs MAX H-SUBGRAPH problem is to find for every pair of vertices u,v, a maximum H-subgraph containing both u and v, if one exists. Our main results are new strongly polynomial algorithms for the all-pairs MAX H-SUBGRAPH problem for vertex weighted graphs. We also give improved algorithms for the MAX-H SUBGRAPH problem for edge weighted graphs, and various related problems, including computing the first k most significant bits of the distance product of two matrices. Some of our algorithms are based, in part, on fast matrix multiplication.\n        \u25b3 Less", "author": "Virginia Williams"}, {"abstract": "The Orthogonal Vectors problem ($\\textsf{OV}$) asks: given $n$ vectors in $\\{0,1\\}^{O(\\log n)}$, are two of them orthogonal? $\\textsf{OV}$ is easily solved in $O(n^2 \\log n)$ time, and it is a central problem in fine-grained complexity: dozens of conditional lower bounds are based on the popular hypothesis that $\\textsf{OV}$ cannot be solved in (say) $n^{1.99}$ time. However, unlike the APSP problem, few other problems are known to be non-trivially equivalent to $\\textsf{OV}$.\n  We show $\\textsf{OV}$ is truly-subquadratic equivalent to several fundamental problems, all of which (a priori) look harder than $\\textsf{OV}$. A partial list is given below:\n  ($\\textsf{Min-IP}/\\textsf{Max-IP}$) Find a red-blue pair of vectors with minimum (respectively, maximum) inner product, among $n$ vectors in $\\{0,1\\}^{O(\\log n)}$.\n  ($\\textsf{Exact-IP}$) Find a red-blue pair of vectors with inner product equal to a given target integer, among $n$ vectors in $\\{0,1\\}^{O(\\log n)}$.\n  ($\\textsf{Apx-Min-IP}/\\textsf{Apx-Max-IP}$) Find a red-blue pair of vectors that is a 100-approximation to the minimum (resp. maximum) inner product, among $n$ vectors in $\\{0,1\\}^{O(\\log n)}$.\n  (Approx. $\\textsf{Bichrom.-$\\ell_p$-Closest-Pair}$) Compute a $(1 + \u03a9(1))$-approximation to the $\\ell_p$-closest red-blue pair (for a constant $p \\in [1,2]$), among $n$ points in $\\mathbb{R}^d$, $d \\le n^{o(1)}$.\n  (Approx. $\\textsf{$\\ell_p$-Furthest-Pair}$) Compute a $(1 + \u03a9(1))$-approximation to the $\\ell_p$-furthest pair (for a constant $p \\in [1,2]$), among $n$ points in $\\mathbb{R}^d$, $d \\le n^{o(1)}$.\n  We also show that there is a $\\text{poly}(n)$ space, $n^{1-\u03b5}$ query time data structure for Partial Match with vectors from $\\{0,1\\}^{O(\\log n)}$ if and only if such a data structure exists for $1+\u03a9(1)$ Approximate Nearest Neighbor Search in Euclidean space.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The recent discovery by Advanced LIGO and Advanced Virgo of a gravitational wave signal from a binary neutron star inspiral has enabled tests of general relativity (GR) with this new type of source. This source, for the first time, permits tests of strong-field dynamics of compact binaries in presence of matter. In this paper, we place constraints on the dipole radiation and possible deviations from GR in the post-Newtonian coefficients that govern the inspiral regime. Bounds on modified dispersion of gravitational waves are obtained; in combination with information from the observed electromagnetic counterpart we can also constrain effects due to large extra dimensions. Finally, the polarization content of the gravitational wave signal is studied. The results of all tests performed here show good agreement with GR.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Astrophysical sources of gravitational waves, such as binary neutron star and black hole mergers or core-collapse supernovae, can drive relativistic outflows, giving rise to non-thermal high-energy emission. High-energy neutrinos are signatures of such outflows. The detection of gravitational waves and high-energy neutrinos from common sources could help establish the connection between the dynamics of the progenitor and the properties of the outflow. We searched for associated emission of gravitational waves and high-energy neutrinos from astrophysical transients with minimal assumptions using data from Advanced LIGO from its first observing run O1, and data from the ANTARES and IceCube neutrino observatories from the same time period. We focused on candidate events whose astrophysical origin could not be determined from a single messenger. We found no significant coincident candidate, which we used to constrain the rate density of astrophysical sources dependent on their gravitational wave and neutrino emission processes.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present a search for prompt gamma-ray counterparts to compact binary coalescence gravitational wave (GW) candidates from Advanced LIGO's first observing run (O1). As demonstrated by the multimessenger observations of GW170817/GRB 170817A, electromagnetic and GW observations provide complementary information about the astrophysical source and, in the case of weaker candidates, may strengthen the case for an astrophysical origin. Here we investigate low-significance GW candidates from the O1 compact-binary coalescence searches using the Fermi Gamma-ray Burst Monitor (GBM), leveraging its all-sky and broad energy coverage. Candidates are ranked and compared to background to measure significance. Those with false alarm rates of less than 10^-5 Hz (about one per day) are used as the search sample for gamma-ray follow-up. No GW candidates were found to be coincident with gamma-ray transients independently identified by blind searches of the GBM data. In addition, GW candidate event times were followed up by a separate targeted search of GBM data. Among the resulting GBM events, the two with lowest false alarm rates were the gamma-ray transient GW150914-GBM presented in Connaughton et al. (2016) and a solar flare in chance coincidence with a GW candidate.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "One unanswered question about the binary neutron star coalescence GW170817 is the nature of its post-merger remnant. A previous search for post-merger gravitational waves targeted high-frequency signals from a possible neutron star remnant with a maximum signal duration of 500 s. Here we revisit the neutron star remnant scenario with a focus on longer signal durations up until the end of the Second Advanced LIGO-Virgo Observing run, 8.5 days after the coalescence of GW170817. The main physical scenario for such emission is the power-law spindown of a massive magnetar-like remnant. We use four independent search algorithms with varying degrees of restrictiveness on the signal waveformand different ways of dealing with noise artefacts. In agreement with theoretical estimates, we find no significant signal candidates. Through simulated signals, we quantify that with the current detector sensitivity, nowhere in the studied parameter space are we sensitive to a signal from more than 1 Mpc away, compared to the actual distance of 40 Mpc. This study however serves as a prototype for post-merger analyses in future observing runs with expected higher sensitivity.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We analyze the impact of a proposed tidal instability coupling $p$-modes and $g$-modes within neutron stars on GW170817. This non-resonant instability transfers energy from the orbit of the binary to internal modes of the stars, accelerating the gravitational-wave driven inspiral. We model the impact of this instability on the phasing of the gravitational wave signal using three parameters per star: an overall amplitude, a saturation frequency, and a spectral index. Incorporating these additional parameters, we compute the Bayes Factor ($\\ln B^{pg}_{!pg}$) comparing our $p$-$g$ model to a standard one. We find that the observed signal is consistent with waveform models that neglect $p$-$g$ effects, with $\\ln B^{pg}_{!pg} = 0.03^{+0.70}_{-0.58}$ (maximum a posteriori and 90% credible region). By injecting simulated signals that do not include $p$-$g$ effects and recovering them with the $p$-$g$ model, we show that there is a $\\simeq 50\\%$ probability of obtaining similar $\\ln B^{pg}_{!pg}$ even when $p$-$g$ effects are absent. We find that the $p$-$g$ amplitude for 1.4 $M_\\odot$ neutron stars is constrained to $\\lesssim \\text{few}\\times10^{-7}$, with maxima a posteriori near $\\sim 10^{-7}$ and $p$-$g$ saturation frequency $\\sim 70\\, \\mathrm{Hz}$. This suggests that there are less than a few hundred excited modes, assuming they all saturate by wave breaking. For comparison, theoretical upper bounds suggest a $p$-$g$ amplitude $\\lesssim 10^{-6}$ and $\\lesssim 10^{3}$ modes saturating by wave breaking. Thus, the measured constraints only rule out extreme values of the $p$-$g$ parameters. They also imply that the instability dissipates $\\lesssim 10^{51}\\, \\mathrm{ergs}$ over the entire inspiral, i.e., less than a few percent of the energy radiated as gravitational waves.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "In this paper, the controlled production of high quality metal-free diamond nanoparticles is demonstrated. Milling with tempered steel is shown to leave behind iron oxide contamination which is difficult to remove. Milling with SiN alleviates this issue but generates more non diamond carbon. Thus the choice of milling materials is critically determined by the acceptable contaminants in the ultimate application. The removal of metal impurities, present in all commercially available nanoparticles, will open new possibilities towards the production of customised diamond nanoparticles, covering the most demanding quantum applications.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present the first Advanced LIGO and Advanced Virgo search for ultracompact binary systems with component masses between 0.2 $M_\\odot$ - 1.0 $M_\\odot$ using data taken between September 12, 2015 and January 19, 2016. We find no viable gravitational wave candidates. Our null result constrains the coalescence rate of monochromatic (delta function) distributions of non-spinning (0.2 $M_\\odot$, 0.2 $M_\\odot$) ultracompact binaries to be less than $1.0 \\times 10^6 \\text{Gpc}^{-3} \\text{yr}^{-1}$ and the coalescence rate of a similar distribution of (1.0 $M_\\odot$, 1.0 $M_\\odot$) ultracompact binaries to be less than $1.9 \\times 10^4 \\text{Gpc}^{-3} \\text{yr}^{-1}$ (at 90 percent confidence). Neither black holes nor neutron stars are expected to form below ~ 1 solar mass through conventional stellar evolution, though it has been proposed that similarly low mass black holes could be formed primordially through density fluctuations in the early universe. Under a particular primordial black hole binary formation scenario, we constrain monochromatic primordial black hole populations of 0.2 $M_\\odot$ to be less than $33\\%$ of the total dark matter density and monochromatic populations of 1.0 $M_\\odot$ to be less than $5\\%$ of the dark matter density. The latter strengthens the presently placed bounds from micro-lensing surveys of MAssive Compact Halo Objects (MACHOs) provided by the MACHO and EROS collaborations.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Machine learning is an important research area in particle physics, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas in machine learning in particle physics with a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "On 17 August 2017, the LIGO and Virgo observatories made the first direct detection of gravitational waves from the coalescence of a neutron star binary system. The detection of this gravitational-wave signal, GW170817, offers a novel opportunity to directly probe the properties of matter at the extreme conditions found in the interior of these stars. The initial, minimal-assumption analysis of the LIGO and Virgo data placed constraints on the tidal effects of the coalescing bodies, which were then translated to constraints on neutron star radii. Here, we expand upon previous analyses by working under the hypothesis that both bodies were neutron stars that are described by the same equation of state and have spins within the range observed in Galactic binary neutron stars. Our analysis employs two methods: the use of equation-of-state-insensitive relations between various macroscopic properties of the neutron stars and the use of an efficient parametrization of the defining function $p(\u03c1)$ of the equation of state itself. From the LIGO and Virgo data alone and the first method, we measure the two neutron star radii as $R_1=10.8^{+2.0}_{-1.7}$ km for the heavier star and $R_2= 10.7^{+2.1}_{-1.5}$ km for the lighter star at the 90% credible level. If we additionally require that the equation of state supports neutron stars with masses larger than $1.97 \\,M_\\odot$ as required from electromagnetic observations and employ the equation-of-state parametrization, we further constrain $R_1= 11.9^{+1.4}_{-1.4}$ km and $R_2= 11.9^{+1.4}_{-1.4}$ km at the 90% credible level. Finally, we obtain constraints on $p(\u03c1)$ at supranuclear densities, with pressure at twice nuclear saturation density measured at $3.5^{+2.7}_{-1.7}\\times 10^{34} \\,\\mathrm{dyn}/\\mathrm{cm}^{2}$ at the 90% level.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "On August 17, 2017, the Advanced LIGO and Advanced Virgo gravitational-wave detectors observed a low-mass compact binary inspiral. The initial sky localization of the source of the gravitational-wave signal, GW170817, allowed electromagnetic observatories to identify NGC 4993 as the host galaxy. In this work we improve initial estimates of the binary's properties, including component masses, spins, and tidal parameters, using the known source location, improved modeling, and re-calibrated Virgo data. We extend the range of gravitational-wave frequencies considered down to 23 Hz, compared to 30 Hz in the initial analysis. We also compare results inferred using several signal models, which are more accurate and incorporate additional physical effects as compared to the initial analysis. We improve the localization of the gravitational-wave source to a 90% credible region of $16~\\mathrm{deg}^2$. We find tighter constraints on the masses, spins, and tidal parameters, and continue to find no evidence for non-zero component spins. The component masses are inferred to lie between 1.00 and 1.89 $M_\\odot$ when allowing for large component spins, and to lie between 1.16 and 1.60 $M_\\odot$ (with a total mass $2.73^{+0.04}_{-0.01} \\, M_\\odot$) when the spins are restricted to be within the range observed in Galactic binary neutron stars. Under minimal assumptions about the nature of the compact objects, our constraints for the tidal deformability parameter $\\tilde \u039b$ are $(0,630)$ when we allow for large component spins, and $300^{+420}_{-230}$ (using a 90% highest posterior density interval) when restricting the magnitude of the component spins, ruling out several equation of state models at the 90% credible level. Finally, with LIGO and GEO600 data, we use a Bayesian analysis to place upper limits on the amplitude and spectral energy density of a possible post-merger signal. (Abridged)\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Double detonations in double white dwarf (WD) binaries undergoing unstable mass transfer have emerged in recent years as one of the most promising Type Ia supernova (SN Ia) progenitor scenarios. One potential outcome of this \"dynamically driven double-degenerate double-detonation\" (D^6) scenario is that the companion WD survives the explosion and is flung away with a velocity equal to its > 1000 km/s pre-SN orbital velocity. We perform a search for these hypervelocity runaway WDs using Gaia's second data release. In this paper, we discuss seven candidates followed up with ground-based instruments. Three sources are likely to be some of the fastest known stars in the Milky Way, with total Galactocentric velocities between 1000 and 3000 km/s, and are consistent with having previously been companion WDs in pre-SN Ia systems. However, although the radial velocity of one of the stars is > 1000 km/s, the radial velocities of the other two stars are puzzlingly consistent with 0. The combined five-parameter astrometric solutions from Gaia and radial velocities from follow-up spectra yield tentative 6D confirmation of the D^6 scenario. The past position of one of these stars places it within a faint, old SN remnant, further strengthening the interpretation of these candidates as hypervelocity runaways from binary systems that underwent SNe Ia.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The DESI Legacy Imaging Surveys are a combination of three public projects (the Dark Energy Camera Legacy Survey, the Beijing-Arizona Sky Survey, and the Mayall z-band Legacy Survey) that will jointly image ~14,000 square degrees of the extragalactic sky visible from the northern hemisphere in three optical bands (g, r, and z) using telescopes at the Kitt Peak National Observatory and the Cerro Tololo Inter-American Observatory. The combined survey footprint is split into two contiguous areas by the Galactic plane. The optical imaging is conducted using a unique strategy of dynamic observing that results in a survey of nearly uniform depth. In addition to calibrated images, the project is delivering an inference-based catalog which includes photometry from the grz optical bands and from four mid-infrared bands (at 3.4um, 4.6um, 12um and 22um) observed by the Wide-field Infrared Survey Explorer (WISE) satellite during its full operational lifetime. The project plans two public data releases each year. All the software used to generate the catalogs is also released with the data. This paper provides an overview of the Legacy Surveys project.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The detection of gravitational waves with Advanced LIGO and Advanced Virgo has enabled novel tests of general relativity, including direct study of the polarization of gravitational waves. While general relativity allows for only two tensor gravitational-wave polarizations, general metric theories can additionally predict two vector and two scalar polarizations. The polarization of gravitational waves is encoded in the spectral shape of the stochastic gravitational-wave background, formed by the superposition of cosmological and individually-unresolved astrophysical sources. Using data recorded by Advanced LIGO during its first observing run, we search for a stochastic background of generically-polarized gravitational waves. We find no evidence for a background of any polarization, and place the first direct bounds on the contributions of vector and scalar polarizations to the stochastic background. Under log-uniform priors for the energy in each polarization, we limit the energy-densities of tensor, vector, and scalar modes at 95% credibility to $\u03a9^T_0 < 5.6 \\times 10^{-8}$, $\u03a9^V_0 < 6.4\\times 10^{-8}$, and $\u03a9^S_0 < 1.1\\times 10^{-7}$ at a reference frequency $f_0 = 25$ Hz.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We consider the problem of representing Boolean functions exactly by \"sparse\" linear combinations (over $\\mathbb{R}$) of functions from some \"simple\" class ${\\cal C}$. In particular, given ${\\cal C}$ we are interested in finding low-complexity functions lacking sparse representations. When ${\\cal C}$ is the set of PARITY functions or the set of conjunctions, this sort of problem has a well-understood answer, the problem becomes interesting when ${\\cal C}$ is \"overcomplete\" and the set of functions is not linearly independent. We focus on the cases where ${\\cal C}$ is the set of linear threshold functions, the set of rectified linear units (ReLUs), and the set of low-degree polynomials over a finite field, all of which are well-studied in different contexts.\n  We provide generic tools for proving lower bounds on representations of this kind. Applying these, we give several new lower bounds for \"semi-explicit\" Boolean functions. For example, we show there are functions in nondeterministic quasi-polynomial time that require super-polynomial size:\n  $\\bullet$ Depth-two neural networks with sign activation function, a special case of depth-two threshold circuit lower bounds.\n  $\\bullet$ Depth-two neural networks with ReLU activation function.\n  $\\bullet$ $\\mathbb{R}$-linear combinations of $O(1)$-degree $\\mathbb{F}_p$-polynomials, for every prime $p$ (related to problems regarding Higher-Order \"Uncertainty Principles\"). We also obtain a function in $E^{NP}$ requiring $2^{\u03a9(n)}$ linear combinations.\n  $\\bullet$ $\\mathbb{R}$-linear combinations of $ACC \\circ THR$ circuits of polynomial size (further generalizing the recent lower bounds of Murray and the author).\n  (The above is a shortened abstract. For the full abstract, see the paper.)\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present an original phenomenological model to describe the evolution of galaxy number counts, morphologies, and spectral energy distributions across a wide range of redshifts (0.2<z<15) and stellar masses [Log10 M/Msun >6]. Our model follows observed mass and luminosity functions of both star-forming and quiescent galaxies, and reproduces the redshift evolution of colors, sizes, star-formation and chemical properties of the observed galaxy population. Unlike other existing approaches, our model includes a self-consistent treatment of stellar and photoionized gas emission and dust attenuation based on the BEAGLE tool. The mock galaxy catalogs generated with our new model can be used to simulate and optimize extragalactic surveys with future facilities such as the James Webb Space Telescope (JWST), and to enable critical assessments of analysis procedures, interpretation tools, and measurement systematics for both photometric and spectroscopic data. As a first application of this work, we make predictions for the upcoming JWST Advanced Deep Extragalactic Survey (JADES), a joint program of the JWST/NIRCam and NIRSpec Guaranteed Time Observations teams. We show that JADES will detect, with NIRCam imaging, thousands of galaxies at z>6, and tens at z>10 at m_AB<30 (5-sigma) within the 236 arcmin^2 of the survey. The JADES data will enable accurate constraints on the evolution of the UV luminosity function at z>8, and resolve the current debate about the rate of evolution of galaxies at z>8. Ready to use mock catalogs and software to generate new realizations are publicly available as the JAdes extraGalactic Ultradeep Artificial Realizations (JAGUAR) package.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We report on a new all-sky search for periodic gravitational waves in the frequency band 475-2000 Hz and with a frequency time derivative in the range of [-1.0e-8, +1e-9] Hz/s. Potential signals could be produced by a nearby spinning and slightly non-axisymmetric isolated neutron star in our galaxy.\n  This search uses the data from Advanced LIGO's first observational run O1. No gravitational wave signals were observed, and upper limits were placed on their strengths. For completeness, results from the separately published low frequency search 20-475 Hz are included as well.\n  Our lowest upper limit on worst-case (linearly polarized) strain amplitude h_0 is 4e-25 near 170 Hz, while at the high end of our frequency range we achieve a worst-case upper limit of 1.3e-24. For a circularly polarized source (most favorable orientation), the smallest upper limit obtained is ~1.5e-25.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Fine-grained reductions have established equivalences between many core problems with $\\tilde{O}(n^3)$-time algorithms on $n$-node weighted graphs, such as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement Paths, Second Shortest Paths, and so on. These problems also have $\\tilde{O}(mn)$-time algorithms on $m$-edge $n$-node weighted graphs, and such algorithms have wider applicability. Are these $mn$ bounds optimal when $m \\ll n^2$?\n  Starting from the hypothesis that the minimum weight $(2\\ell+1)$-Clique problem in edge weighted graphs requires $n^{2\\ell+1-o(1)}$ time, we prove that for all sparsities of the form $m = \u0398(n^{1+1/\\ell})$, there is no $O(n^2 + mn^{1-\u03b5})$ time algorithm for $\u03b5>0$ for \\emph{any} of the below problems:\n  Minimum Weight $(2\\ell+1)$-Cycle in a directed weighted graph,\n  Shortest Cycle in a directed weighted graph,\n  APSP in a directed or undirected weighted graph,\n  Radius (or Eccentricities) in a directed or undirected weighted graph,\n  Wiener index of a directed or undirected weighted graph,\n  Replacement Paths in a directed weighted graph,\n  Second Shortest Path in a directed weighted graph,\n  Betweenness Centrality of a given node in a directed weighted graph.\n  That is, we prove hardness for a variety of sparse graph problems from the hardness of a dense graph problem. Our results also lead to new conditional lower bounds from several related hypothesis for unweighted sparse graph problems including $k$-cycle, shortest cycle, Radius, Wiener index and APSP.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Cosmic strings are topological defects which can be formed in GUT-scale phase transitions in the early universe. They are also predicted to form in the context of string theory. The main mechanism for a network of Nambu-Goto cosmic strings to lose energy is through the production of loops and the subsequent emission of gravitational waves, thus offering an experimental signature for the existence of cosmic strings. Here we report on the analysis conducted to specifically search for gravitational-wave bursts from cosmic string loops in the data of Advanced LIGO 2015-2016 observing run (O1). No evidence of such signals was found in the data, and as a result we set upper limits on the cosmic string parameters for three recent loop distribution models. In this paper, we initially derive constraints on the string tension $G\u03bc$ and the intercommutation probability, using not only the burst analysis performed on the O1 data set, but also results from the previously published LIGO stochastic O1 analysis, pulsar timing arrays, cosmic microwave background and Big-Bang nucleosynthesis experiments. We show that these data sets are complementary in that they probe gravitational waves produced by cosmic string loops during very different epochs. Finally, we show that the data sets exclude large parts of the parameter space of the three loop distribution models we consider.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present the results of a search for long-duration gravitational wave transients in the data of the LIGO Hanford and LIGO Livingston second generation detectors between September 2015 and January 2016, with a total observational time of 49 days. The search targets gravitational wave transients of \\unit[10 -- 500]{s} duration in a frequency band of \\unit[24 -- 2048]{Hz}, with minimal assumptions about the signal waveform, polarization, source direction, or time of occurrence. No significant events were observed. %All candidate triggers were consistent with the expected background, As a result we set 90\\% confidence upper limits on the rate of long-duration gravitational wave transients for different types of gravitational wave signals. We also show that the search is sensitive to sources in the Galaxy emitting at least $\\sim$ \\unit[$10^{-8}$]{$\\mathrm{M_{\\odot} c^2}$} in gravitational waves.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "On June 8, 2017 at 02:01:16.49 UTC, a gravitational-wave signal from the merger of two stellar-mass black holes was observed by the two Advanced LIGO detectors with a network signal-to-noise ratio of 13. This system is the lightest black hole binary so far observed, with component masses $12^{+7}_{-2}\\,M_\\odot$ and $7^{+2}_{-2}\\,M_\\odot$ (90% credible intervals). These lie in the range of measured black hole masses in low-mass X-ray binaries, thus allowing us to compare black holes detected through gravitational waves with electromagnetic observations. The source's luminosity distance is $340^{+140}_{-140}$ Mpc, corresponding to redshift $0.07^{+0.03}_{-0.03}$. We verify that the signal waveform is consistent with the predictions of general relativity.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present the first effort to aggregate, homogenize, and uniformly model the combined ultraviolet, optical, and near-infrared dataset for the electromagnetic counterpart of the binary neutron star merger GW170817. By assembling all of the available data from 18 different papers and 46 different instruments, we are able to identify and mitigate systematic offsets between individual datasets, and to identify clear outlying measurements, with the resulting pruned and adjusted dataset offering an opportunity to expand the study of the kilonova. The unified dataset includes 647 individual flux measurements, spanning 0.45 to 29.4 days post-merger, and thus has greater constraining power for physical models than any single dataset. We test a number of semi-analytical models and find that the data are well modeled with a three-component kilonova model: a \"blue\" lanthanide-poor component with Mej~0.020 Msol and vej~0.27c; an intermediate opacity \"purple\" component with Mej~0.047 Msol and vej~0.15c; and a \"red\" lanthanide-rich component with Mej~0.011 Msol and vej~0.14c. We further explore the possibility of ejecta asymmetry and its impact on the estimated parameters. From the inferred parameters we draw conclusions about the physical mechanisms responsible for the various ejecta components, the properties of the neutron stars, and, combined with an up-to-date merger rate, the implications for r-process enrichment via this channel. To facilitate future studies of this keystone event we make the unified dataset and our modeling code public.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The first observation of a binary neutron star coalescence by the Advanced LIGO and Advanced Virgo gravitational-wave detectors offers an unprecedented opportunity to study matter under the most extreme conditions. After such a merger, a compact remnant is left over whose nature depends primarily on the masses of the inspiralling objects and on the equation of state of nuclear matter. This could be either a black hole or a neutron star (NS), with the latter being either long-lived or too massive for stability implying delayed collapse to a black hole. Here, we present a search for gravitational waves from the remnant of the binary neutron star merger GW170817 using data from Advanced LIGO and Advanced Virgo. We search for short ($\\lesssim1$ s) and intermediate-duration ($\\lesssim 500$ s) signals, which includes gravitational-wave emission from a hypermassive NS or supramassive NS, respectively. We find no signal from the post-merger remnant. Our derived strain upper limits are more than an order of magnitude larger than those predicted by most models. For short signals, our best upper limit on the root-sum-square of the gravitational-wave strain emitted from 1--4 kHz is $h_{\\rm rss}^{50\\%}=2.1\\times 10^{-22}$ Hz$^{-1/2}$ at 50% detection efficiency. For intermediate-duration signals, our best upper limit at 50% detection efficiency is $h_{\\rm rss}^{50\\%}=8.4\\times 10^{-22}$ Hz$^{-1/2}$ for a millisecond magnetar model, and $h_{\\rm rss}^{50\\%}=5.9\\times 10^{-22}$ Hz$^{-1/2}$ for a bar-mode model. These results indicate that post-merger emission from a similar event may be detectable when advanced detectors reach design sensitivity or with next-generation detectors.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The purpose of this note is two give a mathematical treatment to the low energy effective theory of the two-dimensional sigma model. Perhaps surprisingly, our low energy effective theory encodes much of the topology and geometry of the target manifold. In particular, we relate the $\u03b2$-function of our theory to the Ricci curvature of the target, recovering the physical result of Friedan.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The Advanced LIGO and Advanced Virgo observatories recently discovered gravitational waves from a binary neutron star inspiral. A short gamma-ray burst (GRB) that followed the merger of this binary was also recorded by the Fermi Gamma-ray Burst Monitor (Fermi-GBM), and the Anticoincidence Shield for the Spectrometer for the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), indicating particle acceleration by the source. The precise location of the event was determined by optical detections of emission following the merger. We searched for high-energy neutrinos from the merger in the GeV--EeV energy range using the ANTARES, IceCube, and Pierre Auger Observatories. No neutrinos directionally coincident with the source were detected within $\\pm500$ s around the merger time. Additionally, no MeV neutrino burst signal was detected coincident with the merger. We further carried out an extended search in the direction of the source for high-energy neutrinos within the 14-day period following the merger, but found no evidence of emission. We used these results to probe dissipation mechanisms in relativistic outflows driven by the binary neutron star merger. The non-detection is consistent with model predictions of short GRBs observed at a large off-axis angle.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "On 2017 August 17 the merger of two compact objects with masses consistent with two neutron stars was discovered through gravitational-wave (GW170817), gamma-ray (GRB 170817A), and optical (SSS17a/AT 2017gfo) observations. The optical source was associated with the early-type galaxy NGC 4993 at a distance of just $\\sim$40 Mpc, consistent with the gravitational-wave measurement, and the merger was localized to be at a projected distance of $\\sim$2 kpc away from the galaxy's center. We use this minimal set of facts and the mass posteriors of the two neutron stars to derive the first constraints on the progenitor of GW170817 at the time of the second supernova (SN). We generate simulated progenitor populations and follow the three-dimensional kinematic evolution from the binary neutron star (BNS) birth to the merger time, accounting for pre-SN galactic motion, for considerably different input distributions of the progenitor mass, pre-SN semimajor axis, and SN-kick velocity. Though not considerably tight, we find these constraints to be comparable to those for Galactic BNS progenitors. The derived constraints are very strongly influenced by the requirement of keeping the binary bound after the second SN and having the merger occur relatively close to the center of the galaxy. These constraints are insensitive to the galaxy's star formation history, provided the stellar populations are older than 1 Gyr.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The LIGO Scientific and Virgo Collaborations have announced the first detection of gravitational waves from the coalescence of two neutron stars. The merger rate of binary neutron stars estimated from this event suggests that distant, unresolvable binary neutron stars create a significant astrophysical stochastic gravitational-wave background. The binary neutron star background will add to the background from binary black holes, increasing the amplitude of the total astrophysical background relative to previous expectations. In the Advanced LIGO-Virgo frequency band most sensitive to stochastic backgrounds (near 25 Hz), we predict a total astrophysical background with amplitude $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.8_{-1.3}^{+2.7} \\times 10^{-9}$ with $90\\%$ confidence, compared with $\u03a9_{\\rm GW} (f=25 \\text{Hz}) = 1.1_{-0.7}^{+1.2} \\times 10^{-9}$ from binary black holes alone. Assuming the most probable rate for compact binary mergers, we find that the total background may be detectable with a signal-to-noise-ratio of 3 after 40 months of total observation time, based on the expected timeline for Advanced LIGO and Virgo to reach their design sensitivity.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The source of the gravitational-wave signal GW170817, very likely a binary neutron star merger, was also observed electromagnetically, providing the first multi-messenger observations of this type. The two week long electromagnetic counterpart had a signature indicative of an r-process-induced optical transient known as a kilonova. This Letter examines how the mass of the dynamical ejecta can be estimated without a direct electromagnetic observation of the kilonova, using gravitational-wave measurements and a phenomenological model calibrated to numerical simulations of mergers with dynamical ejecta. Specifically, we apply the model to the binary masses inferred from the gravitational-wave measurements, and use the resulting mass of the dynamical ejecta to estimate its contribution (without the effects of wind ejecta) to the corresponding kilonova light curves from various models. The distributions of dynamical ejecta mass range between $M_{ej} = 10^{-3} - 10^{-2} M_{\\odot}$ for various equations of state, assuming the neutron stars are rotating slowly. In addition, we use our estimates of the dynamical ejecta mass and the neutron star merger rates inferred from GW170817 to constrain the contribution of events like this to the r-process element abundance in the Galaxy when ejecta mass from post-merger winds is neglected. We find that if $\\gtrsim10\\%$ of the matter dynamically ejected from BNS mergers is converted to r-process elements, GW170817-like BNS mergers could fully account for the amount of r-process material observed in the Milky Way.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The detection of GW170817 in both gravitational waves and electromagnetic waves heralds the age of gravitational-wave multi-messenger astronomy. On 17 August 2017 the Advanced LIGO and Virgo detectors observed GW170817, a strong signal from the merger of a binary neutron-star system. Less than 2 seconds after the merger, a gamma-ray burst (GRB 170817A) was detected within a region of the sky consistent with the LIGO-Virgo-derived location of the gravitational-wave source. This sky region was subsequently observed by optical astronomy facilities, resulting in the identification of an optical transient signal within $\\sim 10$ arcsec of the galaxy NGC 4993. These multi-messenger observations allow us to use GW170817 as a standard siren, the gravitational-wave analog of an astronomical standard candle, to measure the Hubble constant. This quantity, which represents the local expansion rate of the Universe, sets the overall scale of the Universe and is of fundamental importance to cosmology. Our measurement combines the distance to the source inferred purely from the gravitational-wave signal with the recession velocity inferred from measurements of the redshift using electromagnetic data. This approach does not require any form of cosmic \"distance ladder;\" the gravitational wave analysis can be used to estimate the luminosity distance out to cosmological scales directly, without the use of intermediate astronomical distance measurements. We determine the Hubble constant to be $70.0^{+12.0}_{-8.0} \\, \\mathrm{km} \\, \\mathrm{s}^{-1} \\, \\mathrm{Mpc}^{-1}$ (maximum a posteriori and 68% credible interval). This is consistent with existing measurements, while being completely independent of them. Additional standard-siren measurements from future gravitational-wave sources will provide precision constraints of this important cosmological parameter.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present a near-infrared spectral sequence of the electromagnetic counterpart to the binary neutron star merger GW170817 detected by Advanced LIGO/Virgo. Our dataset comprises seven epochs of J+H spectra taken with FLAMINGOS-2 on Gemini-South between 1.5 and 10.5 days after the merger. In the initial epoch, the spectrum is dominated by a smooth blue continuum due to a high-velocity, lanthanide-poor blue kilonova component. Starting the following night, all subsequent spectra instead show features that are similar to those predicted in model spectra of material with a high concentration of lanthanides, including spectral peaks near 1.07 and 1.55 microns. Our fiducial model with 0.04 M_sun of ejecta, an ejection velocity of v=0.1c, and a lanthanide concentration of X_lan=1e-2 provides a good match to the spectra taken in the first five days, although it over-predicts the late-time fluxes. We also explore models with multiple fitting components, in each case finding that a significant abundance of lanthanide elements is necessary to match the broad spectral peaks that we observe starting at 2.5 d after the merger. These data provide direct evidence that binary neutron star mergers are significant production sites of even the heaviest r-process elements.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Spinning neutron stars asymmetric with respect to their rotation axis are potential sources of continuous gravitational waves for ground-based interferometric detectors. In the case of known pulsars a fully coherent search, based on matched filtering, which uses the position and rotational parameters obtained from electromagnetic observations, can be carried out. Matched filtering maximizes the signal-to-noise (SNR) ratio, but a large sensitivity loss is expected in case of even a very small mismatch between the assumed and the true signal parameters. For this reason, {\\it narrow-band} analyses methods have been developed, allowing a fully coherent search for gravitational waves from known pulsars over a fraction of a hertz and several spin-down values. In this paper we describe a narrow-band search of eleven pulsars using data from Advanced LIGO's first observing run. Although we have found several initial outliers, further studies show no significant evidence for the presence of a gravitational wave signal. Finally, we have placed upper limits on the signal strain amplitude lower than the spin-down limit for 5 of the 11 targets over the bands searched: in the case of J1813-1749 the spin-down limit has been beaten for the first time. For an additional 3 targets, the median upper limit across the search bands is below the spin-down limit. This is the most sensitive narrow-band search for continuous gravitational waves carried out so far.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The first observing run of Advanced LIGO spanned 4 months, from September 12, 2015 to January 19, 2016, during which gravitational waves were directly detected from two binary black hole systems, namely GW150914 and GW151226. Confident detection of gravitational waves requires an understanding of instrumental transients and artifacts that can reduce the sensitivity of a search. Studies of the quality of the detector data yield insights into the cause of instrumental artifacts and data quality vetoes specific to a search are produced to mitigate the effects of problematic data. In this paper, the systematic removal of noisy data from analysis time is shown to improve the sensitivity of searches for compact binary coalescences. The output of the PyCBC pipeline, which is a python-based code package used to search for gravitational wave signals from compact binary coalescences, is used as a metric for improvement. GW150914 was a loud enough signal that removing noisy data did not improve its significance. However, the removal of data with excess noise decreased the false alarm rate of GW151226 by more than two orders of magnitude, from 1 in 770 years to less than 1 in 186000 years.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "On August 14, 2017 at 10:30:43 UTC, the Advanced Virgo detector and the two Advanced LIGO detectors coherently observed a transient gravitational-wave signal produced by the coalescence of two stellar mass black holes, with a false-alarm-rate of $\\lesssim$ 1 in 27000 years. The signal was observed with a three-detector network matched-filter signal-to-noise ratio of 18. The inferred masses of the initial black holes are $30.5_{-3.0}^{+5.7}$ Msun and $25.3_{-4.2}^{+2.8}$ Msun (at the 90% credible level). The luminosity distance of the source is $540_{-210}^{+130}~\\mathrm{Mpc}$, corresponding to a redshift of $z=0.11_{-0.04}^{+0.03}$. A network of three detectors improves the sky localization of the source, reducing the area of the 90% credible region from 1160 deg$^2$ using only the two LIGO detectors to 60 deg$^2$ using all three detectors. For the first time, we can test the nature of gravitational wave polarizations from the antenna response of the LIGO-Virgo network, thus enabling a new class of phenomenological tests of gravity.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present results from the first directed search for nontensorial gravitational waves. While general relativity allows for tensorial (plus and cross) modes only, a generic metric theory may, in principle, predict waves with up to six different polarizations. This analysis is sensitive to continuous signals of scalar, vector or tensor polarizations, and does not rely on any specific theory of gravity. After searching data from the first observation run of the advanced LIGO detectors for signals at twice the rotational frequency of 200 known pulsars, we find no evidence of gravitational waves of any polarization. We report the first upper limits for scalar and vector strains, finding values comparable in magnitude to previously-published limits for tensor strain. Our results may be translated into constraints on specific alternative theories of gravity.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "In the Orthogonal Vectors (OV) problem, we wish to determine if there is an orthogonal pair of vectors among $n$ Boolean vectors in $d$ dimensions. The OV Conjecture (OVC) posits that OV requires $n^{2-o(1)}$ time to solve, for all $d=\u03c9(\\log n)$. Assuming the OVC, optimal time lower bounds have been proved for many prominent problems in $P$.\n  We prove that OVC is true in several computational models of interest:\n  * For all sufficiently large $n$ and $d$, OV for $n$ vectors in $\\{0,1\\}^d$ has branching program complexity $\\tilde\u0398(n\\cdot \\min(n,2^d))$. In particular, the lower bounds match the upper bounds up to polylog factors.\n  * OV has Boolean formula complexity $\\tilde\u0398(n\\cdot \\min(n,2^d))$, over all complete bases of $O(1)$ fan-in.\n  * OV requires $\\tilde\u0398(n\\cdot \\min(n,2^d))$ wires, in formulas comprised of gates computing arbitrary symmetric functions of unbounded fan-in.\n  Our lower bounds basically match the best known (quadratic) lower bounds for any explicit function in those models. Analogous lower bounds hold for many related problems shown to be hard under OVC, such as Batch Partial Match, Batch Subset Queries, and Batch Hamming Nearest Neighbors, all of which have very succinct reductions to OV.\n  The proofs use a certain kind of input restriction that is different from typical random restrictions where variables are assigned independently. We give a sense in which independent random restrictions cannot be used to show hardness, in that OVC is false in the \"average case\" even for $AC^0$ formulas:\n  * For every fixed $p \\in (0,1)$ there is an $\u03b5_p > 0$ such that for every $n$ and $d$, OV instances where input bits are independently set to $1$ with probability $p$ (and $0$ otherwise) can be solved with $AC^0$ formulas of size $O(n^{2-\u03b5_p})$, on all but a $o_n(1)$ fraction of instances.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Point location problems for $n$ points in $d$-dimensional Euclidean space (and $\\ell_p$ spaces more generally) have typically had two kinds of running-time solutions:\n  * (Nearly-Linear) less than $d^{poly(d)} \\cdot n \\log^{O(d)} n$ time, or\n  * (Barely-Subquadratic) $f(d) \\cdot n^{2-1/\u0398(d)}$ time, for various $f$.\n  For small $d$ and large $n$, \"nearly-linear\" running times are generally feasible, while \"barely-subquadratic\" times are generally infeasible. For example, in the Euclidean metric, finding a Closest Pair among $n$ points in ${\\mathbb R}^d$ is nearly-linear, solvable in $2^{O(d)} \\cdot n \\log^{O(1)} n$ time, while known algorithms for Furthest Pair (the diameter of the point set) are only barely-subquadratic, requiring $\u03a9(n^{2-1/\u0398(d)})$ time. Why do these proximity problems have such different time complexities? Is there a barrier to obtaining nearly-linear algorithms for problems which are currently only barely-subquadratic?\n  We give a novel exact and deterministic self-reduction for the Orthogonal Vectors problem on $n$ vectors in $\\{0,1\\}^d$ to $n$ vectors in ${\\mathbb Z}^{\u03c9(\\log d)}$ that runs in $2^{o(d)}$ time. As a consequence, barely-subquadratic problems such as Euclidean diameter, Euclidean bichromatic closest pair, ray shooting, and incidence detection do not have $O(n^{2-\u03b5})$ time algorithms (in Turing models of computation) for dimensionality $d = \u03c9(\\log \\log n)^2$, unless the popular Orthogonal Vectors Conjecture and the Strong Exponential Time Hypothesis are false. That is, while poly-log-log-dimensional Closest Pair is in $n^{1+o(1)}$ time, the analogous case of Furthest Pair can encode larger-dimensional problems conjectured to require $n^{2-o(1)}$ time. We also show that the All-Nearest Neighbors problem in $\u03c9(\\log n)$ dimensions requires $n^{2-o(1)}$ time to solve, assuming either of the above conjectures.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) has been in operation since July 2014. This paper describes the second data release from this phase, and the fourteenth from SDSS overall (making this, Data Release Fourteen or DR14). This release makes public data taken by SDSS-IV in its first two years of operation (July 2014-2016). Like all previous SDSS releases, DR14 is cumulative, including the most recent reductions and calibrations of all data taken by SDSS since the first phase began operations in 2000. New in DR14 is the first public release of data from the extended Baryon Oscillation Spectroscopic Survey (eBOSS); the first data from the second phase of the Apache Point Observatory (APO) Galactic Evolution Experiment (APOGEE-2), including stellar parameter estimates from an innovative data driven machine learning algorithm known as \"The Cannon\"; and almost twice as many data cubes from the Mapping Nearby Galaxies at APO (MaNGA) survey as were in the previous release (N = 2812 in total). This paper describes the location and format of the publicly available data from SDSS-IV surveys. We provide references to the important technical papers describing how these data have been taken (both targeting and observation details) and processed for scientific use. The SDSS website (www.sdss.org) has been updated for this release, and provides links to data downloads, as well as tutorials and examples of data use. SDSS-IV is planning to continue to collect astronomical data until 2020, and will be followed by SDSS-V.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We report results of a deep all-sky search for periodic gravitational waves from isolated neutron stars in data from the first Advanced LIGO observing run. This search investigates the low frequency range of Advanced LIGO data, between 20 and 100 Hz, much of which was not explored in initial LIGO. The search was made possible by the computing power provided by the volunteers of the Einstein@Home project. We find no significant signal candidate and set the most stringent upper limits to date on the amplitude of gravitational wave signals from the target population, corresponding to a sensitivity depth of 48.7 [1/$\\sqrt{\\textrm{Hz}}$]. At the frequency of best strain sensitivity, near 100 Hz, we set 90% confidence upper limits of $1.8 \\times 10^{-25}$. At the low end of our frequency range, 20 Hz, we achieve upper limits of $3.9 \\times 10^{-24}$. At 55 Hz we can exclude sources with ellipticities greater than $10^{-5}$ within 100 pc of Earth with fiducial value of the principal moment of inertia of $10^{38} \\textrm{kg m}^2$.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We report on an all-sky search for periodic gravitational waves in the frequency band 20-475 Hz and with a frequency time derivative in the range of [-1.0, +0.1]e-8 Hz/s. Such a signal could be produced by a nearby spinning and slightly non-axisymmetric isolated neutron star in our galaxy. This search uses the data from Advanced LIGO's first observational run, O1. No periodic gravitational wave signals were observed, and upper limits were placed on their strengths. The lowest upper limits on worst-case (linearly polarized) strain amplitude h0 are 4e-25 near 170 Hz. For a circularly polarized source (most favorable orientation), the smallest upper limits obtained are 1.5e-25. These upper limits refer to all sky locations and the entire range of frequency derivative values. For a population-averaged ensemble of sky locations and stellar orientations, the lowest upper limits obtained for the strain amplitude are 2.5e-25.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present a new distributed model of probabilistically checkable proofs (PCP). A satisfying assignment $x \\in \\{0,1\\}^n$ to a CNF formula $\\varphi$ is shared between two parties, where Alice knows $x_1, \\dots, x_{n/2}$, Bob knows $x_{n/2+1},\\dots,x_n$, and both parties know $\\varphi$. The goal is to have Alice and Bob jointly write a PCP that $x$ satisfies $\\varphi$, while exchanging little or no information. Unfortunately, this model as-is does not allow for nontrivial query complexity. Instead, we focus on a non-deterministic variant, where the players are helped by Merlin, a third party who knows all of $x$.\n  Using our framework, we obtain, for the first time, PCP-like reductions from the Strong Exponential Time Hypothesis (SETH) to approximation problems in P. In particular, under SETH we show that there are no truly-subquadratic approximation algorithms for Bichromatic Maximum Inner Product over {0,1}-vectors, Bichromatic LCS Closest Pair over permutations, Approximate Regular Expression Matching, and Diameter in Product Metric. All our inapproximability factors are nearly-tight. In particular, for the first two problems we obtain nearly-polynomial factors of $2^{(\\log n)^{1-o(1)}}$; only $(1+o(1))$-factor lower bounds (under SETH) were known before.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The design and organization of complex robotic systems traditionally requires laborious trial-and-error processes to ensure both hardware and software components are correctly connected with the resources necessary for computation. This paper presents a novel generalization of the quadratic assignment and routing problem, introducing formalisms for selecting components and interconnections to synthesize a complete system capable of providing some user-defined functionality. By introducing mission context, functional requirements, and modularity directly into the assignment problem, we derive a solution where components are automatically selected and then organized into an optimal hardware and software interconnection structure, all while respecting restrictions on component viability and required functionality. The ability to generate \\emph{complete} functional systems directly from individual components reduces manual design effort by allowing for a guided exploration of the design space. Additionally, our formulation increases resiliency by quantifying resource margins and enabling adaptation of system structure in response to changing environments, hardware or software failure. The proposed formulation is cast as an integer linear program which is provably $\\mathcal{NP}$-hard. Two case studies are developed and analyzed to highlight the expressiveness and complexity of problems that can be addressed by this approach: the first explores the iterative development of a ground-based search-and-rescue robot in a variety of mission contexts, while the second explores the large-scale, complex design of a humanoid disaster robot for the DARPA Robotics Challenge. Numerical simulations quantify real world performance and demonstrate tractable time complexity for the scale of problems encountered in many modern robotic systems.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present the results of a semicoherent search for continuous gravitational waves from the low-mass X-ray binary Scorpius X-1, using data from the first Advanced LIGO observing run. The search method uses details of the modelled, parametrized continuous signal to combine coherently data separated by less than a specified coherence time, which can be adjusted to trade off sensitivity against computational cost. A search was conducted over the frequency range from 25 Hz to 2000 Hz, spanning the current observationally-constrained range of the binary orbital parameters. No significant detection candidates were found, and frequency-dependent upper limits were set using a combination of sensitivity estimates and simulated signal injections. The most stringent upper limit was set at 175 Hz, with comparable limits set across the most sensitive frequency range from 100 Hz to 200 Hz. At this frequency, the 95 pct upper limit on signal amplitude h0 is 2.3e-25 marginalized over the unknown inclination angle of the neutron star's spin, and 8.03e-26 assuming the best orientation (which results in circularly polarized gravitational waves). These limits are a factor of 3-4 stronger than those set by other analyses of the same data, and a factor of about 7 stronger than the best upper limits set using initial LIGO data. In the vicinity of 100 Hz, the limits are a factor of between 1.2 and 3.5 above the predictions of the torque balance model, depending on inclination angle, if the most likely inclination angle of 44 degrees is assumed, they are within a factor of 1.7.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We study a multi-robot assignment problem for multi-target tracking. The proposed problem can be viewed as the mixed packing and covering problem. To deal with a limitation on both sensing and communication ranges, a distributed approach is taken into consideration. A local algorithm gives theoretical bounds on both the running time and approximation ratio to an optimal solution. We employ a local algorithm of max-min linear programs to solve the proposed task. Simulation result shows that a local algorithm is an effective solution to the multi-robot task allocation.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We describe the observation of GW170104, a gravitational-wave signal produced by the coalescence of a pair of stellar-mass black holes. The signal was measured on January 4, 2017 at 10:11:58.6 UTC by the twin advanced detectors of the Laser Interferometer Gravitational-Wave Observatory during their second observing run, with a network signal-to-noise ratio of 13 and a false alarm rate less than 1 in 70,000 years. The inferred component black hole masses are $31.2^{+8.4}_{-6.0}\\,M_\\odot$ and $19.4^{+5.3}_{-5.9}\\,M_\\odot$ (at the 90% credible level). The black hole spins are best constrained through measurement of the effective inspiral spin parameter, a mass-weighted combination of the spin components perpendicular to the orbital plane, $\u03c7_\\mathrm{eff} = -0.12^{+0.21}_{-0.30}.$ This result implies that spin configurations with both component spins positively aligned with the orbital angular momentum are disfavored. The source luminosity distance is $880^{+450}_{-390}~\\mathrm{Mpc}$ corresponding to a redshift of $z = 0.18^{+0.08}_{-0.07}$. We constrain the magnitude of modifications to the gravitational-wave dispersion relation and perform null tests of general relativity. Assuming that gravitons are dispersed in vacuum like massive particles, we bound the graviton mass to $m_g \\le 7.7 \\times 10^{-23}~\\mathrm{eV}/c^2$. In all cases, we find that GW170104 is consistent with general relativity.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "During their first observational run, the two Advanced LIGO detectors attained an unprecedented sensitivity, resulting in the first direct detections of gravitational-wave signals and GW151226, produced by stellar-mass binary black hole systems. This paper reports on an all-sky search for gravitational waves (GWs) from merging intermediate mass black hole binaries (IMBHBs). The combined results from two independent search techniques were used in this study: the first employs a matched-filter algorithm that uses a bank of filters covering the GW signal parameter space, while the second is a generic search for GW transients (bursts). No GWs from IMBHBs were detected, therefore, we constrain the rate of several classes of IMBHB mergers. The most stringent limit is obtained for black holes of individual mass $100\\,M_\\odot$, with spins aligned with the binary orbital angular momentum. For such systems, the merger rate is constrained to be less than $0.93~\\mathrm{Gpc^{-3}\\,yr}^{-1}$ in comoving units at the $90\\%$ confidence level, an improvement of nearly 2 orders of magnitude over previous upper limits.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "Results are presented from a semi-coherent search for continuous gravitational waves from the brightest low-mass X-ray binary, Scorpius X-1, using data collected during the first Advanced LIGO observing run (O1). The search combines a frequency domain matched filter (Bessel-weighted $\\mathcal{F}$-statistic) with a hidden Markov model to track wandering of the neutron star spin frequency. No evidence of gravitational waves is found in the frequency range 60-650 Hz. Frequentist 95% confidence strain upper limits, $h_0^{95\\%} = 4.0\\times10^{-25}$, $8.3\\times10^{-25}$, and $3.0\\times10^{-25}$ for electromagnetically restricted source orientation, unknown polarization, and circular polarization, respectively, are reported at 106 Hz. They are $\\leq 10$ times higher than the theoretical torque-balance limit at 106 Hz.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We use Hubble Space Telescope imaging to study the structural properties of ten of the most massive ($M \\geq 10^{11.25}$ Msun) quiescent galaxies (QGs) in the UKIDSS UDS at $2.5<z<3.0$. The low spatial density of these galaxies required targeted WFC3 $H_{160}$ imaging, as such systems are rare in existing surveys like CANDELS. We fit Sersic models to the 2D light profiles and find that the median half-light radius is $R_e \\sim 3$ kpc, a factor of $\\sim 3$ smaller than QGs with similar masses at $z \\sim 0$. Complementing our sample with similarly massive QGs at lower redshifts, we find that the median size evolves as $R_e \\propto H(z)^{-0.85 \\pm 0.12}$ (or alternatively, $R_e \\propto (1+z)^{-0.90 \\pm 0.12}$). This rate of evolution is slower than that for lower mass QGs. When compared to low redshift QGs, the axis ratio distribution for our high redshift massive QG sample is most consistent with those in which spheroids are dominant. These observations point to earlier size growth among massive QGs that also resulted in spheroidal systems. Finally, we measured residual-corrected surface brightness profiles for our sample. These show that the Sersic parameterization is generally representative out to several effective radii and does not miss excess low surface brightness light. The sizes inferred from the light profiles therefore confirm the compactness of these most massive high redshift QGs.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "The Advanced LIGO observatories detected gravitational waves from two binary black hole mergers during their first observation run (O1). We present a high-energy neutrino follow-up search for the second gravitational wave event, GW151226, as well as for gravitational wave candidate LVT151012. We find 2 and 4 neutrino candidates detected by IceCube, and 1 and 0 detected by ANTARES, within $\\pm500$ s around the respective gravitational wave signals, consistent with the expected background rate. None of these neutrino candidates are found to be directionally coincident with GW151226 or LVT151012. We use non-detection to constrain isotropic-equivalent high-energy neutrino emission from GW151226 adopting the GW event's 3D localization, to less than $2\\times 10^{51}-2\\times10^{54}$ erg.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We directly detect dust emission in an optically-detected, multiply-imaged galaxy lensed by the Frontier Fields cluster MACSJ0717.5+3745. We detect two images of the same galaxy at 1.1mm with the AzTEC camera on the Large Millimeter Telescope leaving no ambiguity in the counterpart identification. This galaxy, MACS071_Az9, is at z>4 and the strong lensing model (mu=7.5) allows us to calculate an intrinsic IR luminosity of 9.7e10 Lsun and an obscured star formation rate of 14.6 +/- 4.5 Msun/yr. The unobscured star formation rate from the UV is only 4.1 +/- 0.3 Msun/yr which means the total star formation rate (18.7 +/- 4.5 Msun/yr) is dominated (75-80%) by the obscured component. With an intrinsic stellar mass of only 6.9e9Msun, MACS0717_Az9 is one of only a handful of z>4 galaxies at these lower masses that is detected in dust emission. This galaxy lies close to the estimated star formation sequence at this epoch. However, it does not lie on the dust obscuration relation (IRX-beta) for local starburst galaxies and is instead consistent with the Small Magellanic Cloud (SMC) attenuation law. This remarkable lower mass galaxy showing signs of both low metallicity and high dust content may challenge our picture of dust production in the early Universe.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We present an all-sky search for muon neutrinos produced during the prompt $\u03b3$-ray emission of 1172 gamma-ray bursts (GRBs) with the IceCube Neutrino Observatory. The detection of these neutrinos would constitute evidence for ultra-high energy cosmic ray (UHECR) production in GRBs, as interactions between accelerated protons and the prompt $\u03b3$-ray field would yield charged pions, which decay to neutrinos. A previously reported search for muon neutrino tracks from Northern Hemisphere GRBs has been extended to include three additional years of IceCube data. A search for such tracks from Southern Hemisphere GRBs in five years of IceCube data has been introduced to enhance our sensitivity to the highest energy neutrinos. No significant correlation between neutrino events and observed GRBs is seen in the new data. Combining this result with previous muon neutrino track searches and a search for cascade signature events from all neutrino flavors, we obtain new constraints for single-zone fireball models of GRB neutrino and UHECR production.\n        \u25b3 Less", "author": "Ryan Williams"}, {"abstract": "We consider the problem of jointly estimating the attitude and spin-rate of a spinning spacecraft. Psiaki (J. Astronautical Sci., 57(1-2):73--92, 2009) has formulated a family of optimization problems that generalize the classical least-squares attitude estimation problem, known as Wahba's problem, to the case of a spinning spacecraft. If the rotation axis is fixed and known, but the spin-rate is unknown (such as for nutation-damped spin-stabilized spacecraft) we show that Psiaki's problem can be reformulated exactly as a type of tractable convex optimization problem called a semidefinite optimization problem. This reformulation allows us to globally solve the problem using standard numerical routines for semidefinite optimization. It also provides a natural semidefinite relaxation-based approach to more complicated variations on the problem.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We study the convex hull of $SO(n)$, thought of as the set of $n\\times n$ orthogonal matrices with unit determinant, from the point of view of semidefinite programming. We show that the convex hull of $SO(n)$ is doubly spectrahedral, i.e. both it and its polar have a description as the intersection of a cone of positive semidefinite matrices with an affine subspace. Our spectrahedral representations are explicit, and are of minimum size, in the sense that there are no smaller spectrahedral representations of these convex bodies.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity $O(k^{2}n)$ using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efficient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or flight networks where the FVS nodes often correspond to a small number of high-degree nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate in $O(kn^2+n^2\\log n)$ if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing a inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank correction with complexity $O(kn^{2}+n^{2}\\log n)$ per iteration. We also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with FVSs of various sizes.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "Bounds on the log partition function are important in a variety of contexts, including approximate inference, model fitting, decision theory, and large deviations analysis. We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model.  In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe free energy, but distinguished by the following desirable properties: i. they are cnvex, and have a unique global minimum; and ii. the global minimum gives an upper bound on the log partition function. The global minimum is defined by stationary conditions very similar to those defining fixed points of belief propagation or tree-based reparameterization Wainwright et al., 2001.  As with BP fixed points, the elements of the minimizing argument can be used as approximations to the marginals of the original model.  The analysis described here can be extended to structures of higher treewidth e.g., hypertrees, thereby making connections with more advanced approximations e.g., Kikuchi and variants Yedidia et al., 2001; Minka, 2001.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "Rejoinder to \"Latent variable graphical model selection via convex optimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We consider the problem of drawing samples from posterior distributions formed under a Dirichlet prior and a truncated multinomial likelihood, by which we mean a Multinomial likelihood function where we condition on one or more counts being zero a priori. Sampling this posterior distribution is of interest in inference algorithms for hierarchical Bayesian models based on the Dirichlet distribution or the Dirichlet process, particularly Gibbs sampling algorithms for the Hierarchical Dirichlet Process Hidden Semi-Markov Model. We provide a data augmentation sampling algorithm that is easy to implement, fast both to mix and to execute, and easily scalable to many dimensions. We demonstrate the algorithm's advantages over a generic Metropolis-Hastings sampling algorithm in several numerical experiments.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "In this paper we establish links between, and new results for, three problems that are not usually considered together. The first is a matrix decomposition problem that arises in areas such as statistical modeling and signal processing: given a matrix $X$ formed as the sum of an unknown diagonal matrix and an unknown low rank positive semidefinite matrix, decompose $X$ into these constituents. The second problem we consider is to determine the facial structure of the set of correlation matrices, a convex set also known as the elliptope. This convex body, and particularly its facial structure, plays a role in applications from combinatorial optimization to mathematical finance. The third problem is a basic geometric question: given points $v_1,v_2,...,v_n\\in \\R^k$ (where $n > k$) determine whether there is a centered ellipsoid passing \\emph{exactly} through all of the points.\n  We show that in a precise sense these three problems are equivalent. Furthermore we establish a simple sufficient condition on a subspace $U$ that ensures any positive semidefinite matrix $L$ with column space $U$ can be recovered from $D+L$ for any diagonal matrix $D$ using a convex optimization-based heuristic known as minimum trace factor analysis. This result leads to a new understanding of the structure of rank-deficient correlation matrices and a simple condition on a set of points that ensures there is a centered ellipsoid passing through them.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the traditional HMM. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markovianity, which has been developed in the parametric setting to allow construction of highly interpretable models that admit natural prior information on state durations. In this paper we introduce the explicitduration HDP-HSMM and develop posterior sampling algorithms for efficient inference in both the direct-assignment and weak-limit approximation settings. We demonstrate the utility of the model and our inference methods on synthetic data as well as experiments on a speaker diarization problem and an example of learning the patterns in Morse code.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous Hidden Markov Model for learning from sequential and time-series data. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markovianity, which has been developed mainly in the parametric frequentist setting, to allow construction of highly interpretable models that admit natural prior information on state durations.\n  In this paper we introduce the explicit-duration Hierarchical Dirichlet Process Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for efficient posterior inference. The methods we introduce also provide new methods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs sampling methods can be embedded in samplers for larger hierarchical Bayesian models, adding semi-Markov chain modeling as another tool in the Bayesian inference toolbox. We demonstrate the utility of the HDP-HSMM and our inference methods on both synthetic and real experiments.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We propose a Bayesian nonparametric approach to the problem of jointly modeling multiple related time series. Our approach is based on the discovery of a set of latent, shared dynamical behaviors. Using a beta process prior, the size of the set and the sharing pattern are both inferred from data. We develop efficient Markov chain Monte Carlo methods based on the Indian buffet process representation of the predictive distribution of the beta process, without relying on a truncated model. In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth and death proposals. We examine the benefits of our proposed feature-based model on several synthetic datasets, and also demonstrate promising results on unsupervised segmentation of visual motion capture data.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We consider the problem of high-dimensional Ising (graphical) model selection. We propose a simple algorithm for structure estimation based on the thresholding of the empirical conditional variation distances. We introduce a novel criterion for tractable graph families, where this method is efficient, based on the presence of sparse local separators between node pairs in the underlying graph. For such graphs, the proposed algorithm has a sample complexity of $n=\u03a9(J_{\\min}^{-2}\\log p)$, where $p$ is the number of variables, and $J_{\\min}$ is the minimum (absolute) edge potential in the model. We also establish nonasymptotic necessary and sufficient conditions for structure estimation.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efficient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples n=omega(J_{min}^{-2} log p), where p is the number of variables and J_{min} is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "While loopy belief propagation (LBP) performs reasonably well for inference in some Gaussian graphical models with cycles, its performance is unsatisfactory for many others. In particular for some models LBP does not converge, and in general when it does converge, the computed variances are incorrect (except for cycle-free graphs for which belief propagation (BP) is non-iterative and exact). In this paper we propose {\\em feedback message passing} (FMP), a message-passing algorithm that makes use of a special set of vertices (called a {\\em feedback vertex set} or {\\em FVS}) whose removal results in a cycle-free graph. In FMP, standard BP is employed several times on the cycle-free subgraph excluding the FVS while a special message-passing scheme is used for the nodes in the FVS. The computational complexity of exact inference is $O(k^2n)$, where $k$ is the number of feedback nodes, and $n$ is the total number of nodes. When the size of the FVS is very large, FMP is intractable. Hence we propose {\\em approximate FMP}, where a pseudo-FVS is used instead of an FVS, and where inference in the non-cycle-free graph obtained by removing the pseudo-FVS is carried out approximately using LBP. We show that, when approximate FMP converges, it yields exact means and variances on the pseudo-FVS and exact means throughout the remainder of the graph. We also provide theoretical results on the convergence and accuracy of approximate FMP. In particular, we prove error bounds on variance computation. Based on these theoretical results, we design efficient algorithms to select a pseudo-FVS of bounded size. The choice of the pseudo-FVS allows us to explicitly trade off between efficiency and accuracy. Experimental results show that using a pseudo-FVS of size no larger than $\\log(n)$, this procedure converges much more often, more quickly, and provides more accurate results than LBP on the entire graph.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "The problem of designing policies for in-network function computation with minimum energy consumption subject to a latency constraint is considered. The scaling behavior of the energy consumption under the latency constraint is analyzed for random networks, where the nodes are uniformly placed in growing regions and the number of nodes goes to infinity. The special case of sum function computation and its delivery to a designated root node is considered first. A policy which achieves order-optimal average energy consumption in random networks subject to the given latency constraint is proposed. The scaling behavior of the optimal energy consumption depends on the path-loss exponent of wireless transmissions and the dimension of the Euclidean region where the nodes are placed. The policy is then extended to computation of a general class of functions which decompose according to maximal cliques of a proximity graph such as the $k$-nearest neighbor graph or the geometric random graph. The modified policy achieves order-optimal energy consumption albeit for a limited range of latency constraints.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "The structural properties of graphs are usually characterized in terms of invariants, which are functions of graphs that do not depend on the labeling of the nodes. In this paper we study convex graph invariants, which are graph invariants that are convex functions of the adjacency matrix of a graph. Some examples include functions of a graph such as the maximum degree, the MAXCUT value (and its semidefinite relaxation), and spectral invariants such as the sum of the $k$ largest eigenvalues. Such functions can be used to construct convex sets that impose various structural constraints on graphs, and thus provide a unified framework for solving a number of interesting graph problems via convex optimization. We give a representation of all convex graph invariants in terms of certain elementary invariants, and describe methods to compute or approximate convex graph invariants tractably. We also compare convex and non-convex invariants, and discuss connections to robust optimization. Finally we use convex graph invariants to provide efficient convex programming solutions to graph problems such as the deconvolution of the composition of two graphs into the individual components, hypothesis testing between graph families, and the generation of graphs with certain desired structural properties.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "In applications throughout science and engineering one is often faced with the challenge of solving an ill-posed inverse problem, where the number of available measurements is smaller than the dimension of the model to be estimated. However in many practical situations of interest, models are constrained structurally so that they only have a few degrees of freedom relative to their ambient dimension. This paper provides a general framework to convert notions of simplicity into convex penalty functions, resulting in convex optimization solutions to linear, underdetermined inverse problems. The class of simple models considered are those formed as the sum of a few atoms from some (possibly infinite) elementary atomic set; examples include well-studied cases such as sparse vectors and low-rank matrices, as well as several others including sums of a few permutations matrices, low-rank tensors, orthogonal matrices, and atomic measures. The convex programming formulation is based on minimizing the norm induced by the convex hull of the atomic set; this norm is referred to as the atomic norm. The facial structure of the atomic norm ball carries a number of favorable properties that are useful for recovering simple models, and an analysis of the underlying convex geometry provides sharp estimates of the number of generic measurements required for exact and robust recovery of models from partial information. These estimates are based on computing the Gaussian widths of tangent cones to the atomic norm ball. When the atomic set has algebraic structure the resulting optimization problems can be solved or approximated via semidefinite programming. The quality of these approximations affects the number of measurements required for recovery. Thus this work extends the catalog of simple models that can be recovered from limited linear information via tractable convex programming.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We consider the problem of learning the structure of ferromagnetic Ising models Markov on sparse Erdos-Renyi random graph. We propose simple local algorithms and analyze their performance in the regime of correlation decay. We prove that an algorithm based on a set of conditional mutual information tests is consistent for structure learning throughout the regime of correlation decay. This algorithm requires the number of samples to scale as \u03c9(\\log n), and has a computational complexity of O(n^4). A simpler algorithm based on correlation thresholding outputs a graph with a constant edit distance to the original graph when there is correlation decay, and the number of samples required is \u03a9(\\log n). Under a more stringent condition, correlation thresholding is consistent for structure estimation. We finally prove a lower bound that \u03a9(c\\log n) samples are also needed for consistent reconstruction of random graphs by any algorithm with positive probability, where c is the average degree. Thus, we establish that consistent structure estimation is possible with almost order-optimal sample complexity throughout the regime of correlation decay.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. We propose two consistent and computationally efficient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. Unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. Our first algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. One of the main contributions of this work is our second algorithm, which we refer to as CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. This global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures) on much smaller subsets of variables. This results in more accurate and efficient learning of latent trees. We also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. We compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden Markov models and star graphs. In addition, we demonstrate the applicability of our methods on real-world datasets by modeling the dependency structure of monthly stock returns in the S&P index and of the words in the 20 newsgroups dataset.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "Suppose we observe samples of a subset of a collection of random variables. No additional information is provided about the number of latent variables, nor of the relationship between the latent and observed variables. Is it possible to discover the number of latent components, and to learn a statistical model over the entire collection of variables? We address this question in the setting in which the latent and observed variables are jointly Gaussian, with the conditional statistics of the observed variables conditioned on the latent variables being specified by a graphical model. As a first step we give natural conditions under which such latent-variable Gaussian graphical models are identifiable given marginal statistics of only the observed variables. Essentially these conditions require that the conditional graphical model among the observed variables is sparse, while the effect of the latent variables is \"spread out\" over most of the observed variables. Next we propose a tractable convex program based on regularized maximum-likelihood for model selection in this latent-variable setting; the regularizer uses both the $\\ell_1$ norm and the nuclear norm. Our modeling framework can be viewed as a combination of dimensionality reduction (to identify latent variables) and graphical modeling (to capture remaining statistical structure not attributable to the latent variables), and it consistently estimates both the number of latent components and the conditional graphical model structure among the observed variables. These results are applicable in the high-dimensional setting in which the number of latent/observed variables grows with the number of samples of the observed variables. The geometric properties of the algebraic varieties of sparse matrices and of low-rank matrices play an important role in our analysis.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under fixed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufficient conditions on (n,d,k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identified; we prove that the independent (resp. tree) model is the hardest (resp. easiest) to learn using the proposed algorithm in terms of error rates for structure learning.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "Many complex dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes.  We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our Bayesian nonparametric approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes.  We additionally employ automatic relevance determination to infer a sparse set of dynamic dependencies allowing us to learn SLDS with varying state dimension or switching VAR processes with varying autoregressive order. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efficient joint sampling of the mode and state sequences. The utility and flexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, the IBOVESPA stock index, and a maneuvering target tracking application.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "Compressed sensing allows perfect recovery of sparse signals (or signals sparse in some basis) using only a small number of random measurements. Existing results in compressed sensing literature have focused on characterizing the achievable performance by bounding the number of samples required for a given level of signal sparsity. However, using these bounds to minimize the number of samples requires a-priori knowledge of the sparsity of the unknown signal, or the decay structure for near-sparse signals. Furthermore, there are some popular recovery methods for which no such bounds are known.\n  In this paper, we investigate an alternative scenario where observations are available in sequence. For any recovery method, this means that there is now a sequence of candidate reconstructions. We propose a method to estimate the reconstruction error directly from the samples themselves, for every candidate in this sequence. This estimate is universal in the sense that it is based only on the measurement ensemble, and not on the recovery method or any assumed level of sparsity of the unknown signal. With these estimates, one can now stop observations as soon as there is reasonable certainty of either exact or sufficiently accurate reconstruction. They also provide a way to obtain \"run-time\" guarantees for recovery methods that otherwise lack a-priori performance bounds.\n  We investigate both continuous (e.g. Gaussian) and discrete (e.g. Bernoulli) random measurement ensembles, both for exactly sparse and general near-sparse signals, and with both noisy and noiseless measurements.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "The problem of learning tree-structured Gaussian graphical models from independent and identically distributed (i.i.d.) samples is considered. The influence of the tree structure and the parameters of the Gaussian distribution on the learning rate as the number of samples increases is discussed. Specifically, the error exponent corresponding to the event that the estimated tree structure differs from the actual unknown tree structure of the distribution is analyzed. Finding the error exponent reduces to a least-squares problem in the very noisy learning regime. In this regime, it is shown that the extremal tree structure that minimizes the error exponent is the star for any fixed set of correlation coefficients on the edges of the tree. If the magnitudes of all the correlation coefficients are less than 0.63, it is also shown that the tree structure that maximizes the error exponent is the Markov chain. In other words, the star and the chain graphs represent the hardest and the easiest structures to learn in the class of tree-structured Gaussian graphical models. This result can also be intuitively explained by correlation decay: pairs of nodes which are far apart, in terms of graph distance, are unlikely to be mistaken as edges by the maximum-likelihood estimator in the asymptotic regime.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "Suppose we are given a matrix that is formed by adding an unknown sparse matrix to an unknown low-rank matrix. Our goal is to decompose the given matrix into its sparse and low-rank components. Such a problem arises in a number of applications in model and system identification, and is NP-hard in general. In this paper we consider a convex optimization formulation to splitting the specified matrix into its components, by minimizing a linear combination of the $\\ell_1$ norm and the nuclear norm of the components. We develop a notion of \\emph{rank-sparsity incoherence}, expressed as an uncertainty principle between the sparsity pattern of a matrix and its row and column spaces, and use it to characterize both fundamental identifiability as well as (deterministic) sufficient conditions for exact recovery. Our analysis is geometric in nature, with the tangent spaces to the algebraic varieties of sparse and low-rank matrices playing a prominent role. When the sparse and low-rank matrices are drawn from certain natural random ensembles, we show that the sufficient conditions for exact recovery are satisfied with high probability. We conclude with simulation results on synthetic matrix decomposition problems.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We consider the problem of speaker diarization, the problem of segmenting an audio recording of a meeting into temporal segments corresponding to individual speakers. The problem is rendered particularly difficult by the fact that we are not allowed to assume knowledge of the number of people participating in the meeting. To address this problem, we take a Bayesian nonparametric approach to speaker diarization that builds on the hierarchical Dirichlet process hidden Markov model (HDP-HMM) of Teh et al. [J. Amer. Statist. Assoc. 101 (2006) 1566--1581]. Although the basic HDP-HMM tends to over-segment the audio data---creating redundant states and rapidly switching among them---we describe an augmented HDP-HMM that provides effective control over the switching rate. We also show that this augmentation makes it possible to treat emission distributions nonparametrically. To scale the resulting architecture to realistic diarization problems, we develop a sampling algorithm that employs a truncated approximation of the Dirichlet process to jointly resample the full state sequence, greatly improving mixing rates. Working with a benchmark NIST data set, we show that our Bayesian nonparametric architecture yields state-of-the-art speaker diarization results.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "The problem of maximum-likelihood (ML) estimation of discrete tree-structured distributions is considered. Chow and Liu established that ML-estimation reduces to the construction of a maximum-weight spanning tree using the empirical mutual information quantities as the edge weights. Using the theory of large-deviations, we analyze the exponent associated with the error probability of the event that the ML-estimate of the Markov tree structure differs from the true tree structure, given a set of independently drawn samples. By exploiting the fact that the output of ML-estimation is a tree, we establish that the error exponent is equal to the exponential rate of decay of a single dominant crossover event. We prove that in this dominant crossover event, a non-neighbor node pair replaces a true edge of the distribution that is along the path of edges in the true tree graph connecting the nodes in the non-neighbor pair. Using ideas from Euclidean information theory, we then analyze the scenario of ML-estimation in the very noisy learning regime and show that the error exponent can be approximated as a ratio, which is interpreted as the signal-to-noise ratio (SNR) for learning tree distributions. We show via numerical experiments that in this regime, our SNR approximation is accurate.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We investigate the use of message-passing algorithms for the problem of finding the max-weight independent set (MWIS) in a graph. First, we study the performance of the classical loopy max-product belief propagation. We show that each fixed point estimate of max-product can be mapped in a natural way to an extreme point of the LP polytope associated with the MWIS problem. However, this extreme point may not be the one that maximizes the value of node weights; the particular extreme point at final convergence depends on the initialization of max-product. We then show that if max-product is started from the natural initialization of uninformative messages, it always solves the correct LP -- if it converges. This result is obtained via a direct analysis of the iterative algorithm, and cannot be obtained by looking only at fixed points.\n  The tightness of the LP relaxation is thus necessary for max-product optimality, but it is not sufficient. Motivated by this observation, we show that a simple modification of max-product becomes gradient descent on (a convexified version of) the dual of the LP, and converges to the dual optimum. We also develop a message-passing algorithm that recovers the primal MWIS solution from the output of the descent algorithm. We show that the MWIS estimate obtained using these two algorithms in conjunction is correct when the graph is bipartite and the MWIS is unique.\n  Finally, we show that any problem of MAP estimation for probability distributions over finite domains can be reduced to an MWIS problem. We believe this reduction will yield new insights and algorithms for MAP estimation.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We develop a general framework for MAP estimation in discrete and Gaussian graphical models using Lagrangian relaxation techniques. The key idea is to reformulate an intractable estimation problem as one defined on a more tractable graph, but subject to additional constraints. Relaxing these constraints gives a tractable dual problem, one defined by a thin graph, which is then optimized by an iterative procedure. When this iterative optimization leads to a consistent estimate, one which also satisfies the constraints, then it corresponds to an optimal MAP estimate of the original model. Otherwise there is a ``duality gap'', and we obtain a bound on the optimal solution. Thus, our approach combines convex optimization with dynamic programming techniques applicable for thin graphs. The popular tree-reweighted max-product (TRMP) method may be seen as solving a particular class of such relaxations, where the intractable graph is relaxed to a set of spanning trees. We also consider relaxations to a set of small induced subgraphs, thin subgraphs (e.g. loops), and a connected tree obtained by ``unwinding'' cycles. In addition, we propose a new class of multiscale relaxations that introduce ``summary'' variables. The potential benefits of such generalizations include: reducing or eliminating the ``duality gap'' in hard problems, reducing the number or Lagrange multipliers in the dual problem, and accelerating convergence of the iterative optimization procedure.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "We develop and analyze methods for computing provably optimal {\\em maximum a posteriori} (MAP) configurations for a subclass of Markov random fields defined on graphs with cycles. By decomposing the original distribution into a convex combination of tree-structured distributions, we obtain an upper bound on the optimal value of the original problem (i.e., the log probability of the MAP assignment) in terms of the combined optimal values of the tree problems. We prove that this upper bound is tight if and only if all the tree distributions share an optimal configuration in common. An important implication is that any such shared configuration must also be a MAP configuration for the original distribution. Next we develop two approaches to attempting to obtain tight upper bounds: (a) a {\\em tree-relaxed linear program} (LP), which is derived from the Lagrangian dual of the upper bounds; and (b) a {\\em tree-reweighted max-product message-passing algorithm} that is related to but distinct from the max-product algorithm. In this way, we establish a connection between a certain LP relaxation of the mode-finding problem, and a reweighted form of the max-product (min-sum) message-passing algorithm.\n        \u25b3 Less", "author": "Alan Willsky"}, {"abstract": "The preponderance of matter over antimatter in the early Universe, the dynamics of the supernova bursts that produced the heavy elements necessary for life and whether protons eventually decay --- these mysteries at the forefront of particle physics and astrophysics are key to understanding the early evolution of our Universe, its current state and its eventual fate. The Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed plan for a world-class experiment dedicated to addressing these questions. LBNE is conceived around three central components: (1) a new, high-intensity neutrino source generated from a megawatt-class proton accelerator at Fermi National Accelerator Laboratory, (2) a near neutrino detector just downstream of the source, and (3) a massive liquid argon time-projection chamber deployed as a far detector deep underground at the Sanford Underground Research Facility. This facility, located at the site of the former Homestake Mine in Lead, South Dakota, is approximately 1,300 km from the neutrino source at Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino charge-parity symmetry violation and mass ordering effects. This ambitious yet cost-effective design incorporates scalability and flexibility and can accommodate a variety of upgrades and contributions. With its exceptional combination of experimental configuration, technical capabilities, and potential for transformative discoveries, LBNE promises to be a vital facility for the field of particle physics worldwide, providing physicists from around the globe with opportunities to collaborate in a twenty to thirty year program of exciting science. In this document we provide a comprehensive overview of LBNE's scientific objectives, its place in the landscape of neutrino physics worldwide, the technologies it will incorporate and the capabilities it will possess.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "Direct monitoring of singlet oxygen (1O2) luminescence is a particularly challenging infrared photodetection problem. 1O2, an excited state of the oxygen molecule, is a crucial intermediate in many biological processes. We employ a low noise superconducting nanowire single-photon detector to record 1O2 luminescence at 1270 nm wavelength from a model photosensitizer (Rose Bengal) in solution. Narrow band spectral filtering and chemical quenching is used to verify the 1O2 signal, and lifetime evolution with the addition of protein is studied. Furthermore, we demonstrate the detection of 1O2 luminescence through a single optical fiber, a marked advance for dose monitoring in clinical treatments such as photodynamic therapy.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "Submillimetre continuum radiation allows us to probe cold objects, particularly the earliest, dusty phases of star formation, high-redshift galaxies and circumstellar disks. The submillimetre window gives a unique view of the physical and dynamical conditions in the neutral and molecular interstellar medium. In the next decade a combination of wide-field surveys with single-dish telescopes and targeted follow-up with ALMA and other facilities should enable rapid progress in answering questions about the origins of planetary systems, stars and galaxies.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "A large sub-mm survey with Herschel will enable many exciting science opportunities, especially in an era of wide-field optical and radio surveys and high resolution cosmic microwave background experiments. The Herschel-SPIRE Legacy Survey (HSLS), will lead to imaging data over 4000 sq. degrees at 250, 350, and 500 micron. Major Goals of HSLS are: (a) produce a catalog of 2.5 to 3 million galaxies down to 26, 27 and 33 mJy (50% completeness; 5 sigma confusion noise) at 250, 350 and 500 micron, respectively, in the southern hemisphere (3000 sq. degrees) and in an equatorial strip (1000 sq. degrees), areas which have extensive multi-wavelength coverage and are easily accessible from ALMA. Two thirds of the of the sources are expected to be at z > 1, one third at z > 2 and about a 1000 at z > 5. (b) Remove point source confusion in secondary anisotropy studies with Planck and ground-based CMB data. (c) Find at least 1200 strongly lensed bright sub-mm sources leading to a 2% test of general relativity. (d) Identify 200 proto-cluster regions at z of 2 and perform an unbiased study of the environmental dependence of star formation. (e) Perform an unbiased survey for star formation and dust at high Galactic latitude and make a census of debris disks and dust around AGB stars and white dwarfs.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "An analog hadron calorimeter (AHCAL) prototype of 5.3 nuclear interaction lengths thickness has been constructed by members of the CALICE Collaboration. The AHCAL prototype consists of a 38-layer sandwich structure of steel plates and highly-segmented scintillator tiles that are read out by wavelength-shifting fibers coupled to SiPMs. The signal is amplified and shaped with a custom-designed ASIC. A calibration/monitoring system based on LED light was developed to monitor the SiPM gain and to measure the full SiPM response curve in order to correct for non-linearity. Ultimately, the physics goals are the study of hadron shower shapes and testing the concept of particle flow. The technical goal consists of measuring the performance and reliability of 7608 SiPMs. The AHCAL was commissioned in test beams at DESY and CERN. The entire prototype was completed in 2007 and recorded hadron showers, electron showers and muons at different energies and incident angles in test beams at CERN and Fermilab.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "The Gamma-Ray Burst Monitor (GBM) will significantly augment the science return from the Fermi Observatory in the study of Gamma-Ray Bursts (GRBs). The primary objective of GBM is to extend the energy range over which bursts are observed downward from the energy range of the Large Area Telescope (LAT) on Fermi into the hard X-ray range where extensive previous data exist. A secondary objective is to compute burst locations on-board to allow re-orientiong the spacecraft so that the LAT can observe delayed emission from bright bursts. GBM uses an array of twelve sodium iodide scintillators and two bismuth germanate scintillators to detect gamma rays from ~8 keV to ~40 MeV over the full unocculted sky. The on-board trigger threshold is ~0.7 photons/cm2/s (50-300 keV, 1 s peak). GBM generates on-board triggers for ~250 GRBs per year.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "The next large NASA mission in the field of gamma-ray astronomy, GLAST, is scheduled for launch in 2007. Aside from the main instrument LAT (Large-Area Telescope), a gamma-ray telescope for the energy range between 20 MeV and > 100 GeV, a secondary instrument, the GLAST burst monitor (GBM), is foreseen. With this monitor one of the key scientific objectives of the mission, the determination of the high-energy behaviour of gamma-ray bursts and transients can be ensured. Its task is to increase the detection rate of gamma-ray bursts for the LAT and to extend the energy range to lower energies (from ~10 keV to \\~30 MeV). It will provide real-time burst locations over a wide FoV with sufficient accuracy to allow repointing the GLAST spacecraft. Time-resolved spectra of many bursts recorded with LAT and the burst monitor will allow the investigation of the relation between the keV and the MeV-GeV emission from GRBs over unprecedented seven decades of energy. This will help to advance our understanding of the mechanisms by which gamma-rays are generated in gamma-ray bursts.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "Using the HST STIS spectrograph we have obtained a grid of [O III] and H-beta emission-line spectra at 0\"05x0\"19 and 60 km/s (FWHM) resolution that covers much of the NLR of NGC 1068. We find emitting knots that have blueshifted radial velocities up to 3200 km/s relative to galaxy systemic, are 70-150 pc NE of the nucleus and up to 40 pc from the radio jet, emit several percent of the NLR line flux but no significant continuum, span velocity extents of up to 1250 km/s but a small fraction of the sky seen from the nucleus, coincide with a region of enhanced IR coronal-line emission, and have ionized masses $\\sim$200 Msun/ne4 (ne4=10^4 cm^{-3}). We argue that the blueshifted knots are ablata from disintegrating molecular clouds that are being photoionized by the AGN, and are being accelerated readiatively by the AGN or mechanically by the radio jet. In their kinematic properties, the knots resemble the associated absorbers seen projected on the UV continua of some AGN. Between 2\"5-4\"5 from the nucleus, emission is redshifted relative to systemic, a pattern that we interpret as gas in the galaxy disk being pushed away from us by the NE radio lobe.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "We present submillimeter observations of the Local Group, metal-poor, irregular dwarf galaxy IC 10, directly relevant to the interaction between interstellar medium and star formation activity in primeval galaxies. Using the JCMT, we have observed the fine structure neutral carbon transition at 492 GHz and the rotational J=3-2 transition of 12CO and 13CO in the most massive giant molecular cloud complex in this galaxy, IC10-SE. We find that, although the I([CII])/I(CO) ratio for this object is a factor of 4 larger than the typical Milky Way value, its [C~I] to CO intensity ratio is I([CI])/I(CO)~18 is similar (only about 50% larger) to that of the Milky Way. Modelling of the [CII]/CO and [CI]/CO intensity ratios with metallicity indicates that, if C+ and C are chiefly produced by UV photodissociation, both ratios should increase sharply with decreasing metallicity (and consequently diminished UV shielding; Bolatto, Jackson, and Ingalls 1999). These data then suggest a different origin for an important fraction of C in these clouds, unrelated to photodissociation. We have also mapped the 850 um continuum in this region using SCUBA. Employing these data in conjunction with KAO and IRAM measurements we find an extremely low emissivity exponent, b~0.5. We conclude that this low exponent is most likely due to the destruction of small dust grains, brought about by the increased penetration of UV radiation in the low metallicity ISM. If a low b in the submillimeter is a general property of metal-poor systems then the interpretation of millimeter and submillimeter surveys of high-z galaxies should be revised.\n        \u25b3 Less", "author": "Gerald Wilson"}, {"abstract": "This work discusses how the MPContribs framework in the Materials Project (MP) allows user-contributed data to be shown and analyzed alongside the core MP database. The Materials Project is a searchable database of electronic structure properties of over 65,000 bulk solid materials that is accessible through a web-based science-gateway. We describe the motivation for enabling user contributions to the materials data and present the framework's features and challenges in the context of two real applications. These use-cases illustrate how scientific collaborations can build applications with their own \"user-contributed\" data using MPContribs. The Nanoporous Materials Explorer application provides a unique search interface to a novel dataset of hundreds of thousands of materials, each with tables of user-contributed values related to material adsorption and density at varying temperature and pressure. The Unified Theoretical and Experimental x-ray Spectroscopy application discusses a full workflow for the association, dissemination and combined analyses of experimental data from the Advanced Light Source with MP's theoretical core data, using MPContribs tools for data formatting, management and exploration. The capabilities being developed for these collaborations are serving as the model for how new materials data can be incorporated into the Materials Project website with minimal staff overhead while giving powerful tools for data search and display to the user community.\n        \u25b3 Less", "author": "Patrick Winston"}, {"abstract": "As scientific discovery becomes increasingly data-driven, software platforms are needed to efficiently organize and disseminate data from disparate sources. This is certainly the case in the field of materials science. For example, Materials Project has generated computational data on over 60,000 chemical compounds and has made that data available through a web portal and REST interface. However, such portals must seek to incorporate community submissions to expand the scope of scientific data sharing. In this paper, we describe MPContribs, a computing/software infrastructure to integrate and organize contributions of simulated or measured materials data from users. Our solution supports complex submissions and provides interfaces that allow contributors to share analyses and graphs. A RESTful API exposes mechanisms for book-keeping, retrieval and aggregation of submitted entries, as well as persistent URIs or DOIs that can be used to reference the data in publications. Our approach isolates contributed data from a host project's quality-controlled core data and yet enables analyses across the entire dataset, programmatically or through customized web apps. We expect the developed framework to enhance collaborative determination of material properties and to maximize the impact of each contributor's dataset. In the long-term, MPContribs seeks to make Materials Project an institutional, and thus community-wide, memory for computational and experimental materials science.\n        \u25b3 Less", "author": "Patrick Winston"}, {"abstract": "Deep neural networks, trained with large amount of labeled data, can fail to generalize well when tested with examples from a \\emph{target domain} whose distribution differs from the training data distribution, referred as the \\emph{source domain}. It can be expensive or even infeasible to obtain required amount of labeled data in all possible domains. Unsupervised domain adaptation sets out to address this problem, aiming to learn a good predictive model for the target domain using labeled examples from the source domain but only unlabeled examples from the target domain. Domain alignment approaches this problem by matching the source and target feature distributions, and has been used as a key component in many state-of-the-art domain adaptation methods. However, matching the marginal feature distributions does not guarantee that the corresponding class conditional distributions will be aligned across the two domains. We propose co-regularized domain alignment for unsupervised domain adaptation, which constructs multiple diverse feature spaces and aligns source and target distributions in each of them individually, while encouraging that alignments agree with each other with regard to the class predictions on the unlabeled target examples. The proposed method is generic and can be used to improve any domain adaptation method which uses domain alignment. We instantiate it in the context of a recent state-of-the-art method and observe that it provides significant performance improvements on several domain adaptation benchmarks.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We characterize the fundamental limits of coded aperture imaging systems up to universal constants by drawing upon a theorem of Nazarov regarding Fourier transforms. Our work is performed under a simple propagation and sensor model model that accounts for thermal and shot noise, scene correlation, and exposure time. By focusing on linear mean square error as a measure of reconstruction quality, we show that appropriate application of a theorem of Nazarov leads to essentially optimal coded apertures, up to a constant multiplicative factor in exposure time. Our theoretical results are accompanied with a heuristically efficient algorithm to generate such patterns that explicitly takes into account scene correlations. Finally, for i.i.d scenes, we show improvements upon prior work by using spectrally flat sequences with bias. We focus on 1D apertures for conceptual clarity, but also discuss how all of our work here generalizes naturally to 2D.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "A loss function measures the discrepancy between the true values and their estimated fits, for a given instance of data. In classification problems, a loss function is said to be proper if the minimizer of the expected loss is the true underlying probability. In this work we show that for binary classification, the divergence associated with smooth, proper and convex loss functions is bounded from above by the Kullback-Leibler (KL) divergence, up to a normalization constant. It implies that by minimizing the log-loss (associated with the KL divergence), we minimize an upper bound to any choice of loss from this set. This property suggests that the log-loss is universal in the sense that it provides performance guarantees to a broad class of accuracy measures. Importantly, our notion of universality is not restricted to a specific problem. This allows us to apply our results to many applications, including predictive modeling, data clustering and sample complexity analysis. Further, we show that the KL divergence bounds from above any separable Bregman divergence that is convex in its second argument (up to a normalization constant). This result introduces a new set of divergence inequalities, similar to Pinsker inequality, and extends well-known $f$-divergence inequality results.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Systems that capture and process analog signals must first acquire them through an analog-to-digital converter. While subsequent digital processing can remove statistical correlations present in the acquired data, the dynamic range of the converter is typically scaled to match that of the input analog signal. The present paper develops an approach for analog-to-digital conversion that aims at minimizing the number of bits per sample at the output of the converter. This is attained by reducing the dynamic range of the analog signal by performing a modulo operation on its amplitude, and then quantizing the result. While the converter itself is universal and agnostic of the statistics of the signal, the decoder operation on the output of the quantizer can exploit the statistical structure in order to unwrap the modulo folding. The performance of this method is shown to approach information theoretical limits, as captured by the rate-distortion function, in various settings. An architecture for modulo analog-to-digital conversion via ring oscillators is suggested, and its merits are numerically demonstrated.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "A loss function measures the discrepancy between the true values (observations) and their estimated fits, for a given instance of data. A loss function is said to be proper (unbiased, Fisher consistent) if the fits are defined over a unit simplex, and the minimizer of the expected loss is the true underlying probability of the data. Typical examples are the zero-one loss, the quadratic loss and the Bernoulli log-likelihood loss (log-loss). In this work we show that for binary classification problems, the divergence associated with smooth, proper and convex loss functions is bounded from above by the Kullback-Leibler (KL) divergence, up to a multiplicative normalization constant. It implies that by minimizing the log-loss (associated with the KL divergence), we minimize an upper bound to any choice of loss functions from this set. This property justifies the broad use of log-loss in regression, decision trees, deep neural networks and many other applications. In addition, we show that the KL divergence bounds from above any separable Bregman divergence that is convex in its second argument (up to a multiplicative normalization constant). This result introduces a new set of divergence inequalities, similar to the well-known Pinsker inequality.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "The ability to see around corners, i.e., recover details of a hidden scene from its reflections in the surrounding environment, is of considerable interest in a wide range of applications. However, the diffuse nature of light reflected from typical surfaces leads to mixing of spatial information in the collected light, precluding useful scene reconstruction. Here, we employ a computational imaging technique that opportunistically exploits the presence of occluding objects, which obstruct probe-light propagation in the hidden scene, to undo the mixing and greatly improve scene recovery. Importantly, our technique obviates the need for the ultrafast time-of-flight measurements employed by most previous approaches to hidden-scene imaging. Moreover, it does so in a photon-efficient manner based on an accurate forward model and a computational algorithm that, together, respect the physics of three-bounce light propagation and single-photon detection. Using our methodology, we demonstrate reconstruction of hidden-surface reflectivity patterns in a meter-scale environment from non-time-resolved measurements. Ultimately, our technique represents an instance of a rich and promising new imaging modality with important potential implications for imaging science.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "There is growing interest in the use of coded aperture imaging systems for a variety of applications. Using an analysis framework based on mutual information, we examine the fundamental limits of such systems---and the associated optimum aperture coding---under simple but meaningful propagation and sensor models. Among other results, we show that when thermal noise dominates, spectrally-flat masks, which have 50% transmissivity, are optimal, but that when shot noise dominates, randomly generated masks with lower transmissivity offer greater performance. We also provide comparisons to classical pinhole cameras.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Active non-line-of-sight imaging systems are of growing interest for diverse applications. The most commonly proposed approaches to date rely on exploiting time-resolved measurements, i.e., measuring the time it takes for short light pulses to transit the scene. This typically requires expensive, specialized, ultrafast lasers and detectors that must be carefully calibrated. We develop an alternative approach that exploits the valuable role that natural occluders in a scene play in enabling accurate and practical image formation in such settings without such hardware complexity. In particular, we demonstrate that the presence of occluders in the hidden scene can obviate the need for collecting time-resolved measurements, and develop an accompanying analysis for such systems and their generalizations. Ultimately, the results suggest the potential to develop increasingly sophisticated future systems that are able to identify and exploit diverse structural features of the environment to reconstruct scenes hidden from view.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider the problem of covert communication over a state-dependent channel, where the transmitter has causal or noncausal knowledge of the channel states. Here, \"covert\" means that a warden on the channel should observe similar statistics when the transmitter is sending a message and when it is not. When a sufficiently long secret key is shared between the transmitter and the receiver, we derive closed-form formulas for the maximum achievable covert communication rate (\"covert capacity\") for discrete memoryless channels and, when the transmitter's channel-state information (CSI) is noncausal, for additive white Gaussian noise (AWGN) channels. For certain channel models, including the AWGN channel, we show that the covert capacity is positive with CSI at the transmitter, but is zero without CSI. We also derive lower bounds on the rate of the secret key that is needed for the transmitter and the receiver to achieve the covert capacity.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "A novel low-complexity wireless neighbor discovery scheme, referred to as sparse orthogonal frequency division multiplexing (sparse-OFDM) is proposed. One area of application is the \"Internet of Things\" (IoT). The number of devices is very large while every device accesses the network with a small probability, so the number of active devices in a frame is much smaller than the total local device population. Sparse OFDM is a one-shot transmission scheme with low complexity, which exploits both the parallel channel access offered by OFDM and the bursty nature of transmissions. When the transmission delay of each device is an integer number of symbol intervals, analysis and simulation show that sparse OFDM enables successful asynchronous neighbor discovery using a much smaller code length than random access schemes.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider the problem of far-field sensing by means of a sensor array. Traditional array geometry design techniques are agnostic to prior information about the far-field scene. However, in many applications such priors are available and may be utilized to design more efficient array topologies. We formulate the problem of array geometry design with scene prior as one of finding a sampling configuration that enables efficient inference, which turns out to be a combinatorial optimization problem. While generic combinatorial optimization problems are NP-hard and resist efficient solvers, we show how for array design problems the theory of submodular optimization may be utilized to obtain efficient algorithms that are guaranteed to achieve solutions within a constant approximation factor from the optimum. We leverage the connection between array design problems and submodular optimization and port several results of interest. We demonstrate efficient methods for designing arrays with constraints on the sensing aperture, as well as arrays respecting combinatorial placement constraints. This novel connection between array design and submodularity suggests the possibility for utilizing other insights and techniques from the growing body of literature on submodular optimization in the field of array design.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "This paper addresses the problem of adding redundancy to a collection of physical objects so that the overall system is more robust to failures. In contrast to its information counterpart, which can exploit parity to protect multiple information symbols from a single erasure, physical redundancy can only be realized through duplication and substitution of objects. We propose a bipartite graph model for designing defect-tolerant systems in which defective objects are replaced by judiciously connected redundant objects. The fundamental limits of this model are characterized under various asymptotic settings and both asymptotic and finite-size systems that approach these limits are constructed. Among other results, we show that simple modular redundancy is in general suboptimal. As we develop, this combinatorial problem of defect tolerant system design has a natural interpretation as one of graph coloring, and the analysis is significantly different from that traditionally used in information redundancy for error-control codes.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Unlike traditional file transfer where only total delay matters, streaming applications impose delay constraints on each packet and require them to be in order. To achieve fast in-order packet decoding, we have to compromise on the throughput. We study this trade-off between throughput and smoothness in packet decoding. We first consider a point-to-point streaming and analyze how the trade-off is affected by the frequency of block-wise feedback, whereby the source receives full channel state feedback at periodic intervals. We show that frequent feedback can drastically improve the throughput-smoothness trade-off. Then we consider the problem of multicasting a packet stream to two users. For both point-to-point and multicast streaming, we propose a spectrum of coding schemes that span different throughput-smoothness tradeoffs. One can choose an appropriate coding scheme from these, depending upon the delay-sensitivity and bandwidth limitations of the application. This work introduces a novel style of analysis using renewal processes and Markov chains to analyze coding schemes.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "The binary (one-bit-per-photon) encoding that most existing quantum key distribution (QKD) protocols employ puts a fundamental limit on their achievable key rates, especially under high channel loss conditions associated with long-distance fiber-optic or satellite-to-ground links. Inspired by the pulse-position-modulation (PPM) approach to photon-starved classical communications, we design and demonstrate the first PPM-QKD, whose security against collective attacks is established through continuous-variable entanglement measurements that also enable a novel decoy-state protocol performed conveniently in post processing. We achieve a throughput of 8.0 Mbit/s (2.5 Mbit/s for loss equivalent to 25 km of fiber) and secret-key capacity up to 4.0 bits per detected photon, thus demonstrating the significant enhancement afforded by high-dimensional encoding. These results point to a new avenue for realizing high-throughput satellite-based or long-haul fiber-optic quantum communications beyond their photon-reception-rate limits.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "In cloud computing systems, assigning a job to multiple servers and waiting for the earliest copy to finish is an effective method to combat the variability in response time of individual servers. Although adding redundant replicas always reduces service time, the total computing time spent per job may be higher, thus increasing waiting time in queue. The total time spent per job is also proportional to the cost of computing resources. We analyze how different redundancy strategies, for eg. number of replicas, and the time when they are issued and canceled, affect the latency and computing cost. We get the insight that the log-concavity of the service time distribution is a key factor in determining whether adding redundancy reduces latency and cost. If the service distribution is log-convex, then adding maximum redundancy reduces both latency and cost. And if it is log-concave, then having fewer replicas and canceling the redundant requests early is more effective.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "In cloud computing systems, assigning a task to multiple servers and waiting for the earliest copy to finish is an effective method to combat the variability in response time of individual servers, and reduce latency. But adding redundancy may result in higher cost of computing resources, as well as an increase in queueing delay due to higher traffic load. This work helps understand when and how redundancy gives a cost-efficient reduction in latency. For a general task service time distribution, we compare different redundancy strategies in terms of the number of redundant tasks, and time when they are issued and canceled. We get the insight that the log-concavity of the task service time creates a dichotomy of when adding redundancy helps. If the service time distribution is log-convex (i.e. log of the tail probability is convex) then adding maximum redundancy reduces both latency and cost. And if it is log-concave (i.e. log of the tail probability is concave), then less redundancy, and early cancellation of redundant tasks is more effective. Using these insights, we design a general redundancy strategy that achieves a good latency-cost trade-off for an arbitrary service time distribution. This work also generalizes and extends some results in the analysis of fork-join queues.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "This paper considers the problem of communication over a discrete memoryless channel (DMC) or an additive white Gaussian noise (AWGN) channel subject to the constraint that the probability that an adversary who observes the channel outputs can detect the communication is low. Specifically, the relative entropy between the output distributions when a codeword is transmitted and when no input is provided to the channel must be sufficiently small. For a DMC whose output distribution induced by the \"off\" input symbol is not a mixture of the output distributions induced by other input symbols, it is shown that the maximum amount of information that can be transmitted under this criterion scales like the square root of the blocklength. The same is true for the AWGN channel. Exact expressions for the scaling constant are also derived.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "In a cloud computing job with many parallel tasks, the tasks on the slowest machines (straggling tasks) become the bottleneck in the job completion. Computing frameworks such as MapReduce and Spark tackle this by replicating the straggling tasks and waiting for any one copy to finish. Despite being adopted in practice, there is little analysis of how replication affects the latency and the cost of additional computing resources. In this paper we provide a framework to analyze this latency-cost trade-off and find the best replication strategy by answering design questions such as: 1) when to replicate straggling tasks, 2) how many replicas to launch, and 3) whether to kill the original copy or not. Our analysis reveals that for certain execution time distributions, a small amount of task replication can drastically reduce both latency as well as the cost of computing resources. We also propose an algorithm to estimate the latency and cost based on the empirical distribution of task execution time. Evaluations using samples in the Google Cluster Trace suggest further latency and cost reduction compared to the existing replication strategy used in MapReduce.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "This work considers the distribution of a secret key over an optical (bosonic) channel in the regime of high photon efficiency, i.e., when the number of secret key bits generated per detected photon is high. While in principle the photon efficiency is unbounded, there is an inherent tradeoff between this efficiency and the key generation rate (with respect to the channel bandwidth). We derive asymptotic expressions for the optimal generation rates in the photon-efficient limit, and propose schemes that approach these limits up to certain approximations. The schemes are practical, in the sense that they use coherent or temporally-entangled optical states and direct photodetection, all of which are reasonably easy to realize in practice, in conjunction with off-the-shelf classical codes.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We investigate lossy compression (source coding) of data in the form of permutations. This problem has direct applications in the storage of ordinal data or rankings, and in the analysis of sorting algorithms. We analyze the rate-distortion characteristic for the permutation space under the uniform distribution, and the minimum achievable rate of compression that allows a bounded distortion after recovery. Our analysis is with respect to different practical and useful distortion measures, including Kendall-tau distance, Spearman's footrule, Chebyshev distance and inversion-$\\ell_1$ distance. We establish equivalence of source code designs under certain distortions and show simple explicit code designs that incur low encoding/decoding complexities and are asymptotically optimal. Finally, we show that for the Mallows model, a popular nonuniform ranking model on the permutation space, both the entropy and the maximum distortion at zero rate are much lower than the uniform counterparts, which motivates the future design of efficient compression schemes for this model.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "An increasing number of streaming applications need packets to be strictly in-order at the receiver. This paper provides a framework for analyzing in-order packet delivery in such applications. We consider the problem of multicasting an ordered stream of packets to two users over independent erasure channels with instantaneous feedback to the source. Depending upon the channel erasures, a packet which is in-order for one user, may be redundant for the other. Thus there is an inter-dependence between throughput and the smoothness of in-order packet delivery to the two users. We use a Markov chain model of packet decoding to analyze these throughput-smoothness trade-offs of the users, and propose coding schemes that can span different points on each trade-off.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "One typical use case of large-scale distributed computing in data centers is to decompose a computation job into many independent tasks and run them in parallel on different machines, sometimes known as the \"embarrassingly parallel\" computation. For this type of computation, one challenge is that the time to execute a task for each machine is inherently variable, and the overall response time is constrained by the execution time of the slowest machine. To address this issue, system designers introduce task replication, which sends the same task to multiple machines, and obtains result from the machine that finishes first. While task replication reduces response time, it usually increases resource usage. In this work, we propose a theoretical framework to analyze the trade-off between response time and resource usage. We show that, while in general, there is a tension between response time and resource usage, there exist scenarios where replicating tasks judiciously reduces completion time and resource usage simultaneously. Given the execution time distribution for machines, we investigate the conditions for a scheduling policy to achieve optimal performance trade-off, and propose efficient algorithms to search for optimal or near-optimal scheduling policies. Our analysis gives insights on when and why replication helps, which can be used to guide scheduler design in large-scale distributed computing systems.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Unlike traditional file transfer where only total delay matters, streaming applications impose delay constraints on each packet and require them to be in order. To achieve fast in-order packet decoding, we have to compromise on the throughput. We study this trade-off between throughput and in-order decoding delay, and in particular how it is affected by the frequency of block-wise feedback to the source. When there is immediate feedback, we can achieve the optimal throughput and delay simultaneously. But as the feedback delay increases, we have to compromise on at least one of these metrics. We present a spectrum of coding schemes that span different points on the throughput-delay trade-off. Depending upon the delay-sensitivity and bandwidth limitations of the application, one can choose an appropriate operating point on this trade-off.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We study the discrete-time Poisson channel under the constraint that its average input power (in photons per channel use) must not exceed some constant E. We consider the wideband, high-photon-efficiency extreme where E approaches zero, and where the channel's \"dark current\" approaches zero proportionally with E. Improving over a previously obtained first-order capacity approximation, we derive a refined approximation, which includes the exact characterization of the second-order term, as well as an asymptotic characterization of the third-order term with respect to the dark current. We also show that pulse-position modulation is nearly optimal in this regime.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Lattice coding and decoding have been shown to achieve the capacity of the additive white Gaussian noise (AWGN) channel. This was accomplished using a minimum mean-square error scaling and randomization to transform the AWGN channel into a modulo-lattice additive noise channel of the same capacity. It has been further shown that when operating at rates below capacity but above the critical rate of the channel, there exists a rate-dependent scaling such that the associated modulo-lattice channel attains the error exponent of the AWGN channel. A geometric explanation for this result is developed. In particular, it is shown how the geometry of typical error events for the modulo-lattice channel coincides with that of a spherical code for the AWGN channel.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Motivated by distributed storage applications, we investigate the degree to which capacity achieving encodings can be efficiently updated when a single information bit changes, and the degree to which such encodings can be efficiently (i.e., locally) repaired when single encoded bit is lost.\n  Specifically, we first develop conditions under which optimum error-correction and update-efficiency are possible, and establish that the number of encoded bits that must change in response to a change in a single information bit must scale logarithmically in the block-length of the code if we are to achieve any nontrivial rate with vanishing probability of error over the binary erasure or binary symmetric channels. Moreover, we show there exist capacity-achieving codes with this scaling.\n  With respect to local repairability, we develop tight upper and lower bounds on the number of remaining encoded bits that are needed to recover a single lost bit of the encoding. In particular, we show that if the code-rate is $\u03b5$ less than the capacity, then for optimal codes, the maximum number of codeword symbols required to recover one lost symbol must scale as $\\log1/\u03b5$.\n  Several variations on---and extensions of---these results are also developed.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We prove an upper bound on the private capacity of the single-mode noiseless bosonic wiretap channel. Combined with a previous lower bound, we obtain the low photon-number asymptotic expression for the private capacity. We then show that the multiple-mode noiseless bosonic wiretap channel is equivalent to parallel single-mode channels, hence the single-mode bounds can be applied. Finally, we consider multiple-spatial-mode propagation through atmospheric turbulence, and derive a private-capacity lower bound that only requires second moments of the channel matrix.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Several aspects of the problem of asynchronous point-to-point communication without feedback are developed when the source is highly intermittent. In the system model of interest, the codeword is transmitted at a random time within a prescribed window whose length corresponds to the level of asynchronism between the transmitter and the receiver. The decoder operates sequentially and communication rate is defined as the ratio between the message size and the elapsed time between when transmission commences and when the decoder makes a decision.\n  For such systems, general upper and lower bounds on capacity as a function of the level of asynchronism are established, and are shown to coincide in some nontrivial cases. From these bounds, several properties of this asynchronous capacity are derived. In addition, the performance of training-based schemes is investigated. It is shown that such schemes, which implement synchronization and information transmission on separate degrees of freedom in the encoding, cannot achieve the asynchronous capacity in general, and that the penalty is particularly significant in the high-rate regime.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We study the capacity of secret-key agreement over a wiretap channel with state parameters. The transmitter communicates to the legitimate receiver and the eavesdropper over a discrete memoryless wiretap channel with a memoryless state sequence. The transmitter and the legitimate receiver generate a shared secret key, that remains secret from the eavesdropper. No public discussion channel is available. The state sequence is known noncausally to the transmitter. We derive lower and upper bounds on the secret-key capacity. The lower bound involves constructing a common state reconstruction sequence at the legitimate terminals and binning the set of reconstruction sequences to obtain the secret-key. For the special case of Gaussian channels with additive interference (secret-keys from dirty paper channel) our bounds differ by 0.5 bit/symbol and coincide in the high signal-to-noise-ratio and high interference-to-noise-ratio regimes. For the case when the legitimate receiver is also revealed the state sequence, we establish that our lower bound achieves the the secret-key capacity. In addition, for this special case, we also propose another scheme that attains the capacity and requires only causal side information at the transmitter and the receiver.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "The capacity of the Gaussian wiretap channel model is analyzed when there are multiple antennas at the sender, intended receiver and eavesdropper. The associated channel matrices are fixed and known to all the terminals. A computable characterization of the secrecy capacity is established as the saddle point solution to a minimax problem. The converse is based on a Sato-type argument used in other broadcast settings, and the coding theorem is based on Gaussian wiretap codebooks.\n  At high signal-to-noise ratio (SNR), the secrecy capacity is shown to be attained by simultaneously diagonalizing the channel matrices via the generalized singular value decomposition, and independently coding across the resulting parallel channels. The associated capacity is expressed in terms of the corresponding generalized singular values. It is shown that a semi-blind \"masked\" multi-input multi-output (MIMO) transmission strategy that sends information along directions in which there is gain to the intended receiver, and synthetic noise along directions in which there is not, can be arbitrarily far from capacity in this regime.\n  Necessary and sufficient conditions for the secrecy capacity to be zero are provided, which simplify in the limit of many antennas when the entries of the channel matrices are independent and identically distributed. The resulting scaling laws establish that to prevent secure communication, the eavesdropper needs 3 times as many antennas as the sender and intended receiver have jointly, and that the optimimum division of antennas between sender and intended receiver is in the ratio of 2:1.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider the recovery of a nonnegative vector x from measurements y = Ax, where A is an m-by-n matrix whos entries are in {0, 1}. We establish that when A corresponds to the adjacency matrix of a bipartite graph with sufficient expansion, a simple message-passing algorithm produces an estimate \\hat{x} of x satisfying ||x-\\hat{x}||_1 \\leq O(n/k) ||x-x(k)||_1, where x(k) is the best k-sparse approximation of x. The algorithm performs O(n (log(n/k))^2 log(k)) computation in total, and the number of measurements required is m = O(k log(n/k)). In the special case when x is k-sparse, the algorithm recovers x exactly in time O(n log(n/k) log(k)). Ultimately, this work is a further step in the direction of more formally developing the broader role of message-passing algorithms in solving compressed sensing problems.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider asynchronous point-to-point communication. Building on a recently developed model, we show that training based schemes, i.e., communication strategies that separate synchronization from information transmission, perform suboptimally at high rate.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider the problem of delivering content cached in a wireless network of n nodes randomly located on a square of area n. The network performance is described by the n2^n-dimensional caching capacity region of the wireless network. We provide an inner bound on this caching capacity region, and, in the high path-loss regime, a matching (in the scaling sense) outer bound. For large path-loss exponent, this provides an information-theoretic scaling characterization of the entire caching capacity region. The proposed communication scheme achieving the inner bound shows that the problems of cache selection and channel coding can be solved separately without loss of order-optimality. On the other hand, our results show that the common architecture of nearest-neighbor cache selection can be arbitrarily bad, implying that cache selection and load balancing need to be performed jointly.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We study the problem of generating a shared secret key between two terminals in a joint source-channel setup -- the sender communicates to the receiver over a discrete memoryless wiretap channel and additionally the terminals have access to correlated discrete memoryless source sequences. We establish lower and upper bounds on the secret-key capacity. These bounds coincide, establishing the capacity, when the underlying channel consists of independent, parallel and reversely degraded wiretap channels. In the lower bound, the equivocation terms of the source and channel components are functionally additive. The secret-key rate is maximized by optimally balancing the the source and channel contributions. This tradeoff is illustrated in detail for the Gaussian case where it is also shown that Gaussian codebooks achieve the capacity. When the eavesdropper also observes a source sequence, the secret-key capacity is established when the sources and channels of the eavesdropper are a degraded version of the legitimate receiver. Finally the case when the terminals also have access to a public discussion channel is studied. We propose generating separate keys from the source and channel components and establish the optimality of this approach when the when the channel outputs of the receiver and the eavesdropper are conditionally independent given the input.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Imaging technologies such as dynamic viewpoint generation are engineered for incoherent radiation using the traditional light field, and for coherent radiation using electromagnetic field theory. We present a model of coherent image formation that strikes a balance between the utility of the light field and the comprehensive predictive power of Maxwell's equations. We synthesize research in optics and signal processing to formulate, capture, and form images from quasi light fields, which extend the light field from incoherent to coherent radiation. Our coherent cameras generalize the classic beamforming algorithm in sensor array processing, and invite further research on alternative notions of image formation.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider the problem of lossy source coding with a mismatched distortion measure. That is, we investigate what distortion guarantees can be made with respect to distortion measure $\\tilde\u03c1$, for a source code designed such that it achieves distortion less than $D$ with respect to distortion measure $\u03c1$. We find a single-letter characterization of this mismatch distortion and study properties of this quantity. These results give insight into the robustness of lossy source coding with respect to modeling errors in the distortion measure. They also provide guidelines on how to choose a good tractable approximation of an intractable distortion measure.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "The MIMOME channel is a Gaussian wiretap channel in which the sender, receiver, and eavesdropper all have multiple antennas. We characterize the secrecy capacity as the saddle-value of a minimax problem. Among other implications, our result establishes that a Gaussian distribution maximizes the secrecy capacity characterization of Csisz{\u00e1}r and K{\u00f6}rner when applied to the MIMOME channel. We also determine a necessary and sufficient condition for the secrecy capacity to be zero. Large antenna array analysis of this condition reveals several useful insights into the conditions under which secure communication is possible.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "The role of multiple antennas for secure communication is investigated within the framework of Wyner's wiretap channel. We characterize the secrecy capacity in terms of generalized eigenvalues when the sender and eavesdropper have multiple antennas, the intended receiver has a single antenna, and the channel matrices are fixed and known to all the terminals, and show that a beamforming strategy is capacity-achieving. In addition, we show that in the high signal-to-noise (SNR) ratio regime the penalty for not knowing eavesdropper's channel is small--a simple ``secure space-time code'' that can be thought of as masked beamforming and radiates power isotropically attains near-optimal performance. In the limit of large number of antennas, we obtain a realization-independent characterization of the secrecy capacity as a function of the number $\u03b2$: the number of eavesdropper antennas per sender antenna. We show that the eavesdropper is comparatively ineffective when $\u03b2<1$, but that for $\u03b2\\ge2$ the eavesdropper can drive the secrecy capacity to zero, thereby blocking secure communication to the intended receiver. Extensions to ergodic fading channels are also provided.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider the `one-shot frame synchronization problem' where a decoder wants to locate a sync pattern at the output of a channel on the basis of sequential observations. We assume that the sync pattern of length N starts being emitted at a random time within some interval of size A, that characterizes the asynchronism level between the transmitter and the receiver. We show that a sequential decoder can optimally locate the sync pattern, i.e., exactly, without delay, and with probability approaching one as N tends to infinity, if and only if the asynchronism level grows as O(exp(N*k)), with k below the `synchronization threshold,' a constant that admits a simple expression depending on the channel. This constant is the same as the one that characterizes the limit for reliable asynchronous communication, as was recently reported by the authors. If k exceeds the synchronization threshold, any decoder, sequential or non-sequential, locates the sync pattern with an error that tends to one as N tends to infinity. Hence, a sequential decoder can locate a sync pattern as well as the (non-sequential) maximum likelihood decoder that operates on the basis of output sequences of maximum length A+N-1, but with much fewer observations.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "A rateless code-i.e., a rate-compatible family of codes-has the property that codewords of the higher rate codes are prefixes of those of the lower rate ones. A perfect family of such codes is one in which each of the codes in the family is capacity-achieving. We show by construction that perfect rateless codes with low-complexity decoding algorithms exist for additive white Gaussian noise channels. Our construction involves the use of layered encoding and successive decoding, together with repetition using time-varying layer weights. As an illustration of our framework, we design a practical three-rate code family. We further construct rich sets of near-perfect rateless codes within our architecture that require either significantly fewer layers or lower complexity than their perfect counterparts. Variations of the basic construction are also developed, including one for time-varying channels in which there is no a priori stochastic model.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider asynchronous communication over point-to-point discrete memoryless channels. The transmitter starts sending one block codeword at an instant that is uniformly distributed within a certain time period, which represents the level of asynchronism. The receiver, by means of a sequential decoder, must isolate the message without knowing when the codeword transmission starts but being cognizant of the asynchronism level A. We are interested in how quickly can the receiver isolate the sent message, particularly in the regime where A is exponentially larger than the codeword length N, which we refer to as `strong asynchronism.'\n  This model of sparse communication may represent the situation of a sensor that remains idle most of the time and, only occasionally, transmits information to a remote base station which needs to quickly take action.\n  The first result shows that vanishing error probability can be guaranteed as N tends to infinity while A grows as Exp(N*k) if and only if k does not exceed the `synchronization threshold,' a constant that admits a simple closed form expression, and is at least as large as the capacity of the synchronized channel. The second result is the characterization of a set of achievable strictly positive rates in the regime where A is exponential in N, and where the rate is defined with respect to the expected delay between the time information starts being emitted until the time the receiver makes a decision.\n  As an application of the first result we consider antipodal signaling over a Gaussian channel and derive a simple necessary condition between A, N, and SNR for achieving reliable communication.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "Wyner's wiretap channel is extended to parallel broadcast channels and fading channels with multiple receivers. In the first part of the paper, we consider the setup of parallel broadcast channels with one sender, multiple intended receivers, and one eavesdropper. We study the situations where the sender broadcasts either a common message or independent messages to the intended receivers. We derive upper and lower bounds on the common-message-secrecy capacity, which coincide when the users are reversely degraded. For the case of independent messages we establish the secrecy sum-capacity when the users are reversely degraded.\n  In the second part of the paper we apply our results to fading channels: perfect channel state information of all intended receivers is known globally, whereas the eavesdropper channel is known only to her. For the common message case, a somewhat surprising result is proven: a positive rate can be achieved independently of the number of intended receivers. For independent messages, an opportunistic transmission scheme is presented that achieves the secrecy sum-capacity in the limit of large number of receivers. Our results are stated for a fast fading channel model. Extensions to the block fading model are also discussed.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "The classical alternating minimization (or projection) algorithm has been successful in the context of solving optimization problems over two variables. The iterative nature and simplicity of the algorithm has led to its application to many areas such as signal processing, information theory, control, and finance. A general set of sufficient conditions for the convergence and correctness of the algorithm is quite well-known when the underlying problem parameters are fixed. In many practical situations, however, the underlying problem parameters are changing over time, and the use of an adaptive algorithm is more appropriate. In this paper, we study such an adaptive version of the alternating minimization algorithm. As a main result of this paper, we provide a general set of sufficient conditions for the convergence and correctness of the adaptive algorithm. Perhaps surprisingly, these conditions seem to be the minimal ones one would expect in such an adaptive setting. We present applications of our results to adaptive decomposition of mixtures, adaptive log-optimal portfolio selection, and adaptive filter design.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "A framework is developed for analyzing capacity gains from user cooperation in slow fading wireless networks when the number of nodes (network size) is large. The framework is illustrated for the case of a simple multipath-rich Rayleigh fading channel model. Both unicasting (one source and one destination) and multicasting (one source and several destinations) scenarios are considered. We introduce a meaningful notion of Shannon capacity for such systems, evaluate this capacity as a function of signal-to-noise ratio (SNR), and develop a simple two-phase cooperative network protocol that achieves it. We observe that the resulting capacity is the same for both unicasting and multicasting, but show that the network size required to achieve any target error probability is smaller for unicasting than for multicasting. Finally, we introduce the notion of a network ``scaling exponent'' to quantify the rate of decay of error probability with network size as a function of the targeted fraction of the capacity. This exponent provides additional insights to system designers by enabling a finer grain comparison of candidate cooperative transmission protocols in even moderately sized networks.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "A generalization of the problem of writing on dirty paper is considered in which one transmitter sends a common message to multiple receivers. Each receiver experiences on its link an additive interference (in addition to the additive noise), which is known noncausally to the transmitter but not to any of the receivers. Applications range from wireless multi-antenna multicasting to robust dirty paper coding.\n  We develop results for memoryless channels in Gaussian and binary special cases. In most cases, we observe that the availability of side information at the transmitter increases capacity relative to systems without such side information, and that the lack of side information at the receivers decreases capacity relative to systems with such side information.\n  For the noiseless binary case, we establish the capacity when there are two receivers. When there are many receivers, we show that the transmitter side information provides a vanishingly small benefit. When the interference is large and independent across the users, we show that time sharing is optimal.\n  For the Gaussian case we present a coding scheme and establish its optimality in the high signal-to-interference-plus-noise limit when there are two receivers. When the interference is large and independent across users we show that time-sharing is again optimal. Connections to the problem of robust dirty paper coding are also discussed.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "In a variety of applications, there is a need to authenticate content that has experienced legitimate editing in addition to potential tampering attacks. We develop one formulation of this problem based on a strict notion of security, and characterize and interpret the associated information-theoretic performance limits. The results can be viewed as a natural generalization of classical approaches to traditional authentication. Additional insights into the structure of such systems and their behavior are obtained by further specializing the results to Bernoulli and Gaussian cases. The associated systems are shown to be substantially better in terms of performance and/or security than commonly advocated approaches based on data hiding and digital watermarking. Finally, the formulation is extended to obtain efficient layered authentication system constructions.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider transmitting a source across a pair of independent, non-ergodic channels with random states (e.g., slow fading channels) so as to minimize the average distortion. The general problem is unsolved. Hence, we focus on comparing two commonly used source and channel encoding systems which correspond to exploiting diversity either at the physical layer through parallel channel coding or at the application layer through multiple description source coding.\n  For on-off channel models, source coding diversity offers better performance. For channels with a continuous range of reception quality, we show the reverse is true. Specifically, we introduce a new figure of merit called the distortion exponent which measures how fast the average distortion decays with SNR. For continuous-state models such as additive white Gaussian noise channels with multiplicative Rayleigh fading, optimal channel coding diversity at the physical layer is more efficient than source coding diversity at the application layer in that the former achieves a better distortion exponent.\n  Finally, we consider a third decoding architecture: multiple description encoding with a joint source-channel decoding. We show that this architecture achieves the same distortion exponent as systems with optimal channel coding diversity for continuous-state channels, and maintains the the advantages of multiple description systems for on-off channels. Thus, the multiple description system with joint decoding achieves the best performance, from among the three architectures considered, on both continuous-state and on-off channels.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We introduce the idea of distortion side information, which does not directly depend on the source but instead affects the distortion measure. We show that such distortion side information is not only useful at the encoder, but that under certain conditions, knowing it at only the encoder is as good as knowing it at both encoder and decoder, and knowing it at only the decoder is useless. Thus distortion side information is a natural complement to the signal side information studied by Wyner and Ziv, which depends on the source but does not involve the distortion measure. Furthermore, when both types of side information are present, we characterize the penalty for deviating from the configuration of encoder-only distortion side information and decoder-only signal side information, which in many cases is as good as full side information knowledge.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider source coding with fixed lag side information at the decoder. We focus on the special case of perfect side information with unit lag corresponding to source coding with feedforward (the dual of channel coding with feedback) introduced by Pradhan. We use this duality to develop a linear complexity algorithm which achieves the rate-distortion bound for any memoryless finite alphabet source and distortion measure.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "We consider lossy source coding when side information affecting the distortion measure may be available at the encoder, decoder, both, or neither. For example, such distortion side information can model reliabilities for noisy measurements, sensor calibration information, or perceptual effects like masking and sensitivity to context. When the distortion side information is statistically independent of the source, we show that in many cases (e.g, for additive or multiplicative distortion side information) there is no penalty for knowing the side information only at the encoder, and there is no advantage to knowing it at the decoder. Furthermore, for quadratic distortion measures scaled by the distortion side information, we evaluate the penalty for lack of encoder knowledge and show that it can be arbitrarily large. In this scenario, we also sketch transform based quantizers constructions which efficiently exploit encoder side information in the high-resolution limit.\n        \u25b3 Less", "author": "Gregory Wornell"}, {"abstract": "This paper explains a flaw in the published proof of the Scalable Commutativity Rule (SCR), presents a revised and formally verified proof of the SCR in the Coq proof assistant, and discusses the insights and open questions raised from our experience proving the SCR.\n        \u25b3 Less", "author": "Nickolai Zeldovich"}, {"abstract": "Report on the NSF Workshop on Formal Methods for Security, held 19-20 November 2015.\n        \u25b3 Less", "author": "Nickolai Zeldovich"}, {"abstract": "One primary focus in multimodal feature extraction is to find the representations of individual modalities that are maximally correlated. As a well-known measure of dependence, the Hirschfeld-Gebelein-R\u00e9nyi (HGR) maximal correlation becomes an appealing objective because of its operational meaning and desirable properties. However, the strict whitening constraints formalized in the HGR maximal correlation limit its application. To address this problem, this paper proposes Soft-HGR, a novel framework to extract informative features from multiple data modalities. Specifically, our framework prevents the \"hard\" whitening constraints, while simultaneously preserving the same feature geometry as in the HGR maximal correlation. The objective of Soft-HGR is straightforward, only involving two inner products, which guarantees the efficiency and stability in optimization. We further generalize the framework to handle more than two modalities and missing modalities. When labels are partially available, we enhance the discriminative power of the feature representations by making a semi-supervised adaptation. Empirical evaluation implies that our approach learns more informative feature mappings and is more efficient to optimize.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "In this paper, we present a local information theoretic approach to explicitly learn probabilistic clustering of a discrete random variable. Our formulation yields a convex maximization problem for which it is NP-hard to find the global optimum. In order to algorithmically solve this optimization problem, we propose two relaxations that are solved via gradient ascent and alternating maximization. Experiments on the MSR Sentence Completion Challenge, MovieLens 100K, and Reuters21578 datasets demonstrate that our approach is competitive with existing techniques and worthy of further investigation.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "We study the problem of designing optical receivers to discriminate between multiple coherent states using coherent processing receivers---i.e., one that uses arbitrary coherent feedback control and quantum-noise-limited direct detection---which was shown by Dolinar to achieve the minimum error probability in discriminating any two coherent states. We first derive and re-interpret Dolinar's binary-hypothesis minimum-probability-of-error receiver as the one that optimizes the information efficiency at each time instant, based on recursive Bayesian updates within the receiver. Using this viewpoint, we propose a natural generalization of Dolinar's receiver design to discriminate $M$ coherent states each of which could now be a codeword, i.e., a sequence of $N$ coherent states each drawn from a modulation alphabet. We analyze the channel capacity of the pure-loss optical channel with a general coherent-processing receiver in the low-photon number regime and compare it with the capacity achievable with direct detection and the Holevo limit (achieving the latter would require a quantum joint-detection receiver). We show compelling evidence that despite the optimal performance of Dolinar's receiver for the binary coherent-state hypothesis test (either in error probability or mutual information), the asymptotic communication rate achievable by such a coherent-processing receiver is only as good as direct detection. This suggests that in the infinitely-long codeword limit, all potential benefits of coherent processing at the receiver can be obtained by designing a good code and direct detection, with no feedback within the receiver.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "In this paper, we propose an open-loop unequal-error-protection querying policy based on superposition coding for the noisy 20 questions problem. In this problem, a player wishes to successively refine an estimate of the value of a continuous random variable by posing binary queries and receiving noisy responses. When the queries are designed non-adaptively as a single block and the noisy responses are modeled as the output of a binary symmetric channel the 20 questions problem can be mapped to an equivalent problem of channel coding with unequal error protection (UEP). A new non-adaptive querying strategy based on UEP superposition coding is introduced whose estimation error decreases with an exponential rate of convergence that is significantly better than that of the UEP repetition coding introduced by Variani et al. (2015). With the proposed querying strategy, the rate of exponential decrease in the number of queries matches the rate of a closed-loop adaptive scheme where queries are sequentially designed with the benefit of feedback. Furthermore, the achievable error exponent is significantly better than that of random block codes employing equal error protection.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "Data processing inequalities for $f$-divergences can be sharpened using constants called \"contraction coefficients\" to produce strong data processing inequalities. For any discrete source-channel pair, the contraction coefficients for $f$-divergences are lower bounded by the contraction coefficient for $\u03c7^2$-divergence. In this paper, we elucidate that this lower bound can be achieved by driving the input $f$-divergences of the contraction coefficients to zero. Then, we establish a linear upper bound on the contraction coefficients for a certain class of $f$-divergences using the contraction coefficient for $\u03c7^2$-divergence, and refine this upper bound for the salient special case of Kullback-Leibler (KL) divergence. Furthermore, we present an alternative proof of the fact that the contraction coefficients for KL and $\u03c7^2$-divergences are equal for a Gaussian source with an additive Gaussian noise channel (where the former coefficient can be power constrained). Finally, we generalize the well-known result that contraction coefficients of channels (after extremizing over all possible sources) for all $f$-divergences with non-linear operator convex $f$ are equal. In particular, we prove that the so called \"less noisy\" preorder over channels can be equivalently characterized by any non-linear operator convex $f$-divergence.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "This paper considers the problem of communication over a discrete memoryless channel (DMC) or an additive white Gaussian noise (AWGN) channel subject to the constraint that the probability that an adversary who observes the channel outputs can detect the communication is low. Specifically, the relative entropy between the output distributions when a codeword is transmitted and when no input is provided to the channel must be sufficiently small. For a DMC whose output distribution induced by the \"off\" input symbol is not a mixture of the output distributions induced by other input symbols, it is shown that the maximum amount of information that can be transmitted under this criterion scales like the square root of the blocklength. The same is true for the AWGN channel. Exact expressions for the scaling constant are also derived.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "In this paper, we extend the information theoretic framework that was developed in earlier work to multi-hop network settings. For a given network, we construct a novel deterministic model that quantifies the ability of the network in transmitting private and common messages across users. Based on this model, we formulate a linear optimization problem that explores the throughput of a multi-layer network, thereby offering the optimal strategy as to what kind of common messages should be generated in the network to maximize the throughput. With this deterministic model, we also investigate the role of feedback for multi-layer networks, from which we identify a variety of scenarios in which feedback can improve transmission efficiency. Our results provide fundamental guidelines as to how to coordinate cooperation between users to enable efficient information exchanges across them.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "Widespread use of the Internet and social networks invokes the generation of big data, which is proving to be useful in a number of applications. To deal with explosively growing amounts of data, data analytics has emerged as a critical technology related to computing, signal processing, and information networking. In this paper, a formalism is considered in which data is modeled as a generalized social network and communication theory and information theory are thereby extended to data analytics. First, the creation of an equalizer to optimize information transfer between two data variables is considered, and financial data is used to demonstrate the advantages. Then, an information coupling approach based on information geometry is applied for dimensionality reduction, with a pattern recognition example to illustrate the effectiveness. These initial trials suggest the potential of communication theoretic data analytics for a wide range of applications.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "Many network information theory problems face the similar difficulty of single-letterization. We argue that this is due to the lack of a geometric structure on the space of probability distribution. In this paper, we develop such a structure by assuming that the distributions of interest are close to each other. Under this assumption, the K-L divergence is reduced to the squared Euclidean metric in an Euclidean space. In addition, we construct the notion of coordinate and inner product, which will facilitate solving communication problems. We will present the application of this approach to the point-to-point channel, general broadcast channel, and the multiple access channel (MAC) with the common source. It can be shown that with this approach, information theory problems, such as the single-letterization, can be reduced to some linear algebra problems. Moreover, we show that for the general broadcast channel, transmitting the common message to receivers can be formulated as the trade-off between linear systems. We also provide an example to visualize this trade-off in a geometric way. Finally, for the MAC with the common source, we observe a coherent combining gain due to the cooperation between transmitters, and this gain can be quantified by applying our technique.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "The relationship between the transmitted signal and the noiseless received signals in correlatively changing fading channels is modeled as a nonlinear mapping over manifolds of different dimensions. Dimension counting argument claims that the dimensionality of the neighborhood in which this mapping is bijective with probability one is achievable as the degrees of freedom of the system.We call the degrees of freedom achieved by the nonlinear decoding methods the nonlinear degrees of freedom.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "A new approach toward the noncoherent communications over the time varying fading channels is presented. In this approach, the relationship between the input signal space and the output signal space of a correlatively changing fading channel is shown to be a nonlinear mapping between manifolds of different dimensions. Studying this mapping, it is shown that using nonlinear decoding algorithms for single input-multiple output (SIMO) and multiple input multiple output (MIMO) systems, extra numbers of degrees of freedom (DOF) are available. We call them the nonlinear degrees of freedom.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "The maximum rate at which classical information can be reliably transmitted per use of a quantum channel strictly increases in general with $N$, the number of channel outputs that are detected jointly by the quantum joint-detection receiver (JDR). This phenomenon is known as superadditivity of the maximum achievable information rate over a quantum channel. We study this phenomenon for a pure-state classical-quantum (cq) channel and provide a lower bound on $C_N/N$, the maximum information rate when the JDR is restricted to making joint measurements over no more than $N$ quantum channel outputs, while allowing arbitrary classical error correction. We also show the appearance of a superadditivity phenomenon---of mathematical resemblance to the aforesaid problem---in the channel capacity of a classical discrete memoryless channel (DMC) when a concatenated coding scheme is employed, and the inner decoder is forced to make hard decisions on $N$-length inner codewords. Using this correspondence, we develop a unifying framework for the above two notions of superadditivity, and show that for our lower bound to $C_N/N$ to be equal to a given fraction of the asymptotic capacity $C$ of the respective channel, $N$ must be proportional to $V/C^2$, where $V$ is the respective channel dispersion quantity.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "Many network information theory problems face the similar difficulty of single letterization. We argue that this is due to the lack of a geometric structure on the space of probability distribution. In this paper, we develop such a structure by assuming that the distributions of interest are close to each other. Under this assumption, the K-L divergence is reduced to the squared Euclidean metric in an Euclidean space. Moreover, we construct the notion of coordinate and inner product, which will facilitate solving communication problems. We will also present the application of this approach to the point-to-point channel and the general broadcast channel, which demonstrates how our technique simplifies information theory problems.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "The bit-wise unequal error protection problem, for the case when the number of groups of bits $\\ell$ is fixed, is considered for variable length block codes with feedback. An encoding scheme based on fixed length block codes with erasures is used to establish inner bounds to the achievable performance for finite expected decoding time. A new technique for bounding the performance of variable length block codes is used to establish outer bounds to the performance for a given expected decoding time. The inner and the outer bounds match one another asymptotically and characterize the achievable region of rate-exponent vectors, completely. The single message message-wise unequal error protection problem for variable length block codes with feedback is also solved as a necessary step on the way.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "This paper studies network information theory problems where the external noise is Gaussian distributed. In particular, the Gaussian broadcast channel with coherent fading and the Gaussian interference channel are investigated. It is shown that in these problems, non-Gaussian code ensembles can achieve higher rates than the Gaussian ones. It is also shown that the strong Shamai-Laroia conjecture on the Gaussian ISI channel does not hold. In order to analyze non-Gaussian code ensembles over Gaussian networks, a geometrical tool using the Hermite polynomials is proposed. This tool provides a coordinate system to analyze a class of non-Gaussian input distributions that are invariant over Gaussian networks.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "Inner and outer bounds are derived on the optimal performance of fixed length block codes on discrete memoryless channels with feedback and errors-and-erasures decoding. First an inner bound is derived using a two phase encoding scheme with communication and control phases together with the optimal decoding rule for the given encoding scheme, among decoding rules that can be represented in terms of pairwise comparisons between the messages. Then an outer bound is derived using a generalization of the straight-line bound to errors-and-erasures decoders and the optimal error exponent trade off of a feedback encoder with two messages. In addition upper and lower bounds are derived, for the optimal erasure exponent of error free block codes in terms of the rate. Finally we present a proof of the fact that the optimal trade off between error exponents of a two message code does not increase with feedback on DMCs.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "Over discrete memoryless channels (DMC), linear decoders (maximizing additive metrics) afford several nice properties. In particular, if suitable encoders are employed, the use of decoding algorithm with manageable complexities is permitted. Maximum likelihood is an example of linear decoder. For a compound DMC, decoders that perform well without the channel's knowledge are required in order to achieve capacity. Several such decoders have been studied in the literature. However, there is no such known decoder which is linear. Hence, the problem of finding linear decoders achieving capacity for compound DMC is addressed, and it is shown that under minor concessions, such decoders exist and can be constructed. This paper also develops a \"local geometric analysis\", which allows in particular, to solve the above problem. By considering very noisy channels, the original problem is reduced, in the limit, to an inner product space problem, for which insightful solutions can be found. The local setting can then provide counterexamples to disproof claims, but also, it is shown how in this problem, results proven locally can be \"lifted\" to results proven globally.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "An information theoretic framework for unequal error protection is developed in terms of the exponential error bounds. The fundamental difference between the bit-wise and message-wise unequal error protection (UEP) is demonstrated, for fixed length block codes on DMCs without feedback. Effect of feedback is investigated via variable length block codes. It is shown that, feedback results in a significant improvement in both bit-wise and message-wise UEP (except the single message case for missed detection). The distinction between false-alarm and missed-detection formalizations for message-wise UEP is also considered. All results presented are at rates close to capacity.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "The concept of a fiber aided wireless network architecture (FAWNA) is introduced in [Ray et al., Allerton Conference 2005], which allows high-speed mobile connectivity by leveraging the speed of optical networks. In this paper, we consider a single-input, multiple-output (SIMO) FAWNA, which consists of a SIMO wireless channel and an optical fiber channel, connected through wireless-optical interfaces. We propose a scheme where the received wireless signal at each interface is quantized and sent over the fiber. Though our architecture is similar to that of the classical CEO problem, our problem is different from it. We show that the capacity of our scheme approaches the capacity of the architecture, exponentially with fiber capacity. We also show that for a given fiber capacity, there is an optimal operating wireless bandwidth and an optimal number of wireless-optical interfaces. The wireless-optical interfaces of our scheme have low complexity and do not require knowledge of the transmitter code book. They are also extendable to FAWNAs with large number of transmitters and interfaces and, offer adaptability to variable rates, changing channel conditions and node positions.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "We consider a multiple-input, multiple-output (MIMO) wideband Rayleigh block fading channel where the channel state is unknown to both the transmitter and the receiver and there is only an average power constraint on the input. We compute the capacity and analyze its dependence on coherence length, number of antennas and receive signal-to-noise ratio (SNR) per degree of freedom. We establish conditions on the coherence length and number of antennas for the non-coherent channel to have a \"near coherent\" performance in the wideband regime. We also propose a signaling scheme that is near-capacity achieving in this regime.\n  We compute the error probability for this wideband non-coherent MIMO channel and study its dependence on SNR, number of transmit and receive antennas and coherence length. We show that error probability decays inversely with coherence length and exponentially with the product of the number of transmit and receive antennas. Moreover, channel outage dominates error probability in the wideband regime. We also show that the critical as well as cut-off rates are much smaller than channel capacity in this regime.\n        \u25b3 Less", "author": "Lizhong Zheng"}, {"abstract": "A wideband fading channel is considered with causal channel state information (CSI) at the transmitter and no receiver CSI. A simple orthogonal code with energy detection rule at the receiver (similar to [6]) is shown to achieve the capacity of this channel in the limit of large bandwidth. This code transmits energy only when the channel gain is large enough. In this limit, this capacity without any receiver CSI is the same as the capacity with full receiver CSI--a phenomenon also true for dirty paper coding. For Rayleigh fading, this capacity (per unit time) is proportional to the logarithm of the bandwidth. Our coding scheme is motivated from the Gel'fand-Pinsker [2,3] coding and dirty paper coding [4]. Nonetheless, for our case, only causal CSI is required at the transmitter in contrast with dirty-paper coding and Gel'fand-Pinsker coding, where non-causal CSI is required.\n  Then we consider a general discrete channel with i.i.d. states. Each input has an associated cost and a zero cost input \"0\" exists. The channel state is assumed be to be known at the transmitter in a causal manner. Capacity per unit cost is found for this channel and a simple orthogonal code is shown to achieve this capacity. Later, a novel orthogonal coding scheme is proposed for the case of causal transmitter CSI and a condition for equivalence of capacity per unit cost for causal and non-causal transmitter CSI is derived. Finally, some connections are made to the case of non-causal transmitter CSI in [8].\n        \u25b3 Less", "author": "Lizhong Zheng"}]